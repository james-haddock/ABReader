<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p272"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="11 Hash Tables">
<p class="chapter-title"><a href="toc.xhtml#chap-11"><strong><span class="blue1">11        Hash Tables</span></strong></a></p>
<p class="noindent">Many applications require a dynamic set that supports only the dictionary operations I<small>NSERT</small>, S<small>EARCH</small>, and D<small>ELETE</small>. For example, a compiler that translates a programming language maintains a symbol table, in which the keys of elements are arbitrary character strings corresponding to identifiers in the language. A hash table is an effective data structure for implementing dictionaries. Although searching for an element in a hash table can take as long as searching for an element in a linked list—Θ(<em>n</em>) time in the worst case—in practice, hashing performs extremely well. Under reasonable assumptions, the average time to search for an element in a hash table is <em>O</em>(1). Indeed, the built-in dictionaries of Python are implemented with hash tables.</p>
<p>A hash table generalizes the simpler notion of an ordinary array. Directly addressing into an ordinary array takes advantage of the <em>O</em>(1) access time for any array element. <a href="chapter011.xhtml#Sec_11.1">Section 11.1</a> discusses direct addressing in more detail. To use direct addressing, you must be able to allocate an array that contains a position for every possible key.</p>
<p>When the number of keys actually stored is small relative to the total number of possible keys, hash tables become an effective alternative to directly addressing an array, since a hash table typically uses an array of size proportional to the number of keys actually stored. Instead of using the key as an array index directly, we <em>compute</em> the array index from the key. <a href="chapter011.xhtml#Sec_11.2">Section 11.2</a> presents the main ideas, focusing on “chaining” as a way to handle “collisions,” in which more than one key maps to the same array index. <a href="chapter011.xhtml#Sec_11.3">Section 11.3</a> describes how to compute array indices from keys using hash functions. We present and analyze several variations on the basic theme. <a href="chapter011.xhtml#Sec_11.4">Section 11.4</a> looks at “open addressing,” which is another way to deal with collisions. The bottom line is that hashing is an extremely effective and practical technique: the basic dictionary operations require only <em>O</em>(1) time on the average. <a href="chapter011.xhtml#Sec_11.5">Section 11.5</a> discusses the hierarchical memory systems of modern computer systems have and illustrates how to design hash tables that work well in such systems.</p>
<a id="p273"/>
<p class="line1"/>
<section title="11.1 Direct-address tables">
<a id="Sec_11.1"/>
<p class="level1" id="h1-62"><a href="toc.xhtml#Rh1-62"><strong>11.1    Direct-address tables</strong></a></p>
<p class="noindent">Direct addressing is a simple technique that works well when the universe <em>U</em> of keys is reasonably small. Suppose that an application needs a dynamic set in which each element has a distinct key drawn from the universe <em>U</em> = {0, 1, …, <em>m</em> − 1}, where <em>m</em> is not too large.</p>
<p>To represent the dynamic set, you can use an array, or <strong><em><span class="blue1">direct-address table</span></em></strong>, denoted by <em>T</em>[0 : <em>m</em> − 1], in which each position, or <strong><em><span class="blue1">slot</span></em></strong>, corresponds to a key in the universe <em>U</em>. <a href="chapter011.xhtml#Fig_11-1">Figure 11.1</a> illustrates this approach. Slot <em>k</em> points to an element in the set with key <em>k</em>. If the set contains no element with key <em>k</em>, then <em>T</em>[<em>k</em>] = <small>NIL</small>.</p>
<p>The dictionary operations D<small>IRECT</small>-A<small>DDRESS</small>-S<small>EARCH</small>, D<small>IRECT</small>-A<small>DDRESS</small>-I<small>NSERT</small>, and D<small>IRECT</small>-A<small>DDRESS</small>-D<small>ELETE</small> on the following page are trivial to implement. Each takes only <em>O</em>(1) time.</p>
<p>For some applications, the direct-address table itself can hold the elements in the dynamic set. That is, rather than storing an element’s key and satellite data in an object external to the direct-address table, with a pointer from a slot in the table to the object, save space by storing the object directly in the slot. To indicate an empty slot, use a special key. Then again, why store the key of the object at all? The index of the object <em>is</em> its key! Of course, then you’d need some way to tell whether slots are empty.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_11-1"><img alt="art" src="images/Art_P378.jpg"/></p>
<p class="caption"><strong>Figure 11.1</strong> How to implement a dynamic set by a direct-address table <em>T</em>. Each key in the universe <em>U</em> = {0, 1, …, 9} corresponds to an index into the table. The set <em>K</em> = {2, 3, 5, 8} of actual keys determines the slots in the table that contain pointers to elements. The other slots, in blue, contain <small>NIL</small>.</p>
</div>
<a id="p274"/>
<div class="pull-quote1">
<p class="box-heading">D<small>IRECT</small>-A<small>DDRESS</small>-S<small>EARCH</small>(<em>T</em>, <em>k</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>return</strong> <em>T</em>[<em>k</em>]</td>
</tr>
</table>
<p class="box-headinga">D<small>IRECT</small>-A<small>DDRESS</small>-I<small>NSERT</small>(<em>T</em>, <em>x</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><em>T</em>[<em>x.key</em>] = <em>x</em></td>
</tr>
</table>
<p class="box-headinga">D<small>IRECT</small>-A<small>DDRESS</small>-D<small>ELETE</small>(<em>T</em>, <em>x</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><em>T</em>[<em>x.key</em>] = <small>NIL</small></td>
</tr>
</table>
</div>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>11.1-1</em></strong></p>
<p class="noindent">A dynamic set <em>S</em> is represented by a direct-address table <em>T</em> of length <em>m</em>. Describe a procedure that finds the maximum element of <em>S</em>. What is the worst-case performance of your procedure?</p>
<p class="level3"><strong><em>11.1-2</em></strong></p>
<p class="noindent">A <strong><em><span class="blue1">bit vector</span></em></strong> is simply an array of bits (each either 0 or 1). A bit vector of length <em>m</em> takes much less space than an array of <em>m</em> pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements drawn from the set {0, 1, …, <em>m</em> − 1} and with no satellite data. Dictionary operations should run in <em>O</em>(1) time.</p>
<p class="level3"><strong><em>11.1-3</em></strong></p>
<p class="noindent">Suggest how to implement a direct-address table in which the keys of stored elements do not need to be distinct and the elements can have satellite data. All three dictionary operations (I<small>NSERT</small>, D<small>ELETE</small>, and S<small>EARCH</small>) should run in <em>O</em>(1) time. (Don’t forget that D<small>ELETE</small> takes as an argument a pointer to an object to be deleted, not a key.)</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.1-4</em></strong></p>
<p class="noindent">Suppose that you want to implement a dictionary by using direct addressing on a <em>huge</em> array. That is, if the array size is <em>m</em> and the dictionary contains at most <em>n</em> elements at any one time, then <em>m</em> <span class="font1">≫</span> <em>n</em>. At the start, the array entries may contain garbage, and initializing the entire array is impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use <em>O</em>(1) space; the operations S<small>EARCH</small>, I<small>NSERT</small>, and D<small>ELETE</small> should take <em>O</em>(1) time each; and initializing the data structure should take <em>O</em>(1) time. (<em>Hint:</em> Use an additional array, treated somewhat like a stack whose size is the number of keys actually stored in the dictionary, to help determine whether a given entry in the huge array is valid or not.)</p>
<a id="p275"/>
</section>
<p class="line1"/>
<section title="11.2 Hash tables">
<a id="Sec_11.2"/>
<p class="level1" id="h1-63"><a href="toc.xhtml#Rh1-63"><strong>11.2    Hash tables</strong></a></p>
<p class="noindent">The downside of direct addressing is apparent: if the universe <em>U</em> is large or infinite, storing a table <em>T</em> of size |<em>U</em>| may be impractical, or even impossible, given the memory available on a typical computer. Furthermore, the set <em>K</em> of keys <em>actually stored</em> may be so small relative to <em>U</em> that most of the space allocated for <em>T</em> would be wasted.</p>
<p>When the set <em>K</em> of keys stored in a dictionary is much smaller than the universe <em>U</em> of all possible keys, a hash table requires much less storage than a direct-address table. Specifically, the storage requirement reduces to Θ(|<em>K</em>|) while maintaining the benefit that searching for an element in the hash table still requires only <em>O</em>(1) time. The catch is that this bound is for the <em>average-case time</em>,<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> whereas for direct addressing it holds for the <em>worst-case time</em>.</p>
<p>With direct addressing, an element with key <em>k</em> is stored in slot <em>k</em>, but with hashing, we use a <strong><em><span class="blue1">hash function</span></em></strong> <em>h</em> to compute the slot number from the key <em>k</em>, so that the element goes into slot <em>h</em>(<em>k</em>). The hash function <em>h</em> maps the universe <em>U</em> of keys into the slots of a <strong><em><span class="blue1">hash table</span></em></strong> <em>T</em>[0 : <em>m</em> − 1]:</p>
<p class="eql"><em>h</em> : <em>U</em> → {0, 1, …, <em>m</em> − 1},</p>
<p class="noindent">where the size <em>m</em> of the hash table is typically much less than |<em>U</em>|. We say that an element with key <em>k</em> <strong><em><span class="blue1">hashes</span></em></strong> to slot <em>h</em>(<em>k</em>), and we also say that <em>h</em>(<em>k</em>) is the <strong><em><span class="blue1">hash value</span></em></strong> of key <em>k</em>. <a href="chapter011.xhtml#Fig_11-2">Figure 11.2</a> illustrates the basic idea. The hash function reduces the range of array indices and hence the size of the array. Instead of a size of |<em>U</em>|, the array can have size <em>m</em>. An example of a simple, but not particularly good, hash function is <em>h</em>(<em>k</em>) = <em>k</em> mod <em>m</em>.</p>
<p>There is one hitch, namely that two keys may hash to the same slot. We call this situation a <strong><em><span class="blue1">collision</span></em></strong>. Fortunately, there are effective techniques for resolving the conflict created by collisions.</p>
<p>Of course, the ideal solution is to avoid collisions altogether. We might try to achieve this goal by choosing a suitable hash function <em>h</em>. One idea is to make <em>h</em> appear to be “random,” thus avoiding collisions or at least minimizing their number. The very term “to hash,” evoking images of random mixing and chopping, captures the spirit of this approach. (Of course, a hash function <em>h</em> must be deterministic in that a given input <em>k</em> must always produce the same output <em>h</em>(<em>k</em>).) Because |<em>U</em>| &gt; <em>m</em>, however, there must be at least two keys that have the same hash value, <a id="p276"/>and avoiding collisions altogether is impossible. Thus, although a well-designed, “random”-looking hash function can reduce the number of collisions, we still need a method for resolving the collisions that do occur.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_11-2"><img alt="art" src="images/Art_P379.jpg"/></p>
<p class="caption"><strong>Figure 11.2</strong> Using a hash function <em>h</em> to map keys to hash-table slots. Because keys <em>k</em><sub>2</sub> and <em>k</em><sub>5</sub> map to the same slot, they collide.</p>
</div>
<p>The remainder of this section first presents a definition of “independent uniform hashing,” which captures the simplest notion of what it means for a hash function to be “random.” It then presents and analyzes the simplest collision resolution technique, called chaining. <a href="chapter011.xhtml#Sec_11.4">Section 11.4</a> introduces an alternative method for resolving collisions, called open addressing.</p>
<p class="level4"><strong>Independent uniform hashing</strong></p>
<p class="noindent">An “ideal” hashing function <em>h</em> would have, for each possible input <em>k</em> in the domain <em>U</em>, an output <em>h</em>(<em>k</em>) that is an element randomly and independently chosen uniformly from the range {0, 1, …, <em>m</em> − 1}. Once a value <em>h</em>(<em>k</em>) is randomly chosen, each subsequent call to <em>h</em> with the same input <em>k</em> yields the same output <em>h</em>(<em>k</em>).</p>
<p>We call such an ideal hash function an <strong><em><span class="blue1">independent uniform hash function</span></em></strong>. Such a function is also often called a <strong><em><span class="blue1">random oracle</span></em></strong> [43]. When hash tables are implemented with an independent uniform hash function, we say we are using <strong><em><span class="blue1">independent uniform hashing</span></em></strong>.</p>
<p>Independent uniform hashing is an ideal theoretical abstraction, but it is not something that can reasonably be implemented in practice. Nonetheless, we’ll analyze the efficiency of hashing under the assumption of independent uniform hashing and then present ways of achieving useful practical approximations to this ideal.</p>
<a id="p277"/>
<div class="divimage">
<p class="fig-imga" id="Fig_11-3"><img alt="art" class="width100" src="images/Art_P380.jpg"/></p>
<p class="caption"><strong>Figure 11.3</strong> Collision resolution by chaining. Each nonempty hash-table slot <em>T</em>[<em>j</em>] points to a linked list of all the keys whose hash value is <em>j</em>. For example, <em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>4</sub>) and <em>h</em>(<em>k</em><sub>5</sub>) = <em>h</em>(<em>k</em><sub>2</sub>) = <em>h</em>(<em>k</em><sub>7</sub>). The list can be either singly or doubly linked. We show it as doubly linked because deletion may be faster that way when the deletion procedure knows which list element (not just which key) is to be deleted.</p>
</div>
<p class="level4"><strong>Collision resolution by chaining</strong></p>
<p class="noindent">At a high level, you can think of hashing with chaining as a nonrecursive form of divide-and-conquer: the input set of <em>n</em> elements is divided randomly into <em>m</em> subsets, each of approximate size <em>n</em>/<em>m</em>. A hash function determines which subset an element belongs to. Each subset is managed independently as a list.</p>
<p><a href="chapter011.xhtml#Fig_11-3">Figure 11.3</a> shows the idea behind <strong><em><span class="blue1">chaining</span></em></strong>: each nonempty slot points to a linked list, and all the elements that hash to the same slot go into that slot’s linked list. Slot <em>j</em> contains a pointer to the head of the list of all stored elements with hash value <em>j</em>. If there are no such elements, then slot <em>j</em> contains <small>NIL</small>.</p>
<p>When collisions are resolved by chaining, the dictionary operations are straightforward to implement. They appear on the next page and use the linked-list procedures from <a href="chapter010.xhtml#Sec_10.2">Section 10.2</a>. The worst-case running time for insertion is <em>O</em>(1). The insertion procedure is fast in part because it assumes that the element <em>x</em> being inserted is not already present in the table. To enforce this assumption, you can search (at additional cost) for an element whose key is <em>x.key</em> before inserting. For searching, the worst-case running time is proportional to the length of the list. (We’ll analyze this operation more closely below.) Deletion takes <em>O</em>(1) time if the lists are doubly linked, as in <a href="chapter011.xhtml#Fig_11-3">Figure 11.3</a>. (Since C<small>HAINED</small>-H<small>ASH</small>-D<small>ELETE</small> takes as input an element <em>x</em> and not its key <em>k</em>, no search is needed. If the hash table supports deletion, then its linked lists should be doubly linked in order to delete an item quickly. If the lists were only singly linked, then by Exercise 10.2-1, deletion <a id="p278"/>could take time proportional to the length of the list. With singly linked lists, both deletion and searching would have the same asymptotic running times.)</p>
<div class="pull-quote1">
<p class="box-heading">C<small>HAINED</small>-H<small>ASH</small>-I<small>NSERT</small>(<em>T</em>, <em>x</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1">L<small>IST</small>-P<small>REPEND</small>(<em>T</em>[<em>h</em>(<em>x.key</em>)], <em>x</em>)</td>
</tr>
</table>
<p class="box-headinga">C<small>HAINED</small>-H<small>ASH</small>-S<small>EARCH</small>(<em>T</em>, <em>k</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>return</strong> L<small>IST</small>-S<small>EARCH</small>(<em>T</em>[<em>h</em>(<em>k</em>)], <em>k</em>)</td>
</tr>
</table>
<p class="box-headinga">C<small>HAINED</small>-H<small>ASH</small>-D<small>ELETE</small>(<em>T</em>, <em>x</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1">L<small>IST</small>-D<small>ELETE</small>(<em>T</em>[<em>h</em>(<em>x.key</em>)], <em>x</em>)</td>
</tr>
</table>
</div>
<p class="level4"><strong>Analysis of hashing with chaining</strong></p>
<p class="noindent">How well does hashing with chaining perform? In particular, how long does it take to search for an element with a given key?</p>
<p>Given a hash table <em>T</em> with <em>m</em> slots that stores <em>n</em> elements, we define the <strong><em><span class="blue1">load factor</span></em></strong> <em>α</em> for <em>T</em> as <em>n</em>/<em>m</em>, that is, the average number of elements stored in a chain. Our analysis will be in terms of <em>α</em>, which can be less than, equal to, or greater than 1.</p>
<p>The worst-case behavior of hashing with chaining is terrible: all <em>n</em> keys hash to the same slot, creating a list of length <em>n</em>. The worst-case time for searching is thus Θ(<em>n</em>) plus the time to compute the hash function—no better than using one linked list for all the elements. We clearly don’t use hash tables for their worst-case performance.</p>
<p>The average-case performance of hashing depends on how well the hash function <em>h</em> distributes the set of keys to be stored among the <em>m</em> slots, on the average (meaning with respect to the distribution of keys to be hashed and with respect to the choice of hash function, if this choice is randomized). <a href="chapter011.xhtml#Sec_11.3">Section 11.3</a> discusses these issues, but for now we assume that any given element is equally likely to hash into any of the <em>m</em> slots. That is, the hash function is <strong><em><span class="blue1">uniform</span></em></strong>. We further assume that where a given element hashes to is <em>independent</em> of where any other elements hash to. In other words, we assume that we are using <strong><em><span class="blue1">independent uniform hashing</span></em></strong>.</p>
<p>Because hashes of distinct keys are assumed to be independent, independent uniform hashing is <strong><em><span class="blue1">universal</span></em></strong>: the chance that any two distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> collide is at most 1/<em>m</em>. Universality is important in our analysis and also in the specification of universal families of hash functions, which we’ll see in <a href="chapter011.xhtml#Sec_11.3.2">Section 11.3.2</a>.</p>
<p>For <em>j</em> = 0, 1, …, <em>m</em> − 1, denote the length of the list <em>T</em>[<em>j</em>] by <em>n<sub>j</sub></em>, so that</p>
<a id="p279"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P381.jpg"/></p>
<p class="noindent">and the expected value of <em>n<sub>j</sub></em> is E[<em>n<sub>j</sub></em>] = <em>α</em> = <em>n</em>/<em>m</em>.</p>
<p>We assume that <em>O</em>(1) time suffices to compute the hash value <em>h</em>(<em>k</em>), so that the time required to search for an element with key <em>k</em> depends linearly on the length <em>n</em><sub><em>h</em>(<em>k</em>)</sub> of the list <em>T</em>[<em>h</em>(<em>k</em>)]. Setting aside the <em>O</em>(1) time required to compute the hash function and to access slot <em>h</em>(<em>k</em>), we’ll consider the expected number of elements examined by the search algorithm, that is, the number of elements in the list <em>T</em>[<em>h</em>(<em>k</em>)] that the algorithm checks to see whether any have a key equal to <em>k</em>. We consider two cases. In the first, the search is unsuccessful: no element in the table has key <em>k</em>. In the second, the search successfully finds an element with key <em>k</em>.</p>
<p class="theo"><strong><em>Theorem 11.1</em></strong></p>
<p class="noindent">In a hash table in which collisions are resolved by chaining, an unsuccessful search takes Θ(1 + <em>α</em>) time on average, under the assumption of independent uniform hashing.</p>
<p class="proof"><strong><em>Proof</em></strong>   Under the assumption of independent uniform hashing, any key <em>k</em> not already stored in the table is equally likely to hash to any of the <em>m</em> slots. The expected time to search unsuccessfully for a key <em>k</em> is the expected time to search to the end of list <em>T</em>[<em>h</em>(<em>k</em>)], which has expected length E[<em>n</em><sub><em>h</em>(<em>k</em>)</sub>] = <em>α</em>. Thus, the expected number of elements examined in an unsuccessful search is <em>α</em>, and the total time required (including the time for computing <em>h</em>(<em>k</em>)) is Θ(1 + <em>α</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The situation for a successful search is slightly different. An unsuccessful search is equally likely to go to any slot of the hash table. A successful search, however, cannot go to an empty slot, since it is for an element that is present in one of the linked lists. We assume that the element searched for is equally likely to be any one of the elements in the table, so the longer the list, the more likely that the search is for one of its elements. Even so, the expected search time still turns out to be Θ(1 + <em>α</em>).</p>
<p class="theo"><strong><em>Theorem 11.2</em></strong></p>
<p class="noindent">In a hash table in which collisions are resolved by chaining, a successful search takes Θ(1 + <em>α</em>) time on average, under the assumption of independent uniform hashing.</p>
<p class="proof"><strong><em>Proof</em></strong>   We assume that the element being searched for is equally likely to be any of the <em>n</em> elements stored in the table. The number of elements examined during a successful search for an element <em>x</em> is 1 more than the number of elements that appear before <em>x</em> in <em>x</em>’s list. Because new elements are placed at the front of the list, <a id="p280"/>elements before <em>x</em> in the list were all inserted after <em>x</em> was inserted. Let <em>x<sub>i</sub></em> denote the <em>i</em>th element inserted into the table, for <em>i</em> = 1, 2, …, <em>n</em>, and let <em>k<sub>i</sub></em> = <em>x<sub>i</sub>.key</em>.</p>
<p>Our analysis uses indicator random variables extensively. For each slot <em>q</em> in the table and for each pair of distinct keys <em>k<sub>i</sub></em> and <em>k<sub>j</sub></em>, we define the indicator random variable</p>
<p class="eql"><em>X<sub>ijq</sub></em> = I {the search is for <em>x<sub>i</sub></em>, <em>h</em>(<em>k<sub>i</sub></em>) = <em>q</em>, and <em>h</em>(<em>k<sub>j</sub></em>) = <em>q</em>}.</p>
<p class="noindent">That is, <em>X<sub>ijq</sub></em> = 1 when keys <em>k<sub>i</sub></em> and <em>k<sub>j</sub></em> collide at slot <em>q</em> and the search is for element <em>x<sub>i</sub></em>. Because Pr{the search is for <em>x<sub>i</sub></em>} = 1/<em>n</em>, Pr{<em>h</em>(<em>k<sub>i</sub></em>) = <em>q</em>} = 1/<em>m</em>, Pr{<em>h</em>(<em>k<sub>j</sub></em>) = <em>q</em>} = 1/<em>m</em>, and these events are all independent, we have that Pr{<em>X<sub>ijq</sub></em> = 1} = 1/<em>nm</em><sup>2</sup>. Lemma 5.1 on page 130 gives E[<em>X<sub>ijq</sub></em>] = 1/<em>nm</em><sup>2</sup>.</p>
<p>Next, we define, for each element <em>x<sub>j</sub></em>, the indicator random variable</p>
<table class="table2b">
<tr>
<td class="td2"><em>Y<sub>j</sub></em></td>
<td class="td2m">=</td>
<td class="td2">I {<em>x<sub>j</sub></em> appears in a list prior to the element being searched for}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><img alt="art" src="images/Art_P382.jpg"/>,</td>
</tr>
</table>
<p class="noindent">since at most one of the <em>X<sub>ijq</sub></em> equals 1, namely when the element <em>x<sub>i</sub></em> being searched for belongs to the same list as <em>x<sub>j</sub></em> (pointed to by slot <em>q</em>), and <em>i</em> &lt; <em>j</em> (so that <em>x<sub>i</sub></em> appears after <em>x<sub>j</sub></em> in the list).</p>
<p>Our final random variable is <em>Z</em>, which counts how many elements appear in the list prior to the element being searched for:</p>
<p class="eql"><img alt="art" src="images/Art_P383.jpg"/></p>
<p class="noindent">Because we must count the element being searched for as well as all those preceding it in its list, we wish to compute E[<em>Z</em> + 1]. Using linearity of expectation (equation (C.24) on page 1192), we have</p>
<p class="eql"><img alt="art" src="images/Art_P384.jpg"/></p>
<a id="p281"/>
<p class="noindent">Thus, the total time required for a successful search (including the time for computing the hash function) is Θ(2 + <em>α/</em>2 − <em>α/</em>2<em>n</em>) = Θ(1 + <em>α</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">What does this analysis mean? If the number of elements in the table is at most proportional to the number of hash-table slots, we have <em>n</em> = <em>O</em>(<em>m</em>) and, consequently, <em>α</em> = <em>n</em>/<em>m</em> = <em>O</em>(<em>m</em>)/<em>m</em> = <em>O</em>(1). Thus, searching takes constant time on average. Since insertion takes <em>O</em>(1) worst-case time and deletion takes <em>O</em>(1) worst-case time when the lists are doubly linked (assuming that the list element to be deleted is known, and not just its key), we can support all dictionary operations in <em>O</em>(1) time on average.</p>
<p>The analysis in the preceding two theorems depends only on two essential properties of independent uniform hashing: uniformity (each key is equally likely to hash to any one of the <em>m</em> slots), and independence (so any two distinct keys collide with probability 1/<em>m</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>11.2-1</em></strong></p>
<p class="noindent">You use a hash function <em>h</em> to hash <em>n</em> distinct keys into an array <em>T</em> of length <em>m</em>. Assuming independent uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of {{<em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>} : <em>k</em><sub>1</sub> ≠ <em>k</em><sub>2</sub> and <em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>2</sub>)}?</p>
<p class="level3"><strong><em>11.2-2</em></strong></p>
<p class="noindent">Consider a hash table with 9 slots and the hash function <em>h</em>(<em>k</em>) = <em>k</em> mod 9. Demonstrate what happens upon inserting the keys 5, 28, 19, 15, 20, 33, 12, 17, 10 with collisions resolved by chaining.</p>
<a id="p282"/>
<p class="level3"><strong><em>11.2-3</em></strong></p>
<p class="noindent">Professor Marley hypothesizes that he can obtain substantial performance gains by modifying the chaining scheme to keep each list in sorted order. How does the professor’s modification affect the running time for successful searches, unsuccessful searches, insertions, and deletions?</p>
<p class="level3"><strong><em>11.2-4</em></strong></p>
<p class="noindent">Suggest how to allocate and deallocate storage for elements within the hash table itself by creating a “free list”: a linked list of all the unused slots. Assume that one slot can store a flag and either one element plus a pointer or two pointers. All dictionary and free-list operations should run in <em>O</em>(1) expected time. Does the free list need to be doubly linked, or does a singly linked free list suffice?</p>
<p class="level3"><strong><em>11.2-5</em></strong></p>
<p class="noindent">You need to store a set of <em>n</em> keys in a hash table of size <em>m</em>. Show that if the keys are drawn from a universe <em>U</em> with |<em>U</em>| &gt; (<em>n</em> − 1)<em>m</em>, then <em>U</em> has a subset of size <em>n</em> consisting of keys that all hash to the same slot, so that the worst-case searching time for hashing with chaining is Θ(<em>n</em>).</p>
<p class="level3"><strong><em>11.2-6</em></strong></p>
<p class="noindent">You have stored <em>n</em> keys in a hash table of size <em>m</em>, with collisions resolved by chaining, and you know the length of each chain, including the length <em>L</em> of the longest chain. Describe a procedure that selects a key uniformly at random from among the keys in the hash table and returns it in expected time <em>O</em>(<em>L</em> · (1 + 1/<em>α</em>)).</p>
</section>
<p class="line1"/>
<section title="11.3 Hash functions">
<a id="Sec_11.3"/>
<p class="level1" id="h1-64"><a href="toc.xhtml#Rh1-64"><strong>11.3    Hash functions</strong></a></p>
<p class="noindent">For hashing to work well, it needs a good hash function. Along with being efficiently computable, what properties does a good hash function have? How do you design good hash functions?</p>
<p>This section first attempts to answer these questions based on two ad hoc approaches for creating hash functions: hashing by division and hashing by multiplication. Although these methods work well for some sets of input keys, they are limited because they try to provide a single fixed hash function that works well on any data—an approach called <strong><em><span class="blue1">static hashing</span></em></strong>.</p>
<p>We then see that provably good average-case performance for <em>any</em> data can be obtained by designing a suitable <em>family</em> of hash functions and choosing a hash function at random from this family at runtime, independent of the data to be hashed. The approach we examine is called random hashing. A particular kind of random <a id="p283"/>hashing, universal hashing, works well. As we saw with quicksort in <a href="chapter007.xhtml">Chapter 7</a>, randomization is a powerful algorithmic design tool.</p>
<p class="level4"><strong>What makes a good hash function?</strong></p>
<p class="noindent">A good hash function satisfies (approximately) the assumption of independent uniform hashing: each key is equally likely to hash to any of the <em>m</em> slots, independently of where any other keys have hashed to. What does “equally likely” mean here? If the hash function is fixed, any probabilities would have to be based on the probability distribution of the input keys.</p>
<p>Unfortunately, you typically have no way to check this condition, unless you happen to know the probability distribution from which the keys are drawn. Moreover, the keys might not be drawn independently.</p>
<p>Occasionally you might know the distribution. For example, if you know that the keys are random real numbers <em>k</em> independently and uniformly distributed in the range 0 ≤ <em>k</em> &lt; 1, then the hash function</p>
<p class="eql"><em>h</em>(<em>k</em>) = <span class="font1">⌊</span><em>km</em><span class="font1">⌋</span></p>
<p class="noindent">satisfies the condition of independent uniform hashing.</p>
<p>A good static hashing approach derives the hash value in a way that you expect to be independent of any patterns that might exist in the data. For example, the “division method” (discussed in <a href="chapter011.xhtml#Sec_11.3.1">Section 11.3.1</a>) computes the hash value as the remainder when the key is divided by a specified prime number. This method may give good results, if you (somehow) choose a prime number that is unrelated to any patterns in the distribution of keys.</p>
<p>Random hashing, described in <a href="chapter011.xhtml#Sec_11.3.2">Section 11.3.2</a>, picks the hash function to be used at random from a suitable family of hashing functions. This approach removes any need to know anything about the probability distribution of the input keys, as the randomization necessary for good average-case behavior then comes from the (known) random process used to pick the hash function from the family of hash functions, rather than from the (unknown) process used to create the input keys. We recommend that you use random hashing.</p>
<p class="level4"><strong>Keys are integers, vectors, or strings</strong></p>
<p class="noindent">In practice, a hash function is designed to handle keys that are one of the following two types:</p>
<ul class="ulnoindent" epub:type="list">
<li>A short nonnegative integer that fits in a <em>w</em>-bit machine word. Typical values for <em>w</em> would be 32 or 64.<a id="p284"/></li>
<li class="litop">A short vector of nonnegative integers, each of bounded size. For example, each element might be an 8-bit byte, in which case the vector is often called a (byte) string. The vector might be of variable length.</li></ul>
<p class="noindent">To begin, we assume that keys are short nonnegative integers. Handling vector keys is more complicated and discussed in <a href="chapter011.xhtml#Sec_11.3.5">Sections 11.3.5</a> and <a href="chapter011.xhtml#Sec_11.5.2">11.5.2</a>.</p>
<section title="11.3.1 Static hashing">
<p class="level2" id="Sec_11.3.1"><strong>11.3.1    Static hashing</strong></p>
<p class="noindent">Static hashing uses a single, fixed hash function. The only randomization available is through the (usually unknown) distribution of input keys. This section discusses two standard approaches for static hashing: the division method and the multiplication method. Although static hashing is no longer recommended, the multiplication method also provides a good foundation for “nonstatic” hashing—better known as random hashing—where the hash function is chosen at random from a suitable family of hash functions.</p>
<p class="level4"><strong>The division method</strong></p>
<p class="noindent">The <strong><em><span class="blue1">division method</span></em></strong> for creating hash functions maps a key <em>k</em> into one of <em>m</em> slots by taking the remainder of <em>k</em> divided by <em>m</em>. That is, the hash function is</p>
<p class="eql"><em>h</em>(<em>k</em>) = <em>k</em> mod <em>m</em>.</p>
<p class="noindent">For example, if the hash table has size <em>m</em> = 12 and the key is <em>k</em> = 100, then <em>h</em>(<em>k</em>) = 4. Since it requires only a single division operation, hashing by division is quite fast.</p>
<p>The division method may work well when <em>m</em> is a prime not too close to an exact power of 2. There is no guarantee that this method provides good average-case performance, however, and it may complicate applications since it constrains the size of the hash tables to be prime.</p>
<p class="level4"><strong>The multiplication method</strong></p>
<p class="noindent">The general <strong><em><span class="blue1">multiplication method</span></em></strong> for creating hash functions operates in two steps. First, multiply the key <em>k</em> by a constant <em>A</em> in the range 0 &lt; <em>A</em> &lt; 1 and extract the fractional part of <em>kA</em>. Then, multiply this value by <em>m</em> and take the floor of the result. That is, the hash function is</p>
<p class="eql"><em>h</em>(<em>k</em>) = <span class="font1">⌊</span><em>m</em> (<em>kA</em> mod 1)<span class="font1">⌋</span>,</p>
<p class="noindent">where “<em>kA</em> mod 1” means the fractional part of <em>kA</em>, that is, <em>kA</em> − <span class="font1">⌊</span><em>kA</em><span class="font1">⌋</span>. The general multiplication method has the advantage that the value of <em>m</em> is not critical and you can choose it independently of how you choose the multiplicative constant <em>A</em>.</p>
<a id="p285"/>
<div class="divimage">
<p class="fig-imga" id="Fig_11-4"><img alt="art" src="images/Art_P385.jpg"/></p>
<p class="caption"><strong>Figure 11.4</strong> The multiply-shift method to compute a hash function. The <em>w</em>-bit representation of the key <em>k</em> is multiplied by the <em>w</em>-bit value <em>a</em> = <em>A</em> · 2<em><sup>w</sup></em>. The <em>ℓ</em> highest-order bits of the lower <em>w</em>-bit half of the product form the desired hash value <em>h<sub>a</sub></em>(<em>k</em>).</p>
</div>
<p class="level4"><strong>The multiply-shift method</strong></p>
<p class="noindent">In practice, the multiplication method is best in the special case where the number <em>m</em> of hash-table slots is an exact power of 2, so that <em>m</em> = 2<em><sup>ℓ</sup></em> for some integer <em>ℓ</em>, where <em>ℓ</em> ≤ <em>w</em> and <em>w</em> is the number of bits in a machine word. If you choose a fixed <em>w</em>-bit positive integer <em>a</em> = <em>A</em> 2<em><sup>w</sup></em>, where 0 &lt; <em>A</em> &lt; 1 as in the multiplication method so that <em>a</em> is in the range 0 &lt; <em>a</em> &lt; 2<em><sup>w</sup></em>, you can implement the function on most computers as follows. We assume that a key <em>k</em> fits into a single <em>w</em>-bit word.</p>
<p>Referring to <a href="chapter011.xhtml#Fig_11-4">Figure 11.4</a>, first multiply <em>k</em> by the <em>w</em>-bit integer <em>a</em>. The result is a 2<em>w</em>-bit value <em>r</em><sub>1</sub>2<em><sup>w</sup></em> + <em>r</em><sub>0</sub>, where <em>r</em><sub>1</sub> is the high-order <em>w</em>-bit word of the product and <em>r</em><sub>0</sub> is the low-order <em>w</em>-bit word of the product. The desired <em>ℓ</em>-bit hash value consists of the <em>ℓ</em> most significant bits of <em>r</em><sub>0</sub>. (Since <em>r</em><sub>1</sub> is ignored, the hash function can be implemented on a computer that produces only a <em>w</em>-bit product given two <em>w</em>-bit inputs, that is, where the multiplication operation computes modulo 2<em><sup>w</sup></em>.)</p>
<p>In other words, you define the hash function <em>h</em> = <em>h<sub>a</sub></em>, where</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P386.jpg"/></p>
<p class="noindent">for a fixed nonzero <em>w</em>-bit value <em>a</em>. Since the product <em>ka</em> of two <em>w</em>-bit words occupies 2<em>w</em> bits, taking this product modulo 2<em><sup>w</sup></em> zeroes out the high-order <em>w</em> bits (<em>r</em><sub>1</sub>), leaving only the low-order <em>w</em> bits (<em>r</em><sub>0</sub>). The <span class="font1">⋙</span> operator performs a logical right shift by <em>w</em> − <em>ℓ</em> bits, shifting zeros into the vacated positions on the left, so that the <em>ℓ</em> most significant bits of <em>r</em><sub>0</sub> move into the <em>ℓ</em> rightmost positions. (It’s the same as dividing by 2<sup><em>w</em>−<em>ℓ</em></sup> and taking the floor of the result.) The resulting value equals the <em>ℓ</em> most significant bits of <em>r</em><sub>0</sub>. The hash function <em>h<sub>a</sub></em> can be implemented with three machine instructions: multiplication, subtraction, and logical right shift.</p>
<p>As an example, suppose that <em>k</em> = 123456, <em>ℓ</em> = 14, <em>m</em> = 2<sup>14</sup> = 16384, and <em>w</em> = 32. Suppose further that we choose <em>a</em> = 2654435769 (following a suggestion <a id="p286"/>of Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_261">261</a>]). Then <em>ka</em> = 327706022297664 = (76300 · 2<sup>32</sup>) + 17612864, and so <em>r</em><sub>1</sub> = 76300 and <em>r</em><sub>0</sub> = 17612864. The 14 most significant bits of <em>r</em><sub>0</sub> yield the value <em>h<sub>a</sub></em>(<em>k</em>) = 67.</p>
<p>Even though the multiply-shift method is fast, it doesn’t provide any guarantee of good average-case performance. The universal hashing approach presented in the next section provides such a guarantee. A simple randomized variant of the multiply-shift method works well on the average, when the program begins by picking <em>a</em> as a randomly chosen odd integer.</p>
</section>
<section title="11.3.2 Random hashing">
<p class="level2" id="Sec_11.3.2"><strong>11.3.2    Random hashing</strong></p>
<p class="noindent">Suppose that a malicious adversary chooses the keys to be hashed by some fixed hash function. Then the adversary can choose <em>n</em> keys that all hash to the same slot, yielding an average retrieval time of Θ(<em>n</em>). Any static hash function is vulnerable to such terrible worst-case behavior. The only effective way to improve the situation is to choose the hash function <em>randomly</em> in a way that is <em>independent</em> of the keys that are actually going to be stored. This approach is called <strong><em><span class="blue1">random hashing</span></em></strong>. A special case of this approach, called <strong><em><span class="blue1">universal hashing</span></em></strong>, can yield provably good performance on average when collisions are handled by chaining, no matter which keys the adversary chooses.</p>
<p>To use random hashing, at the beginning of program execution you select the hash function at random from a suitable family of functions. As in the case of quicksort, randomization guarantees that no single input always evokes worst-case behavior. Because you randomly select the hash function, the algorithm can behave differently on each execution, even for the same set of keys to be hashed, guaranteeing good average-case performance.</p>
<p>Let <span class="scriptfont">H</span> be a finite family of hash functions that map a given universe <em>U</em> of keys into the range {0, 1, …, <em>m</em> − 1}. Such a family is said to be <strong><em><span class="blue1">universal</span></em></strong> if for each pair of distinct keys <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub> ∈ <em>U</em>, the number of hash functions <em>h</em> ∈ <span class="scriptfont">H</span> for which <em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>2</sub>) is at most |<span class="scriptfont">H</span>|/<em>m</em>. In other words, with a hash function randomly chosen from <span class="scriptfont">H</span>, the chance of a collision between distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> is no more than the chance 1/<em>m</em> of a collision if <em>h</em>(<em>k</em><sub>1</sub>) and <em>h</em>(<em>k</em><sub>2</sub>) were randomly and independently chosen from the set {0, 1, …, <em>m</em> − 1}.</p>
<p>Independent uniform hashing is the same as picking a hash function uniformly at random from a family of <em>m<sup>n</sup></em> hash functions, each member of that family mapping the <em>n</em> keys to the <em>m</em> hash values in a different way.</p>
<p>Every independent uniform random family of hash function is universal, but the converse need not be true: consider the case where <em>U</em> = {0, 1, …, <em>m</em> − 1} and the only hash function in the family is the identity function. The probability that two distinct keys collide is zero, even though each key is hashes to a fixed value.</p>
<a id="p287"/>
<p>The following corollary to Theorem 11.2 on page 279 says that universal hashing provides the desired payoff: it becomes impossible for an adversary to pick a sequence of operations that forces the worst-case running time.</p>
<p class="cor"><strong><em>Corollary 11.3</em></strong></p>
<p class="noindent">Using universal hashing and collision resolution by chaining in an initially empty table with <em>m</em> slots, it takes Θ(<em>s</em>) expected time to handle any sequence of <em>s</em> I<small>NSERT</small>, S<small>EARCH</small>, and D<small>ELETE</small> operations containing <em>n</em> = <em>O</em>(<em>m</em>) I<small>NSERT</small> operations.</p>
<p class="proof"><strong><em>Proof</em></strong>   The I<small>NSERT</small> and D<small>ELETE</small> operations take constant time. Since the number <em>n</em> of insertions is <em>O</em>(<em>m</em>), we have that <em>α</em> = <em>O</em>(1). Furthermore, the expected time for each S<small>EARCH</small> operation is <em>O</em>(1), which can be seen by examining the proof of Theorem 11.2. That analysis depends only on collision probabilities, which are 1/<em>m</em> for any pair <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub> of keys by the choice of an independent uniform hash function in that theorem. Using a universal family of hash functions here instead of using independent uniform hashing changes the probability of collision from 1/<em>m</em> to at most 1/<em>m</em>. By linearity of expectation, therefore, the expected time for the entire sequence of <em>s</em> operations is <em>O</em>(<em>s</em>). Since each operation takes Ω(1) time, the Θ(<em>s</em>) bound follows.</p>
<p class="right"><span class="font1">▪</span></p>
</section>
<section title="11.3.3 Achievable properties of random hashing">
<p class="level2" id="Sec_11.3.3"><strong>11.3.3    Achievable properties of random hashing</strong></p>
<p class="noindent">There is a rich literature on the properties a family <span class="scriptfont">H</span> of hash functions can have, and how they relate to the efficiency of hashing. We summarize a few of the most interesting ones here.</p>
<p>Let <span class="scriptfont">H</span> be a family of hash functions, each with domain <em>U</em> and range {0, 1, …, <em>m</em> − 1}, and let <em>h</em> be any hash function that is picked uniformly at random from <span class="scriptfont">H</span>. The probabilities mentioned are probabilities over the picks of <em>h</em>.</p>
<ul class="ulnoindent1" epub:type="list">
<li class="litop">The family <span class="scriptfont">H</span> is <strong><em><span class="blue1">uniform</span></em></strong> if for any key <em>k</em> in <em>U</em> and any slot <em>q</em> in the range {0, 1, …, <em>m</em> − 1}, the probability that <em>h</em>(<em>k</em>) = <em>q</em> is 1/<em>m</em>.</li>
<li class="litop">The family <span class="scriptfont">H</span> is <strong><em><span class="blue1">universal</span></em></strong> if for any distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> in <em>U</em>, the probability that <em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>2</sub>) is at most 1/<em>m</em>.</li>
<li class="litop">The family <span class="scriptfont">H</span> of hash functions is <strong><em><span class="blue1"><span class="font1">ϵ</span>-universal</span></em></strong> if for any distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> in <em>U</em>, the probability that <em>h</em>(<em>k</em><sub>1</sub>) = <em>h</em>(<em>k</em><sub>2</sub>) is at most <em><span class="font1">ϵ</span></em>. Therefore, a universal family of hash functions is also 1/<em>m</em>-universal.<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup><a id="p288"/></li>
<li class="litop">The family <span class="scriptfont">H</span> is <strong><em><span class="blue1">d-independent</span></em></strong> if for any distinct keys <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>d</sub></em> in <em>U</em> and any slots <em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, …, <em>q<sub>d</sub></em>, not necessarily distinct, in {0, 1, …, <em>m</em> − 1} the probability that <em>h</em>(<em>k<sub>i</sub></em>) = <em>q<sub>i</sub></em> for <em>i</em> = 1, 2, …, <em>d</em> is 1/<em>m<sup>d</sup></em>.</li></ul>
<p class="noindent">Universal hash-function families are of particular interest, as they are the simplest type supporting provably efficient hash-table operations for any input data set. Many other interesting and desirable properties, such as those noted above, are also possible and allow for efficient specialized hash-table operations.</p>
</section>
<section title="11.3.4 Designing a universal family of hash functions">
<p class="level2" id="Sec_11.3.4"><strong>11.3.4    Designing a universal family of hash functions</strong></p>
<p class="noindent">This section present two ways to design a universal (or <em><span class="font1">ϵ</span></em>-universal) family of hash functions: one based on number theory and another based on a randomized variant of the multiply-shift method presented in <a href="chapter011.xhtml#Sec_11.3.1">Section 11.3.1</a>. The first method is a bit easier to prove universal, but the second method is newer and faster in practice.</p>
<p class="level4"><strong>A universal family of hash functions based on number theory</strong></p>
<p class="noindent">We can design a universal family of hash functions using a little number theory. You may wish to refer to <a href="chapter031.xhtml">Chapter 31</a> if you are unfamiliar with basic concepts in number theory.</p>
<p>Begin by choosing a prime number <em>p</em> large enough so that every possible key <em>k</em> lies in the range 0 to <em>p</em> − 1, inclusive. We assume here that <em>p</em> has a “reasonable” length. (See <a href="chapter011.xhtml#Sec_11.3.5">Section 11.3.5</a> for a discussion of methods for handling long input keys, such as variable-length strings.) Let <span class="font1">ℤ</span><em><sub>p</sub></em> denote the set {0, 1, …, <em>p</em> − 1}, and let <img alt="art" src="images/Art_zpast.jpg"/> denote the set {1, 2, …, <em>p</em> − 1}. Since <em>p</em> is prime, we can solve equations modulo <em>p</em> with the methods given in <a href="chapter031.xhtml">Chapter 31</a>. Because the size of the universe of keys is greater than the number of slots in the hash table (otherwise, just use direct addressing), we have <em>p</em> &gt; <em>m</em>.</p>
<p>Given any <img alt="art" src="images/Art_P387.jpg"/> and any <em>b</em> ∈ <span class="font1">ℤ</span><sub><em>p</em></sub>, define the hash function <em>h<sub>ab</sub></em> as a linear transformation followed by reductions modulo <em>p</em> and then modulo <em>m</em>:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P388.jpg"/></p>
<p class="noindent">For example, with <em>p</em> = 17 and <em>m</em> = 6, we have</p>
<table class="table2b">
<tr>
<td class="td2"><em>h</em><sub>3,4</sub>(8)</td>
<td class="td2">=</td>
<td class="td2">((3 · 8 + 4) mod 17) mod 6</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">(28 mod 17) mod 6</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">11 mod 6</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">5.</td>
</tr>
</table>
<p class="noindent">Given <em>p</em> and <em>m</em>, the family of all such hash functions is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P389.jpg"/></p>
<a id="p289"/>
<p class="noindent">Each hash function <em>h<sub>ab</sub></em> maps <span class="font1">ℤ</span><em><sub>p</sub></em> to <span class="font1">ℤ</span><em><sub>m</sub></em>. This family of hash functions has the nice property that the size <em>m</em> of the output range (which is the size of the hash table) is arbitrary—it need not be prime. Since you can choose from among <em>p</em> − 1 values for <em>a</em> and <em>p</em> values for <em>b</em>, the family <em><span class="scriptfont">H</span><sub>pm</sub></em> contains <em>p</em>(<em>p</em> − 1) hash functions.</p>
<p class="theo"><strong><em>Theorem 11.4</em></strong></p>
<p class="noindent">The family <em><span class="scriptfont">H</span><sub>pm</sub></em> of hash functions defined by equations (11.3) and (11.4) is universal.</p>
<p class="proof"><strong><em>Proof</em></strong>   Consider two distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> from <span class="font1">ℤ</span><em><sub>p</sub></em>, so that <em>k</em><sub>1</sub> ≠ <em>k</em><sub>2</sub>. For a given hash function <em>h<sub>ab</sub></em>, let</p>
<p class="eql-a"><em>r</em><sub>1</sub> = (<em>ak</em><sub>1</sub> + <em>b</em>) mod <em>p</em>,</p>
<p class="eql-b"><em>r</em><sub>2</sub> = (<em>ak</em><sub>2</sub> + <em>b</em>) mod <em>p</em>.</p>
<p class="noindent">We first note that <em>r</em><sub>1</sub> ≠ <em>r</em><sub>2</sub>. Why? Since we have <em>r</em><sub>1</sub> − <em>r</em><sub>2</sub> = <em>a</em>(<em>k</em><sub>1</sub> − <em>k</em><sub>2</sub>) (mod <em>p</em>), it follows that <em>r</em><sub>1</sub> ≠ <em>r</em><sub>2</sub> because <em>p</em> is prime and both <em>a</em> and (<em>k</em><sub>1</sub> − <em>k</em><sub>2</sub>) are nonzero modulo <em>p</em>. By Theorem 31.6 on page 908, their product must also be nonzero modulo <em>p</em>. Therefore, when computing any <em>h<sub>ab</sub></em> ∈ <em><span class="scriptfont">H</span><sub>pm</sub></em>, distinct inputs <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> map to distinct values <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub> modulo <em>p</em>, and there are no collisions yet at the “mod <em>p</em> level.” Moreover, each of the possible <em>p</em>(<em>p</em> − 1) choices for the pair (<em>a</em>, <em>b</em>) with <em>a</em> ≠ 0 yields a <em>different</em> resulting pair (<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) with <em>r</em><sub>1</sub> ≠ <em>r</em><sub>2</sub>, since we can solve for <em>a</em> and <em>b</em> given <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub>:</p>
<p class="eql-a"><em>a</em> = ((<em>r</em> − <em>r</em><sub>2</sub>)((<em>k</em><sub>1</sub> − <em>k</em><sub>2</sub>)<sup>−1</sup> mod <em>p</em>)) mod <em>p</em>,</p>
<p class="eql-b"><em>b</em> = (<em>r</em><sub>1</sub> − <em>ak</em><sub>1</sub>) mod <em>p</em>,</p>
<p class="noindent">where ((<em>k</em><sub>1</sub> − <em>k</em><sub>2</sub>)<sup>−1</sup> mod <em>p</em>) denotes the unique multiplicative inverse, modulo <em>p</em>, of <em>k</em><sub>1</sub> − <em>k</em><sub>2</sub>. For each of the <em>p</em> possible values of <em>r</em><sub>1</sub>, there are only <em>p</em> − 1 possible values of <em>r</em><sub>2</sub> that do not equal <em>r</em><sub>1</sub>, making only <em>p</em>(<em>p</em> − 1) possible pairs (<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) with <em>r</em><sub>1</sub> ≠ <em>r</em><sub>2</sub>. Therefore, there is a one-to-one correspondence between pairs (<em>a</em>, <em>b</em>) with <em>a</em> ≠ 0 and pairs (<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) with <em>r</em><sub>1</sub> ≠ <em>r</em><sub>2</sub>. Thus, for any given pair of distinct inputs <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub>, if we pick (<em>a</em>, <em>b</em>) uniformly at random from <img alt="art" src="images/Art_P390.jpg"/>, the resulting pair (<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>) is equally likely to be any pair of distinct values modulo <em>p</em>.</p>
<p>Therefore, the probability that distinct keys <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> collide is equal to the probability that <em>r</em><sub>1</sub> = <em>r</em><sub>2</sub> (mod <em>m</em>) when <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub> are randomly chosen as distinct values modulo <em>p</em>. For a given value of <em>r</em><sub>1</sub>, of the <em>p</em> − 1 possible remaining values for <em>r</em><sub>2</sub>, the number of values <em>r</em><sub>2</sub> such that <em>r</em><sub>2</sub> ≠ <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub> = <em>r</em><sub>1</sub> (mod <em>m</em>) is at most</p>
<p class="eql"><img alt="art" src="images/Art_P391.jpg"/></p>
<a id="p290"/>
<p class="noindent">The probability that <em>r</em><sub>2</sub> collides with <em>r</em><sub>1</sub> when reduced modulo <em>m</em> is at most ((<em>p</em> − 1)/<em>m</em>)/(<em>p</em> − 1) = 1/<em>m</em>, since <em>r</em><sub>2</sub> is equally likely to be any of the <em>p</em> − 1 values in <em>Z<sub>p</sub></em> that are different from <em>r</em><sub>1</sub>, but at most (<em>p</em> − 1)/<em>m</em> of those values are equivalent to <em>r</em><sub>1</sub> modulo <em>m</em>.</p>
<p>Therefore, for any pair of distinct values <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub> ∈ <span class="font1">ℤ</span><sub><em>p</em></sub>,</p>
<p class="eql">Pr{<em>h<sub>ab</sub></em>(<em>k</em><sub>1</sub>) = <em>h<sub>ab</sub></em>(<em>k</em><sub>2</sub>)} ≤ 1/<em>m</em>,</p>
<p class="noindent">so that <em><span class="scriptfont">H</span><sub>pm</sub></em> is indeed universal.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>A 2/<em>m</em>-universal family of hash functions based on the multiply-shift method</strong></p>
<p class="noindent">We recommend that in practice you use the following hash-function family based on the multiply-shift method. It is exceptionally efficient and (although we omit the proof) provably 2/<em>m</em>-universal. Define <span class="scriptfont">H</span> to be the family of multiply-shift hash functions with odd constants <em>a</em>:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P392.jpg"/></p>
<p class="theo"><strong><em>Theorem 11.5</em></strong></p>
<p class="noindent">The family of hash functions <span class="scriptfont">H</span> given by equation (11.5) is 2/<em>m</em>-universal.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">That is, the probability that any two distinct keys collide is at most 2/<em>m</em>. In many practical situations, the speed of computing the hash function more than compensates for the higher upper bound on the probability that two distinct keys collide when compared with a universal hash function.</p>
</section>
<section title="11.3.5 Hashing long inputs such as vectors or strings">
<p class="level2" id="Sec_11.3.5"><strong>11.3.5    Hashing long inputs such as vectors or strings</strong></p>
<p class="noindent">Sometimes hash function inputs are so long that they cannot be easily encoded modulo a reasonably sized prime number <em>p</em> or encoded within a single word of, say, 64 bits. As an example, consider the class of vectors, such as vectors of 8-bit bytes (which is how strings in many programming languages are stored). A vector might have an arbitrary nonnegative length, in which case the length of the input to the hash function may vary from input to input.</p>
<p class="level4"><strong>Number-theoretic approaches</strong></p>
<p class="noindent">One way to design good hash functions for variable-length inputs is to extend the ideas used in <a href="chapter011.xhtml#Sec_11.3.4">Section 11.3.4</a> to design universal hash functions. Exercise 11.3-6 explores one such approach.</p>
<a id="p291"/>
<p class="level4"><strong>Cryptographic hashing</strong></p>
<p class="noindent">Another way to design a good hash function for variable-length inputs is to use a hash function designed for cryptographic applications. <strong><em><span class="blue1">Cryptographic hash functions</span></em></strong> are complex pseudorandom functions, designed for applications requiring properties beyond those needed here, but are robust, widely implemented, and usable as hash functions for hash tables.</p>
<p>A cryptographic hash function takes as input an arbitrary byte string and returns a fixed-length output. For example, the NIST standard deterministic cryptographic hash function SHA-256 [<a epub:type="noteref" href="bibliography001.xhtml#endnote_346">346</a>] produces a 256-bit (32-byte) output for any input.</p>
<p>Some chip manufacturers include instructions in their CPU architectures to provide fast implementations of some cryptographic functions. Of particular interest are instructions that efficiently implement rounds of the Advanced Encryption Standard (AES), the “AES-NI” instructions. These instructions execute in a few tens of nanoseconds, which is generally fast enough for use with hash tables. A message authentication code such as CBC-MAC based on AES and the use of the AES-NI instructions could be a useful and efficient hash function. We don’t pursue the potential use of specialized instruction sets further here.</p>
<p>Cryptographic hash functions are useful because they provide a way of implementing an approximate version of a random oracle. As noted earlier, a random oracle is equivalent to an independent uniform hash function family. From a theoretical point of view, a random oracle is an unachievable ideal: a deterministic function that provides a randomly selected output for each input. Because it is deterministic, it provides the same output if queried again for the same input. From a practical point of view, constructions of hash function families based on cryptographic hash functions are sensible substitutes for random oracles.</p>
<p>There are many ways to use a cryptographic hash function as a hash function. For example, we could define</p>
<p class="eql"><em>h</em>(<em>k</em>) = SHA-256(<em>k</em>) mod <em>m</em>.</p>
<p class="noindent">To define a family of such hash functions one may prepend a “salt” string <em>a</em> to the input before hashing it, as in</p>
<p class="eql"><em>h<sub>a</sub></em>(<em>k</em>) = SHA-256(<em>a</em> <span class="font1">‖</span> <em>k</em>) mod <em>m</em>,</p>
<p class="noindent">where <em>a</em> <span class="font1">‖</span> <em>k</em> denotes the string formed by concatenating the strings <em>a</em> and <em>k</em>. The literature on message authentication codes (MACs) provides additional approaches.</p>
<p>Cryptographic approaches to hash-function design are becoming more practical as computers arrange their memories in hierarchies of differing capacities and speeds. <a href="chapter011.xhtml#Sec_11.5">Section 11.5</a> discusses one hash-function design based on the RC6 encryption method.</p>
<a id="p292"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>11.3-1</em></strong></p>
<p class="noindent">You wish to search a linked list of length <em>n</em>, where each element contains a key <em>k</em> along with a hash value <em>h</em>(<em>k</em>). Each key is a long character string. How might you take advantage of the hash values when searching the list for an element with a given key?</p>
<p class="level3"><strong><em>11.3-2</em></strong></p>
<p class="noindent">You hash a string of <em>r</em> characters into <em>m</em> slots by treating it as a radix-128 number and then using the division method. You can represent the number <em>m</em> as a 32-bit computer word, but the string of <em>r</em> characters, treated as a radix-128 number, takes many words. How can you apply the division method to compute the hash value of the character string without using more than a constant number of words of storage outside the string itself?</p>
<p class="level3"><strong><em>11.3-3</em></strong></p>
<p class="noindent">Consider a version of the division method in which <em>h</em>(<em>k</em>) = <em>k</em> mod <em>m</em>, where <em>m</em> = 2<em><sup>p</sup></em> − 1 and <em>k</em> is a character string interpreted in radix 2<em><sup>p</sup></em>. Show that if string <em>x</em> can be converted to string <em>y</em> by permuting its characters, then <em>x</em> and <em>y</em> hash to the same value. Give an example of an application in which this property would be undesirable in a hash function.</p>
<p class="level3"><strong><em>11.3-4</em></strong></p>
<p class="noindent">Consider a hash table of size <em>m</em> = 1000 and a corresponding hash function <em>h</em>(<em>k</em>) = <span class="font1">⌊</span><em>m</em> (<em>kA</em> mod 1)<span class="font1">⌋</span> for <img alt="art" src="images/Art_P393.jpg"/>. Compute the locations to which the keys 61, 62, 63, 64, and 65 are mapped.</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.3-5</em></strong></p>
<p class="noindent">Show that any <em><span class="font1">ϵ</span></em>-universal family <span class="scriptfont">H</span> of hash functions from a finite set <em>U</em> to a finite set <em>Q</em> has <em><span class="font1">ϵ</span></em> ≥ 1/|<em>Q</em>| − 1/|<em>U</em>|.</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.3-6</em></strong></p>
<p class="noindent">Let <em>U</em> be the set of <em>d</em>-tuples of values drawn from <span class="font1">ℤ</span><em><sub>p</sub></em>, and let <em>Q</em> = <span class="font1">ℤ</span><em><sub>p</sub></em>, where <em>p</em> is prime. Define the hash function <em>h<sub>b</sub></em> : <em>U</em> → <em>Q</em> for <em>b</em> ∈ <span class="font1">ℤ</span><em><sub>p</sub></em> on an input <em>d</em>-tuple <span class="font1">〈</span><em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, …, <em>a</em><sub><em>d</em>−1</sub><span class="font1">〉</span> from <em>U</em> as</p>
<p class="eql"><img alt="art" src="images/Art_P394.jpg"/></p>
<p class="noindent">and let <span class="scriptfont">H</span> = {<em>h<sub>b</sub></em> : <em>b</em> ∈ <span class="font1">ℤ</span><sub><em>p</em></sub>}. Argue that <span class="scriptfont">H</span> is <em><span class="font1">ϵ</span></em>-universal for <em><span class="font1">ϵ</span></em> = (<em>d</em> − 1)/<em>p</em>. (<em>Hint:</em> See Exercise 31.4-4.)</p>
<a id="p293"/>
</section>
</section>
<p class="line1"/>
<section title="11.4 Open addressing">
<a id="Sec_11.4"/>
<p class="level1" id="h1-65"><a href="toc.xhtml#Rh1-65"><strong>11.4    Open addressing</strong></a></p>
<p class="noindent">This section describes open addressing, a method for collision resolution that, unlike chaining, does not make use of storage outside of the hash table itself. In <strong><em><span class="blue1">open addressing</span></em></strong>, all elements occupy the hash table itself. That is, each table entry contains either an element of the dynamic set or <small>NIL</small>. No lists or elements are stored outside the table, unlike in chaining. Thus, in open addressing, the hash table can “fill up” so that no further insertions can be made. One consequence is that the load factor <em>α</em> can never exceed 1.</p>
<p>Collisions are handled as follows: when a new element is to be inserted into the table, it is placed in its “first-choice” location if possible. If that location is already occupied, the new element is placed in its “second-choice” location. The process continues until an empty slot is found in which to place the new element. Different elements have different preference orders for the locations.</p>
<p>To search for an element, systematically examine the preferred table slots for that element, in order of decreasing preference, until either you find the desired element or you find an empty slot and thus verify that the element is not in the table.</p>
<p>Of course, you could use chaining and store the linked lists inside the hash table, in the otherwise unused hash-table slots (see Exercise 11.2-4), but the advantage of open addressing is that it avoids pointers altogether. Instead of following pointers, you compute the sequence of slots to be examined. The memory freed by not storing pointers provides the hash table with a larger number of slots in the same amount of memory, potentially yielding fewer collisions and faster retrieval.</p>
<p>To perform insertion using open addressing, successively examine, or <strong><em><span class="blue1">probe</span></em></strong>, the hash table until you find an empty slot in which to put the key. Instead of being fixed in the order 0, 1, …, <em>m</em> − 1 (which implies a Θ(<em>n</em>) search time), the sequence of positions probed depends upon the key being inserted. To determine which slots to probe, the hash function includes the probe number (starting from 0) as a second input. Thus, the hash function becomes</p>
<p class="eql"><em>h</em> : <em>U</em> × {0, 1, …, <em>m</em> − 1} → {0, 1, …, <em>m</em> − 1}.</p>
<p class="noindent">Open addressing requires that for every key <em>k</em>, the <strong><em><span class="blue1">probe sequence</span></em></strong> <span class="font1">〈</span><em>h</em>(<em>k</em>, 0), <em>h</em>(<em>k</em>, 1), …, <em>h</em>(<em>k</em>, <em>m</em> − 1)<span class="font1">〉</span> be a permutation of <span class="font1">〈</span>0, 1, …, <em>m</em> − 1<span class="font1">〉</span>, so that every hash-table position is eventually considered as a slot for a new key as the table fills up. The H<small>ASH</small>-I<small>NSERT</small> procedure on the following page assumes that the elements in the hash table <em>T</em> are keys with no satellite information: the key <em>k</em> is identical to the element containing key <em>k</em>. Each slot contains either a key or <small>NIL</small> (if the slot is empty). The H<small>ASH</small>-I<small>NSERT</small> procedure takes as input a hash table <em>T</em> and a key <em>k</em> <a id="p294"/>that is assumed to be not already present in the hash table. It either returns the slot number where it stores key <em>k</em> or flags an error because the hash table is already full.</p>
<div class="pull-quote1">
<p class="box-heading">H<small>ASH</small>-I<small>NSERT</small>(<em>T</em>, <em>k</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><em>i</em> = 0</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><strong>repeat</strong></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>q</em> = <em>h</em>(<em>k</em>, <em>i</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>T</em>[<em>q</em>] == <small>NIL</small></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p3"><em>T</em>[<em>q</em>] = <em>k</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p3"><strong>return</strong> <em>q</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p2"><strong>else</strong> <em>i</em> = <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><strong>until</strong> <em>i</em> == <em>m</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">9</span></td>
<td class="td1"><strong>error</strong> “hash table overflow”</td>
</tr>
</table>
<p class="box-headinga">H<small>ASH</small>-S<small>EARCH</small>(<em>T</em>, <em>k</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><em>i</em> = 0</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><strong>repeat</strong></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>q</em> = <em>h</em>(<em>k</em>, <em>i</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>T</em>[<em>q</em>] == <em>k</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p3"><strong>return</strong> <em>q</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2"><em>i</em> = <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><strong>until</strong> <em>T</em>[<em>q</em>] == <small>NIL</small> or <em>i</em> == <em>m</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><strong>return</strong> <small>NIL</small></td>
</tr>
</table>
</div>
<p>The algorithm for searching for key <em>k</em> probes the same sequence of slots that the insertion algorithm examined when key <em>k</em> was inserted. Therefore, the search can terminate (unsuccessfully) when it finds an empty slot, since <em>k</em> would have been inserted there and not later in its probe sequence. The procedure H<small>ASH</small>-S<small>EARCH</small> takes as input a hash table <em>T</em> and a key <em>k</em>, returning <em>q</em> if it finds that slot <em>q</em> contains key <em>k</em>, or <small>NIL</small> if key <em>k</em> is not present in table <em>T</em>.</p>
<p>Deletion from an open-address hash table is tricky. When you delete a key from slot <em>q</em>, it would be a mistake to mark that slot as empty by simply storing <small>NIL</small> in it. If you did, you might be unable to retrieve any key <em>k</em> for which slot <em>q</em> was probed and found occupied when <em>k</em> was inserted. One way to solve this problem is by marking the slot, storing in it the special value <small>DELETED</small> instead of <small>NIL</small>. The H<small>ASH</small>-I<small>NSERT</small> procedure then has to treat such a slot as empty so that it can insert a new key there. The H<small>ASH</small>-S<small>EARCH</small> procedure passes over <small>DELETED</small> values while searching, since slots containing <small>DELETED</small> were filled when the key being searched for was inserted. Using the special value <small>DELETED</small>, however, means that search times no longer depend on the load factor <em>α</em>, and for this reason chaining is <a id="p295"/>frequently selected as a collision resolution technique when keys must be deleted. There is a simple special case of open addressing, linear probing, that avoids the need to mark slots with <small>DELETED</small>. <a href="chapter011.xhtml#Sec_11.5.1">Section 11.5.1</a> shows how to delete from a hash table when using linear probing.</p>
<p>In our analysis, we assume <strong><em><span class="blue1">independent uniform permutation hashing</span></em></strong> (also confusingly known as <strong><em><span class="blue1">uniform hashing</span></em></strong> in the literature): the probe sequence of each key is equally likely to be any of the <em>m</em>! permutations of <span class="font1">〈</span>0, 1, …, <em>m</em> − 1<span class="font1">〉</span>. Independent uniform permutation hashing generalizes the notion of independent uniform hashing defined earlier to a hash function that produces not just a single slot number, but a whole probe sequence. True independent uniform permutation hashing is difficult to implement, however, and in practice suitable approximations (such as double hashing, defined below) are used.</p>
<p>We’ll examine both double hashing and its special case, linear probing. These techniques guarantee that <span class="font1">〈</span><em>h</em>(<em>k</em>, 0), <em>h</em>(<em>k</em>, 1), …, <em>h</em>(<em>k</em>, <em>m</em> − 1)<span class="font1">〉</span> is a permutation of <span class="font1">〈</span>0, 1, …, <em>m</em> − 1<span class="font1">〉</span> for each key <em>k</em>. (Recall that the second parameter to the hash function <em>h</em> is the probe number.) Neither double hashing nor linear probing meets the assumption of independent uniform permutation hashing, however. Double hashing cannot generate more than <em>m</em><sup>2</sup> different probe sequences (instead of the <em>m</em>! that independent uniform permutation hashing requires). Nonetheless, double hashing has a large number of possible probe sequences and, as you might expect, seems to give good results. Linear probing is even more restricted, capable of generating only <em>m</em> different probe sequences.</p>
<p class="level4"><strong>Double hashing</strong></p>
<p class="noindent">Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations. <strong><em><span class="blue1">Double hashing</span></em></strong> uses a hash function of the form</p>
<p class="eql"><em>h</em>(<em>k</em>, <em>i</em>) = (<em>h</em><sub>1</sub>(<em>k</em>) + <em>ih</em><sub>2</sub>(<em>k</em>)) mod <em>m</em>,</p>
<p class="noindent">where both <em>h</em><sub>1</sub> and <em>h</em><sub>2</sub> are <strong><em><span class="blue1">auxiliary hash functions</span></em></strong>. The initial probe goes to position <em>T</em>[<em>h</em><sub>1</sub>(<em>k</em>)], and successive probe positions are offset from previous positions by the amount <em>h</em><sub>2</sub>(<em>k</em>), modulo <em>m</em>. Thus, the probe sequence here depends in two ways upon the key <em>k</em>, since the initial probe position <em>h</em><sub>1</sub>(<em>k</em>), the step size <em>h</em><sub>2</sub>(<em>k</em>), or both, may vary. <a href="chapter011.xhtml#Fig_11-5">Figure 11.5</a> gives an example of insertion by double hashing.</p>
<p>In order for the entire hash table to be searched, the value <em>h</em><sub>2</sub>(<em>k</em>) must be relatively prime to the hash-table size <em>m</em>. (See Exercise 11.4-5.) A convenient way to ensure this condition is to let <em>m</em> be an exact power of 2 and to design <em>h</em><sub>2</sub> so that it always produces an odd number. Another way is to let <em>m</em> be prime and to design <em>h</em><sub>2</sub> so that it always returns a positive integer less than <em>m</em>. For example, you <a id="p296"/>could choose <em>m</em> prime and let</p>
<div class="divimage">
<p class="fig-imga" id="Fig_11-5"><img alt="art" src="images/Art_P395.jpg"/></p>
<p class="caption"><strong>Figure 11.5</strong> Insertion by double hashing. The hash table has size 13 with <em>h</em><sub>1</sub>(<em>k</em>) = <em>k</em> mod 13 and <em>h</em><sub>2</sub>(<em>k</em>) = 1 + (<em>k</em> mod 11). Since 14 = 1 (mod 13) and 14 = 3 (mod 11), the key 14 goes into empty slot 9, after slots 1 and 5 are examined and found to be occupied.</p>
</div>
<p class="eql-a"><em>h</em><sub>1</sub>(<em>k</em>) = <em>k</em> mod <em>m</em>,</p>
<p class="eql-b"><em>h</em><sub>2</sub>(<em>k</em>) = 1 + (<em>k</em> mod <em>m</em>′),</p>
<p class="noindent">where <em>m</em>′ is chosen to be slightly less than <em>m</em> (say, <em>m</em> − 1). For example, if <em>k</em> = 123456, <em>m</em> = 701, and <em>m</em>′ = 700, then <em>h</em><sub>1</sub>(<em>k</em>) = 80 and <em>h</em><sub>2</sub>(<em>k</em>) = 257, so that the first probe goes to position 80, and successive probes examine every 257th slot (modulo <em>m</em>) until the key has been found or every slot has been examined.</p>
<p>Although values of <em>m</em> other than primes or exact powers of 2 can in principle be used with double hashing, in practice it becomes more difficult to efficiently generate <em>h</em><sub>2</sub>(<em>k</em>) (other than choosing <em>h</em><sub>2</sub>(<em>k</em>) = 1, which gives linear probing) in a way that ensures that it is relatively prime to <em>m</em>, in part because the relative density <em><span class="symbolfont">ϕ</span></em>(<em>m</em>)/<em>m</em> of such numbers for general <em>m</em> may be small (see equation (31.25) on page 921).</p>
<p>When <em>m</em> is prime or an exact power of 2, double hashing produces Θ(<em>m</em><sup>2</sup>) probe sequences, since each possible (<em>h</em><sub>1</sub>(<em>k</em>), <em>h</em><sub>2</sub>(<em>k</em>)) pair yields a distinct probe sequence. As a result, for such values of <em>m</em>, double hashing appears to perform close to the “ideal” scheme of independent uniform permutation hashing.</p>
<a id="p297"/>
<p class="level4"><strong>Linear probing</strong></p>
<p class="noindent"><strong><em><span class="blue1">Linear probing</span></em></strong>, a special case of double hashing, is the simplest open-addressing approach to resolving collisions. As with double hashing, an auxiliary hash function <em>h</em><sub>1</sub> determines the first probe position <em>h</em><sub>1</sub>(<em>k</em>) for inserting an element. If slot <em>T</em>[<em>h</em><sub>1</sub>(<em>k</em>)] is already occupied, probe the next position <em>T</em>[<em>h</em><sub>1</sub>(<em>k</em>) + 1]. Keep going as necessary, on up to slot <em>T</em>[<em>m</em> − 1], and then wrap around to slots <em>T</em>[0], <em>T</em>[1], and so on, but never going past slot <em>T</em>[<em>h</em><sub>1</sub>(<em>k</em>) − 1]. To view linear probing as a special case of double hashing, just set the double-hashing step function <em>h</em><sub>2</sub> to be fixed at 1: <em>h</em><sub>2</sub>(<em>k</em>) = 1 for all <em>k</em>. That is, the hash function is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P396.jpg"/></p>
<p class="noindent">for <em>i</em> = 0, 1, …, <em>m</em> − 1. The value of <em>h</em><sub>1</sub>(<em>k</em>) determines the entire probe sequence, and so assuming that <em>h</em><sub>1</sub>(<em>k</em>) can take on any value in {0, 1, …, <em>m</em> − 1}, linear probing allows only <em>m</em> distinct probe sequences.</p>
<p>We’ll revisit linear probing in <a href="chapter011.xhtml#Sec_11.5.1">Section 11.5.1</a>.</p>
<p class="level4"><strong>Analysis of open-address hashing</strong></p>
<p class="noindent">As in our analysis of chaining in <a href="chapter011.xhtml#Sec_11.2">Section 11.2</a>, we analyze open addressing in terms of the load factor <em>α</em> = <em>n</em>/<em>m</em> of the hash table. With open addressing, at most one element occupies each slot, and thus <em>n</em> ≤ <em>m</em>, which implies <em>α</em> ≤ 1. The analysis below requires <em>α</em> to be strictly less than 1, and so we assume that at least one slot is empty. Because deleting from an open-address hash table does not really free up a slot, we assume as well that no deletions occur.</p>
<p>For the hash function, we assume independent uniform permutation hashing. In this idealized scheme, the probe sequence <span class="font1">〈</span><em>h</em>(<em>k</em>, 0), <em>h</em>(<em>k</em>, 1), …, <em>h</em>(<em>k</em>, <em>m</em> − 1)<span class="font1">〉</span> used to insert or search for each key <em>k</em> is equally likely to be any permutation of <span class="font1">〈</span>0, 1, …, <em>m</em> − 1<span class="font1">〉</span>. Of course, any given key has a unique fixed probe sequence associated with it. What we mean here is that, considering the probability distribution on the space of keys and the operation of the hash function on the keys, each possible probe sequence is equally likely.</p>
<p>We now analyze the expected number of probes for hashing with open addressing under the assumption of independent uniform permutation hashing, beginning with the expected number of probes made in an unsuccessful search (assuming, as stated above, that <em>α</em> &lt; 1).</p>
<p>The bound proven, of 1/(1 − <em>α</em>) = 1 + <em>α</em> + <em>α</em><sup>2</sup> + <em>α</em><sup>3</sup> + <span class="font1">⋯</span>, has an intuitive interpretation. The first probe always occurs. With probability approximately <em>α</em>, the first probe finds an occupied slot, so that a second probe happens. With probability approximately <em>α</em><sup>2</sup>, the first two slots are occupied so that a third probe ensues, and so on.</p>
<a id="p298"/>
<p class="theo"><strong><em>Theorem 11.6</em></strong></p>
<p class="noindent">Given an open-address hash table with load factor <em>α</em> = <em>n</em>/<em>m</em> &lt; 1, the expected number of probes in an unsuccessful search is at most 1/(1 − <em>α</em>), assuming independent uniform permutation hashing and no deletions.</p>
<p class="proof"><strong><em>Proof</em></strong>   In an unsuccessful search, every probe but the last accesses an occupied slot that does not contain the desired key, and the last slot probed is empty. Let the random variable <em>X</em> denote the number of probes made in an unsuccessful search, and define the event <em>A<sub>i</sub></em>, for <em>i</em> = 1, 2, …, as the event that an <em>i</em>th probe occurs and it is to an occupied slot. Then the event {<em>X</em> ≥ <em>i</em>} is the intersection of events <em>A</em><sub>1</sub> <span class="font1">⋂</span> <em>A</em><sub>2</sub> <span class="font1">⋂</span> <span class="font1">⋯</span> <span class="font1">⋂</span> <em>A</em><sub><em>i</em>−1</sub>. We bound Pr{<em>X</em> ≥ <em>i</em>} by bounding Pr{<em>A</em><sub>1</sub> <span class="font1">⋂</span> <em>A</em><sub>2</sub> <span class="font1">⋂</span> <span class="font1">⋯</span> <span class="font1">⋂</span> <em>A</em><sub><em>i</em>−1</sub>}. By Exercise C.2-5 on page 1190,</p>
<table class="table2b">
<tr>
<td class="td2">Pr{<em>A</em><sub>1</sub> <span class="font1">⋂</span> <em>A</em><sub>2</sub> <span class="font1">⋂</span> <span class="font1">⋯</span> <span class="font1">⋂</span> <em>A</em><sub><em>i</em>−1</sub>}</td>
<td class="td2">=</td>
<td class="td2">Pr{<em>A</em><sub>1</sub>} · Pr{<em>A</em><sub>2</sub> | <em>A</em><sub>1</sub>} · Pr {<em>A</em><sub>3</sub> | <em>A</em><sub>1</sub> <span class="font1">⋂</span> <em>A</em><sub>2</sub>} <span class="font1">⋯</span></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"/>
<td class="td2">Pr{<em>A</em><sub><em>i</em>−1</sub> | <em>A</em><sub>1</sub> <span class="font1">⋂</span> <em>A</em><sub>2</sub> <span class="font1">⋂</span> <span class="font1">⋯</span> <span class="font1">⋂</span> <em>A</em><sub><em>i</em>−2</sub>}.</td>
</tr>
</table>
<p class="noindent">Since there are <em>n</em> elements and <em>m</em> slots, Pr{<em>A</em><sub>1</sub>} = <em>n</em>/<em>m</em>. For <em>j</em> &gt; 1, the probability that there is a <em>j</em>th probe and it is to an occupied slot, given that the first <em>j</em> − 1 probes were to occupied slots, is (<em>n</em> − <em>j</em> + 1)/(<em>m</em> − <em>j</em> + 1). This probability follows because the <em>j</em>th probe would be finding one of the remaining (<em>n</em> − (<em>j</em> − 1)) elements in one of the (<em>m</em> − (<em>j</em> − 1)) unexamined slots, and by the assumption of independent uniform permutation hashing, the probability is the ratio of these quantities. Since <em>n</em> &lt; <em>m</em> implies that (<em>n</em> − <em>j</em>)/(<em>m</em> − <em>j</em>) ≤ <em>n</em>/<em>m</em> for all <em>j</em> in the range 0 ≤ <em>j</em> &lt; <em>m</em>, it follows that for all <em>i</em> in the range 1 ≤ <em>i</em> ≤ <em>m</em>, we have</p>
<p class="eql"><img alt="art" src="images/Art_P397.jpg"/></p>
<p class="noindent">The product in the first line has <em>i</em> − 1 factors. When <em>i</em> = 1, the product is 1, the identity for multiplication, and we get Pr{<em>X</em> ≥ 1} = 1, which makes sense, since there must always be at least 1 probe. If each of the first <em>n</em> probes is to an occupied slot, then all occupied slots have been probed. Then, the (<em>n</em> + 1)st probe must be to an empty slot, which gives Pr{<em>X</em> ≥ <em>i</em>} = 0 for <em>i</em> &gt; <em>n</em> + 1. Now, we use equation (C.28) on page 1193 to bound the expected number of probes:</p>
<p class="eql"><img alt="art" src="images/Art_P398.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<a id="p299"/>
<p class="space-break">If <em>α</em> is a constant, Theorem 11.6 predicts that an unsuccessful search runs in <em>O</em>(1) time. For example, if the hash table is half full, the average number of probes in an unsuccessful search is at most 1/(1 − .5) = 2. If it is 90% full, the average number of probes is at most 1/(1 − .9) = 10.</p>
<p>Theorem 11.6 yields almost immediately how well the H<small>ASH</small>-I<small>NSERT</small> procedure performs.</p>
<p class="cor"><strong><em>Corollary 11.7</em></strong></p>
<p class="noindent">Inserting an element into an open-address hash table with load factor <em>α</em>, where <em>α</em> &lt; 1, requires at most 1/(1 − <em>α</em>) probes on average, assuming independent uniform permutation hashing and no deletions.</p>
<p class="proof"><strong><em>Proof</em></strong>   An element is inserted only if there is room in the table, and thus <em>α</em> &lt; 1. Inserting a key requires an unsuccessful search followed by placing the key into the first empty slot found. Thus, the expected number of probes is at most 1/(1 − <em>α</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">It takes a little more work to compute the expected number of probes for a successful search.</p>
<p class="theo"><strong><em>Theorem 11.8</em></strong></p>
<p class="noindent">Given an open-address hash table with load factor <em>α</em> &lt; 1, the expected number of probes in a successful search is at most</p>
<p class="eql"><img alt="art" src="images/Art_P399.jpg"/></p>
<p class="noindent">assuming independent uniform permutation hashing with no deletions and assuming that each key in the table is equally likely to be searched for.</p>
<p class="proof"><strong><em>Proof</em></strong>   A search for a key <em>k</em> reproduces the same probe sequence as when the element with key <em>k</em> was inserted. If <em>k</em> was the (<em>i</em> + 1)st key inserted into the hash table, then the load factor at the time it was inserted was <em>i</em>/<em>m</em>, and so by Corollary 11.7, the expected number of probes made in a search for <em>k</em> is at most 1/(1 − <em>i</em>/<em>m</em>) = <em>m</em>/(<em>m</em> − <em>i</em>). Averaging over all <em>n</em> keys in the hash table gives us <a id="p300"/>the expected number of probes in a successful search:</p>
<p class="eql"><img alt="art" src="images/Art_P400.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">If the hash table is half full, the expected number of probes in a successful search is less than 1.387. If the hash table is 90% full, the expected number of probes is less than 2.559. If <em>α</em> = 1, then in an unsuccessful search, all <em>m</em> slots must be probed. Exercise 11.4-4 asks you to analyze a successful search when <em>α</em> = 1.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>11.4-1</em></strong></p>
<p class="noindent">Consider inserting the keys 10, 22, 31, 4, 15, 28, 17, 88, 59 into a hash table of length <em>m</em> = 11 using open addressing. Illustrate the result of inserting these keys using linear probing with <em>h</em>(<em>k</em>, <em>i</em>) = (<em>k</em> + <em>i</em>) mod <em>m</em> and using double hashing with <em>h</em><sub>1</sub>(<em>k</em>) = <em>k</em> and <em>h</em><sub>2</sub>(<em>k</em>) = 1 + (<em>k</em> mod (<em>m</em> − 1)).</p>
<p class="level3"><strong><em>11.4-2</em></strong></p>
<p class="noindent">Write pseudocode for H<small>ASH</small>-D<small>ELETE</small> that fills the deleted key’s slot with the special value <small>DELETED</small>, and modify H<small>ASH</small>-S<small>EARCH</small> and H<small>ASH</small>-I<small>NSERT</small> as needed to handle <small>DELETED</small>.</p>
<p class="level3"><strong><em>11.4-3</em></strong></p>
<p class="noindent">Consider an open-address hash table with independent uniform permutation hashing and no deletions. Give upper bounds on the expected number of probes in an unsuccessful search and on the expected number of probes in a successful search when the load factor is 3/4 and when it is 7/8.</p>
<a id="p301"/>
<p class="level3"><strong><em>11.4-4</em></strong></p>
<p class="noindent">Show that the expected number of probes required for a successful search when <em>α</em> = 1 (that is, when <em>n</em> = <em>m</em>), is <em>H<sub>m</sub></em>, the <em>m</em>th harmonic number.</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.4-5</em></strong></p>
<p class="noindent">Show that, with double hashing, if <em>m</em> and <em>h</em><sub>2</sub>(<em>k</em>) have greatest common divisor <em>d</em> ≥ 1 for some key <em>k</em>, then an unsuccessful search for key <em>k</em> examines (1/<em>d</em>)th of the hash table before returning to slot <em>h</em><sub>1</sub>(<em>k</em>). Thus, when <em>d</em> = 1, so that <em>m</em> and <em>h</em><sub>2</sub>(<em>k</em>) are relatively prime, the search may examine the entire hash table. (<em>Hint:</em> See <a href="chapter031.xhtml">Chapter 31</a>.)</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.4-6</em></strong></p>
<p class="noindent">Consider an open-address hash table with a load factor <em>α</em>. Approximate the nonzero value <em>α</em> for which the expected number of probes in an unsuccessful search equals twice the expected number of probes in a successful search. Use the upper bounds given by Theorems 11.6 and 11.8 for these expected numbers of probes.</p>
</section>
<p class="line1"/>
<section title="11.5 Practical considerations">
<a id="Sec_11.5"/>
<p class="level1" id="h1-66"><a href="toc.xhtml#Rh1-66"><strong>11.5    Practical considerations</strong></a></p>
<p class="noindent">Efficient hash table algorithms are not only of theoretical interest, but also of immense practical importance. Constant factors can matter. For this reason, this section discusses two aspects of modern CPUs that are not included in the standard RAM model presented in <a href="chapter002.xhtml#Sec_2.2">Section 2.2</a>:</p>
<p class="para-hang-top"><strong>Memory hierarchies:</strong> The memory of modern CPUs has a number of levels, from the fast registers, through one or more levels of <strong><em><span class="blue1">cache memory</span></em></strong>, to the main-memory level. Each successive level stores more data than the previous level, but access is slower. As a consequence, a complex computation (such as a complicated hash function) that works entirely within the fast registers can take less time than a single read operation from main memory. Furthermore, cache memory is organized in <strong><em><span class="blue1">cache blocks</span></em></strong> of (say) 64 bytes each, which are always fetched together from main memory. There is a substantial benefit for ensuring that memory usage is local: reusing the same cache block is much more efficient than fetching a different cache block from main memory.</p>
<p class="php">The standard RAM model measures efficiency of a hash-table operation by counting the number of hash-table slots probed. In practice, this metric is only a crude approximation to the truth, since once a cache block is in the cache, successive probes to that cache block are much faster than probes that must access main memory.</p>
<a id="p302"/>
<p class="para-hang-top"><strong>Advanced instruction sets:</strong> Modern CPUs may have sophisticated instruction sets that implement advanced primitives useful for encryption or other forms of cryptography. These instructions may be useful in the design of exceptionally efficient hash functions.</p>
<p class="noindent1-top"><a href="chapter011.xhtml#Sec_11.5.1">Section 11.5.1</a> discusses linear probing, which becomes the collision-resolution method of choice in the presence of a memory hierarchy. <a href="chapter011.xhtml#Sec_11.5.2">Section 11.5.2</a> suggests how to construct “advanced” hash functions based on cryptographic primitives, suitable for use on computers with hierarchical memory models.</p>
<section title="11.5.1 Linear probing">
<p class="level2" id="Sec_11.5.1"><strong>11.5.1    Linear probing</strong></p>
<p class="noindent">Linear probing is often disparaged because of its poor performance in the standard RAM model. But linear probing excels for hierarchical memory models, because successive probes are usually to the same cache block of memory.</p>
<p class="level4"><strong>Deletion with linear probing</strong></p>
<p class="noindent">Another reason why linear probing is often not used in practice is that deletion seems complicated or impossible without using the special <small>DELETED</small> value. Yet we’ll now see that deletion from a hash table based on linear probing is not all that difficult, even without the <small>DELETED</small> marker. The deletion procedure works for linear probing, but not for open-address probing in general, because with linear probing keys all follow the same simple cyclic probing sequence (albeit with different starting points).</p>
<p>The deletion procedure relies on an “inverse” function to the linear-probing hash function <em>h</em>(<em>k</em>, <em>i</em>) = (<em>h</em><sub>1</sub>(<em>k</em>) + <em>i</em>) mod <em>m</em>, which maps a key <em>k</em> and a probe number <em>i</em> to a slot number in the hash table. The inverse function <em>g</em> maps a key <em>k</em> and a slot number <em>q</em>, where 0 ≤ <em>q</em> &lt; <em>m</em>, to the probe number that reaches slot <em>q</em>:</p>
<p class="eql"><em>g</em>(<em>k</em>, <em>q</em>) = (<em>q</em> − <em>h</em><sub>1</sub>(<em>k</em>)) mod <em>m</em>.</p>
<p class="noindent">If <em>h</em>(<em>k</em>, <em>i</em>) = <em>q</em>, then <em>g</em>(<em>k</em>, <em>q</em>) = <em>i</em>, and so <em>h</em>(<em>k</em>, <em>g</em>(<em>k</em>, <em>q</em>)) = <em>q</em>.</p>
<p>The procedure L<small>INEAR</small>-P<small>ROBING</small>-H<small>ASH</small>-D<small>ELETE</small> on the facing page deletes the key stored in position <em>q</em> from hash table <em>T</em>. <a href="chapter011.xhtml#Fig_11-6">Figure 11.6</a> shows how it works. The procedure first deletes the key in position <em>q</em> by setting <em>T</em>[<em>q</em>] to <small>NIL</small> in line 2. It then searches for a slot <em>q</em>′ (if any) that contains a key that should be moved to the slot <em>q</em> just vacated by key <em>k</em>. Line 9 asks the critical question: does the key <em>k</em>′ in slot <em>q</em>′ need to be moved to the vacated slot <em>q</em> in order to preserve the accessibility of <em>k</em>′? If <em>g</em>(<em>k</em>′, <em>q</em>) &lt; <em>g</em>(<em>k</em>′, <em>q</em>′), then during the insertion of <em>k</em>′ into the table, slot <em>q</em> was examined but found to be already occupied. But now slot <em>q</em>, where a search will look for <em>k</em>′, is empty. In this case, key <em>k</em>′ moves to slot <em>q</em> in line 10, and the <a id="p303"/>search continues, to see whether any later key also needs to be moved to the slot <em>q</em>′ that was just freed up when <em>k</em>′ moved.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_11-6"><img alt="art" src="images/Art_P401.jpg"/></p>
<p class="caption"><strong>Figure 11.6</strong> Deletion in a hash table that uses linear probing. The hash table has size 10 with <em>h</em><sub>1</sub>(<em>k</em>) = <em>k</em> mod 10. <strong>(a)</strong> The hash table after inserting keys in the order 74, 43, 93, 18, 82, 38, 92. <strong>(b)</strong> The hash table after deleting the key 43 from slot 3. Key 93 moves up to slot 3 to keep it accessible, and then key 92 moves up to slot 5 just vacated by key 93. No other keys need to be moved.</p>
</div>
<div class="pull-quote1">
<p class="box-heading">L<small>INEAR</small>-P<small>ROBING</small>-H<small>ASH</small>-D<small>ELETE</small>(<em>T</em>, <em>q</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1"><strong>while</strong> <small>TRUE</small></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="p2"><em>T</em>[<em>q</em>] = <small>NIL</small></p></td>
<td class="td1"><span class="red"><strong>//</strong> make slot <em>q</em> empty</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="p2"><em>q</em>′ = <em>q</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> starting point for search</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="p2"><strong>repeat</strong></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p3"><em>q</em>′ = (<em>q</em>′ + 1) mod <em>m</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> next slot number with linear probing</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="p3"><em>k</em>′ = <em>T</em>[<em>q</em>′]</p></td>
<td class="td1"><span class="red"><strong>//</strong> next key to try to move</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1"><p class="p3"><strong>if</strong> <em>k</em>′ == <small>NIL</small></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="p4"><strong>return</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> return when an empty slot is found</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1"><p class="p2"><strong>until</strong> <em>g</em>(<em>k</em>′, <em>q</em>) &lt; <em>g</em>(<em>k</em>′, <em>q</em>′)</p></td>
<td class="td1"><span class="red"><strong>//</strong> was empty slot <em>q</em> probed before <em>q</em>′?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><p class="p2"><em>T</em>[<em>q</em>] = <em>k</em>′</p></td>
<td class="td1"><span class="red"><strong>//</strong> move <em>k</em>′ into slot <em>q</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1"><p class="p2"><em>q</em> = <em>q</em>′</p></td>
<td class="td1"><span class="red"><strong>//</strong> free up slot <em>q</em>′</span></td>
</tr>
</table>
</div>
<p class="level4"><strong>Analysis of linear probing</strong></p>
<p class="noindent">Linear probing is popular to implement, but it exhibits a phenomenon known as <strong><em><span class="blue1">primary clustering</span></em></strong>. Long runs of occupied slots build up, increasing the average <a id="p304"/>search time. Clusters arise because an empty slot preceded by <em>i</em> full slots gets filled next with probability (<em>i</em> + 1)/<em>m</em>. Long runs of occupied slots tend to get longer, and the average search time increases.</p>
<p>In the standard RAM model, primary clustering is a problem, and general double hashing usually performs better than linear probing. By contrast, in a hierarchical memory model, primary clustering is a beneficial property, as elements are often stored together in the same cache block. Searching proceeds through one cache block before advancing to search the next cache block. With linear probing, the running time for a key <em>k</em> of H<small>ASH</small>-I<small>NSERT</small>, H<small>ASH</small>-S<small>EARCH</small>, or L<small>INEAR</small>-P<small>ROBING</small>-H<small>ASH</small>-D<small>ELETE</small> is at most proportional to the distance from <em>h</em><sub>1</sub>(<em>k</em>) to the next empty slot.</p>
<p>The following theorem is due to Pagh et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_351">351</a>]. A more recent proof is given by Thorup [<a epub:type="noteref" href="bibliography001.xhtml#endnote_438">438</a>]. We omit the proof here. The need for 5-independence is by no means obvious; see the cited proofs.</p>
<p class="theo"><strong><em>Theorem 11.9</em></strong></p>
<p class="noindent">If <em>h</em><sub>1</sub> is 5-independent and <em>α</em> ≤ 2/3, then it takes expected constant time to search for, insert, or delete a key in a hash table using linear probing.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="noindent1-top">(Indeed, the expected operation time is <em>O</em>(1/<em><span class="font1">ϵ</span></em> <sup>2</sup>) for <em>α</em> = 1 − <em><span class="font1">ϵ</span></em>.)</p>
</section>
<section title="⋆ 11.5.2 Hash functions for hierarchical memory models">
<p class="level2" id="Sec_11.5.2"><span class="font1">★</span> <strong>11.5.2 Hash functions for hierarchical memory models</strong></p>
<p class="noindent">This section illustrates an approach for designing efficient hash tables in a modern computer system having a memory hierarchy.</p>
<p>Because of the memory hierarchy, linear probing is a good choice for resolving collisions, as probe sequences are sequential and tend to stay within cache blocks. But linear probing is most efficient when the hash function is complex (for example, 5-independent as in Theorem 11.9). Fortunately, having a memory hierarchy means that complex hash functions can be implemented efficiently.</p>
<p>As noted in <a href="chapter011.xhtml#Sec_11.3.5">Section 11.3.5</a>, one approach is to use a cryptographic hash function such as SHA-256. Such functions are complex and sufficiently random for hash table applications. On machines with specialized instructions, cryptographic functions can be quite efficient.</p>
<p>Instead, we present here a simple hash function based only on addition, multiplication, and swapping the halves of a word. This function can be implemented entirely within the fast registers, and on a machine with a memory hierarchy, its latency is small compared with the time taken to access a random slot of the hash table. It is related to the RC6 encryption algorithm and can for practical purposes be considered a “random oracle.”</p>
<a id="p305"/>
<p class="level4"><strong>The wee hash function</strong></p>
<p class="noindent">Let <em>w</em> denote the word size of the machine (e.g., <em>w</em> = 64), assumed to be even, and let <em>a</em> and <em>b</em> be <em>w</em>-bit unsigned (nonnegative) integers such that <em>a</em> is odd. Let swap(<em>x</em>) denote the <em>w</em>-bit result of swapping the two <em>w/</em>2-bit halves of <em>w</em>-bit input <em>x</em>. That is,</p>
<p class="eql">swap(<em>x</em>) = (<em>x</em> <span class="font1">⋙</span> (<em>w</em>/2)) + (<em>x</em> <span class="font1">⋘</span> (<em>w</em>/2))</p>
<p class="noindent">where “<span class="font1">⋙</span>” is “logical right shift” (as in equation (11.2)) and “<span class="font1">⋘</span> is “left shift.” Define</p>
<p class="eql"><em>f<sub>a</sub></em>(<em>k</em>) = swap((2<em>k</em><sup>2</sup> + <em>ak</em>) mod 2<em><sup>w</sup></em>).</p>
<p class="noindent">Thus, to compute <em>f<sub>a</sub></em>(<em>k</em>), evaluate the quadratic function 2<em>k</em><sup>2</sup> + <em>ak</em> modulo 2<em><sup>w</sup></em> and then swap the left and right halves of the result.</p>
<p>Let <em>r</em> denote a desired number of “rounds” for the computation of the hash function. We’ll use <em>r</em> = 4, but the hash function is well defined for any nonnegative <em>r</em>. Denote by <img alt="art" src="images/Art_P402.jpg"/> the result of iterating <em>f<sub>a</sub></em> a total of <em>r</em> times (that is, <em>r</em> rounds) starting with input value <em>k</em>. For any odd <em>a</em> and any <em>r</em> ≥ 0, the function <img alt="art" src="images/far_1.jpg"/>, although complicated, is one-to-one (see Exercise 11.5-1). A cryptographer would view <img alt="art" src="images/far_1.jpg"/> as a simple block cipher operating on <em>w</em>-bit input blocks, with <em>r</em> rounds and key <em>a</em>.</p>
<p>We first define the wee hash function <em>h</em> for short inputs, where by “short” we means “whose length <em>t</em> is at most <em>w</em>-bits,” so that the input fits within one computer word. We would like inputs of different lengths to be hashed differently. The <strong><em><span class="blue1">wee hash function</span></em></strong> <em>h</em><sub><em>a</em>,<em>b</em>,<em>t</em>,<em>r</em></sub>(<em>k</em>) for parameters <em>a</em>, <em>b</em>, and <em>r</em> on <em>t</em>-bit input <em>k</em> is defined as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P403.jpg"/></p>
<p class="noindent">That is, the hash value for <em>t</em>-bit input <em>k</em> is obtained by applying <img alt="art" src="images/Art_P404.jpg"/> to <em>k</em> + <em>b</em>, then taking the final result modulo <em>m</em>. Adding the value <em>b</em> provides hash-dependent randomization of the input, in a way that ensures that for variable-length inputs the 0-length input does not have a fixed hash value. Adding the value 2<em>t</em> to <em>a</em> ensures that the hash function acts differently for inputs of different lengths. (We use 2<em>t</em> rather than <em>t</em> to ensure that the key <em>a</em> + 2<em>t</em> is odd if <em>a</em> is odd.) We call this hash function “wee” because it uses a tiny amount of memory—more precisely, it can be implemented efficiently using only the computer’s fast registers. (This hash function does not have a name in the literature; it is a variant we developed for this textbook.)</p>
<p class="level4"><strong>Speed of the wee hash function</strong></p>
<p class="noindent">It is surprising how much efficiency can be bought with locality. Experiments (unpublished, by the authors) suggest that evaluating the wee hash function takes less <a id="p306"/>time than probing a <em>single</em> randomly chosen slot in a hash table. These experiments were run on a laptop (2019 MacBook Pro) with <em>w</em> = 64 and <em>a</em> = 123. For large hash tables, evaluating the wee hash function was 2 to 10 times faster than performing a single probe of the hash table.</p>
<p class="level4"><strong>The wee hash function for variable-length inputs</strong></p>
<p class="noindent">Sometimes inputs are long—more than one <em>w</em>-bit word in length—or have variable length, as discussed in <a href="chapter011.xhtml#Sec_11.3.5">Section 11.3.5</a>. We can extend the wee hash function, defined above for inputs that are at most single <em>w</em>-bit word in length, to handle long or variable-length inputs. Here is one method for doing so.</p>
<p>Suppose that an input <em>k</em> has length <em>t</em> (measured in bits). Break <em>k</em> into a sequence <span class="font1">〈</span><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>u</sub></em><span class="font1">〉</span> of <em>w</em>-bit words, where <em>u</em> = <span class="font1">⌈</span><em>t</em>/<em>w</em><span class="font1">⌉</span>, <em>k</em><sub>1</sub> contains the least-significant <em>w</em> bits of <em>k</em>, and <em>k<sub>u</sub></em> contains the most significant bits. If <em>t</em> is not a multiple of <em>w</em>, then <em>k<sub>u</sub></em> contains fewer than <em>w</em> bits, in which case, pad out the unused high-order bits of <em>k<sub>u</sub></em> with 0-bits. Define the function chop to return a sequence of the <em>w</em>-bit words in <em>k</em>:</p>
<p class="eql">chop(<em>k</em>) = <span class="font1">〈</span><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>u</sub></em><span class="font1">〉</span>.</p>
<p class="noindent">The most important property of the chop operation is that it is one-to-one, given <em>t</em>: for any two <em>t</em>-bit keys <em>k</em> and <em>k</em>′, if <em>k</em> ≠ <em>k</em>′ then chop(<em>k</em>) ≠ chop(<em>k</em>′), and <em>k</em> can be derived from chop(<em>k</em>) and <em>t</em>. The chop operation also has the useful property that a single-word input key yields a single-word output sequence: chop(<em>k</em>) = <span class="font1">〈</span><em>k</em><span class="font1">〉</span>.</p>
<p>With the chop function in hand, we specify the wee hash function <em>h</em><sub><em>a</em>,<em>b</em>,<em>t</em>,<em>r</em></sub>(<em>k</em>) for an input <em>k</em> of length <em>t</em> bits as follows:</p>
<p class="eql"><em>h</em><sub><em>a</em>,<em>b</em>,<em>t</em>,<em>r</em></sub>(<em>k</em>) = W<small>EE</small>(<em>k</em>, <em>a</em>, <em>b</em>, <em>t</em>, <em>r</em>, <em>m</em>),</p>
<p class="noindent">where the procedure W<small>EE</small> defined on the facing page iterates through the elements of the <em>w</em>-bit words returned by chop(<em>k</em>), applying <img alt="art" src="images/far.jpg"/> to the sum of the current word <em>k<sub>i</sub></em> and the previously computed hash value so far, finally returning the result obtained modulo <em>m</em>. This definition for variable-length and long (multiple-word) inputs is a consistent extension of the definition in equation (11.7) for short (single-word) inputs. For practical use, we recommend that <em>a</em> be a randomly chosen odd <em>w</em>-bit word, <em>b</em> be a randomly chosen <em>w</em>-bit word, and that <em>r</em> = 4.</p>
<p>Note that the wee hash function is really a hash function family, with individual hash functions determined by parameters <em>a</em>, <em>b</em>, <em>t</em>, <em>r</em>, and <em>m</em>. The (approximate) 5-independence of the wee hash function family for variable-length inputs can be argued based on the assumption that the 1-word wee hash function is a random oracle and on the security of the cipher-block-chaining message authentication code (CBC-MAC), as studied by Bellare et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_42">42</a>]. The case here is actually simpler than that studied in the literature, since if two messages have different lengths <em>t</em> and <em>t</em>′, then their “keys” are different: <em>a</em> + 2<em>t</em> ≠ <em>a</em> + 2<em>t</em>′. We omit the details.</p>
<a id="p307"/>
<div class="pull-quote1">
<p class="box-heading">W<small>EE</small>(<em>k</em>, <em>a</em>, <em>b</em>, <em>t</em>, <em>r</em>, <em>m</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><em>u</em> = <span class="font1">⌈</span><em>t</em>/<em>w</em><span class="font1">⌉</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><span class="font1">〈</span><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>u</sub></em><span class="font1">〉</span> = chop(<em>k</em>)</td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><em>q</em> = <em>b</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>u</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P405.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><strong>return</strong> <em>q</em> mod <em>m</em></td>
</tr>
</table>
</div>
<p>This definition of a cryptographically inspired hash-function family is meant to be realistic, yet only illustrative, and many variations and improvements are possible. See the chapter notes for suggestions.</p>
<p>In summary, we see that when the memory system is hierarchical, it becomes advantageous to use linear probing (a special case of double hashing), since successive probes tend to stay in the same cache block. Furthermore, hash functions that can be implemented using only the computer’s fast registers are exceptionally efficient, so they can be quite complex and even cryptographically inspired, providing the high degree of independence needed for linear probing to work most efficiently.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><span class="font1">★</span> <strong><em>11.5-1</em></strong></p>
<p class="noindent">Complete the argument that for any odd positive integer <em>a</em> and any integer <em>r</em> ≥ 0, the function <img alt="art" src="images/far_1.jpg"/> is one-to-one. Use a proof by contradiction and make use of the fact that the function <em>f<sub>a</sub></em> works modulo 2<em><sup>w</sup></em>.</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.5-2</em></strong></p>
<p class="noindent">Argue that a random oracle is 5-independent.</p>
<p class="level3"><span class="font1">★</span> <strong><em>11.5-3</em></strong></p>
<p class="noindent">Consider what happens to the value <img alt="art" src="images/fark.jpg"/> as we flip a single bit <em>k<sub>i</sub></em> of the input value <em>k</em>, for various values of <em>r</em>. Let <img alt="art" src="images/Art_P406.jpg"/> and <img alt="art" src="images/Art_P407.jpg"/> define the bit values <em>k<sub>i</sub></em> in the input (with <em>k</em><sub>0</sub> the least-significant bit) and the bit values <em>b<sub>j</sub></em> in <em>g<sub>a</sub></em>(<em>k</em>) = (2<em>k</em><sup>2</sup> + <em>ak</em>) mod 2<em><sup>w</sup></em> (where <em>g<sub>a</sub></em>(<em>k</em>) is the value that, when its halves are swapped, becomes <em>f<sub>a</sub></em>(<em>k</em>)). Suppose that flipping a single bit <em>k<sub>i</sub></em> of the input <em>k</em> may cause any bit <em>b<sub>j</sub></em> of <em>g<sub>a</sub></em>(<em>k</em>) to flip, for <em>j</em> ≥ <em>i</em>. What is the least value of <em>r</em> for which flipping the value of any single bit <em>k<sub>i</sub></em> may cause <em>any</em> bit of the output <img alt="art" src="images/fark.jpg"/> to flip? Explain.</p>
<a id="p308"/>
</section>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-67"><strong>Problems</strong></p>
<section title="11-1 Longest-probe bound for hashing">
<p class="level2"><strong><em>11-1     Longest-probe bound for hashing</em></strong></p>
<p class="noindent">Suppose you are using an open-addressed hash table of size <em>m</em> to store <em>n</em> ≤ <em>m</em>/2 items.</p>
<p class="nl"><strong><em>a.</em></strong> Assuming independent uniform permutation hashing, show that for <em>i</em> = 1, 2, …, <em>n</em>, the probability is at most 2<sup>−<em>p</em></sup> that the <em>i</em>th insertion requires strictly more than <em>p</em> probes.</p>
<p class="nl"><strong><em>b.</em></strong> Show that for <em>i</em> = 1, 2, …, <em>n</em>, the probability is <em>O</em>(1/<em>n</em><sup>2</sup>) that the <em>i</em>th insertion requires more than 2 lg <em>n</em> probes.</p>
<p class="noindent">Let the random variable <em>X<sub>i</sub></em> denote the number of probes required by the <em>i</em>th insertion. You have shown in part (b) that Pr{<em>X<sub>i</sub></em> &gt; 2 lg <em>n</em>} = <em>O</em>(1/<em>n</em><sup>2</sup>). Let the random variable <em>X</em> = max {<em>X<sub>i</sub></em> : 1 ≤ <em>i</em> ≤ <em>n</em>} denote the maximum number of probes required by any of the <em>n</em> insertions.</p>
<p class="nl"><strong><em>c.</em></strong> Show that Pr{<em>X</em> &gt; 2 lg <em>n</em>} = <em>O</em>(1/<em>n</em>).</p>
<p class="nl"><strong><em>d.</em></strong> Show that the expected length E[<em>X</em>] of the longest probe sequence is <em>O</em>(lg <em>n</em>).</p>
</section>
<section title="11-2 Searching a static set">
<p class="level2"><strong><em>11-2     Searching a static set</em></strong></p>
<p class="noindent">You are asked to implement a searchable set of <em>n</em> elements in which the keys are numbers. The set is static (no I<small>NSERT</small> or D<small>ELETE</small> operations), and the only operation required is S<small>EARCH</small>. You are given an arbitrary amount of time to preprocess the <em>n</em> elements so that S<small>EARCH</small> operations run quickly.</p>
<p class="nl"><strong><em>a.</em></strong> Show how to implement S<small>EARCH</small> in <em>O</em>(lg <em>n</em>) worst-case time using no extra storage beyond what is needed to store the elements of the set themselves.</p>
<p class="nl"><strong><em>b.</em></strong> Consider implementing the set by open-address hashing on <em>m</em> slots, and assume independent uniform permutation hashing. What is the minimum amount of extra storage <em>m</em> − <em>n</em> required to make the average performance of an unsuccessful S<small>EARCH</small> operation be at least as good as the bound in part (a)? Your answer should be an asymptotic bound on <em>m</em> − <em>n</em> in terms of <em>n</em>.</p>
</section>
<section title="11-3 Slot-size bound for chaining">
<p class="level2"><strong><em>11-3     Slot-size bound for chaining</em></strong></p>
<p class="noindent">Given a hash table with <em>n</em> slots, with collisions resolved by chaining, suppose that <em>n</em> keys are inserted into the table. Each key is equally likely to be hashed to each slot. Let <em>M</em> be the maximum number of keys in any slot after all the keys have <a id="p309"/>been inserted. Your mission is to prove an <em>O</em>(lg <em>n</em> / lg lg <em>n</em>) upper bound on E[<em>M</em>], the expected value of <em>M</em>.</p>
<p class="nl"><strong><em>a.</em></strong> Argue that the probability <em>Q<sub>k</sub></em> that exactly <em>k</em> keys hash to a particular slot is given by</p>
<p class="eqnl"><img alt="art" src="images/Art_P408.jpg"/></p>
<p class="nl"><strong><em>b.</em></strong> Let <em>P<sub>k</sub></em> be the probability that <em>M</em> = <em>k</em>, that is, the probability that the slot containing the most keys contains <em>k</em> keys. Show that <em>P<sub>k</sub></em> ≤ <em>nQ<sub>k</sub></em>.</p>
<a id="p310"/>
<p class="nl"><strong><em>c.</em></strong> Show that <em>Q<sub>k</sub></em> &lt; <em>e<sup>k</sup></em>/<em>k<sup>k</sup></em>. <em>Hint:</em> Use Stirling’s approximation, equation (3.25) on page 67.</p>
<p class="nl"><strong><em>d.</em></strong> Show that there exists a constant <em>c</em> &gt; 1 such that <img alt="art" src="images/Art_P409.jpg"/> for <em>k</em><sub>0</sub> = <em>c</em> lg <em>n</em> / lg lg <em>n</em>. Conclude that <em>P<sub>k</sub></em> &lt; 1/<em>n</em><sup>2</sup> for <em>k</em> ≥ <em>k</em><sub>0</sub> = <em>c</em> lg <em>n</em> / lg lg <em>n</em>.</p>
<p class="nl"><strong><em>e.</em></strong> Argue that</p>
<p class="eqnl"><img alt="art" src="images/Art_P410.jpg"/></p>
<p class="nl-para">Conclude that E[<em>M</em>] = <em>O</em>(lg <em>n</em> / lg lg <em>n</em>).</p>
</section>
<section title="11-4 Hashing and authentication">
<p class="level2"><strong><em>11-4     Hashing and authentication</em></strong></p>
<p class="noindent">Let <span class="scriptfont">H</span> be a family of hash functions in which each hash function <em>h</em> ∈ <span class="scriptfont">H</span> maps the universe <em>U</em> of keys to {0, 1, …, <em>m</em> − 1}.</p>
<p class="nl"><strong><em>a.</em></strong> Show that if the family <span class="scriptfont">H</span> of hash functions is 2-independent, then it is universal.</p>
<p class="nl"><strong><em>b.</em></strong> Suppose that the universe <em>U</em> is the set of <em>n</em>-tuples of values drawn from <span class="font1">ℤ</span><sub><em>p</em></sub> = {0, 1, …, <em>p</em> − 1}, where <em>p</em> is prime. Consider an element <em>x</em> = <span class="font1">〈</span><em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, …, <em>x</em><sub><em>n</em>−1</sub><span class="font1">〉</span> ∈ <em>U</em>. For any <em>n</em>-tuple <em>a</em> = <span class="font1">〈</span><em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, …, <em>a</em><sub><em>n</em>−1</sub><span class="font1">〉</span> ∈ <em>U</em>, define the hash function <em>h<sub>a</sub></em> by</p>
<p class="eqnl"><img alt="art" src="images/Art_P411.jpg"/></p>
<p class="nl-para">Let <span class="scriptfont">H</span> = {<em>h<sub>a</sub></em> : <em>a</em> ∈ <em>U</em>}. Show that <span class="scriptfont">H</span> is universal, but not 2-independent. (<em>Hint:</em> Find a key for which all hash functions in <span class="scriptfont">H</span> produce the same value.)</p>
<p class="nl"><strong><em>c.</em></strong> Suppose that we modify <span class="scriptfont">H</span> slightly from part (b): for any <em>a</em> ∈ <em>U</em> and for any <em>b</em> ∈ <span class="font1">ℤ</span><em><sub>p</sub></em>, define</p>
<p class="eqnl"><img alt="art" src="images/Art_P412.jpg"/></p>
<p class="nl-para">and <img alt="art" src="images/Art_P413.jpg"/>. Argue that <span class="scriptfont">H</span>′ is 2-independent. (<em>Hint:</em> Consider fixed <em>n</em>-tuples <em>x</em> ∈ <em>U</em> and <em>y</em> ∈ <em>U</em>, with <em>x<sub>i</sub></em> ≠ <em>y<sub>i</sub></em> for some <em>i</em>. What happens to <img alt="art" src="images/Art_P414.jpg"/> and <img alt="art" src="images/Art_P415.jpg"/> as <em>a<sub>i</sub></em> and <em>b</em> range over <span class="font1">ℤ</span><em><sub>p</sub></em>?)</p>
<p class="nl"><strong><em>d.</em></strong> Alice and Bob secretly agree on a hash function <em>h</em> from a 2-independent family <span class="scriptfont">H</span> of hash functions. Each <em>h</em> ∈ <span class="scriptfont">H</span> maps from a universe of keys <em>U</em> to <span class="font1">ℤ</span><em><sub>p</sub></em>, where <em>p</em> is prime. Later, Alice sends a message <em>m</em> to Bob over the internet, where <em>m</em> ∈ <em>U</em>. She authenticates this message to Bob by also sending an authentication tag <em>t</em> = <em>h</em>(<em>m</em>), and Bob checks that the pair (<em>m</em>, <em>t</em>) he receives indeed satisfies <em>t</em> = <em>h</em>(<em>m</em>). Suppose that an adversary intercepts (<em>m</em>, <em>t</em>) en route and tries to fool Bob by replacing the pair (<em>m</em>, <em>t</em>) with a different pair (<em>m</em>′, <em>t</em>′). Argue that the probability that the adversary succeeds in fooling Bob into accepting (<em>m</em>′, <em>t</em>′) is at most 1/<em>p</em>, no matter how much computing power the adversary has, even if the adversary knows the family <span class="scriptfont">H</span> of hash functions used.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-68"><strong>Chapter notes</strong></p>
<p class="noindent">The books by Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_261">261</a>] and Gonnet and Baeza-Yates [<a epub:type="noteref" href="bibliography001.xhtml#endnote_193">193</a>] are excellent references for the analysis of hashing algorithms. Knuth credits H. P. Luhn (1953) for inventing hash tables, along with the chaining method for resolving collisions. At about the same time, G. M. Amdahl originated the idea of open addressing. The notion of a random oracle was introduced by Bellare et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_43">43</a>]. Carter and Wegman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_80">80</a>] introduced the notion of universal families of hash functions in 1979.</p>
<p>Dietzfelbinger et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_113">113</a>] invented the multiply-shift hash function and gave a proof of Theorem 11.5. Thorup [<a epub:type="noteref" href="bibliography001.xhtml#endnote_437">437</a>] provides extensions and additional analysis. Thorup [<a epub:type="noteref" href="bibliography001.xhtml#endnote_438">438</a>] gives a simple proof that linear probing with 5-independent hashing takes constant expected time per operation. Thorup also describes the method for deletion in a hash table using linear probing.</p>
<p>Fredman, Komlós, and Szemerédi [<a epub:type="noteref" href="bibliography001.xhtml#endnote_154">154</a>] developed a perfect hashing scheme for static sets—“perfect” because all collisions are avoided. An extension of their method to dynamic sets, handling insertions and deletions in amortized expected time <em>O</em>(1), has been given by Dietzfelbinger et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_114">114</a>].</p>
<p>The wee hash function is based on the RC6 encryption algorithm [<a epub:type="noteref" href="bibliography001.xhtml#endnote_379">379</a>]. Leiserson et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_292">292</a>] propose an “RC6<small>MIX</small>” function that is essentially the same as the <a id="p311"/>wee hash function. They give experimental evidence that it has good randomness, and they also give a “D<small>OT</small>M<small>IX</small>” function for dealing with variable-length inputs. Bellare et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_42">42</a>] provide an analysis of the security of the cipher-block-chaining message authentication code. This analysis implies that the wee hash function has the desired pseudorandomness properties.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> The definition of “average-case” requires care—are we assuming an input distribution over the keys, or are we randomizing the choice of hash function itself? We’ll consider both approaches, but with an emphasis on the use of a randomly chosen hash function.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> In the literature, a (<em>c</em>/<em>m</em>)-universal hash function is sometimes called <em>c</em>-universal or <em>c</em>-approximately universal. We’ll stick with the notation (<em>c</em>/<em>m</em>)-universal.</p>
</section>
</section>
</div>
</body>
</html>