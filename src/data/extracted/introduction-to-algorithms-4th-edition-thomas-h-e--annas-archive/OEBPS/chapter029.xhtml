<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p850"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="29 Linear Programming">
<p class="chapter-title"><a href="toc.xhtml#chap-29"><strong><span class="blue1">29        Linear Programming</span></strong></a></p>
<p class="noindent">Many problems take the form of maximizing or minimizing an objective, given limited resources and competing constraints. If you can specify the objective as a linear function of certain variables, and if you can specify the constraints on resources as equalities or inequalities on those variables, then you have a <strong><em><span class="blue1">linear-programming problem</span></em></strong>. Linear programs arise in a variety of practical applications. We begin by studying an application in electoral politics.</p>
<p class="level4"><strong>A political problem</strong></p>
<p class="noindent">Suppose that you are a politician trying to win an election. Your district has three different types of areas—urban, suburban, and rural. These areas have, respectively, 100,000, 200,000, and 50,000 registered voters. Although not all the registered voters actually go to the polls, you decide that to govern effectively, you would like at least half the registered voters in each of the three regions to vote for you. You are honorable and would never consider supporting policies you don’t believe in. You realize, however, that certain issues may be more effective in winning votes in certain places. Your primary issues are preparing for a zombie apocalypse, equipping sharks with lasers, building highways for flying cars, and allowing dolphins to vote.</p>
<p>According to your campaign staff’s research, you can estimate how many votes you win or lose from each population segment by spending $1,000 on advertising on each issue. This information appears in the table of <a href="chapter029.xhtml#Fig_29-1">Figure 29.1</a>. In this table, each entry indicates the number of thousands of either urban, suburban, or rural voters who would be won over by spending $1,000 on advertising in support of a particular issue. Negative entries denote votes that would be lost. Your task is to figure out the minimum amount of money that you need to spend in order to win 50,000 urban votes, 100,000 suburban votes, and 25,000 rural votes.</p>
<p>You could, by trial and error, devise a strategy that wins the required number of votes, but the strategy you come up with might not be the least expensive one. For example, you could devote $20,000 of advertising to preparing for a zombie <a id="p851"/>apocalypse, $0 to equipping sharks with lasers, $4,000 to building highways for flying cars, and $9,000 to allowing dolphins to vote. In this case, you would win (20 · −2) + (0 · 8) + (4 · 0) + (9 · 10) = 50 thousand urban votes, (20 · 5) + (0 · 2) + (4 · 0) + (9 · 0) = 100 thousand suburban votes, and (20 · 3) + (0 · −5) + (4 · 10) + (9 · −2) = 82 thousand rural votes. You would win the exact number of votes desired in the urban and suburban areas and more than enough votes in the rural area. (In fact, according to your model, in the rural area you would receive more votes than there are voters.) In order to garner these votes, you would have paid for 20 + 0 + 4 + 9 = 33 thousand dollars of advertising.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_29-1"><img alt="art" src="images/Art_P976.jpg"/></p>
<p class="caption"><strong>Figure 29.1</strong> The effects of policies on voters. Each entry describes the number of thousands of urban, suburban, or rural voters who could be won over by spending $1,000 on advertising support of a policy on a particular issue. Negative entries denote votes that would be lost.</p>
</div>
<p>It’s natural to wonder whether this strategy is the best possible. That is, can you achieve your goals while spending less on advertising? Additional trial and error might help you to answer this question, but a better approach is to formulate (or <strong><em><span class="blue1">model</span></em></strong>) this question mathematically.</p>
<p>The first step is to decide what decisions you have to make and to introduce variables that capture these decisions. Since you have four decisions, you introduce four <strong><em><span class="blue1">decision variables</span></em></strong>:</p>
<ul class="ulnoindent" epub:type="list">
<li><em>x</em><sub>1</sub> is the number of thousands of dollars spent on advertising on preparing for a zombie apocalypse,</li>
<li class="litop"><em>x</em><sub>2</sub> is the number of thousands of dollars spent on advertising on equipping sharks with lasers,</li>
<li class="litop"><em>x</em><sub>3</sub> is the number of thousands of dollars spent on advertising on building highways for flying cars, and</li>
<li class="litop"><em>x</em><sub>4</sub> is the number of thousands of dollars spent on advertising on allowing dolphins to vote.</li></ul>
<p class="noindent">You then think about <strong><em><span class="blue1">constraints</span></em></strong>, which are limits, or restrictions, on the values that the decision variables can take. You can write the requirement that you win at least 50,000 urban votes as</p>
<p class="eqr"><img alt="art" src="images/Art_P977.jpg"/></p>
<a id="p852"/>
<p class="noindent">Similarly, you can write the requirements that you win at least 100,000 suburban votes and 25,000 rural votes as</p>
<p class="eqr"><img alt="art" src="images/Art_P978.jpg"/></p>
<p class="noindent">and</p>
<p class="eqr"><img alt="art" src="images/Art_P979.jpg"/></p>
<p class="noindent">Any setting of the variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, <em>x</em><sub>4</sub> that satisfies inequalities (29.1)–(29.3) yields a strategy that wins a sufficient number of each type of vote.</p>
<p>Finally, you think about your <strong><em><span class="blue1">objective</span></em></strong>, which is the quantity that you wish to either minimize or maximize. In order to keep costs as small as possible, you would like to minimize the amount spent on advertising. That is, you want to minimize the expression</p>
<p class="eqr"><img alt="art" src="images/Art_P980.jpg"/></p>
<p class="noindent">Although negative advertising often occurs in political campaigns, there is no such thing as negative-cost advertising. Consequently, you require that</p>
<p class="eqr"><img alt="art" src="images/Art_P981.jpg"/></p>
<p class="noindent">Combining inequalities (29.1)–(29.3) and (29.5) with the objective of minimizing (29.4) produces what is known as a “linear program.” We can format this problem tabularly as</p>
<p class="eqr"><img alt="art" src="images/Art_P982.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P983.jpg"/></p>
<p class="noindent">The solution to this linear program yields your optimal strategy.</p>
<p>The remainder of this chapter covers how to formulate linear programs and is an introduction to modeling in general. Modeling refers to the general process of converting a problem into a mathematical form amenable to solution by an algorithm. <a href="chapter029.xhtml#Sec_29.1">Section 29.1</a> discusses briefly the algorithmic aspects of linear programming, although it does not include the details of a linear-programming algorithm. Throughout this book, we have seen ways to model problems, such as by shortest paths and connectivity in a graph. When modeling a problem as a linear program, you go through the steps used in this political example—identifying the decision variables, specifying the constraints, and formulating the objective function. In order to model a problem as a linear program, the constraints and objectives must be <a id="p853"/>linear. In <a href="chapter029.xhtml#Sec_29.2">Section 29.2</a>, we will see several other examples of modeling via linear programs. <a href="chapter029.xhtml#Sec_29.3">Section 29.3</a> discusses duality, an important concept in linear programming and other optimization algorithms.</p>
<p class="line1"/>
<section title="29.1 Linear programming formulations and algorithms">
<a id="Sec_29.1"/>
<p class="level1" id="h1-167"><a href="toc.xhtml#Rh1-167"><strong>29.1    Linear programming formulations and algorithms</strong></a></p>
<p class="noindent">Linear programs take a particular form, which we will examine in this section. Multiple algorithms have been developed to solve linear programs. Some run in polynomial time, some do not, but they are all too complicated to show here. Instead, we will give an example that demonstrates some ideas behind the simplex algorithm, which is currently the most commonly deployed solution method.</p>
<p class="level4"><strong>General linear programs</strong></p>
<p class="noindent">In the general linear-programming problem, we wish to optimize a linear function subject to a set of linear inequalities. Given a set of real numbers <em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em> and a set of variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>, we define a <strong><em><span class="blue1">linear function</span></em></strong> <em>f</em> on those variables by</p>
<p class="eql"><img alt="art" src="images/Art_P984.jpg"/></p>
<p class="noindent">If <em>b</em> is a real number and <em>f</em> is a linear function, then the equation</p>
<p class="eql"><em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>) = <em>b</em></p>
<p class="noindent">is a <strong><em><span class="blue1">linear equality</span></em></strong> and the inequalities</p>
<p class="eql"><em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>) ≤ <em>b</em> and <em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>) ≥ <em>b</em></p>
<p class="noindent">are <strong><em><span class="blue1">linear inequalities</span></em></strong>. We use the general term <strong><em><span class="blue1">linear constraints</span></em></strong> to denote either linear equalities or linear inequalities. Linear programming does not allow strict inequalities. Formally, a <strong><em><span class="blue1">linear-programming problem</span></em></strong> is the problem of either minimizing or maximizing a linear function subject to a finite set of linear constraints. If minimizing, we call the linear program a <strong><em><span class="blue1">minimization linear program</span></em></strong>, and if maximizing, we call the linear program a <strong><em><span class="blue1">maximization linear program</span></em></strong>.</p>
<p>In order to discuss linear-programming algorithms and properties, it will be helpful to use a standard notation for the input. By convention, a maximization linear program takes as input <em>n</em> real numbers <em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, … , <em>c<sub>n</sub></em>; <em>m</em> real numbers <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>m</sub></em>; and <em>mn</em> real numbers <em>a<sub>ij</sub></em> for <em>i</em> = 1, 2, … , <em>m</em> and <em>j</em> = 1, 2, … , <em>n</em>.</p>
<a id="p854"/>
<p class="noindent">The goal is to find <em>n</em> real numbers <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em> that</p>
<p class="eqr"><img alt="art" src="images/Art_P985.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P986.jpg"/></p>
<p class="noindent">We call expression (29.11) the <strong><em><span class="blue1">objective function</span></em></strong> and the <em>n</em> + <em>m</em> inequalities in lines (29.12) and (29.13) the <strong><em><span class="blue1">constraints</span></em></strong>. The <em>n</em> constraints in line (29.13) are the <strong><em><span class="blue1">nonnegativity constraints</span></em></strong>. It can sometimes be more convenient to express a linear program in a more compact form. If we create an <em>m</em> × <em>n</em> matrix <em>A</em> = (<em>a<sub>ij</sub></em>), an <em>m</em>-vector <em>b</em> = (<em>b<sub>i</sub></em>), an <em>n</em>-vector <em>c</em> = (<em>c<sub>j</sub></em>), and an <em>n</em>-vector <em>x</em> = (<em>x<sub>j</sub></em>), then we can rewrite the linear program defined in (29.11)–(29.13) as</p>
<p class="eqr"><img alt="art" src="images/Art_P987.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P988.jpg"/></p>
<p class="noindent">In line (29.14), <em>c</em><sup>T</sup><em>x</em> is the inner product of two <em>n</em>-vectors. In inequality (29.15), <em>Ax</em> is the <em>m</em>-vector that is the product of an <em>m</em> × <em>n</em> matrix and an <em>n</em>-vector, and in inequality (29.16), <em>x</em> ≥ 0 means that each entry of the vector <em>x</em> must be nonnegative. We call this representation the <strong><em><span class="blue1">standard form</span></em></strong> for a linear program, and we adopt the convention that <em>A</em>, <em>b</em>, and <em>c</em> always have the dimensions given above.</p>
<p>The standard form above may not naturally correspond to real-life situations you are trying to model. For example, you might have equality constraints or variables that can take on negative values. Exercises 29.1-6 and 29.1-7 ask you to show how to convert any linear program into this standard form.</p>
<p>We now introduce terminology to describe solutions to linear programs. We denote a particular setting of the values in a variable, say <em>x</em>, by putting a bar over the variable name: <em><span class="overline">x</span></em>. If <em><span class="overline">x</span></em> satisfies all the constraints, then it is a <strong><em><span class="blue1">feasible solution</span></em></strong>, but if it fails to satisfy at least one constraint, then it is an <strong><em><span class="blue1">infeasible solution</span></em></strong>. We say that a solution <em><span class="overline">x</span></em> has <strong><em><span class="blue1">objective value</span></em></strong> <em>c</em><sup>T</sup><em><span class="overline">x</span></em>. A feasible solution <em><span class="overline">x</span></em> whose objective value is maximum over all feasible solutions is an <strong><em><span class="blue1">optimal solution</span></em></strong>, and we call its objective value <em>c</em><sup>T</sup><em><span class="overline">x</span></em> the <strong><em><span class="blue1">optimal objective value</span></em></strong>. If a linear program has no feasible solutions, we say that the linear program is <strong><em><span class="blue1">infeasible</span></em></strong>, and otherwise, it is <strong><em><span class="blue1">feasible</span></em></strong>. The set of points that satisfy all the constraints is the <strong><em><span class="blue1">feasible region</span></em></strong>. If a linear program has some feasible solutions but does not have a finite optimal objective value, then the feasible region is <strong><em><span class="blue1">unbounded</span></em></strong> and so is the linear program. Exercise 29.1-5 asks you to show that a linear program can have a finite optimal objective value even if the feasible region is unbounded.</p>
<a id="p855"/>
<p>One of the reasons for the power and popularity of linear programming is that linear programs can, in general, be solved efficiently. There are two classes of algorithms, known as ellipsoid algorithms and interior-point algorithms, that solve linear programs in polynomial time. In addition, the simplex algorithm is widely used. Although it does not run in polynomial time in the worst case, it tends to perform well in practice.</p>
<p>We will not give a detailed algorithm for linear programming, but will discuss a few important ideas. First, we will give an example of using a geometric procedure to solve a two-variable linear program. Although this example does not immediately generalize to an efficient algorithm for larger problems, it introduces some important concepts for linear programming and for optimization in general.</p>
<p class="level4"><strong>A two-variable linear program</strong></p>
<p class="noindent">Let us first consider the following linear program with two variables:</p>
<p class="eqr"><img alt="art" src="images/Art_P989.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P990.jpg"/></p>
<p class="noindent"><a href="chapter029.xhtml#Fig_29-2">Figure 29.2(a)</a> graphs the constraints in the (<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>)-Cartesian coordinate system. The feasible region in the two-dimensional space (highlighted in blue in the figure) is convex.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> Conceptually, you could evaluate the objective function <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> at each point in the feasible region, and then identify a point that has the maximum objective value as an optimal solution. For this example (and for most linear programs), however, the feasible region contains an infinite number of points, and so to solve this linear program, you need an efficient way to find a point that achieves the maximum objective value without explicitly evaluating the objective function at every point in the feasible region.</p>
<p>In two dimensions, you can optimize via a graphical procedure. The set of points for which <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = <em>z</em>, for any <em>z</em>, is a line with a slope of −1. Plotting <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 0 produces the line with slope −1 through the origin, as in <a href="chapter029.xhtml#Fig_29-2">Figure 29.2(b)</a>. The intersection of this line and the feasible region is the set of feasible solutions that have an objective value of 0. In this case, that intersection of the line with the feasible region is the single point (0, 0). More generally, for any value <em>z</em>, the <a id="p856"/>intersection of the line <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = <em>z</em> and the feasible region is the set of feasible solutions that have objective value <em>z</em>. <a href="chapter029.xhtml#Fig_29-2">Figure 29.2(b)</a> shows the lines <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 0, <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 4, and <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 8. Because the feasible region in <a href="chapter029.xhtml#Fig_29-2">Figure 29.2</a> is bounded, there must be some maximum value <em>z</em> for which the intersection of the line <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = <em>z</em> and the feasible region is nonempty. Any point in the feasible region that maximizes <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> is an optimal solution to the linear program, which in this case is the vertex of the feasible region at <em>x</em><sub>1</sub> = 2 and <em>x</em><sub>2</sub> = 6, with objective value 8.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_29-2"><img alt="art" src="images/Art_P991.jpg"/></p>
<p class="caption"><strong>Figure 29.2 (a)</strong> The linear program given in (29.18)–(29.21). Each constraint is represented by a line and a direction. The intersection of the constraints, which is the feasible region, is highlighted in blue. <strong>(b)</strong> The red lines show, respectively, the points for which the objective value is 0, 4, and 8. The optimal solution to the linear program is <em>x</em><sub>1</sub> = 2 and <em>x</em><sub>2</sub> = 6 with objective value 8.</p>
</div>
<p>It is no accident that an optimal solution to the linear program occurs at a vertex of the feasible region. The maximum value of <em>z</em> for which the line <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = <em>z</em> intersects the feasible region must be on the boundary of the feasible region, and thus the intersection of this line with the boundary of the feasible region is either a single vertex or a line segment. If the intersection is a single vertex, then there is just one optimal solution, and it is that vertex. If the intersection is a line segment, every point on that line segment must have the same objective value. In particular, both endpoints of the line segment are optimal solutions. Since each endpoint of a line segment is a vertex, there is an optimal solution at a vertex in this case as well.</p>
<p>Although you cannot easily graph linear programs with more than two variables, the same intuition holds. If you have three variables, then each constraint corresponds to a half-space in three-dimensional space. The intersection of these half-spaces <a id="p857"/>forms the feasible region. The set of points for which the objective function obtains a given value <em>z</em> is now a plane (assuming no degenerate conditions). If all coefficients of the objective function are nonnegative, and if the origin is a feasible solution to the linear program, then as you move this plane away from the origin, in a direction normal to the objective function, you find points of increasing objective value. (If the origin is not feasible or if some coefficients in the objective function are negative, the intuitive picture becomes slightly more complicated.) As in two dimensions, because the feasible region is convex, the set of points that achieve the optimal objective value must include a vertex of the feasible region. Similarly, if you have <em>n</em> variables, each constraint defines a half-space in <em>n</em>-dimensional space. We call the feasible region formed by the intersection of these half-spaces a <strong><em><span class="blue1">simplex</span></em></strong>. The objective function is now a hyperplane and, because of convexity, an optimal solution still occurs at a vertex of the simplex. Any algorithm for linear programming must also identify linear programs that have no solutions, as well as linear programs that have no finite optimal solution.</p>
<p>The <strong><em><span class="blue1">simplex algorithm</span></em></strong> takes as input a linear program and returns an optimal solution. It starts at some vertex of the simplex and performs a sequence of iterations. In each iteration, it moves along an edge of the simplex from a current vertex to a neighboring vertex whose objective value is no smaller than that of the current vertex (and usually is larger.) The simplex algorithm terminates when it reaches a local maximum, which is a vertex from which all neighboring vertices have a smaller objective value. Because the feasible region is convex and the objective function is linear, this local optimum is actually a global optimum. In <a href="chapter029.xhtml#Sec_29.3">Section 29.3</a>, we’ll see an important concept called “duality,” which we’ll use to prove that the solution returned by the simplex algorithm is indeed optimal.</p>
<p>The simplex algorithm, when implemented carefully, often solves general linear programs quickly in practice. With some carefully contrived inputs, however, the simplex algorithm can require exponential time. The first polynomial-time algorithm for linear programming was the <strong><em><span class="blue1">ellipsoid algorithm</span></em></strong>, which runs slowly in practice. A second class of polynomial-time algorithms are known as <strong><em><span class="blue1">interior-point methods</span></em></strong>. In contrast to the simplex algorithm, which moves along the exterior of the feasible region and maintains a feasible solution that is a vertex of the simplex at each iteration, these algorithms move through the interior of the feasible region. The intermediate solutions, while feasible, are not necessarily vertices of the simplex, but the final solution is a vertex. For large inputs, interior-point algorithms can run as fast as, and sometimes faster than, the simplex algorithm. The chapter notes point you to more information about these algorithms.</p>
<p>If you add to a linear program the additional requirement that all variables take on integer values, you have an <strong><em><span class="blue1">integer linear program</span></em></strong>. Exercise 34.5-3 on page 1098 asks you to show that just finding a feasible solution to this problem is NP-hard. Since no polynomial-time algorithms are known for any NP-hard problems, <a id="p858"/>there is no known polynomial-time algorithm for integer linear programming. In contrast, a general linear-programming problem can be solved in polynomial time.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>29.1-1</em></strong></p>
<p class="noindent">Consider the linear program</p>
<p class="eql">minimize −2<em>x</em><sub>1</sub> + 3<em>x</em><sub>2</sub></p>
<p class="noindent">subject to</p>
<table class="table2-d">
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">7</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub> − 2<em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="noindent">4</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub></p></td>
<td class="td2"><p class="center">≥</p></td>
<td class="td2"><p class="noindent">0.</p></td>
</tr>
</table>
<p class="noindent">Give three feasible solutions to this linear program. What is the objective value of each one?</p>
<p class="level3"><strong><em>29.1-2</em></strong></p>
<p class="noindent">Consider the following linear program, which has a nonpositivity constraint:</p>
<p class="eql">minimize 2<em>x</em><sub>1</sub> + 7<em>x</em><sub>2</sub> + <em>x</em><sub>3</sub></p>
<p class="noindent">subject to</p>
<table class="table2-d">
<tr>
<td class="td2"><p class="noindent"><em>x</em><sub>1</sub></p></td>
<td class="td2"><p class="right">− <em>x</em><sub>3</sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">7</p></td>
</tr>
<tr>
<td class="td2"><p class="noindent">3<em>x</em><sub>1</sub> + <em>x</em><sub>2</sub></p></td>
<td class="td2"/>
<td class="td2"><p class="center">≥</p></td>
<td class="td2"><p class="right">24</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="right"><em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≥</p></td>
<td class="td2"><p class="right">0</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="right"><em>x</em><sub>3</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="right">0.</p></td>
</tr>
</table>
<p class="noindent">Give three feasible solutions to this linear program. What is the objective value of each one?</p>
<p class="level3"><strong><em>29.1-3</em></strong></p>
<p class="noindent">Show that the following linear program is infeasible:</p>
<p class="eql">maximize 3<em>x</em><sub>1</sub> − 2<em>x</em><sub>2</sub></p>
<p class="noindent">subject to</p>
<table class="table2-d">
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="right">2</p></td>
</tr>
<tr>
<td class="td2"><p class="right">−2<em>x</em><sub>1</sub> − 2<em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="right">−10</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≥</p></td>
<td class="td2"><p class="right">0.</p></td>
</tr>
</table>
<a id="p859"/>
<p class="level3"><strong><em>29.1-4</em></strong></p>
<p class="noindent">Show that the following linear program is unbounded:</p>
<p class="eql">maximize <em>x</em><sub>1</sub> − <em>x</em><sub>2</sub></p>
<p class="noindent">subject to</p>
<table class="table2-d">
<tr>
<td class="td2"><p class="right">−2<em>x</em><sub>1</sub> + <em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="right">−1</p></td>
</tr>
<tr>
<td class="td2"><p class="right">−<em>x</em><sub>1</sub> − 2<em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2"><p class="right">−2</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></p></td>
<td class="td2"><p class="center">≥</p></td>
<td class="td2"><p class="right">0.</p></td>
</tr>
</table>
<p class="level3"><strong><em>29.1-5</em></strong></p>
<p class="noindent">Give an example of a linear program for which the feasible region is not bounded, but the optimal objective value is finite.</p>
<p class="level3"><strong><em>29.1-6</em></strong></p>
<p class="noindent">Sometimes, in a linear program, you need to convert constraints from one form to another.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Show how to convert an equality constraint into an equivalent set of inequalities. That is, given a constraint <img alt="art" src="images/Art_P992.jpg"/>, give a set of inequalities that will be satisfied if and only if <img alt="art" src="images/Art_P993.jpg"/>,</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Show how to convert an inequality constraint <img alt="art" src="images/Art_P994.jpg"/> into an equality constraint and a nonnegativity constraint. You will need to introduce an additional variable <em>s</em>, and use the constraint that <em>s</em> ≥ 0.</p>
<p class="level3"><strong><em>29.1-7</em></strong></p>
<p class="noindent">Explain how to convert a minimization linear program to an equivalent maximization linear program, and argue that your new linear program is equivalent to the original one.</p>
<p class="level3"><strong><em>29.1-8</em></strong></p>
<p class="noindent">In the political problem at the beginning of this chapter, there are feasible solutions that correspond to winning more voters than there actually are in the district. For example, you can set <em>x</em><sub>2</sub> to 200, <em>x</em><sub>3</sub> to 200, and <em>x</em><sub>1</sub> = <em>x</em><sub>4</sub> = 0. That solution is feasible, yet it seems to say that you will win 400,000 suburban voters, even though there are only 200,000 actual suburban voters. What constraints can you add to the linear program to ensure that you never seem to win more voters than there actually are? Even if you don’t add these constraints, argue that the optimal solution to this linear program can never win more voters than there actually are in the district.</p>
<a id="p860"/>
</section>
<p class="line1"/>
<section title="29.2 Formulating problems as linear programs">
<a id="Sec_29.2"/>
<p class="level1" id="h1-168"><a href="toc.xhtml#Rh1-168"><strong>29.2    Formulating problems as linear programs</strong></a></p>
<p class="noindent">Linear programming has many applications. Any textbook on operations research is filled with examples of linear programming, and linear programming has become a standard tool taught to students in most business schools. The election scenario is one typical example. Here are two more examples:</p>
<ul class="ulnoindent" epub:type="list">
<li>An airline wishes to schedule its flight crews. The Federal Aviation Administration imposes several constraints, such as limiting the number of consecutive hours that each crew member can work and insisting that a particular crew work only on one model of aircraft during each month. The airline wants to schedule crews on all of its flights using as few crew members as possible.</li>
<li class="litop">An oil company wants to decide where to drill for oil. Siting a drill at a particular location has an associated cost and, based on geological surveys, an expected payoff of some number of barrels of oil. The company has a limited budget for locating new drills and wants to maximize the amount of oil it expects to find, given this budget.</li></ul>
<p>Linear programs also model and solve graph and combinatorial problems, such as those appearing in this book. We have already seen a special case of linear programming used to solve systems of difference constraints in <a href="chapter022.xhtml#Sec_22.4">Section 22.4</a>. In this section, we’ll study how to formulate several graph and network-flow problems as linear programs. <a href="chapter035.xhtml#Sec_35.4">Section 35.4</a> uses linear programming as a tool to find an approximate solution to another graph problem.</p>
<p>Perhaps the most important aspect of linear programming is to be able to recognize when you can formulate a problem as a linear program. Once you cast a problem as a polynomial-sized linear program, you can solve it in polynomial time by the ellipsoid algorithm or interior-point methods. Several linear-programming software packages can solve problems efficiently, so that once the problem is in the form of a linear program, such a package can solve it.</p>
<p>We’ll look at several concrete examples of linear-programming problems. We start with two problems that we have already studied: the single-source shortest-paths problem from <a href="chapter022.xhtml">Chapter 22</a> and the maximum-flow problem from <a href="chapter024.xhtml">Chapter 24</a>. We then describe the minimum-cost-flow problem. (Although the minimum-cost-flow problem has a polynomial-time algorithm that is not based on linear programming, we won’t describe the algorithm.) Finally, we describe the multicommodity-flow problem, for which the only known polynomial-time algorithm is based on linear programming.</p>
<p>When we solved graph problems in <a href="part006.xhtml">Part VI</a>, we used attribute notation, such as <em>v</em>.<em>d</em> and (<em>u</em>, <em>v</em>).<em>f</em>. Linear programs typically use subscripted variables rather than <a id="p861"/>objects with attached attributes, however. Therefore, when we express variables in linear programs, we indicate vertices and edges through subscripts. For example, we denote the shortest-path weight for vertex <em>v</em> not by <em>v</em>.<em>d</em> but by <em>d<sub>v</sub></em>, and we denote the flow from vertex <em>u</em> to vertex <em>v</em> not by (<em>u</em>, <em>v</em>).<em>f</em> but by <em>f<sub>uv</sub></em>. For quantities that are given as inputs to problems, such as edge weights or capacities, we continue to use notations such as <em>w</em>(<em>u</em>, <em>v</em>) and <em>c</em>(<em>u</em>, <em>v</em>).</p>
<p class="level4"><strong>Shortest paths</strong></p>
<p class="noindent">We can formulate the single-source shortest-paths problem as a linear program. We’ll focus on how to formulate the single-pair shortest-path problem, leaving the extension to the more general single-source shortest-paths problem as Exercise 29.2-2.</p>
<p>In the single-pair shortest-path problem, the input is a weighted, directed graph <em>G</em> = (<em>V</em>, <em>E</em>), with weight function <em>w</em> : <em>E</em> → <span class="double"><span class="font1">ℝ</span></span> mapping edges to real-valued weights, a source vertex <em>s</em>, and destination vertex <em>t</em>. The goal is to compute the value <em>d<sub>t</sub></em>, which is the weight of a shortest path from <em>s</em> to <em>t</em>. To express this problem as a linear program, you need to determine a set of variables and constraints that define when you have a shortest path from <em>s</em> to <em>t</em>. The triangle inequality (Lemma 22.10 on page 633) gives <em>d<sub>v</sub></em> ≤ <em>d<sub>u</sub></em> + <em>w</em>(<em>u</em>, <em>v</em>) for each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em>. The source vertex initially receives a value <em>d<sub>s</sub></em> = 0, which never changes. Thus the following linear program expresses the shortest-path weight from <em>s</em> to <em>t</em>:</p>
<p class="eqr"><img alt="art" src="images/Art_P995.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P996.jpg"/></p>
<p class="noindent">You might be surprised that this linear program maximizes an objective function when it is supposed to compute shortest paths. Minimizing the objective function would be a mistake, because when all the edge weights are nonnegative, setting <em><span class="overline">d</span><sub>v</sub></em> = 0 for all <em>v</em> ∈ <em>V</em> (recall that a bar over a variable name denotes a specific setting of the variable’s value) would yield an optimal solution to the linear program without solving the shortest-paths problem. Maximizing is the right thing to do because an optimal solution to the shortest-paths problem sets each <em><span class="overline">d</span><sub>v</sub></em> to min {<em><span class="overline">d</span><sub>u</sub></em> + <em>w</em>(<em>u</em>, <em>v</em>) : <em>u</em> ∈ <em>V</em> and (<em>u</em>, <em>v</em>) ∈ <em>E</em>}, so that <em><span class="overline">d</span><sub>v</sub></em> is the largest value that is less than or equal to all of the values in the set {<em><span class="overline">d</span><sub>u</sub></em> + <em>w</em>(<em>u</em>, <em>v</em>). Therefore, it makes sense to maximize <em>d<sub>v</sub></em> for all vertices <em>v</em> on a shortest path from <em>s</em> to <em>t</em> subject to these constraints, and maximizing <em>d<sub>t</sub></em> achieves this goal.</p>
<p>This linear program has |<em>V</em>| variables <em>d<sub>v</sub></em>, one for each vertex <em>v</em> ∈ <em>V</em>. It also has |<em>E</em>| + 1 constraints: one for each edge, plus the additional constraint that the source vertex’s shortest-path weight always has the value 0.</p>
<a id="p862"/>
<p class="level4"><strong>Maximum flow</strong></p>
<p class="noindent">Next, let’s express the maximum-flow problem as a linear program. Recall that the input is a directed graph <em>G</em> = (<em>V</em>, <em>E</em>) in which each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em> has a nonnegative capacity <em>c</em>(<em>u</em>, <em>v</em>) ≥ 0, and two distinguished vertices: a source <em>s</em> and a sink <em>t</em>. As defined in <a href="chapter024.xhtml#Sec_24.1">Section 24.1</a>, a flow is a nonnegative real-valued function <em>f</em> : <em>V</em> × <em>V</em> → <span class="double"><span class="font1">ℝ</span></span> that satisfies the capacity constraint and flow conservation. A maximum flow is a flow that satisfies these constraints and maximizes the flow value, which is the total flow coming out of the source minus the total flow into the source. A flow, therefore, satisfies linear constraints, and the value of a flow is a linear function. Recalling also that we assume that <em>c</em>(<em>u</em>, <em>v</em>) = 0 if (<em>u</em>, <em>v</em>) ∉ <em>E</em> and that there are no antiparallel edges, the maximum-flow problem can be expressed as a linear program:</p>
<p class="eqr"><img alt="art" src="images/Art_P997.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P998.jpg"/></p>
<p class="noindent">This linear program has |<em>V</em>|<sup>2</sup> variables, corresponding to the flow between each pair of vertices, and it has 2 |<em>V</em>|<sup>2</sup> + |<em>V</em>| − 2 constraints.</p>
<p>It is usually more efficient to solve a smaller-sized linear program. The linear program in (29.25)–(29.28) has, for ease of notation, a flow and capacity of 0 for each pair of vertices <em>u</em>, <em>v</em> with (<em>u</em>, <em>v</em>) ∉ <em>E</em>. It is more efficient to rewrite the linear program so that it has <em>O</em>(<em>V</em> + <em>E</em>) constraints. Exercise 29.2-4 asks you to do so.</p>
<p class="level4"><strong>Minimum-cost flow</strong></p>
<p class="noindent">In this section, we have used linear programming to solve problems for which we already knew efficient algorithms. In fact, an efficient algorithm designed specifically for a problem, such as Dijkstra’s algorithm for the single-source shortest-paths problem, will often be more efficient than linear programming, both in theory and in practice.</p>
<p>The real power of linear programming comes from the ability to solve new problems. Recall the problem faced by the politician in the beginning of this chapter. The problem of obtaining a sufficient number of votes, while not spending too much money, is not solved by any of the algorithms that we have studied in this book, yet it can be solved by linear programming. Books abound with such real-world problems that linear programming can solve. Linear programming is also <a id="p863"/>particularly useful for solving variants of problems for which we may not already know of an efficient algorithm.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_29-3"><img alt="art" src="images/Art_P999.jpg"/></p>
<p class="caption"><strong>Figure 29.3 (a)</strong> An example of a minimum-cost-flow problem. Capacities are denoted by <em>c</em> and costs by <em>a</em>. Vertex <em>s</em> is the source, and vertex <em>t</em> is the sink. The goal is to send 4 units of flow from <em>s</em> to <em>t</em>. <strong>(b)</strong> A solution to the minimum-cost flow problem in which 4 units of flow are sent from <em>s</em> to <em>t</em>. For each edge, the flow and capacity are written as flow/capacity.</p>
</div>
<p>Consider, for example, the following generalization of the maximum-flow problem. Suppose that, in addition to a capacity <em>c</em>(<em>u</em>, <em>v</em>) for each edge (<em>u</em>, <em>v</em>), you are given a real-valued cost <em>a</em>(<em>u</em>, <em>v</em>). As in the maximum-flow problem, assume that <em>c</em>(<em>u</em>, <em>v</em>) = 0 if (<em>u</em>, <em>v</em>) ∉ <em>E</em> and that there are no antiparallel edges. If you send <em>f<sub>uv</sub></em> units of flow over edge (<em>u</em>, <em>v</em>), you incur a cost of <em>a</em>(<em>u</em>, <em>v</em>) · <em>f<sub>uv</sub></em>. You are also given a flow demand <em>d</em>. You wish to send <em>d</em> units of flow from <em>s</em> to <em>t</em> while minimizing the total cost ∑<sub>(<em>u</em>,<em>v</em>)∈<em>E</em></sub> <em>a</em>(<em>u</em>, <em>v</em>) · <em>f<sub>uv</sub></em> incurred by the flow. This problem is known as the <strong><em><span class="blue1">minimum-cost-flow problem</span></em></strong>.</p>
<p><a href="chapter029.xhtml#Fig_29-3">Figure 29.3(a)</a> shows an example of the minimum-cost-flow problem, with a goal of sending 4 units of flow from <em>s</em> to <em>t</em> while incurring the minimum total cost. Any particular legal flow, that is, a function <em>f</em> satisfying constraints (29.26)–(29.28), incurs a total cost of ∑<sub>(<em>u</em>,<em>v</em>)∈<em>E</em></sub> <em>a</em>(<em>u</em>, <em>v</em>) · <em>f<sub>uv</sub></em>. What is the particular 4-unit flow that minimizes this cost? <a href="chapter029.xhtml#Fig_29-3">Figure 29.3(b)</a> shows an optimal solution, with total cost ∑<sub>(<em>u</em>,<em>v</em>)∈<em>E</em></sub> <em>a</em>(<em>u</em>, <em>v</em>) · <em>f<sub>uv</sub></em> = (2 · 2) + (5 · 2) + (3 · 1) + (7 · 1) + (1 · 3) = 27.</p>
<p>There are polynomial-time algorithms specifically designed for the minimum-cost-flow problem, but they are beyond the scope of this book. The minimum-cost-flow problem can be expressed as a linear program, however. The linear program looks similar to the one for the maximum-flow problem with the additional constraint that the value of the flow must be exactly <em>d</em> units, and with the new objective function of minimizing the cost:</p>
<a id="p864"/>
<p class="eqr"><img alt="art" src="images/Art_P1000.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1001.jpg"/></p>
<p class="level4"><strong>Multicommodity flow</strong></p>
<p class="noindent">As a final example, let’s consider another flow problem. Suppose that the Lucky Puck company from <a href="chapter024.xhtml#Sec_24.1">Section 24.1</a> decides to diversify its product line and ship not only hockey pucks, but also hockey sticks and hockey helmets. Each piece of equipment is manufactured in its own factory, has its own warehouse, and must be shipped, each day, from factory to warehouse. The sticks are manufactured in Vancouver and are needed in Saskatoon, and the helmets are manufactured in Edmonton and must be shipped to Regina. The capacity of the shipping network does not change, however, and the different items, or <strong><em><span class="blue1">commodities</span></em></strong>, must share the same network.</p>
<p>This example is an instance of a <strong><em><span class="blue1">multicommodity-flow problem</span></em></strong>. The input to this problem is once again a directed graph <em>G</em> = (<em>V</em>, <em>E</em>) in which each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em> has a nonnegative capacity <em>c</em>(<em>u</em>, <em>v</em>) ≥ 0. As in the maximum-flow problem, implicitly assume that <em>c</em>(<em>u</em>, <em>v</em>) = 0 for (<em>u</em>, <em>v</em>) ∉ <em>E</em> and that the graph has no antiparallel edges. In addition, there are <em>k</em> different commodities, <em>K</em><sub>1</sub>, <em>K</em><sub>2</sub>, … , <em>K<sub>k</sub></em>, with commodity <em>i</em> specified by the triple <em>K<sub>i</sub></em> = (<em>s<sub>i</sub></em>, <em>t<sub>i</sub></em>, <em>d<sub>i</sub></em>). Here, vertex <em>s<sub>i</sub></em> is the source of commodity <em>i</em>, vertex <em>t<sub>i</sub></em> is the sink of commodity <em>i</em>, and <em>d<sub>i</sub></em> is the demand for commodity <em>i</em>, which is the desired flow value for the commodity from <em>s<sub>i</sub></em> to <em>t<sub>i</sub></em>. We define a flow for commodity <em>i</em>, denoted by <em>f<sub>i</sub></em>, (so that <em>f<sub>iuv</sub></em> is the flow of commodity <em>i</em> from vertex <em>u</em> to vertex <em>v</em>) to be a real-valued function that satisfies the flow-conservation and capacity constraints. We define <em>f<sub>uv</sub></em>, the <strong><em><span class="blue1">aggregate flow</span></em></strong>, to be the sum of the various commodity flows, so that <img alt="art" src="images/Art_P1002.jpg"/>. The aggregate flow on edge (<em>u</em>, <em>v</em>) must be no more than the capacity of edge (<em>u</em>, <em>v</em>). This problem has no objective function: the question is to determine whether such a flow exists. Thus the linear program for this problem has a “null” objective function:</p>
<a id="p865"/>
<p class="eql"><img alt="art" src="images/Art_P1003.jpg"/></p>
<p class="noindent">The only known polynomial-time algorithm for this problem expresses it as a linear program and then solves it with a polynomial-time linear-programming algorithm.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>29.2-1</em></strong></p>
<p class="noindent">Write out explicitly the linear program corresponding to finding the shortest path from vertex <em>s</em> to vertex <em>x</em> in <a href="chapter022.xhtml#Fig_22-2">Figure 22.2(a)</a> on page 609.</p>
<p class="level3"><strong><em>29.2-2</em></strong></p>
<p class="noindent">Given a graph <em>G</em>, write a linear program for the single-source shortest-paths problem. The solution should have the property that <em>d<sub>v</sub></em> is the shortest-path weight from the source vertex <em>s</em> to <em>v</em> for each vertex <em>v</em> ∈ <em>V</em>.</p>
<p class="level3"><strong><em>29.2-3</em></strong></p>
<p class="noindent">Write out explicitly the linear program corresponding to finding the maximum flow in <a href="chapter024.xhtml#Fig_24-1">Figure 24.1(a)</a>.</p>
<p class="level3"><strong><em>29.2-4</em></strong></p>
<p class="noindent">Rewrite the linear program for maximum flow (29.25)–(29.28) so that it uses only <em>O</em>(<em>V</em> + <em>E</em>) constraints.</p>
<p class="level3"><strong><em>29.2-5</em></strong></p>
<p class="noindent">Write a linear program that, given a bipartite graph <em>G</em> = (<em>V</em>, <em>E</em>), solves the maximum-bipartite-matching problem.</p>
<p class="level3"><strong><em>29.2-6</em></strong></p>
<p class="noindent">There can be more than one way to model a particular problem as a linear program. This exercise gives an alternative formulation for the maximum-flow problem. Let <em><span class="font1">P</span></em> = {<em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P<sub>p</sub></em>} be the set of <em>all</em> possible directed simple paths from source <em>s</em> <a id="p866"/> to sink <em>t</em>. Using decision variables <em>x</em><sub>1</sub>, … , <em>x<sub>p</sub></em>, where <em>x<sub>i</sub></em> is the amount of flow on path <em>i</em>, formulate a linear program for the maximum-flow problem. What is an upper bound on <em>p</em>, the number of directed simple paths from <em>s</em> to <em>t</em>?</p>
<p class="level3"><strong><em>29.2-7</em></strong></p>
<p class="noindent">In the <strong><em><span class="blue1">minimum-cost multicommodity-flow problem</span></em></strong>, the input is a directed graph <em>G</em> = (<em>V</em>, <em>E</em>) in which each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em> has a nonnegative capacity <em>c</em>(<em>u</em>, <em>v</em>) ≥ 0 and a cost <em>a</em>(<em>u</em>, <em>v</em>). As in the multicommodity-flow problem, there are <em>k</em> different commodities, <em>K</em><sub>1</sub>, <em>K</em><sub>2</sub>, … , <em>K<sub>k</sub></em>, with commodity <em>i</em> specified by the triple <em>K<sub>i</sub></em> = (<em>s<sub>i</sub></em>, <em>t<sub>i</sub></em>, <em>d<sub>i</sub></em>). We define the flow <em>f<sub>i</sub></em> for commodity <em>i</em> and the aggregate flow <em>f<sub>uv</sub></em> on edge (<em>u</em>, <em>v</em>) as in the multicommodity-flow problem. A feasible flow is one in which the aggregate flow on each edge (<em>u</em>, <em>v</em>) is no more than the capacity of edge (<em>u</em>, <em>v</em>). The cost of a flow is ∑<sub><em>u</em>,<em>v</em>∈<em>E</em></sub> <em>a</em>(<em>u</em>, <em>v</em>) · <em>f<sub>uv</sub></em>, and the goal is to find the feasible flow of minimum cost. Express this problem as a linear program.</p>
</section>
<p class="line1"/>
<section title="29.3 Duality">
<a id="Sec_29.3"/>
<p class="level1" id="h1-169"><a href="toc.xhtml#Rh1-169"><strong>29.3    Duality</strong></a></p>
<p class="noindent">We will now introduce a powerful concept called <strong><em><span class="blue1">linear-programming duality</span></em></strong>. In general, given a maximization problem, duality allows you to formulate a related minimization problem that has the same objective value. The idea of duality is actually more general than linear programming, but we restrict our attention to linear programming in this section.</p>
<p>Duality enables us to prove that a solution is indeed optimal. We saw an example of duality in <a href="chapter024.xhtml">Chapter 24</a> with Theorem 24.6, the max-flow min-cut theorem. Suppose that, given an instance of a maximum-flow problem, you find a flow <em>f</em> with value |<em>f</em>|. How do you know whether <em>f</em> is a maximum flow? By the max-flow min-cut theorem, if you can find a cut whose value is also |<em>f</em>|, then you have verified that <em>f</em> is indeed a maximum flow. This relationship provides an example of duality: given a maximization problem, define a related minimization problem such that the two problems have the same optimal objective values.</p>
<p>Given a linear program in standard form in which the objective is to maximize, let’s see how to formulate a <strong><em><span class="blue1">dual</span></em></strong> linear program in which the objective is to minimize and whose optimal value is identical to that of the original linear program. When referring to dual linear programs, we call the original linear program the <strong><em><span class="blue1">primal</span></em></strong>.</p>
<p>Given the primal linear program</p>
<a id="p867"/>
<p class="eqr"><img alt="art" src="images/Art_P1004.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1005.jpg"/></p>
<p class="noindent">its dual is</p>
<p class="eqr"><img alt="art" src="images/Art_P1006.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1007.jpg"/></p>
<p>Mechanically, to form the dual, change the maximization to a minimization, exchange the roles of coefficients on the right-hand sides and in the objective function, and replace each ≤ by ≥. Each of the <em>m</em> constraints in the primal corresponds to a variable <em>y<sub>i</sub></em> in the dual. Likewise, each of the <em>n</em> constraints in the dual corresponds to a variable <em>x<sub>j</sub></em> in the primal. For example, consider the following primal linear program:</p>
<p class="eqr"><img alt="art" src="images/Art_P1008.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1009.jpg"/></p>
<p class="noindent">Its dual is</p>
<p class="eqr"><img alt="art" src="images/Art_P1010.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1011.jpg"/></p>
<p>Although forming the dual can be considered a mechanical operation, there is an intuitive explanation. Consider the primal maximization problem (29.37)–(29.41). Each constraint gives an upper bound on the objective function. In addition, if you <a id="p868"/>take one or more constraints and add together nonnegative multiples of them, you get a valid constraint. For example, you can add constraints (29.38) and (29.39) to obtain the constraint 3<em>x</em><sub>1</sub> + 3<em>x</em><sub>2</sub> + 8<em>x</em><sub>3</sub> ≤ 54. Any feasible solution to the primal must satisfy this new constraint, but there is something else interesting about it. Comparing this new constraint to the objective function (29.37), you can see that for each variable, the corresponding coefficient is at least as large as the coefficient in the objective function. Thus, since the variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub> and <em>x</em><sub>3</sub> are nonnegative, we have that</p>
<p class="eql">3<em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> + 4<em>x</em><sub>3</sub> ≤ 3<em>x</em><sub>1</sub> + 3<em>x</em><sub>2</sub> + 8<em>x</em><sub>3</sub> ≤ 54,</p>
<p class="noindent">and so the solution value to the primal is at most 54. In other words, adding these two constraints together has generated an upper bound on the objective value.</p>
<p>In general, for any nonnegative multipliers <em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, and <em>y</em><sub>3</sub>, you can generate a constraint</p>
<p class="eql"><em>y</em><sub>1</sub>(<em>x</em><sub>1</sub>+<em>x</em><sub>2</sub>+3<em>x</em><sub>3</sub>)+<em>y</em><sub>2</sub>(2<em>x</em><sub>1</sub>+2<em>x</em><sub>2</sub>+5<em>x</em><sub>3</sub>)+<em>y</em><sub>3</sub>(4<em>x</em><sub>1</sub>+<em>x</em><sub>2</sub>+2<em>x</em><sub>3</sub>) ≤ 30<em>y</em><sub>1</sub>+24<em>y</em><sub>2</sub>+36<em>y</em><sub>3</sub></p>
<p class="noindent">from the primal constraints or, by distributing and regrouping,</p>
<p class="eql">(<em>y</em><sub>1</sub>+2<em>y</em><sub>2</sub>+4<em>y</em><sub>3</sub>)<em>x</em><sub>1</sub>+(<em>y</em><sub>1</sub>+2<em>y</em><sub>2</sub>+<em>y</em><sub>3</sub>)<em>x</em><sub>2</sub>+(3<em>y</em><sub>1</sub>+5<em>y</em><sub>2</sub>+2<em>y</em><sub>3</sub>)<em>x</em><sub>3</sub> ≤ 30<em>y</em><sub>1</sub>+24<em>y</em><sub>2</sub>+36<em>y</em><sub>3</sub>.</p>
<p class="noindent">Now, as long as this constraint has coefficients of <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, and <em>x</em><sub>3</sub> that are at least their objective-function coefficients, it is a valid upper bound. That is, as long as</p>
<table class="table2b">
<tr>
<td class="td2"><em>y</em><sub>1</sub> + 2<em>y</em><sub>2</sub> + 4<em>y</em><sub>3</sub></td>
<td class="td2">≥</td>
<td class="td2">3,</td>
</tr>
<tr>
<td class="td2"><em>y</em><sub>1</sub> + 2<em>y</em><sub>2</sub> + <em>y</em><sub>3</sub></td>
<td class="td2">≥</td>
<td class="td2">1,</td>
</tr>
<tr>
<td class="td2">3<em>y</em><sub>1</sub> + 5<em>y</em><sub>2</sub> + 2<em>y</em><sub>3</sub></td>
<td class="td2">≥</td>
<td class="td2">4,</td>
</tr>
</table>
<p class="noindent">you have a valid upper bound of 30<em>y</em><sub>1</sub>+24<em>y</em><sub>2</sub>+36<em>y</em><sub>3</sub>. The multipliers <em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, and <em>y</em><sub>3</sub> must be nonnegative, because otherwise you cannot combine the inequalities. Of course, you would like the upper bound to be as small as possible, and so you want to choose <em>y</em> to minimize 30<em>y</em><sub>1</sub> + 24<em>y</em><sub>2</sub> + 36<em>y</em><sub>3</sub>. Observe that we have just described the dual linear program as the problem of finding the smallest possible upper bound on the primal.</p>
<p>We’ll formalize this idea and show in Theorem 29.4 that, if the linear program and its dual are feasible and bounded, then the optimal value of the dual linear program is always equal to the optimal value of the primal linear program. We begin by demonstrating <strong><em><span class="blue1">weak duality</span></em></strong>, which states that any feasible solution to the primal linear program has a value no greater than that of any feasible solution to the dual linear program.</p>
<p class="lem"><strong><em>Lemma 29.1 (Weak linear-programming duality)</em></strong></p>
<p class="noindent">Let <em><span class="overline">x</span></em> be any feasible solution to the primal linear program in (29.31)–(29.33), and let <em><span class="font1">ӯ</span></em> be any feasible solution to its dual linear program in (29.34)–(29.36). Then</p>
<a id="p869"/>
<p class="eql"><img alt="art" src="images/Art_P1012.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   We have</p>
<p class="eql"><img alt="art" src="images/Art_P1013.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="cor"><strong><em>Corollary 29.2</em></strong></p>
<p class="noindent">Let <em><span class="overline">x</span></em> be a feasible solution to the primal linear program in (29.31)–(29.33), and let <em><span class="font1">ӯ</span></em> be a feasible solution to its dual linear program in (29.34)–(29.36). If</p>
<p class="eql"><img alt="art" src="images/Art_P1014.jpg"/></p>
<p class="noindent">then <em><span class="overline">x</span></em> and <em><span class="font1">ӯ</span></em> are optimal solutions to the primal and dual linear programs, respectively.</p>
<p class="prof"><strong><em>Proof</em></strong>   By Lemma 29.1, the objective value of a feasible solution to the primal cannot exceed that of a feasible solution to the dual. The primal linear program is a maximization problem and the dual is a minimization problem. Thus, if feasible solutions <em><span class="overline">x</span></em> and <em><span class="font1">ӯ</span></em> have the same objective value, neither can be improved.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We now show that, at optimality, the primal and dual objective values are indeed equal. To prove linear programming duality, we will require one lemma from linear algebra, known as Farkas’s lemma, the proof of which Problem 29-4 asks you to provide. Farkas’s lemma can take several forms, each of which is about when a set of linear equalities has a solution. In stating the lemma, we use <em>m</em> + 1 as a dimension because it matches our use below.</p>
<p class="lem"><strong><em>Lemma 29.3 (Farkas’s lemma)</em></strong></p>
<p class="noindent">Given <em>M</em> ∈ <span class="double"><span class="font1">ℝ</span></span><sup>(<em>m</em>+1)×<em>n</em></sup> and <em>g</em> ∈ <span class="double"><span class="font1">ℝ</span></span><sup><em>m</em>+1</sup>, exactly one of the following statements is true:</p>
<ol class="olnoindent" epub:type="list">
<li>There exists <em>v</em> ∈ <span class="double"><span class="font1">ℝ</span></span><em><sup>n</sup></em> such that <em>Mv</em> ≤ <em>g</em>,</li>
<li class="litop">There exists <em>w</em> ∈ <span class="double"><span class="font1">ℝ</span></span><sup><em>m</em>+1</sup> such that <em>w</em> ≥ 0, <em>w</em><sup>T</sup><em>M</em> = 0 (an <em>n</em>-vector of all zeros), and <em>w</em><sup>T</sup>g &lt; 0.</li></ol>
<p class="right"><span class="font1">▪</span></p>
<a id="p870"/>
<p class="theo"><strong><em>Theorem 29.4 (Linear-programming duality)</em></strong></p>
<p class="noindent">Given the primal linear program in (29.31)–(29.33) and its corresponding dual in (29.34)–(29.36), if both are feasible and bounded, then for optimal solutions <em>x</em>* and <em>y</em>*, we have <em>c</em><sup>T</sup><em>x</em>* = <em>b</em><sup>T</sup><em>y</em>*.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let <em>μ</em> = <em>b</em><sup>T</sup><em>y</em>* be the optimal value of the dual linear program given in (29.34)–(29.36). Consider an augmented set of primal constraints in which we add a constraint to (29.31)–(29.33) that the objective value is at least <em>μ</em>. We write out this <strong><em><span class="blue1">augmented primal</span></em></strong> as</p>
<p class="eqr"><img alt="art" src="images/Art_P1015.jpg"/></p>
<p class="noindent">We can multiply (29.48) through by −1 and rewrite (29.47)–(29.48) as</p>
<p class="eqr"><img alt="art" src="images/Art_P1016.jpg"/></p>
<p class="noindent">Here, <img alt="art" src="images/Art_P1017.jpg"/> denotes an (<em>m</em>+1)×<em>n</em> matrix, <em>x</em> is an <em>n</em>-vector, and <img alt="art" src="images/Art_P1018.jpg"/> denotes an (<em>m</em> + 1)-vector.</p>
<p>We claim that if there is a feasible solution <em><span class="overline">x</span></em> to the augmented primal, then the theorem is proved. To establish this claim, observe that <em><span class="overline">x</span></em> is also a feasible solution to the original primal and that it has objective value at least <em>μ</em>. We can then apply Lemma 29.1, which states that the objective value of the primal is at most <em>μ</em>, to complete the proof of the theorem.</p>
<p>It therefore remains to show that the augmented primal has a feasible solution. Suppose, for the purpose of contradiction, that the augmented primal is infeasible, which means that there is no <em>v</em> ∈ <span class="double"><span class="font1">ℝ</span></span><em><sup>n</sup></em> such that <img alt="art" src="images/Art_P1019.jpg"/>. We can apply Farkas’s lemma, Lemma 29.3, to inequalty (29.49) with</p>
<p class="eql"><img alt="art" src="images/Art_P1020.jpg"/></p>
<p class="noindent">Because the augmented primal is infeasible, condition 1 of Farkas’s lemma does not hold. Therefore, condition 2 must apply, so that there must exist a <em>w</em> ∈ <span class="double"><span class="font1">ℝ</span></span><sup><em>m</em>+1</sup> such that <em>w</em> ≥ 0, <em>w</em><sup>T</sup><em>M</em> = 0, and <em>w</em><sup>T</sup><em>g</em> &lt; 0. Let’s write <em>w</em> as <img alt="art" src="images/Art_P1021.jpg"/> for some <em><span class="font1">ӯ</span></em> ∈ <span class="double"><span class="font1">ℝ</span></span><em><sup>m</sup></em> and <em>λ</em> ∈ <span class="double"><span class="font1">ℝ</span></span>, where <em><span class="font1">ӯ</span></em> ≥ 0 and <em>λ</em> ≥ 0. Substituting for <em>w</em>, <em>M</em>, and <em>g</em> in condition 2 gives</p>
<p class="eql"><img alt="art" src="images/Art_P1022.jpg"/></p>
<a id="p871"/>
<p class="noindent">Unpacking the matrix notation gives</p>
<p class="eqr"><img alt="art" src="images/Art_P1023.jpg"/></p>
<p class="noindent">We now show that the requirements in (29.50) contradict the assumption that <em>μ</em> is the optimal solution value for the dual linear program. We consider two cases.</p>
<p>The first case is when <em>λ</em> = 0. In this case, (29.50) simplifies to</p>
<p class="eqr"><img alt="art" src="images/Art_P1024.jpg"/></p>
<p class="noindent">We’ll now construct a dual feasible solution <em>y</em>′ with an objective value smaller than <em>b</em><sup>T</sup><em>y</em>*. Set <em>y</em>′ = <em>y</em>* + <em><span class="font1">ϵ</span> <span class="font1">ӯ</span></em>, for any <em><span class="font1">ϵ</span></em> &gt; 0. Since</p>
<table class="table2b">
<tr>
<td class="td2"><em>y</em>′<sup>T</sup><em>A</em></td>
<td class="td2">=</td>
<td class="td2">(<em>y</em>* + <em><span class="font1">ϵ</span> <span class="font1">ӯ</span></em>)<sup>T</sup><em>A</em></td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>y</em>*<sup>T</sup><em>A</em> + <em><span class="font1">ϵ</span> <span class="font1">ӯ</span></em><sup>T</sup><em>A</em></td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>y</em>*<sup>T</sup><em>A</em></td>
<td class="td2">(by (29.51))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≥</td>
<td class="td2"><em>c</em><sup>T</sup></td>
<td class="td2">(because <em>y</em>* is feasible),</td>
</tr>
</table>
<p class="noindent"><em>y</em>′ is feasible. Now consider the objective value</p>
<table class="table2b">
<tr>
<td class="td2"><em>b</em><sup>T</sup><em>y</em>′</td>
<td class="td2">=</td>
<td class="td2"><em>b</em><sup>T</sup>(<em>y</em>* + <em><span class="font1">ϵ</span> <span class="font1">ӯ</span></em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>b</em><sup>T</sup><em>y</em>* + <em><span class="font1">ϵ</span> b</em><sup>T</sup><em><span class="font1">ӯ</span></em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">&lt;</td>
<td class="td2"><em>b</em><sup>T</sup><em>y</em>*,</td>
</tr>
</table>
<p class="noindent">where the last inequality follows because <em><span class="font1">ϵ</span></em> &gt; 0 and, by (29.51), <em><span class="font1">ӯ</span></em><sup>T</sup><em>b</em> = <em>b</em><sup>T</sup><em><span class="font1">ӯ</span></em> &lt; 0 (since both <em><span class="font1">ӯ</span></em><sup>T</sup><em>b</em> and <em>b</em><sup>T</sup><em><span class="font1">ӯ</span></em> are the inner product of <em>b</em> and <em><span class="font1">ӯ</span></em>), and so their product is negative. Thus we have a feasible dual solution of value less than <em>μ</em>, which contradicts <em>μ</em> being the optimal objective value.</p>
<p>We now consider the second case, where <em>λ</em> &gt; 0. In this case, we can take (29.50) and divide through by <em>λ</em> to obtain</p>
<p class="eqr"><img alt="art" src="images/Art_P1025.jpg"/></p>
<p class="noindent">Now set <em>y</em>′ = <em><span class="font1">ӯ</span></em>/<em>λ</em> in (29.52), giving</p>
<p class="eql"><em>y</em>′<sup>T</sup><em>A</em> = <em>c</em><sup>T</sup> and <em>y</em>′<sup>T</sup><em>b</em> &lt; <em>μ</em>.</p>
<p class="noindent">Thus, <em>y</em>′ is a feasible dual solution with objective value strictly less than <em>μ</em>, a contradiction. We conclude that the augmented primal has a feasible solution, and the theorem is proved.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Fundamental theorem of linear programming</strong></p>
<p class="noindent">We conclude this chapter by stating the fundamental theorem of linear programming, which extends Theorem 29.4 to the cases when the linear program may be either feasible or unbounded. Exercise 29.3-8 asks you to provide the proof.</p>
<a id="p872"/>
<p class="theo"><strong><em>Theorem 29.5 (Fundamental theorem of linear programming)</em></strong></p>
<p class="noindent">Any linear program, given in standard form, either</p>
<ol class="olnoindent" epub:type="list">
<li>has an optimal solution with a finite objective value,</li>
<li class="litop">is infeasible, or</li>
<li class="litop">is unbounded.</li></ol>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>29.3-1</em></strong></p>
<p class="noindent">Formulate the dual of the linear program given in lines (29.6)–(29.10) on page 852.</p>
<p class="level3"><strong><em>29.3-2</em></strong></p>
<p class="noindent">You have a linear program that is not in standard form. You could produce the dual by first converting it to standard form, and then taking the dual. It would be more convenient, however, to produce the dual directly. Explain how to directly take the dual of an arbitrary linear program.</p>
<p class="level3"><strong><em>29.3-3</em></strong></p>
<p class="noindent">Write down the dual of the maximum-flow linear program, as given in lines (29.25)–(29.28) on page 862. Explain how to interpret this formulation as a minimum-cut problem.</p>
<p class="level3"><strong><em>29.3-4</em></strong></p>
<p class="noindent">Write down the dual of the minimum-cost-flow linear program, as given in lines (29.29)–(29.30) on page 864. Explain how to interpret this problem in terms of graphs and flows.</p>
<p class="level3"><strong><em>29.3-5</em></strong></p>
<p class="noindent">Show that the dual of the dual of a linear program is the primal linear program.</p>
<p class="level3"><strong><em>29.3-6</em></strong></p>
<p class="noindent">Which result from <a href="chapter024.xhtml">Chapter 24</a> can be interpreted as weak duality for the maximum-flow problem?</p>
<p class="level3"><strong><em>29.3-7</em></strong></p>
<p class="noindent">Consider the following 1-variable primal linear program:</p>
<table class="table2b">
<tr>
<td class="td2">maximize</td>
<td class="td2"><em>tx</em></td>
<td class="td2"/>
<td class="td2"/>
</tr>
<tr>
<td class="td2">subject to</td>
<td class="td2"/>
<td class="td2"/>
<td class="td2"/>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><em>rx</em></td>
<td class="td2">≤</td>
<td class="td2"><em>s</em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><em>x</em></td>
<td class="td2">≥</td>
<td class="td2">0,</td>
</tr>
</table>
<a id="p873"/>
<p class="noindent">where <em>r</em>, <em>s</em>, and <em>t</em> are arbitrary real numbers. State for which values of <em>r</em>, <em>s</em>, and <em>t</em> you can assert that</p>
<ol class="olnoindent" epub:type="list">
<li>Both the primal linear program and its dual have optimal solutions with finite objective values.</li>
<li class="litop">The primal is feasible, but the dual is infeasible.</li>
<li class="litop">The dual is feasible, but the primal is infeasible.</li>
<li class="litop">Neither the primal nor the dual is feasible.</li></ol>
<p class="level3"><strong><em>29.3-8</em></strong></p>
<p class="noindent">Prove the fundamental theorem of linear programming, Theorem 29.5.</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-170"><strong>Problems</strong></p>
<section title="29-1 Linear-inequality feasibility">
<p class="level2"><strong><em>29-1     Linear-inequality feasibility</em></strong></p>
<p class="noindent">Given a set of <em>m</em> linear inequalities on <em>n</em> variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>, the <strong><em><span class="blue1">linear-inequality feasibility problem</span></em></strong> asks whether there is a setting of the variables that simultaneously satisfies each of the inequalities.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Given an algorithm for the linear-programming problem, show how to use it to solve a linear-inequality feasibility problem. The number of variables and constraints that you use in the linear-programming problem should be polynomial in <em>n</em> and <em>m</em>.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Given an algorithm for the linear-inequality feasibility problem, show how to use it to solve a linear-programming problem. The number of variables and linear inequalities that you use in the linear-inequality feasibility problem should be polynomial in <em>n</em> and <em>m</em>, the number of variables and constraints in the linear program.</p>
</section>
<section title="29-2 Complementary slackness">
<p class="level2"><strong><em>29-2     Complementary slackness</em></strong></p>
<p class="noindent"><strong><em><span class="blue1">Complementary slackness</span></em></strong> describes a relationship between the values of primal variables and dual constraints and between the values of dual variables and primal constraints. Let <em><span class="overline">x</span></em> be a feasible solution to the primal linear program given in (29.31)–(29.33), and let <em><span class="font1">ӯ</span></em> be a feasible solution to the dual linear program given in (29.34)–(29.36). Complementary slackness states that the following conditions are necessary and sufficient for <em><span class="overline">x</span></em> and <em><span class="font1">ӯ</span></em> to be optimal:</p>
<a id="p874"/>
<p class="eql"><img alt="art" src="images/Art_P1026.jpg"/></p>
<p class="noindent">and</p>
<p class="eql"><img alt="art" src="images/Art_P1027.jpg"/></p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Verify that complementary slackness holds for the linear program in lines (29.37)–(29.41).</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Prove that complementary slackness holds for any primal linear program and its corresponding dual.</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Prove that a feasible solution <em><span class="overline">x</span></em> to a primal linear program given in lines (29.31)–(29.33) is optimal if and only if there exist values <em><span class="font1">ӯ</span></em> = (<em><span class="font1">ӯ</span></em><sub>1</sub>, <em><span class="font1">ӯ</span></em><sub>2</sub>, … , <em><span class="font1">ӯ</span><sub>m</sub></em>) such that</p>
<ol class="olnoindent-d" epub:type="list">
<li><em><span class="font1">ӯ</span></em> is a feasible solution to the dual linear program given in (29.34)–(29.36),</li>
<li class="litop"><img alt="art" src="images/Art_P1028.jpg"/> for all <em>j</em> such that <em><span class="overline">x</span><sub>j</sub></em> &gt; 0, and</li>
<li class="litop"><em><span class="font1">ӯ</span><sub>i</sub></em> = 0 for all <em>i</em> such that <img alt="art" src="images/Art_P1029.jpg"/>.</li></ol>
</section>
<section title="29-3 Integer linear programming">
<p class="level2"><strong><em>29-3     Integer linear programming</em></strong></p>
<p class="noindent">An <strong><em><span class="blue1">integer linear-programming problem</span></em></strong> is a linear-programming problem with the additional constraint that the variables <em>x</em> must take on integer values. Exercise 34.5-3 on page 1098 shows that just determining whether an integer linear program has a feasible solution is NP-hard, which means that there is no known polynomial-time algorithm for this problem.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Show that weak duality (Lemma 29.1) holds for an integer linear program.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Show that duality (Theorem 29.4) does not always hold for an integer linear program.</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Given a primal linear program in standard form, let <em>P</em> be the optimal objective value for the primal linear program, <em>D</em> be the optimal objective value for its dual, <em>IP</em> be the optimal objective value for the integer version of the primal (that is, the primal with the added constraint that the variables take on integer values), and <em>ID</em> be the optimal objective value for the integer version of the dual. Assuming that both the primal integer program and the dual integer program are feasible and bounded, show that</p>
<p class="nl-1list-dp1"><em>IP</em> ≤ <em>P</em> = <em>D</em> ≤ <em>ID</em>.</p>
<a id="p875"/>
</section>
<section title="29-4 Farkas’s lemma">
<p class="level2"><strong><em>29-4     Farkas’s lemma</em></strong></p>
<p class="noindent">Prove Farkas’s lemma, Lemma 29.3.</p>
</section>
<section title="29-5 Minimum-cost circulation">
<p class="level2"><strong><em>29-5     Minimum-cost circulation</em></strong></p>
<p class="noindent">This problem considers a variant of the minimum-cost-flow problem from <a href="chapter029.xhtml#Sec_29.2">Section 29.2</a> in which there is no demand, source, or sink. Instead, the input, as before, contains a flow network, capacity constraints <em>c</em>(<em>u</em>, <em>v</em>), and edge costs <em>a</em>(<em>u</em>, <em>v</em>). A flow is feasible if it satisfies the capacity constraint on every edge and flow conservation at <em>every</em> vertex. The goal is to find, among all feasible flows, the one of minimum cost. We call this problem the <strong><em><span class="blue1">minimum-cost-circulation problem.</span></em></strong></p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Formulate the minimum-cost-circulation problem as a linear program.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Suppose that for all edges (<em>u</em>, <em>v</em>) ∈ <em>E</em>, we have <em>a</em>(<em>u</em>, <em>v</em>) &gt; 0. What does an optimal solution to the minimum-cost-circulation problem look like?</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Formulate the maximum-flow problem as a minimum-cost-circulation problem linear program. That is, given a maximum-flow problem instance <em>G</em> = (<em>V</em>, <em>E</em>) with source <em>s</em>, sink <em>t</em> and edge capacities <em>c</em>, create a minimum-cost-circulation problem by giving a (possibly different) network <em>G</em>′ = (<em>V</em>′, <em>E</em>′) with edge capacities <em>c</em>′ and edge costs <em>a</em>′ such that you can derive a solution to the maximum-flow problem from a solution to the minimum-cost-circulation problem.</p>
<p class="nl-1list-d"><strong><em>d.</em></strong> Formulate the single-source shortest-path problem as a minimum-cost-circulation problem linear program.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-171"><strong>Chapter notes</strong></p>
<p class="noindent">This chapter only begins to study the wide field of linear programming. A number of books are devoted exclusively to linear programming, including those by Chvátal [<a epub:type="noteref" href="bibliography001.xhtml#endnote_94">94</a>], Gass [<a epub:type="noteref" href="bibliography001.xhtml#endnote_178">178</a>], Karloff [<a epub:type="noteref" href="bibliography001.xhtml#endnote_246">246</a>], Schrijver [<a epub:type="noteref" href="bibliography001.xhtml#endnote_398">398</a>], and Vanderbei [<a epub:type="noteref" href="bibliography001.xhtml#endnote_444">444</a>]. Many other books give a good coverage of linear programming, including those by Papadimitriou and Steiglitz [<a epub:type="noteref" href="bibliography001.xhtml#endnote_353">353</a>] and Ahuja, Magnanti, and Orlin [<a epub:type="noteref" href="bibliography001.xhtml#endnote_7">7</a>]. The coverage in this chapter draws on the approach taken by Chvátal.</p>
<p>The simplex algorithm for linear programming was invented by G. Dantzig in 1947. Shortly after, researchers discovered how to formulate a number of problems in a variety of fields as linear programs and solve them with the simplex algorithm. As a result, applications of linear programming flourished, along with several algorithms. Variants of the simplex algorithm remain the most popular <a id="p876"/>methods for solving linear-programming problems. This history appears in a number of places, including the notes in [<a epub:type="noteref" href="bibliography001.xhtml#endnote_94">94</a>] and [<a epub:type="noteref" href="bibliography001.xhtml#endnote_246">246</a>].</p>
<p>The ellipsoid algorithm was the first polynomial-time algorithm for linear programming and is due to L. G. Khachian in 1979. It was based on earlier work by N. Z. Shor, D. B. Judin, and A. S. Nemirovskii. Grötschel, Lovász, and Schrijver [<a epub:type="noteref" href="bibliography001.xhtml#endnote_201">201</a>] describe how to use the ellipsoid algorithm to solve a variety of problems in combinatorial optimization. To date, the ellipsoid algorithm does not appear to be competitive with the simplex algorithm in practice.</p>
<p>Karmarkar’s paper [<a epub:type="noteref" href="bibliography001.xhtml#endnote_247">247</a>] includes a description of the first interior-point algorithm. Many subsequent researchers designed interior-point algorithms. Good surveys appear in the article of Goldfarb and Todd [<a epub:type="noteref" href="bibliography001.xhtml#endnote_189">189</a>] and the book by Ye [<a epub:type="noteref" href="bibliography001.xhtml#endnote_463">463</a>].</p>
<p>Analysis of the simplex algorithm remains an active area of research. V. Klee and G. J. Minty constructed an example on which the simplex algorithm runs through 2<em><sup>n</sup></em> − 1 iterations. The simplex algorithm usually performs well in practice, and many researchers have tried to give theoretical justification for this empirical observation. A line of research begun by K. H. Borgwardt, and carried on by many others, shows that under certain probabilistic assumptions on the input, the simplex algorithm converges in expected polynomial time. Spielman and Teng [<a epub:type="noteref" href="bibliography001.xhtml#endnote_421">421</a>] made progress in this area, introducing the “smoothed analysis of algorithms” and applying it to the simplex algorithm.</p>
<p>The simplex algorithm is known to run efficiently in certain special cases. Particularly noteworthy is the network-simplex algorithm, which is the simplex algorithm, specialized to network-flow problems. For certain network problems, including the shortest-paths, maximum-flow, and minimum-cost-flow problems, variants of the network-simplex algorithm run in polynomial time. See, for example, the article by Orlin [<a epub:type="noteref" href="bibliography001.xhtml#endnote_349">349</a>] and the citations therein.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> An intuitive definition of a convex region is that it fulfills the requirement that for any two points in the region, all points on a line segment between them are also in the region.</p>
</section>
</section>
</div>
</body>
</html>