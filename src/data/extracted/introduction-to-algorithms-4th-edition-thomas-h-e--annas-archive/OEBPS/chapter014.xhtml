<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p362"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="14 Dynamic Programming">
<p class="chapter-title"><a href="toc.xhtml#chap-14"><strong><span class="blue1">14        Dynamic Programming</span></strong></a></p>
<p class="noindent">Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems. (“Programming” in this context refers to a tabular method, not to writing computer code.) As we saw in <a href="chapter002.xhtml">Chapters 2</a> and <a href="chapter004.xhtml">4</a>, divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem. In contrast, dynamic programming applies when the subproblems overlap—that is, when subproblems share subsubproblems. In this context, a divide-and-conquer algorithm does more work than necessary, repeatedly solving the common subsubproblems. A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblem.</p>
<p>Dynamic programming typically applies to <strong><em><span class="blue1">optimization problems</span></em></strong>. Such problems can have many possible solutions. Each solution has a value, and you want to find a solution with the optimal (minimum or maximum) value. We call such a solution <em>an</em> optimal solution to the problem, as opposed to <em>the</em> optimal solution, since there may be several solutions that achieve the optimal value.</p>
<p>To develop a dynamic-programming algorithm, follow a sequence of four steps:</p>
<ol class="olnoindent" epub:type="list">
<li>Characterize the structure of an optimal solution.</li>
<li class="litop">Recursively define the value of an optimal solution.</li>
<li class="litop">Compute the value of an optimal solution, typically in a bottom-up fashion.</li>
<li class="litop">Construct an optimal solution from computed information.</li></ol>
<p class="noindent">Steps 1–3 form the basis of a dynamic-programming solution to a problem. If you need only the value of an optimal solution, and not the solution itself, then you can omit step 4. When you do perform step 4, it often pays to maintain additional information during step 3 so that you can easily construct an optimal solution.</p>
<p>The sections that follow use the dynamic-programming method to solve some optimization problems. <a href="chapter014.xhtml#Sec_14.1">Section 14.1</a> examines the problem of cutting a rod into <a id="p363"/>rods of smaller length in a way that maximizes their total value. <a href="chapter014.xhtml#Sec_14.2">Section 14.2</a> shows how to multiply a chain of matrices while performing the fewest total scalar multiplications. Given these examples of dynamic programming, <a href="chapter014.xhtml#Sec_14.3">Section 14.3</a> discusses two key characteristics that a problem must have for dynamic programming to be a viable solution technique. <a href="chapter014.xhtml#Sec_14.4">Section 14.4</a> then shows how to find the longest common subsequence of two sequences via dynamic programming. Finally, <a href="chapter014.xhtml#Sec_14.5">Section 14.5</a> uses dynamic programming to construct binary search trees that are optimal, given a known distribution of keys to be looked up.</p>
<p class="line1"/>
<section title="14.1 Rod cutting">
<a id="Sec_14.1"/>
<p class="level1" id="h1-81"><a href="toc.xhtml#Rh1-81"><strong>14.1    Rod cutting</strong></a></p>
<p class="noindent">Our first example uses dynamic programming to solve a simple problem in deciding where to cut steel rods. Serling Enterprises buys long steel rods and cuts them into shorter rods, which it then sells. Each cut is free. The management of Serling Enterprises wants to know the best way to cut up the rods.</p>
<p>Serling Enterprises has a table giving, for <em>i</em> = 1, 2, …, the price <em>p<sub>i</sub></em> in dollars that they charge for a rod of length <em>i</em> inches. The length of each rod in inches is always an integer. <a href="chapter014.xhtml#Fig_14-1">Figure 14.1</a> gives a sample price table.</p>
<p>The <strong><em><span class="blue1">rod-cutting problem</span></em></strong> is the following. Given a rod of length <em>n</em> inches and a table of prices <em>p<sub>i</sub></em> for <em>i</em> = 1, 2, …, <em>n</em>, determine the maximum revenue <em>r<sub>n</sub></em> obtainable by cutting up the rod and selling the pieces. If the price <em>p<sub>n</sub></em> for a rod of length <em>n</em> is large enough, an optimal solution might require no cutting at all.</p>
<p>Consider the case when <em>n</em> = 4. <a href="chapter014.xhtml#Fig_14-2">Figure 14.2</a> shows all the ways to cut up a rod of 4 inches in length, including the way with no cuts at all. Cutting a 4-inch rod into two 2-inch pieces produces revenue <em>p</em><sub>2</sub> + <em>p</em><sub>2</sub> = 5 + 5 = 10, which is optimal.</p>
<p>Serling Enterprises can cut up a rod of length <em>n</em> in 2<sup><em>n</em>−1</sup> different ways, since they have an independent option of cutting, or not cutting, at distance <em>i</em> inches from the left end, for <em>i</em> = 1, 2, …, <em>n</em> − 1.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> We denote a decomposition into pieces using ordinary additive notation, so that 7 = 2 + 2 + 3 indicates that a rod of length 7 is cut into three pieces—two of length 2 and one of length 3. If an optimal solution cuts the rod into <em>k</em> pieces, for some 1 ≤ <em>k</em> ≤ <em>n</em>, then an optimal decomposition</p>
<p class="eql"><em>n</em> = <em>i</em><sub>1</sub> + <em>i</em><sub>2</sub> + <span class="font1">⋯</span> + <em>i<sub>k</sub></em></p>
<a id="p364"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-1"><img alt="art" src="images/Art_P447.jpg"/></p>
<p class="caption"><strong>Figure 14.1</strong> A sample price table for rods. Each rod of length <em>i</em> inches earns the company <em>p<sub>i</sub></em> dollars of revenue.</p>
</div>
<p class="block"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-2"><img alt="art" class="width100" src="images/Art_P448.jpg"/></p>
<p class="caption"><strong>Figure 14.2</strong> The 8 possible ways of cutting up a rod of length 4. Above each piece is the value of that piece, according to the sample price chart of <a href="chapter014.xhtml#Fig_14-1">Figure 14.1</a>. The optimal strategy is part (c)—cutting the rod into two pieces of length 2—which has total value 10.</p>
</div>
<p class="noindent">of the rod into pieces of lengths <em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, …, <em>i<sub>k</sub></em> provides maximum corresponding revenue</p>
<p class="eql"><img alt="art" src="images/Art_P449.jpg"/></p>
<p>For the sample problem in <a href="chapter014.xhtml#Fig_14-1">Figure 14.1</a>, you can determine the optimal revenue figures <em>r<sub>i</sub></em>, for <em>i</em> = 1, 2, …, 10, by inspection, with the corresponding optimal decompositions</p>
<table class="table1a">
<tr>
<td class="td2"><em>r</em><sub>1</sub> = 1</td>
<td class="td2">from solution 1 = 1</td>
<td class="td2">(no cuts),</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>2</sub> = 5</td>
<td class="td2">from solution 2 = 2</td>
<td class="td2">(no cuts),</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>3</sub> = 8</td>
<td class="td2">from solution 3 = 3</td>
<td class="td2">(no cuts),</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>4</sub> = 10</td>
<td class="td2" colspan="2">from solution 4 = 2 + 2,</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>5</sub> = 13</td>
<td class="td2" colspan="2">from solution 5 = 2 + 3,</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>6</sub> = 17</td>
<td class="td2">from solution 6 = 6</td>
<td class="td2">(no cuts),</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>7</sub> = 18</td>
<td class="td2" colspan="2">from solution 7 = 1 + 6 or 7 = 2 + 2 + 3,</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>8</sub> = 22</td>
<td class="td2" colspan="2">from solution 8 = 2 + 6,</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>9</sub> = 25</td>
<td class="td2" colspan="2">from solution 9 = 3 + 6,</td>
</tr>
<tr>
<td class="td2"><em>r</em><sub>10</sub> = 30</td>
<td class="td2">from solution 10 = 10</td>
<td class="td2">(no cuts).</td>
</tr>
</table>
<a id="p365"/>
<p>More generally, we can express the values <em>r<sub>n</sub></em> for <em>n</em> ≥ 1 in terms of optimal revenues from shorter rods:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P450.jpg"/></p>
<p class="noindent">The first argument, <em>p<sub>n</sub></em>, corresponds to making no cuts at all and selling the rod of length <em>n</em> as is. The other <em>n</em> − 1 arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size <em>i</em> and <em>n</em> − <em>i</em>, for each <em>i</em> = 1, 2, …, <em>n</em> − 1, and then optimally cutting up those pieces further, obtaining revenues <em>r<sub>i</sub></em> and <em>r</em><sub><em>n</em>−<em>i</em></sub> from those two pieces. Since you don’t know ahead of time which value of <em>i</em> optimizes revenue, you have to consider all possible values for <em>i</em> and pick the one that maximizes revenue. You also have the option of picking no <em>i</em> at all if the greatest revenue comes from selling the rod uncut.</p>
<p>To solve the original problem of size <em>n</em>, you solve smaller problems of the same type. Once you make the first cut, the two resulting pieces form independent instances of the rod-cutting problem. The overall optimal solution incorporates optimal solutions to the two resulting subproblems, maximizing revenue from each of those two pieces. We say that the rod-cutting problem exhibits <strong><em><span class="blue1">optimal substructure</span></em></strong>: optimal solutions to a problem incorporate optimal solutions to related subproblems, which you may solve independently.</p>
<p>In a related, but slightly simpler, way to arrange a recursive structure for the rod-cutting problem, let’s view a decomposition as consisting of a first piece of length <em>i</em> cut off the left-hand end, and then a right-hand remainder of length <em>n</em> − <em>i</em>. Only the remainder, and not the first piece, may be further divided. Think of every decomposition of a length-<em>n</em> rod in this way: as a first piece followed by some decomposition of the remainder. Then we can express the solution with no cuts at all by saying that the first piece has size <em>i</em> = <em>n</em> and revenue <em>p<sub>n</sub></em> and that the remainder has size 0 with corresponding revenue <em>r</em><sub>0</sub> = 0. We thus obtain the following simpler version of equation (14.1):</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P451.jpg"/></p>
<p class="noindent">In this formulation, an optimal solution embodies the solution to only <em>one</em> related subproblem—the remainder—rather than two.</p>
<p class="level4"><strong>Recursive top-down implementation</strong></p>
<p class="noindent">The C<small>UT</small>-R<small>OD</small> procedure on the following page implements the computation implicit in equation (14.2) in a straightforward, top-down, recursive manner. It takes as input an array <em>p</em>[1 : <em>n</em>] of prices and an integer <em>n</em>, and it returns the maximum revenue possible for a rod of length <em>n</em>. For length <em>n</em> = 0, no revenue is possible, and so C<small>UT</small>-R<small>OD</small> returns 0 in line 2. Line 3 initializes the maximum revenue <em>q</em> to −∞, so that the <strong>for</strong> loop in lines 4–5 correctly computes <a id="p366"/><em>q</em> = max {<em>p<sub>i</sub></em> + C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em> − <em>i</em>) : 1 ≤ <em>i</em> ≤ <em>n</em>}. Line 6 then returns this value. A simple induction on <em>n</em> proves that this answer is equal to the desired answer <em>r<sub>n</sub></em>, using equation (14.2).</p>
<div class="pull-quote1">
<p class="box-heading">C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>if</strong> <em>n</em> == 0</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>return</strong> 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><em>q</em> = −∞</td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><em>q</em> = max {<em>q</em>, <em>p</em>[<em>i</em>] + C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em> − <em>i</em>)}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><strong>return</strong> <em>q</em></td>
</tr>
</table>
</div>
<p>If you code up C<small>UT</small>-R<small>OD</small> in your favorite programming language and run it on your computer, you’ll find that once the input size becomes moderately large, your program takes a long time to run. For <em>n</em> = 40, your program may take several minutes and possibly more than an hour. For large values of <em>n</em>, you’ll also discover that each time you increase <em>n</em> by 1, your program’s running time approximately doubles.</p>
<p>Why is C<small>UT</small>-R<small>OD</small> so inefficient? The problem is that C<small>UT</small>-R<small>OD</small> calls itself recursively over and over again with the same parameter values, which means that it solves the same subproblems repeatedly. <a href="chapter014.xhtml#Fig_14-3">Figure 14.3</a> shows a recursion tree demonstrating what happens for <em>n</em> = 4: C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>) calls C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em> − <em>i</em>) for <em>i</em> = 1, 2, …, <em>n</em>. Equivalently, C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>) calls C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>j</em>) for each <em>j</em> = 0, 1, …, <em>n</em> − 1. When this process unfolds recursively, the amount of work done, as a function of <em>n</em>, grows explosively.</p>
<p>To analyze the running time of C<small>UT</small>-R<small>OD</small>, let <em>T</em>(<em>n</em>) denote the total number of calls made to C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>) for a particular value of <em>n</em>. This expression equals the number of nodes in a subtree whose root is labeled <em>n</em> in the recursion tree. The count includes the initial call at its root. Thus, <em>T</em>(0) = 1 and</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P452.jpg"/></p>
<p class="noindent">The initial 1 is for the call at the root, and the term <em>T</em>(<em>j</em>) counts the number of calls (including recursive calls) due to the call C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em> − <em>i</em>), where <em>j</em> = <em>n</em> − <em>i</em>. As Exercise 14.1-1 asks you to show,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P453.jpg"/></p>
<p class="noindent">and so the running time of C<small>UT</small>-R<small>OD</small> is exponential in <em>n</em>.</p>
<p>In retrospect, this exponential running time is not so surprising. C<small>UT</small>-R<small>OD</small> explicitly considers all possible ways of cutting up a rod of length <em>n</em>. How many ways <a id="p367"/>are there? A rod of length <em>n</em> has <em>n</em> − 1 potential locations to cut. Each possible way to cut up the rod makes a cut at some subset of these <em>n</em> − 1 locations, including the empty set, which makes for no cuts. Viewing each cut location as a distinct member of a set of <em>n</em> − 1 elements, you can see that there are 2<sup><em>n</em>−1</sup> subsets. Each leaf in the recursion tree of <a href="chapter014.xhtml#Fig_14-3">Figure 14.3</a> corresponds to one possible way to cut up the rod. Hence, the recursion tree has 2<sup><em>n</em>−1</sup> leaves. The labels on the simple path from the root to a leaf give the sizes of each remaining right-hand piece before making each cut. That is, the labels give the corresponding cut points, measured from the right-hand end of the rod.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_14-3"><img alt="art" src="images/Art_P454.jpg"/></p>
<p class="caption"><strong>Figure 14.3</strong> The recursion tree showing recursive calls resulting from a call C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>) for <em>n</em> = 4. Each node label gives the size <em>n</em> of the corresponding subproblem, so that an edge from a parent with label <em>s</em> to a child with label <em>t</em> corresponds to cutting off an initial piece of size <em>s</em> − <em>t</em> and leaving a remaining subproblem of size <em>t</em>. A path from the root to a leaf corresponds to one of the 2<sup><em>n</em>−1</sup> ways of cutting up a rod of length <em>n</em>. In general, this recursion tree has 2<sup><em>n</em></sup> nodes and 2<sup><em>n</em>−1</sup> leaves.</p>
</div>
<p class="level4"><strong>Using dynamic programming for optimal rod cutting</strong></p>
<p class="noindent">Now, let’s see how to use dynamic programming to convert C<small>UT</small>-R<small>OD</small> into an efficient algorithm.</p>
<p>The dynamic-programming method works as follows. Instead of solving the same subproblems repeatedly, as in the naive recursion solution, arrange for each subproblem to be solved <em>only once</em>. There’s actually an obvious way to do so: the first time you solve a subproblem, <em>save its solution</em>. If you need to refer to this subproblem’s solution again later, just look it up, rather than recomputing it.</p>
<p>Saving subproblem solutions comes with a cost: the additional memory needed to store solutions. Dynamic programming thus serves as an example of a <strong><em><span class="blue1">time-memory trade-off</span></em></strong>. The savings may be dramatic. For example, we’re about to use dynamic programming to go from the exponential-time algorithm for rod cutting <a id="p368"/>down to a Θ(<em>n</em><sup>2</sup>)-time algorithm. A dynamic-programming approach runs in polynomial time when the number of <em>distinct</em> subproblems involved is polynomial in the input size and you can solve each such subproblem in polynomial time.</p>
<p>There are usually two equivalent ways to implement a dynamic-programming approach. Solutions to the rod-cutting problem illustrate both of them.</p>
<p>The first approach is <strong><em><span class="blue1">top-down</span></em></strong> with <strong><em><span class="blue1">memoization</span></em></strong>.<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> In this approach, you write the procedure recursively in a natural manner, but modified to save the result of each subproblem (usually in an array or hash table). The procedure now first checks to see whether it has previously solved this subproblem. If so, it returns the saved value, saving further computation at this level. If not, the procedure computes the value in the usual manner but also saves it. We say that the recursive procedure has been <strong><em><span class="blue1">memoized</span></em></strong>: it “remembers” what results it has computed previously.</p>
<p>The second approach is the <strong><em><span class="blue1">bottom-up method</span></em></strong>. This approach typically depends on some natural notion of the “size” of a subproblem, such that solving any particular subproblem depends only on solving “smaller” subproblems. Solve the subproblems in size order, smallest first, storing the solution to each subproblem when it is first solved. In this way, when solving a particular subproblem, there are already saved solutions for all of the smaller subproblems its solution depends upon. You need to solve each subproblem only once, and when you first see it, you have already solved all of its prerequisite subproblems.</p>
<p>These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems. The bottom-up approach often has much better constant factors, since it has lower overhead for procedure calls.</p>
<p>The procedures M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> and M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small> on the facing page demonstrate how to memoize the top-down C<small>UT</small>-R<small>OD</small> procedure. The main procedure M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> initializes a new auxiliary array <em>r</em>[0 : <em>n</em>] with the value −∞ which, since known revenue values are always nonnegative, is a convenient choice for denoting “unknown.” M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> then calls its helper procedure, M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small>, which is just the memoized version of the exponential-time procedure, C<small>UT</small>-R<small>OD</small>. It first checks in line 1 to see whether the desired value is already known and, if it is, then line 2 returns it. Otherwise, lines 3–7 compute the desired value <em>q</em> in the usual manner, line 8 saves it in <em>r</em>[<em>n</em>], and line 9 returns it.</p>
<p>The bottom-up version, B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> on the next page, is even simpler. Using the bottom-up dynamic-programming approach, B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> takes advantage of the natural ordering of the subproblems: a subproblem of size <a id="p369"/><em>i</em> is “smaller” than a subproblem of size <em>j</em> if <em>i</em> &lt; <em>j</em>. Thus, the procedure solves subproblems of sizes <em>j</em> = 0, 1, …, <em>n</em>, in that order.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1">let <em>r</em>[0 : <em>n</em>] be a new array</td>
<td class="td1"><span class="red"><strong>//</strong> will remember solution values in <em>r</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 0 <strong>to</strong> <em>n</em></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>r</em>[<em>i</em>] = −∞</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><strong>return</strong> M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small>(<em>p</em>, <em>n</em>, <em>r</em>)</td>
</tr>
</table>
<p class="box-headinga">M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small>(<em>p</em>, <em>n</em>, <em>r</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>if</strong> <em>r</em>[<em>n</em>] ≥ 0</td>
<td class="td1"><span class="red"><strong>//</strong> already have a solution for length <em>n</em>?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>return</strong> <em>r</em>[<em>n</em>]</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><strong>if</strong> <em>n</em> == 0</td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>q</em> = 0</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><strong>else</strong> <em>q</em> = −∞</td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>i</em> is the position of the first cut</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2">
<p class="p3"><em>q</em> = max {<em>q</em>, <em>p</em>[<em>i</em>] + M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small>(<em>p</em>, <em>n</em> − <em>i</em>, <em>r</em>)}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><em>r</em>[<em>n</em>] = <em>q</em></td>
<td class="td1"><span class="red"><strong>//</strong> remember the solution value for length <em>n</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">9</span></td>
<td class="td1"><strong>return</strong> <em>q</em></td>
<td class="td1"/>
</tr>
</table>
<p class="box-headinga">B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1">let <em>r</em>[0 : <em>n</em>] be a new array</td>
<td class="td1"><span class="red"><strong>//</strong> will remember solution values in <em>r</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><em>r</em>[0] = 0</td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></td>
<td class="td1"><span class="red"><strong>//</strong> for increasing rod length <em>j</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>q</em> = −∞</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>j</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>i</em> is the position of the first cut</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2">
<p class="p3"><em>q</em> = max {<em>q</em>, <em>p</em>[<em>i</em>] + <em>r</em>[<em>j</em> − <em>i</em>]}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p2"><em>r</em>[<em>j</em>] = <em>q</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> remember the solution value for length <em>j</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><strong>return</strong> <em>r</em>[<em>n</em>]</td>
<td class="td1"/>
</tr>
</table>
</div>
<p>Line 1 of B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> creates a new array <em>r</em>[0 : <em>n</em>] in which to save the results of the subproblems, and line 2 initializes <em>r</em>[0] to 0, since a rod of length 0 earns no revenue. Lines 3–6 solve each subproblem of size <em>j</em>, for <em>j</em> = 1, 2, …, <em>n</em>, in order of increasing size. The approach used to solve a problem of a particular size <em>j</em> is the same as that used by C<small>UT</small>-R<small>OD</small>, except that line 6 now directly references array entry <em>r</em>[<em>j</em> − <em>i</em>] instead of making a recursive call to solve the subproblem of size <em>j</em> − <em>i</em>. Line 7 saves in <em>r</em>[<em>j</em>] the solution to the subproblem of size <em>j</em>. Finally, line 8 returns <em>r</em>[<em>n</em>], which equals the optimal value <em>r<sub>n</sub></em>.</p>
<p>The bottom-up and top-down versions have the same asymptotic running time. The running time of B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> is Θ(<em>n</em><sup>2</sup>), due to its doubly nested <a id="p370"/>loop structure. The number of iterations of its inner <strong>for</strong> loop, in lines 5–6, forms an arithmetic series. The running time of its top-down counterpart, M<small>EMOIZED</small>C<small>UT</small>-R<small>OD</small>, is also Θ(<em>n</em><sup>2</sup>), although this running time may be a little harder to see. Because a recursive call to solve a previously solved subproblem returns immediately, M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> solves each subproblem just once. It solves subproblems for sizes 0, 1, …, <em>n</em>. To solve a subproblem of size <em>n</em>, the <strong>for</strong> loop of lines 6–7 iterates <em>n</em> times. Thus, the total number of iterations of this <strong>for</strong> loop, over all recursive calls of M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>, forms an arithmetic series, giving a total of Θ(<em>n</em><sup>2</sup>) iterations, just like the inner <strong>for</strong> loop of B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>. (We actually are using a form of aggregate analysis here. We’ll see aggregate analysis in detail in <a href="chapter016.xhtml#Sec_16.1">Section 16.1</a>.)</p>
<div class="divimage">
<p class="fig-imga" id="Fig_14-4"><img alt="art" src="images/Art_P455.jpg"/></p>
<p class="caption"><strong>Figure 14.4</strong> The subproblem graph for the rod-cutting problem with <em>n</em> = 4. The vertex labels give the sizes of the corresponding subproblems. A directed edge (<em>x</em>, <em>y</em>) indicates that solving subproblem <em>x</em> requires a solution to subproblem <em>y</em>. This graph is a reduced version of the recursion tree of <a href="chapter014.xhtml#Fig_14-3">Figure 14.3</a>, in which all nodes with the same label are collapsed into a single vertex and all edges go from parent to child.</p>
</div>
<p class="level4"><strong>Subproblem graphs</strong></p>
<p class="noindent">When you think about a dynamic-programming problem, you need to understand the set of subproblems involved and how subproblems depend on one another.</p>
<p>The <strong><em><span class="blue1">subproblem graph</span></em></strong> for the problem embodies exactly this information. <a href="chapter014.xhtml#Fig_14-4">Figure 14.4</a> shows the subproblem graph for the rod-cutting problem with <em>n</em> = 4. It is a directed graph, containing one vertex for each distinct subproblem. The subproblem graph has a directed edge from the vertex for subproblem <em>x</em> to the vertex for subproblem <em>y</em> if determining an optimal solution for subproblem <em>x</em> involves directly considering an optimal solution for subproblem <em>y</em>. For example, the subproblem graph contains an edge from <em>x</em> to <em>y</em> if a top-down recursive procedure for solving <em>x</em> directly calls itself to solve <em>y</em>. You can think of the subproblem graph as <a id="p371"/>a “reduced” or “collapsed” version of the recursion tree for the top-down recursive method, with all nodes for the same subproblem coalesced into a single vertex and all edges directed from parent to child.</p>
<p>The bottom-up method for dynamic programming considers the vertices of the subproblem graph in such an order that you solve the subproblems <em>y</em> adjacent to a given subproblem <em>x</em> before you solve subproblem <em>x</em>. (As <a href="appendix002.xhtml#Sec_B.4">Section B.4</a> notes, the adjacency relation in a directed graph is not necessarily symmetric.) Using terminology that we’ll see in <a href="chapter020.xhtml#Sec_20.4">Section 20.4</a>, in a bottom-up dynamic-programming algorithm, you consider the vertices of the subproblem graph in an order that is a “reverse topological sort,” or a “topological sort of the transpose” of the subproblem graph. In other words, no subproblem is considered until all of the subproblems it depends upon have been solved. Similarly, using notions that we’ll visit in <a href="chapter020.xhtml#Sec_20.3">Section 20.3</a>, you can view the top-down method (with memoization) for dynamic programming as a “depth-first search” of the subproblem graph.</p>
<p>The size of the subproblem graph <em>G</em> = (<em>V</em>, <em>E</em>) can help you determine the running time of the dynamic-programming algorithm. Since you solve each subproblem just once, the running time is the sum of the times needed to solve each subproblem. Typically, the time to compute the solution to a subproblem is proportional to the degree (number of outgoing edges) of the corresponding vertex in the subproblem graph, and the number of subproblems is equal to the number of vertices in the subproblem graph. In this common case, the running time of dynamic programming is linear in the number of vertices and edges.</p>
<p class="level4"><strong>Reconstructing a solution</strong></p>
<p class="noindent">The procedures M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> and B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> return the <em>value</em> of an optimal solution to the rod-cutting problem, but they do not return the solution <em>itself</em>: a list of piece sizes.</p>
<p>Let’s see how to extend the dynamic-programming approach to record not only the optimal <em>value</em> computed for each subproblem, but also a <em>choice</em> that led to the optimal value. With this information, you can readily print an optimal solution. The procedure E<small>XTENDED</small>-B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> on the next page computes, for each rod size <em>j</em>, not only the maximum revenue <em>r<sub>j</sub></em>, but also <em>s<sub>j</sub></em>, the optimal size of the first piece to cut off. It’s similar to B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>, except that it creates the array <em>s</em> in line 1, and it updates <em>s</em>[<em>j</em>] in line 8 to hold the optimal size <em>i</em> of the first piece to cut off when solving a subproblem of size <em>j</em>.</p>
<p>The procedure P<small>RINT</small>-C<small>UT</small>-R<small>OD</small>-S<small>OLUTION</small> on the following page takes as input an array <em>p</em>[1 : <em>n</em>] of prices and a rod size <em>n</em>. It calls E<small>XTENDED</small>-B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small> to compute the array <em>s</em>[1 : <em>n</em>] of optimal first-piece sizes. Then it prints out the complete list of piece sizes in an optimal decomposition of a <a id="p372"/>rod of length <em>n</em>. For the sample price chart appearing in <a href="chapter014.xhtml#Fig_14-1">Figure 14.1</a>, the call E<small>XTENDED</small>-B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>(<em>p</em>, 10) returns the following arrays:</p>
<table class="table1c">
<tr>
<td class="th2"><p class="center"><em>i</em></p></td>
<td class="th1"><p class="center">0</p></td>
<td class="th1"><p class="center">1</p></td>
<td class="th1"><p class="center">2</p></td>
<td class="th1"><p class="center">3</p></td>
<td class="th1"><p class="center">4</p></td>
<td class="th1"><p class="center">5</p></td>
<td class="th1"><p class="center">6</p></td>
<td class="th1"><p class="center">7</p></td>
<td class="th1"><p class="center">8</p></td>
<td class="th1"><p class="center">9</p></td>
<td class="th1"><p class="center">10</p></td>
</tr>
<tr>
<td class="td1r"><p class="center"><em>r</em>[<em>i</em>]</p></td>
<td class="td1"><p class="center">0</p></td>
<td class="td1"><p class="center">1</p></td>
<td class="td1"><p class="center">5</p></td>
<td class="td1"><p class="center">8</p></td>
<td class="td1"><p class="center">10</p></td>
<td class="td1"><p class="center">13</p></td>
<td class="td1"><p class="center">17</p></td>
<td class="td1"><p class="center">18</p></td>
<td class="td1"><p class="center">22</p></td>
<td class="td1"><p class="center">25</p></td>
<td class="td1"><p class="center">30</p></td>
</tr>
<tr>
<td class="td1r"><p class="center"><em>s</em>[<em>i</em>]</p></td>
<td class="td1"/>
<td class="td1"><p class="center">1</p></td>
<td class="td1"><p class="center">2</p></td>
<td class="td1"><p class="center">3</p></td>
<td class="td1"><p class="center">2</p></td>
<td class="td1"><p class="center">2</p></td>
<td class="td1"><p class="center">6</p></td>
<td class="td1"><p class="center">1</p></td>
<td class="td1"><p class="center">2</p></td>
<td class="td1"><p class="center">3</p></td>
<td class="td1"><p class="center">10</p></td>
</tr>
</table>
<p class="noindent">A call to P<small>RINT</small>-C<small>UT</small>-R<small>OD</small>-S<small>OLUTION</small>(<em>p</em>, 10) prints just 10, but a call with <em>n</em> = 7 prints the cuts 1 and 6, which correspond to the first optimal decomposition for <em>r</em><sub>7</sub> given earlier.</p>
<div class="pull-quote1">
<p class="box-heading">E<small>XTENDED</small>-B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="2">let <em>r</em>[0 : <em>n</em>] and <em>s</em>[1 : <em>n</em>] be new arrays</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><em>r</em>[0] = 0</td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></td>
<td class="td1"><span class="red"><strong>//</strong> for increasing rod length <em>j</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="p2"><em>q</em> = −∞</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>j</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>i</em> is the position of the first cut</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="p3"><strong>if</strong> <em>q</em> &lt; <em>p</em>[<em>i</em>] + <em>r</em>[<em>j</em> − <em>i</em>]</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>q</em> = <em>p</em>[<em>i</em>] + <em>r</em>[<em>j</em> − <em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="p4"><em>s</em>[<em>j</em>] = <em>i</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> best cut location so far for length <em>j</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1"><p class="p2"><em>r</em>[<em>j</em>] = <em>q</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> remember the solution value for length <em>j</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><strong>return</strong> <em>r</em> and <em>s</em></td>
<td class="td1"/>
</tr>
</table>
<p class="box-headinga">P<small>RINT</small>-C<small>UT</small>-R<small>OD</small>-S<small>OLUTION</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2">(<em>r</em>, <em>s</em>) = E<small>XTENDED</small>-B<small>OTTOM</small>-U<small>P</small>-C<small>UT</small>-R<small>OD</small>(<em>p</em>, <em>n</em>)</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><strong>while</strong> <em>n</em> &gt; 0</td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2">print <em>s</em>[<em>n</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> cut location for length <em>n</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>n</em> = <em>n</em> − <em>s</em>[<em>n</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> length of the remainder of the rod</span></td>
</tr>
</table>
</div>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>14.1-1</em></strong></p>
<p class="noindent">Show that equation (14.4) follows from equation (14.3) and the initial condition <em>T</em>(0) = 1.</p>
<p class="level3"><strong><em>14.1-2</em></strong></p>
<p class="noindent">Show, by means of a counterexample, that the following “greedy” strategy does not always determine an optimal way to cut rods. Define the <strong><em><span class="blue1">density</span></em></strong> of a rod of length <em>i</em> to be <em>p<sub>i</sub></em>/<em>i</em>, that is, its value per inch. The greedy strategy for a rod of length <em>n</em> cuts off a first piece of length <em>i</em>, where 1 ≤ <em>i</em> ≤ <em>n</em>, having maximum <a id="p373"/>density. It then continues by applying the greedy strategy to the remaining piece of length <em>n</em> − <em>i</em>.</p>
<p class="level3"><strong><em>14.1-3</em></strong></p>
<p class="noindent">Consider a modification of the rod-cutting problem in which, in addition to a price <em>p<sub>i</sub></em> for each rod, each cut incurs a fixed cost of <em>c</em>. The revenue associated with a solution is now the sum of the prices of the pieces minus the costs of making the cuts. Give a dynamic-programming algorithm to solve this modified problem.</p>
<p class="level3"><strong><em>14.1-4</em></strong></p>
<p class="noindent">Modify C<small>UT</small>-R<small>OD</small> and M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small>-A<small>UX</small> so that their <strong>for</strong> loops go up to only <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>, rather than up to <em>n</em>. What other changes to the procedures do you need to make? How are their running times affected?</p>
<p class="level3"><strong><em>14.1-5</em></strong></p>
<p class="noindent">Modify M<small>EMOIZED</small>-C<small>UT</small>-R<small>OD</small> to return not only the value but the actual solution.</p>
<p class="level3"><strong><em>14.1-6</em></strong></p>
<p class="noindent">The Fibonacci numbers are defined by recurrence (3.31) on page 69. Give an <em>O</em>(<em>n</em>)-time dynamic-programming algorithm to compute the <em>n</em>th Fibonacci number. Draw the subproblem graph. How many vertices and edges does the graph contain?</p>
</section>
<p class="line1"/>
<section title="14.2 Matrix-chain multiplication">
<a id="Sec_14.2"/>
<p class="level1" id="h1-82"><a href="toc.xhtml#Rh1-82"><strong>14.2    Matrix-chain multiplication</strong></a></p>
<p class="noindent">Our next example of dynamic programming is an algorithm that solves the problem of matrix-chain multiplication. Given a sequence (chain) <span class="font1">〈</span><em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, …, <em>A<sub>n</sub></em><span class="font1">〉</span> of <em>n</em> matrices to be multiplied, where the matrices aren’t necessarily square, the goal is to compute the product</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P456.jpg"/></p>
<p class="noindent">using the standard algorithm<sup><a epub:type="footnote" href="#footnote_3" id="footnote_ref_3">3</a></sup> for multiplying rectangular matrices, which we’ll see in a moment, while minimizing the number of scalar multiplications.</p>
<p>You can evaluate the expression (14.5) using the algorithm for multiplying pairs of rectangular matrices as a subroutine once you have parenthesized it to resolve all ambiguities in how the matrices are multiplied together. Matrix multiplication is associative, and so all parenthesizations yield the same product. A product of <a id="p374"/>matrices is <strong><em><span class="blue1">fully parenthesized</span></em></strong> if it is either a single matrix or the product of two fully parenthesized matrix products, surrounded by parentheses. For example, if the chain of matrices is <span class="font1">〈</span><em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, <em>A</em><sub>3</sub>, <em>A</em><sub>4</sub><span class="font1">〉</span>, then you can fully parenthesize the product <em>A</em><sub>1</sub><em>A</em><sub>2</sub><em>A</em><sub>3</sub><em>A</em><sub>4</sub> in five distinct ways:</p>
<table class="table2">
<tr>
<td class="td1"><p class="noindent">(<em>A</em><sub>1</sub>(<em>A</em><sub>2</sub>(<em>A</em><sub>3</sub><em>A</em><sub>4</sub>))),</p>
<p class="noindent">(<em>A</em><sub>1</sub>((<em>A</em><sub>2</sub><em>A</em><sub>3</sub>)<em>A</em><sub>4</sub>)),</p>
<p class="noindent">((<em>A</em><sub>1</sub><em>A</em><sub>2</sub>)(<em>A</em><sub>3</sub><em>A</em><sub>4</sub>)),</p>
<p class="noindent">((<em>A</em><sub>1</sub>(<em>A</em><sub>2</sub><em>A</em><sub>3</sub>))<em>A</em><sub>4</sub>),</p>
<p class="noindent">(((<em>A</em><sub>1</sub><em>A</em><sub>2</sub>)<em>A</em><sub>3</sub>)<em>A</em><sub>4</sub>).</p></td>
</tr>
</table>
<p>How you parenthesize a chain of matrices can have a dramatic impact on the cost of evaluating the product. Consider first the cost of multiplying two rectangular matrices. The standard algorithm is given by the procedure R<small>ECTANGULAR</small>-M<small>ATRIX</small>-M<small>ULTIPLY</small>, which generalizes the square-matrix multiplication procedure M<small>ATRIX</small>-M<small>ULTIPLY</small> on page 81. The R<small>ECTANGULAR</small>-M<small>ATRIX</small>-M<small>ULTIPLY</small> procedure computes <em>C</em> = <em>C</em> + <em>A</em> ·<em>B</em> for three matrices <em>A</em> = (<em>a<sub>ij</sub></em>), <em>B</em> = (<em>b<sub>ij</sub></em>), and <em>C</em> = (<em>c<sub>ij</sub></em>), where <em>A</em> is <em>p</em> × <em>q</em>, <em>B</em> is <em>q</em> × <em>r</em>, and <em>C</em> is <em>p</em> × <em>r</em>.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ECTANGULAR</small>-M<small>ATRIX</small>-M<small>ULTIPLY</small>(<em>A</em>, <em>B</em>, <em>C</em>, <em>p</em>, <em>q</em>, <em>r</em>)</p>
<table class="table1c">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>p</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>r</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p3"><strong>for</strong> <em>k</em> = 1 <strong>to</strong> <em>q</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p4"><em>c<sub>ij</sub></em> = <em>c<sub>ij</sub></em> + <em>a<sub>ik</sub></em> · <em>b<sub>kj</sub></em></p></td>
</tr>
</table>
</div>
<p>The running time of R<small>ECTANGULAR</small>-M<small>ATRIX</small>-M<small>ULTIPLY</small> is dominated by the number of scalar multiplications in line 4, which is <em>pqr</em>. Therefore, we’ll consider the cost of multiplying matrices to be the number of scalar multiplications. (The number of scalar multiplications dominates even if we consider initializing <em>C</em> = 0 to perform just <em>C</em> = <em>A</em> ·<em>B</em>.)</p>
<p>To illustrate the different costs incurred by different parenthesizations of a matrix product, consider the problem of a chain <span class="font1">〈</span><em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, <em>A</em><sub>3</sub><span class="font1">〉</span> of three matrices. Suppose that the dimensions of the matrices are 10 × 100, 100 × 5, and 5 × 50, respectively. Multiplying according to the parenthesization ((<em>A</em><sub>1</sub><em>A</em><sub>2</sub>)<em>A</em><sub>3</sub>) performs 10 · 100 · 5 = 5000 scalar multiplications to compute the 10 × 5 matrix product <em>A</em><sub>1</sub><em>A</em><sub>2</sub>, plus another 10 · 5 · 50 = 2500 scalar multiplications to multiply this matrix by <em>A</em><sub>3</sub>, for a <a id="p375"/>total of 7500 scalar multiplications. Multiplying according to the alternative parenthesization (<em>A</em><sub>1</sub>(<em>A</em><sub>2</sub><em>A</em><sub>3</sub>)) performs 100 · 5 · 50 = 25,000 scalar multiplications to compute the 100 × 50 matrix product <em>A</em><sub>2</sub><em>A</em><sub>3</sub>, plus another 10 · 100 · 50 = 50,000 scalar multiplications to multiply <em>A</em><sub>1</sub> by this matrix, for a total of 75,000 scalar multiplications. Thus, computing the product according to the first parenthesization is 10 times faster.</p>
<p>We state the <strong><em><span class="blue1">matrix-chain multiplication problem</span></em></strong> as follows: given a chain <span class="font1">〈</span><em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, …, <em>A<sub>n</sub></em><span class="font1">〉</span> of <em>n</em> matrices, where for <em>i</em> = 1, 2, …, <em>n</em>, matrix <em>A<sub>i</sub></em> has dimension <em>p</em><sub><em>i</em>−1</sub> × <em>p<sub>i</sub></em>, fully parenthesize the product <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>n</sub></em> in a way that minimizes the number of scalar multiplications. The input is the sequence of dimensions <span class="font1">〈</span><em>p</em><sub>0</sub>, <em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, …, <em>p<sub>n</sub></em><span class="font1">〉</span>.</p>
<p>The matrix-chain multiplication problem does not entail actually multiplying matrices. The goal is only to determine an order for multiplying matrices that has the lowest cost. Typically, the time invested in determining this optimal order is more than paid for by the time saved later on when actually performing the matrix multiplications (such as performing only 7500 scalar multiplications instead of 75,000).</p>
<p class="level4"><strong>Counting the number of parenthesizations</strong></p>
<p class="noindent">Before solving the matrix-chain multiplication problem by dynamic programming, let us convince ourselves that exhaustively checking all possible parenthesizations is not an efficient algorithm. Denote the number of alternative parenthesizations of a sequence of <em>n</em> matrices by <em>P</em>(<em>n</em>). When <em>n</em> = 1, the sequence consists of just one matrix, and therefore there is only one way to fully parenthesize the matrix product. When <em>n</em> ≥ 2, a fully parenthesized matrix product is the product of two fully parenthesized matrix subproducts, and the split between the two subproducts may occur between the <em>k</em>th and (<em>k</em> + 1)st matrices for any <em>k</em> = 1, 2, …, <em>n</em> − 1. Thus, we obtain the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P457.jpg"/></p>
<p class="noindent">Problem 12-4 on page 329 asked you to show that the solution to a similar recurrence is the sequence of <strong><em><span class="blue1">Catalan numbers</span></em></strong>, which grows as Ω(4<sup><em>n</em></sup>/<em>n</em><sup>3/2</sup>). A simpler exercise (see Exercise 14.2-3) is to show that the solution to the recurrence (14.6) is Ω(2<em><sup>n</sup></em>). The number of solutions is thus exponential in <em>n</em>, and the brute-force method of exhaustive search makes for a poor strategy when determining how to optimally parenthesize a matrix chain.</p>
<p class="level4"><strong>Applying dynamic programming</strong></p>
<p class="noindent">Let’s use the dynamic-programming method to determine how to optimally parenthesize a matrix chain, by following the four-step sequence that we stated at the beginning of this chapter:</p>
<a id="p376"/>
<ol class="olnoindent" epub:type="list">
<li>Characterize the structure of an optimal solution.</li>
<li class="litop">Recursively define the value of an optimal solution.</li>
<li class="litop">Compute the value of an optimal solution.</li>
<li class="litop">Construct an optimal solution from computed information.</li></ol>
<p class="noindent">We’ll go through these steps in order, demonstrating how to apply each step to the problem.</p>
<p class="level4"><strong>Step 1: The structure of an optimal parenthesization</strong></p>
<p class="noindent">In the first step of the dynamic-programming method, you find the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems. To perform this step for the matrix-chain multiplication problem, it’s convenient to first introduce some notation. Let <em>A</em><sub><em>i</em>:<em>j</em></sub>, where <em>i</em> ≤ <em>j</em>, denote the matrix that results from evaluating the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. If the problem is nontrivial, that is, <em>i</em> &lt; <em>j</em>, then to parenthesize the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, the product must split between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub> for some integer <em>k</em> in the range <em>i</em> ≤ <em>k</em> &lt; <em>j</em>. That is, for some value of <em>k</em>, first compute the matrices <em>A</em><sub><em>i</em>:<em>k</em></sub> and <em>A</em><sub><em>k</em>+1:<em>j</em></sub>, and then multiply them together to produce the final product <em>A</em><sub><em>i</em>:<em>j</em></sub>. The cost of parenthesizing this way is the cost of computing the matrix <em>A</em><sub><em>i</em>:<em>k</em></sub>, plus the cost of computing <em>A</em><sub><em>k</em>+1:<em>j</em></sub>, plus the cost of multiplying them together.</p>
<p>The optimal substructure of this problem is as follows. Suppose that to optimally parenthesize <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, you split the product between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub>. Then the way you parenthesize the “prefix” subchain <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> within this optimal parenthesization of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> must be an optimal parenthesization of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em>. Why? If there were a less costly way to parenthesize <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em>, then you could substitute that parenthesization in the optimal parenthesization of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> to produce another way to parenthesize <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> whose cost is lower than the optimum: a contradiction. A similar observation holds for how to parenthesize the subchain <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> in the optimal parenthesization of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>: it must be an optimal parenthesization of <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>.</p>
<p>Now let’s use the optimal substructure to show how to construct an optimal solution to the problem from optimal solutions to subproblems. Any solution to a nontrivial instance of the matrix-chain multiplication problem requires splitting the product, and any optimal solution contains within it optimal solutions to subproblem instances. Thus, to build an optimal solution to an instance of the matrix-chain multiplication problem, split the problem into two subproblems (optimally parenthesizing <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>), find optimal solutions to the two subproblem instances, and then combine these optimal subproblem solutions. To ensure that you’ve examined the optimal split, you must consider all possible splits.</p>
<a id="p377"/>
<p class="level4"><strong>Step 2: A recursive solution</strong></p>
<p class="noindent">The next step is to define the cost of an optimal solution recursively in terms of the optimal solutions to subproblems. For the matrix-chain multiplication problem, a subproblem is to determine the minimum cost of parenthesizing <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> for 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>. Given the input dimensions <span class="font1">〈</span><em>p</em><sub>0</sub>, <em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, …, <em>p<sub>n</sub></em><span class="font1">〉</span>, an index pair <em>i</em>, <em>j</em> specifies a subproblem. Let <em>m</em>[<em>i</em>, <em>j</em>] be the minimum number of scalar multiplications needed to compute the matrix <em>A</em><sub><em>i</em>:<em>j</em></sub>. For the full problem, the lowest-cost way to compute <em>A</em><sub>1:<em>n</em></sub> is thus <em>m</em>[1, <em>n</em>].</p>
<p>We can define <em>m</em>[<em>i</em>, <em>j</em>] recursively as follows. If <em>i</em> = <em>j</em>, the problem is trivial: the chain consists of just one matrix <em>A</em><sub><em>i</em>:<em>i</em></sub> = <em>A<sub>i</sub></em>, so that no scalar multiplications are necessary to compute the product. Thus, <em>m</em>[<em>i</em>, <em>i</em>] = 0 for <em>i</em> = 1, 2, …, <em>n</em>. To compute <em>m</em>[<em>i</em>, <em>j</em>] when <em>i</em> &lt; <em>j</em>, we take advantage of the structure of an optimal solution from step 1. Suppose that an optimal parenthesization splits the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub>, where <em>i</em> ≤ <em>k</em> &lt; <em>j</em>. Then, <em>m</em>[<em>i</em>, <em>j</em>] equals the minimum cost <em>m</em>[<em>i</em>, <em>k</em>] for computing the subproduct <em>A</em><sub><em>i</em>:<em>k</em></sub>, plus the minimum cost <em>m</em>[<em>k</em>+1, <em>j</em>] for computing the subproduct, <em>A</em><sub><em>k</em>+1:<em>j</em></sub>, plus the cost of multiplying these two matrices together. Because each matrix <em>A<sub>i</sub></em> is <em>p</em><sub><em>i</em>−1</sub> × <em>p<sub>i</sub></em>, computing the matrix product <em>A</em><sub><em>i</em>:<em>k</em></sub><em>A</em><sub><em>k</em>+1:<em>j</em></sub> takes <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em> scalar multiplications. Thus, we obtain</p>
<p class="eql"><em>m</em>[<em>i</em>, <em>j</em>] = <em>m</em>[<em>i</em>, <em>k</em>] + <em>m</em>[<em>k</em> + 1, <em>j</em>] + <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em>.</p>
<p>This recursive equation assumes that you know the value of <em>k</em>. But you don’t, at least not yet. You have to try all possible values of <em>k</em>. How many are there? Just <em>j</em> − <em>i</em>, namely <em>k</em> = <em>i</em>, <em>i</em> + 1, …, <em>j</em> − 1. Since the optimal parenthesization must use one of these values for <em>k</em>, you need only check them all to find the best. Thus, the recursive definition for the minimum cost of parenthesizing the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> becomes</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P458.jpg"/></p>
<p>The <em>m</em>[<em>i</em>, <em>j</em>] values give the costs of optimal solutions to subproblems, but they do not provide all the information you need to construct an optimal solution. To help you do so, let’s define <em>s</em>[<em>i</em>, <em>j</em>] to be a value of <em>k</em> at which you split the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> in an optimal parenthesization. That is, <em>s</em>[<em>i</em>, <em>j</em>] equals a value <em>k</em> such that <em>m</em>[<em>i</em>, <em>j</em>] = <em>m</em>[<em>i</em>, <em>k</em>] + <em>m</em>[<em>k</em> + 1, <em>j</em>] + <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em>.</p>
<p class="level4"><strong>Step 3: Computing the optimal costs</strong></p>
<p class="noindent">At this point, you could write a recursive algorithm based on recurrence (14.7) to compute the minimum cost <em>m</em>[1, <em>n</em>] for multiplying <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>n</sub></em>. But as we saw <a id="p378"/>for the rod-cutting problem, and as we shall see in <a href="chapter014.xhtml#Sec_14.3">Section 14.3</a>, this recursive algorithm takes exponential time. That’s no better than the brute-force method of checking each way of parenthesizing the product.</p>
<p>Fortunately, there aren’t all that many distinct subproblems: just one subproblem for each choice of <em>i</em> and <em>j</em> satisfying 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>, or <img alt="art" src="images/Art_P459.jpg"/> in all.<sup><a epub:type="footnote" href="#footnote_4" id="footnote_ref_4">4</a></sup> A recursive algorithm may encounter each subproblem many times in different branches of its recursion tree. This property of overlapping subproblems is the second hallmark of when dynamic programming applies (the first hallmark being optimal substructure).</p>
<p>Instead of computing the solution to recurrence (14.7) recursively, let’s compute the optimal cost by using a tabular, bottom-up approach, as in the procedure M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>. (The corresponding top-down approach using memoization appears in <a href="chapter014.xhtml#Sec_14.3">Section 14.3</a>.) The input is a sequence <em>p</em> = <span class="font1">〈</span><em>p</em><sub>0</sub>, <em>p</em><sub>1</sub>, …, <em>p<sub>n</sub></em><span class="font1">〉</span> of matrix dimensions, along with <em>n</em>, so that for <em>i</em> = 1, 2, …, <em>n</em>, matrix <em>A<sub>i</sub></em> has dimensions <em>p</em><sub><em>i</em>−1</sub> × <em>p<sub>i</sub></em>. The procedure uses an auxiliary table <em>m</em>[1 : <em>n</em>, 1 : <em>n</em>] to store the <em>m</em>[<em>i</em>, <em>j</em>] costs and another auxiliary table <em>s</em>[1 : <em>n</em> − 1, 2 : <em>n</em>] that records which index <em>k</em> achieved the optimal cost in computing <em>m</em>[<em>i</em>, <em>j</em>]. The table <em>s</em> will help in constructing an optimal solution.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="2">let <em>m</em>[1 : <em>n</em>, 1 : <em>n</em>] and <em>s</em>[1 : <em>n</em> − 1, 2 : <em>n</em>] be new tables</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></td>
<td class="td1"><span class="red"><strong>//</strong> chain length 1</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1" colspan="2"><p class="p2"><em>m</em>[<em>i</em>, <em>i</em>] = 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><strong>for</strong> <em>l</em> = 2 <strong>to</strong> <em>n</em></td>
<td class="td1"><span class="red"><strong>//</strong> <em>l</em> is the chain length</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> − <em>l</em> + 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> chain begins at <em>A<sub>i</sub></em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="p3"><em>j</em> = <em>i</em> + <em>l</em> − 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> chain ends at <em>A<sub>j</sub></em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2"><p class="p3"><em>m</em>[<em>i</em>, <em>j</em>] = ∞</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="p3"><strong>for</strong> <em>k</em> = <em>i</em> <strong>to</strong> <em>j</em> − 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> try <em>A</em><sub><em>i</em>:<em>k</em></sub><em>A</em><sub><em>k</em>+1:<em>j</em></sub></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2"><p class="p4"><em>q</em> = <em>m</em>[<em>i</em>, <em>k</em>] + <em>m</em>[<em>k</em> + 1, <em>j</em>] + <em>p</em><sub><em>i</em>−1</sub><em>p<sub>k</sub> p<sub>j</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1" colspan="2"><p class="p4"><strong>if</strong> <em>q</em> &lt; <em>m</em>[<em>i</em>, <em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1"><p class="p5"><em>m</em>[<em>i</em>, <em>j</em>] = <em>q</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> remember this cost</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1"><p class="p5"><em>s</em>[<em>i</em>, <em>j</em>] = <em>k</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> remember this index</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2"><strong>return</strong> <em>m</em> and <em>s</em></td>
</tr>
</table>
</div>
<p>In what order should the algorithm fill in the table entries? To answer this question, let’s see which entries of the table need to be accessed when computing the <a id="p379"/>cost <em>m</em>[<em>i</em>, <em>j</em>]. Equation (14.7) tells us that to compute the cost of matrix product <em>A</em><sub><em>i</em>:<em>j</em></sub>, first the costs of the products <em>A</em><sub><em>i</em>:<em>k</em></sub> and <em>A</em><sub><em>k</em>+1:<em>j</em></sub> need to have been computed for all <em>k</em> = <em>i</em>, <em>i</em> + 1, …, <em>j</em> − 1. The chain <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> consists of <em>j</em> − <em>i</em> + 1 matrices, and the chains <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> … <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub> <em>A</em><sub><em>k</em>+2</sub> … <em>A<sub>j</sub></em> consist of <em>k</em> − <em>i</em> + 1 and <em>j</em> − <em>k</em> matrices, respectively. Since <em>k</em> &lt; <em>j</em>, a chain of <em>k</em> − <em>i</em> + 1 matrices consists of fewer than <em>j</em> − <em>i</em> + 1 matrices. Likewise, since <em>k</em> ≥ <em>i</em>, a chain of <em>j</em> − <em>k</em> matrices consists of fewer than <em>j</em> − <em>i</em> + 1 matrices. Thus, the algorithm should fill in the table <em>m</em> from shorter matrix chains to longer matrix chains. That is, for the subproblem of optimally parenthesizing the chain <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, it makes sense to consider the subproblem size as the length <em>j</em> − <em>i</em> + 1 of the chain.</p>
<p>Now, let’s see how the M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> procedure fills in the <em>m</em>[<em>i</em>, <em>j</em>] entries in order of increasing chain length. Lines 2–3 initialize <em>m</em>[<em>i</em>, <em>i</em>] = 0 for <em>i</em> = 1, 2, …, <em>n</em>, since any matrix chain with just one matrix requires no scalar multiplications. In the <strong>for</strong> loop of lines 4–12, the loop variable <em>l</em> denotes the length of matrix chains whose minimum costs are being computed. Each iteration of this loop uses recurrence (14.7) to compute <em>m</em>[<em>i</em>, <em>i</em> + <em>l</em> − 1] for <em>i</em> = 1, 2, …, <em>n</em> − <em>l</em> + 1. In the first iteration, <em>l</em> = 2, and so the loop computes <em>m</em>[<em>i</em>, <em>i</em> + 1] for <em>i</em> = 1, 2, …, <em>n</em> − 1: the minimum costs for chains of length <em>l</em> = 2. The second time through the loop, it computes <em>m</em>[<em>i</em>, <em>i</em> + 2] for <em>i</em> = 1, 2, …, <em>n</em> − 2: the minimum costs for chains of length <em>l</em> = 3. And so on, ending with a single matrix chain of length <em>l</em> = <em>n</em> and computing <em>m</em>[1, <em>n</em>]. When lines 7–12 compute an <em>m</em>[<em>i</em>, <em>j</em>] cost, this cost depends only on table entries <em>m</em>[<em>i</em>, <em>k</em>] and <em>m</em>[<em>k</em> + 1, <em>j</em>], which have already been computed.</p>
<p><a href="chapter014.xhtml#Fig_14-5">Figure 14.5</a> illustrates the <em>m</em> and <em>s</em> tables, as filled in by the M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> procedure on a chain of <em>n</em> = 6 matrices. Since <em>m</em>[<em>i</em>, <em>j</em>] is defined only for <em>i</em> ≤ <em>j</em>, only the portion of the table <em>m</em> on or above the main diagonal is used. The figure shows the table rotated to make the main diagonal run horizontally. The matrix chain is listed along the bottom. Using this layout, the minimum cost <em>m</em>[<em>i</em>, <em>j</em>] for multiplying a subchain <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> of matrices appears at the intersection of lines running northeast from <em>A<sub>i</sub></em> and northwest from <em>A<sub>j</sub></em>. Reading across, each diagonal in the table contains the entries for matrix chains of the same length. M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> computes the rows from bottom to top and from left to right within each row. It computes each entry <em>m</em>[<em>i</em>, <em>j</em>] using the products <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em> for <em>k</em> = <em>i</em>, <em>i</em> + 1, …, <em>j</em> − 1 and all entries southwest and southeast from <em>m</em>[<em>i</em>, <em>j</em>].</p>
<p>A simple inspection of the nested loop structure of M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> yields a running time of <em>O</em>(<em>n</em><sup>3</sup>) for the algorithm. The loops are nested three deep, and each loop index (<em>l</em>, <em>i</em>, and <em>k</em>) takes on at most <em>n</em> − 1 values. Exercise 14.2-5 asks you to show that the running time of this algorithm is in fact also Ω(<em>n</em><sup>3</sup>). The algorithm requires Θ(<em>n</em><sup>2</sup>) space to store the <em>m</em> and <em>s</em> tables. Thus, M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> is much more efficient than the exponential-time method of enumerating all possible parenthesizations and checking each one.</p>
<a id="p380"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-5"><img alt="art" class="width100" src="images/Art_P460.jpg"/></p>
<p class="caption"><strong>Figure 14.5</strong> The <em>m</em> and <em>s</em> tables computed by M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> for <em>n</em> = 6 and the following matrix dimensions:</p>
<table class="table1s">
<tr>
<td class="th2"><p class="center">matrix</p></td>
<td class="th1"><p class="center"><em>A</em><sub>1</sub></p></td>
<td class="th1"><p class="center"><em>A</em><sub>2</sub></p></td>
<td class="th1"><p class="center"><em>A</em><sub>3</sub></p></td>
<td class="th1"><p class="center"><em>A</em><sub>4</sub></p></td>
<td class="th1"><p class="center"><em>A</em><sub>5</sub></p></td>
<td class="th1"><p class="center"><em>A</em><sub>6</sub></p></td>
</tr>
<tr>
<td class="td1r"><p class="center">dimension</p></td>
<td class="td1"><p class="center">30 × 35</p></td>
<td class="td1"><p class="center">35 × 15</p></td>
<td class="td1"><p class="center">15 × 5</p></td>
<td class="td1"><p class="center">5 × 10</p></td>
<td class="td1"><p class="center">10 × 20</p></td>
<td class="td1"><p class="center">20 × 25</p></td>
</tr>
</table>
<p class="caption1">The tables are rotated so that the main diagonal runs horizontally. The <em>m</em> table uses only the main diagonal and upper triangle, and the <em>s</em> table uses only the upper triangle. The minimum number of scalar multiplications to multiply the 6 matrices is <em>m</em>[1, 6] = 15,125. Of the entries that are not tan, the pairs that have the same color are taken together in line 9 when computing</p>
<p class="eql"><img alt="art" src="images/Art_P461.jpg"/></p>
</div>
<p class="level4"><strong>Step 4: Constructing an optimal solution</strong></p>
<p class="noindent">Although M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> determines the optimal number of scalar multiplications needed to compute a matrix-chain product, it does not directly show how to multiply the matrices. The table <em>s</em>[1 : <em>n</em> − 1, 2 : <em>n</em>] provides the information needed to do so. Each entry <em>s</em>[<em>i</em>, <em>j</em>] records a value of <em>k</em> such that an optimal parenthesization of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> splits the product between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub>. The final matrix multiplication in computing <em>A</em><sub>1:<em>n</em></sub> optimally is <em>A</em><sub>1:<em>s</em>[1,<em>n</em>]</sub><em>A</em><sub><em>s</em>[1,<em>n</em>]+1:<em>n</em></sub>. The <em>s</em> table contains the information needed to determine the earlier matrix multiplications as well, using recursion: <em>s</em>[1, <em>s</em>[1, <em>n</em>]] determines the last matrix multiplication when computing <em>A</em><sub>1:<em>s</em>[1,<em>n</em>]</sub> and <em>s</em>[<em>s</em>[1,<em>n</em>] + 1, <em>n</em>] determines the last matrix multiplication when computing <em>A</em><sub><em>s</em>[1,<em>n</em>]+1:<em>n</em></sub>. The recursive procedure P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small> on the facing page prints an optimal parenthesization of the matrix chain product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, given the <em>s</em> table computed by M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> and the indices <a id="p381"/><em>i</em> and <em>j</em>. The initial call P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small>(<em>s</em>, 1, <em>n</em>) prints an optimal parenthesization of the full matrix chain product <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>n</sub></em>. In the example of <a href="chapter014.xhtml#Fig_14-5">Figure 14.5</a>, the call P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small>(<em>s</em>, 1, 6) prints the optimal parenthesization ((<em>A</em><sub>1</sub>(<em>A</em><sub>2</sub><em>A</em><sub>3</sub>))((<em>A</em><sub>4</sub><em>A</em><sub>5</sub>)<em>A</em><sub>6</sub>)).</p>
<div class="pull-quote1">
<p class="box-heading">P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small>(<em>s</em>, <em>i</em>, <em>j</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>if</strong> <em>i</em> == <em>j</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2">print “<em>A</em>”<em><sub>i</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><strong>else</strong> print “(”</td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2">P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small>(<em>s</em>, <em>i</em>, <em>s</em>[<em>i</em>, <em>j</em>])</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2">P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small>(<em>s</em>, <em>s</em>[<em>i</em>, <em>j</em>] + 1, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2">print “)”</p></td>
</tr>
</table>
</div>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>14.2-1</em></strong></p>
<p class="noindent">Find an optimal parenthesization of a matrix-chain product whose sequence of dimensions is <span class="font1">〈</span>5, 10, 3, 12, 5, 50, 6<span class="font1">〉</span>.</p>
<p class="level3"><strong><em>14.2-2</em></strong></p>
<p class="noindent">Give a recursive algorithm M<small>ATRIX</small>-C<small>HAIN</small>-M<small>ULTIPLY</small>(<em>A</em>, <em>s</em>, <em>i</em>, <em>j</em>) that actually performs the optimal matrix-chain multiplication, given the sequence of matrices <span class="font1">〈</span><em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, …, <em>A<sub>n</sub></em><span class="font1">〉</span>, the <em>s</em> table computed by M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>, and the indices <em>i</em> and <em>j</em>. (The initial call is M<small>ATRIX</small>-C<small>HAIN</small>-M<small>ULTIPLY</small>(<em>A</em>, <em>s</em>, 1, <em>n</em>).) Assume that the call R<small>ECTANGULAR</small>-M<small>ATRIX</small>-M<small>ULTIPLY</small>(<em>A</em>, <em>B</em>) returns the product of matrices <em>A</em> and <em>B</em>.</p>
<p class="level3"><strong><em>14.2-3</em></strong></p>
<p class="noindent">Use the substitution method to show that the solution to the recurrence (14.6) is Ω(2<em><sup>n</sup></em>).</p>
<p class="level3"><strong><em>14.2-4</em></strong></p>
<p class="noindent">Describe the subproblem graph for matrix-chain multiplication with an input chain of length <em>n</em>. How many vertices does it have? How many edges does it have, and which edges are they?</p>
<p class="level3"><strong><em>14.2-5</em></strong></p>
<p class="noindent">Let <em>R</em>(<em>i</em>, <em>j</em>) be the number of times that table entry <em>m</em>[<em>i</em>, <em>j</em>] is referenced while computing other table entries in a call of M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>. Show that the total number of references for the entire table is</p>
<a id="p382"/>
<p class="eql"><img alt="art" src="images/Art_P462.jpg"/></p>
<p class="noindent">(<em>Hint:</em> You may find equation (A.4) on page 1141 useful.)</p>
<p class="level3"><strong><em>14.2-6</em></strong></p>
<p class="noindent">Show that a full parenthesization of an <em>n</em>-element expression has exactly <em>n</em> − 1 pairs of parentheses.</p>
</section>
<p class="line1"/>
<section title="14.3 Elements of dynamic programming">
<a id="Sec_14.3"/>
<p class="level1" id="h1-83"><a href="toc.xhtml#Rh1-83"><strong>14.3    Elements of dynamic programming</strong></a></p>
<p class="noindent">Although you have just seen two complete examples of the dynamic-programming method, you might still be wondering just when the method applies. From an engineering perspective, when should you look for a dynamic-programming solution to a problem? In this section, we’ll examine the two key ingredients that an optimization problem must have in order for dynamic programming to apply: optimal substructure and overlapping subproblems. We’ll also revisit and discuss more fully how memoization might help you take advantage of the overlapping-subproblems property in a top-down recursive approach.</p>
<p class="level4"><strong>Optimal substructure</strong></p>
<p class="noindent">The first step in solving an optimization problem by dynamic programming is to characterize the structure of an optimal solution. Recall that a problem exhibits <strong><em><span class="blue1">optimal substructure</span></em></strong> if an optimal solution to the problem contains within it optimal solutions to subproblems. When a problem exhibits optimal substructure, that gives you a good clue that dynamic programming might apply. (As <a href="chapter015.xhtml">Chapter 15</a> discusses, it also might mean that a greedy strategy applies, however.) Dynamic programming builds an optimal solution to the problem from optimal solutions to subproblems. Consequently, you must take care to ensure that the range of subproblems you consider includes those used in an optimal solution.</p>
<p>Optimal substructure was key to solving both of the previous problems in this chapter. In <a href="chapter014.xhtml#Sec_14.1">Section 14.1</a>, we observed that the optimal way of cutting up a rod of length <em>n</em> (if Serling Enterprises makes any cuts at all) involves optimally cutting up the two pieces resulting from the first cut. In <a href="chapter014.xhtml#Sec_14.2">Section 14.2</a>, we noted that an optimal parenthesization of the matrix chain product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> that splits the product between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub> contains within it optimal solutions to the problems of parenthesizing <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>.</p>
<a id="p383"/>
<p>You will find yourself following a common pattern in discovering optimal substructure:</p>
<ol class="olnoindent" epub:type="list">
<li>You show that a solution to the problem consists of making a choice, such as choosing an initial cut in a rod or choosing an index at which to split the matrix chain. Making this choice leaves one or more subproblems to be solved.</li>
<li class="litop">You suppose that for a given problem, you are given the choice that leads to an optimal solution. You do not concern yourself yet with how to determine this choice. You just assume that it has been given to you.</li>
<li class="litop">Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems.</li>
<li class="litop">You show that the solutions to the subproblems used within an optimal solution to the problem must themselves be optimal by using a “cut-and-paste” technique. You do so by supposing that each of the subproblem solutions is not optimal and then deriving a contradiction. In particular, by “cutting out” the nonoptimal solution to each subproblem and “pasting in” the optimal one, you show that you can get a better solution to the original problem, thus contradicting your supposition that you already had an optimal solution. If an optimal solution gives rise to more than one subproblem, they are typically so similar that you can modify the cut-and-paste argument for one to apply to the others with little effort.</li></ol>
<p>To characterize the space of subproblems, a good rule of thumb says to try to keep the space as simple as possible and then expand it as necessary. For example, the space of subproblems for the rod-cutting problem contained the problems of optimally cutting up a rod of length <em>i</em> for each size <em>i</em>. This subproblem space worked well, and it was not necessary to try a more general space of subproblems.</p>
<p>Conversely, suppose that you tried to constrain the subproblem space for matrix-chain multiplication to matrix products of the form <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. As before, an optimal parenthesization must split this product between <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub> for some 1 ≤ <em>k</em> &lt; <em>j</em>. Unless you can guarantee that <em>k</em> always equals <em>j</em> − 1, you will find that you have subproblems of the form <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. Moreover, the latter subproblem does not have the form <em>A</em><sub>1</sub><em>A</em><sub>2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. To solve this problem by dynamic programming, you need to allow the subproblems to vary at “both ends.” That is, both <em>i</em> and <em>j</em> need to vary in the subproblem of parenthesizing the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>.</p>
<p>Optimal substructure varies across problem domains in two ways:</p>
<ol class="olnoindent" epub:type="list">
<li>how many subproblems an optimal solution to the original problem uses, and</li>
<li class="litop">how many choices you have in determining which subproblem(s) to use in an optimal solution.</li></ol>
<a id="p384"/>
<p class="noindent">In the rod-cutting problem, an optimal solution for cutting up a rod of size <em>n</em> uses just one subproblem (of size <em>n</em> − <em>i</em>), but we have to consider <em>n</em> choices for <em>i</em> in order to determine which one yields an optimal solution. Matrix-chain multiplication for the subchain <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> serves an example with two subproblems and <em>j</em> − <em>i</em> choices. For a given matrix <em>A<sub>k</sub></em> where the product splits, two subproblems arise—parenthesizing <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> and parenthesizing <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>—and we have to solve <em>both</em> of them optimally. Once we determine the optimal solutions to subproblems, we choose from among <em>j</em> − <em>i</em> candidates for the index <em>k</em>.</p>
<p>Informally, the running time of a dynamic-programming algorithm depends on the product of two factors: the number of subproblems overall and how many choices you look at for each subproblem. In rod cutting, we had Θ(<em>n</em>) subproblems overall, and at most <em>n</em> choices to examine for each, yielding an <em>O</em>(<em>n</em><sup>2</sup>) running time. Matrix-chain multiplication had Θ(<em>n</em><sup>2</sup>) subproblems overall, and each had at most <em>n</em> − 1 choices, giving an <em>O</em>(<em>n</em><sup>3</sup>) running time (actually, a Θ(<em>n</em><sup>3</sup>) running time, by Exercise 14.2-5).</p>
<p>Usually, the subproblem graph gives an alternative way to perform the same analysis. Each vertex corresponds to a subproblem, and the choices for a subproblem are the edges incident from that subproblem. Recall that in rod cutting, the subproblem graph has <em>n</em> vertices and at most <em>n</em> edges per vertex, yielding an <em>O</em>(<em>n</em><sup>2</sup>) running time. For matrix-chain multiplication, if you were to draw the subproblem graph, it would have Θ(<em>n</em><sup>2</sup>) vertices and each vertex would have degree at most <em>n</em> − 1, giving a total of <em>O</em>(<em>n</em><sup>3</sup>) vertices and edges.</p>
<p>Dynamic programming often uses optimal substructure in a bottom-up fashion. That is, you first find optimal solutions to subproblems and, having solved the subproblems, you find an optimal solution to the problem. Finding an optimal solution to the problem entails making a choice among subproblems as to which you will use in solving the problem. The cost of the problem solution is usually the subproblem costs plus a cost that is directly attributable to the choice itself. In rod cutting, for example, first we solved the subproblems of determining optimal ways to cut up rods of length <em>i</em> for <em>i</em> = 0, 1, …, <em>n</em> − 1, and then we determined which of these subproblems yielded an optimal solution for a rod of length <em>n</em>, using equation (14.2). The cost attributable to the choice itself is the term <em>p<sub>i</sub></em> in equation (14.2). In matrix-chain multiplication, we determined optimal parenthesizations of subchains of <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, and then we chose the matrix <em>A<sub>k</sub></em> at which to split the product. The cost attributable to the choice itself is the term <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em>.</p>
<p><a href="chapter015.xhtml">Chapter 15</a> explores “greedy algorithms,” which have many similarities to dynamic programming. In particular, problems to which greedy algorithms apply have optimal substructure. One major difference between greedy algorithms and dynamic programming is that instead of first finding optimal solutions to subproblems and then making an informed choice, greedy algorithms first make a “greedy” choice—the choice that looks best at the time—and then solve a resulting subproblem, <a id="p385"/>without bothering to solve all possible related smaller subproblems. Surprisingly, in some cases this strategy works!</p>
<p class="noindent-top"><strong><em>Subtleties</em></strong></p>
<p class="noindent">You should be careful not to assume that optimal substructure applies when it does not. Consider the following two problems whose input consists of a directed graph <em>G</em> = (<em>V</em>, <em>E</em>) and vertices <em>u</em>, <em>v</em> ∈ <em>V</em>.</p>
<p class="para-hang-top"><strong>Unweighted shortest path:</strong><sup><a epub:type="footnote" href="#footnote_5" id="footnote_ref_5">5</a></sup> Find a path from <em>u</em> to <em>v</em> consisting of the fewest edges. Such a path must be simple, since removing a cycle from a path produces a path with fewer edges.</p>
<p class="para-hang-top"><strong>Unweighted longest simple path:</strong> Find a simple path from <em>u</em> to <em>v</em> consisting of the most edges. (Without the requirement that the path must be simple, the problem is undefined, since repeatedly traversing a cycle creates paths with an arbitrarily large number of edges.)</p>
<p class="space-break">The unweighted shortest-path problem exhibits optimal substructure. Here’s how. Suppose that <em>u</em> ≠ <em>v</em>, so that the problem is nontrivial. Then, any path <em>p</em> from <em>u</em> to <em>v</em> must contain an intermediate vertex, say <em>w</em>. (Note that <em>w</em> may be <em>u</em> or <em>v</em>.) Then, we can decompose the path <img alt="art" src="images/upv.jpg"/> into subpaths <img alt="art" src="images/up1wp2v.jpg"/>. The number of edges in <em>p</em> equals the number of edges in <em>p</em><sub>1</sub> plus the number of edges in <em>p</em><sub>2</sub>. We claim that if <em>p</em> is an optimal (i.e., shortest) path from <em>u</em> to <em>v</em>, then <em>p</em><sub>1</sub> must be a shortest path from <em>u</em> to <em>w</em>. Why? As suggested earlier, use a “cut-and-paste” argument: if there were another path, say <img alt="art" src="images/psubsup1prime.jpg"/>, from <em>u</em> to <em>w</em> with fewer edges than <em>p</em><sub>1</sub>, then we could cut out <em>p</em><sub>1</sub> and paste in <img alt="art" src="images/psubsup1prime.jpg"/> to produce a path <img alt="art" src="images/uwvp1primep2.jpg"/> with fewer edges than <em>p</em>, thus contradicting <em>p</em>’s optimality. Likewise, <em>p</em><sub>2</sub> must be a shortest path from <em>w</em> to <em>v</em>. Thus, to find a shortest path from <em>u</em> to <em>v</em>, consider all intermediate vertices <em>w</em>, find a shortest path from <em>u</em> to <em>w</em> and a shortest path from <em>w</em> to <em>v</em>, and choose an intermediate vertex <em>w</em> that yields the overall shortest path. <a href="chapter023.xhtml#Sec_23.2">Section 23.2</a> uses a variant of this observation of optimal substructure to find a shortest path between every pair of vertices on a weighted, directed graph.</p>
<p>You might be tempted to assume that the problem of finding an unweighted longest simple path exhibits optimal substructure as well. After all, if we decompose a longest simple path <img alt="art" src="images/upv.jpg"/> into subpaths <img alt="art" src="images/up1wp2v.jpg"/>, then mustn’t <em>p</em><sub>1</sub> be a longest simple path from <em>u</em> to <em>w</em>, and mustn’t <em>p</em><sub>2</sub> be a longest simple path from <em>w</em> to <em>v</em>? The answer is no! <a href="chapter014.xhtml#Fig_14-6">Figure 14.6</a> supplies an example. Consider the <a id="p386"/>path <em>q</em> → <em>r</em> → <em>t</em>, which is a longest simple path from <em>q</em> to <em>t</em>. Is <em>q</em> → <em>r</em> a longest simple path from <em>q</em> to <em>r</em>? No, for the path <em>q</em> → <em>s</em> → <em>t</em> → <em>r</em> is a simple path that is longer. Is <em>r</em> → <em>t</em> a longest simple path from <em>r</em> to <em>t</em>? No again, for the path <em>r</em> → <em>q</em> → <em>s</em> → <em>t</em> is a simple path that is longer.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_14-6"><img alt="art" src="images/Art_P463.jpg"/></p>
<p class="caption"><strong>Figure 14.6</strong> A directed graph showing that the problem of finding a longest simple path in an unweighted directed graph does not have optimal substructure. The path <em>q</em> → <em>r</em> → <em>t</em> is a longest simple path from <em>q</em> to <em>t</em>, but the subpath <em>q</em> → <em>r</em> is not a longest simple path from <em>q</em> to <em>r</em>, nor is the subpath <em>r</em> → <em>t</em> a longest simple path from <em>r</em> to <em>t</em>.</p>
</div>
<p>This example shows that for longest simple paths, not only does the problem lack optimal substructure, but you cannot necessarily assemble a “legal” solution to the problem from solutions to subproblems. If you combine the longest simple paths <em>q</em> → <em>s</em> → <em>t</em> → <em>r</em> and <em>r</em> → <em>q</em> → <em>s</em> → <em>t</em>, you get the path <em>q</em> → <em>s</em> → <em>t</em> → <em>r</em> → <em>q</em> → <em>s</em> → <em>t</em>, which is not simple. Indeed, the problem of finding an unweighted longest simple path does not appear to have any sort of optimal substructure. No efficient dynamic-programming algorithm for this problem has ever been found. In fact, this problem is NP-complete, which—as we shall see in <a href="chapter034.xhtml">Chapter 34</a>—means that we are unlikely to find a way to solve it in polynomial time.</p>
<p>Why is the substructure of a longest simple path so different from that of a shortest path? Although a solution to a problem for both longest and shortest paths uses two subproblems, the subproblems in finding the longest simple path are not <strong><em><span class="blue1">independent</span></em></strong>, whereas for shortest paths they are. What do we mean by subproblems being independent? We mean that the solution to one subproblem does not affect the solution to another subproblem of the same problem. For the example of <a href="chapter014.xhtml#Fig_14-6">Figure 14.6</a>, we have the problem of finding a longest simple path from <em>q</em> to <em>t</em> with two subproblems: finding longest simple paths from <em>q</em> to <em>r</em> and from <em>r</em> to <em>t</em>. For the first of these subproblems, we chose the path <em>q</em> → <em>s</em> → <em>t</em> → <em>r</em>, which used the vertices <em>s</em> and <em>t</em>. These vertices cannot appear in a solution to the second subproblem, since the combination of the two solutions to subproblems yields a path that is not simple. If vertex <em>t</em> cannot be in the solution to the second problem, then there is no way to solve it, since <em>t</em> is required to be on the path that forms the solution, and it is not the vertex where the subproblem solutions are “spliced” together (that vertex being <em>r</em>). Because vertices <em>s</em> and <em>t</em> appear in one subproblem solution, they cannot appear in the other subproblem solution. One of them must be in the solution to the other subproblem, however, and an optimal solution requires both. <a id="p387"/>Thus, we say that these subproblems are not independent. Looked at another way, using resources in solving one subproblem (those resources being vertices) renders them unavailable for the other subproblem.</p>
<p>Why, then, are the subproblems independent for finding a shortest path? The answer is that by nature, the subproblems do not share resources. We claim that if a vertex <em>w</em> is on a shortest path <em>p</em> from <em>u</em> to <em>v</em>, then we can splice together <em>any</em> shortest path <img alt="art" src="images/up1w.jpg"/> and <em>any</em> shortest path <img alt="art" src="images/wp2v.jpg"/> to produce a shortest path from <em>u</em> to <em>v</em>. We are assured that, other than <em>w</em>, no vertex can appear in both paths <em>p</em><sub>1</sub> and <em>p</em><sub>2</sub>. Why? Suppose that some vertex <em>x</em> ≠ <em>w</em> appears in both <em>p</em><sub>1</sub> and <em>p</em><sub>2</sub>, so that we can decompose <em>p</em><sub>1</sub> as <img alt="art" src="images/upuxxw.jpg"/> and <em>p</em><sub>2</sub> as <img alt="art" src="images/wxpxvv.jpg"/>. By the optimal substructure of this problem, path <em>p</em> has as many edges as <em>p</em><sub>1</sub> and <em>p</em><sub>2</sub> together. Let’s say that <em>p</em> has <em>e</em> edges. Now let us construct a path <img alt="art" src="images/Art_P464.jpg"/> from <em>u</em> to <em>v</em>. Because we have excised the paths from <em>x</em> to <em>w</em> and from <em>w</em> to <em>x</em>, each of which contains at least one edge, path <em>p</em>′ contains at most <em>e</em> − 2 edges, which contradicts the assumption that <em>p</em> is a shortest path. Thus, we are assured that the subproblems for the shortest-path problem are independent.</p>
<p>The two problems examined in <a href="chapter014.xhtml#Sec_14.1">Sections 14.1</a> and <a href="chapter014.xhtml#Sec_14.2">14.2</a> have independent subproblems. In matrix-chain multiplication, the subproblems are multiplying subchains <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>k</sub></em> and <em>A</em><sub><em>k</em>+1</sub><em>A</em><sub><em>k</em>+2</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. These subchains are disjoint, so that no matrix could possibly be included in both of them. In rod cutting, to determine the best way to cut up a rod of length <em>n</em>, we looked at the best ways of cutting up rods of length <em>i</em> for <em>i</em> = 0, 1, …, <em>n</em> − 1. Because an optimal solution to the length-<em>n</em> problem includes just one of these subproblem solutions (after cutting off the first piece), independence of subproblems is not an issue.</p>
<p class="level4"><strong>Overlapping subproblems</strong></p>
<p class="noindent">The second ingredient that an optimization problem must have for dynamic programming to apply is that the space of subproblems must be “small” in the sense that a recursive algorithm for the problem solves the same subproblems over and over, rather than always generating new subproblems. Typically, the total number of distinct subproblems is a polynomial in the input size. When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problem has <strong><em><span class="blue1">overlapping subproblems</span></em></strong>.<sup><a epub:type="footnote" href="#footnote_6" id="footnote_ref_6">6</a></sup> In contrast, a problem for which a divide-and-conquer <a id="p388"/>approach is suitable usually generates brand-new problems at each step of the recursion. Dynamic-programming algorithms typically take advantage of overlapping subproblems by solving each subproblem once and then storing the solution in a table where it can be looked up when needed, using constant time per lookup.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_14-7"><img alt="art" src="images/Art_P465.jpg"/></p>
<p class="caption"><strong>Figure 14.7</strong> The recursion tree for the computation of R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, 1, 4). Each node contains the parameters <em>i</em> and <em>j</em>. The computations performed in a subtree shaded blue are replaced by a single table lookup in M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small>.</p>
</div>
<p>In <a href="chapter014.xhtml#Sec_14.1">Section 14.1</a>, we briefly examined how a recursive solution to rod cutting makes exponentially many calls to find solutions of smaller subproblems. The dynamic-programming solution reduces the running time from the exponential time of the recursive algorithm down to quadratic time.</p>
<p>To illustrate the overlapping-subproblems property in greater detail, let’s revisit the matrix-chain multiplication problem. Referring back to <a href="chapter014.xhtml#Fig_14-5">Figure 14.5</a>, observe that M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> repeatedly looks up the solution to subproblems in lower rows when solving subproblems in higher rows. For example, it references entry <em>m</em>[3, 4] four times: during the computations of <em>m</em>[2, 4], <em>m</em>[1, 4], <em>m</em>[3, 5], and <em>m</em>[3, 6]. If the algorithm were to recompute <em>m</em>[3, 4] each time, rather than just looking it up, the running time would increase dramatically. To see how, consider the inefficient recursive procedure R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small> on the facing page, which determines <em>m</em>[<em>i</em>, <em>j</em>], the minimum number of scalar multiplications needed to compute the matrix-chain product <em>A</em><sub><em>i</em>:<em>j</em></sub> = <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>. The procedure is based directly on the recurrence (14.7). <a href="chapter014.xhtml#Fig_14-7">Figure 14.7</a> shows the recursion tree produced by the call R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, 1, 4). Each node is labeled by the values of the parameters <em>i</em> and <em>j</em>. Observe that some pairs of values occur many times.</p>
<p>In fact, the time to compute <em>m</em>[1, <em>n</em>] by this recursive procedure is at least exponential in <em>n</em>. To see why, let <em>T</em>(<em>n</em>) denote the time taken by R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small> <a id="p389"/>to compute an optimal parenthesization of a chain of <em>n</em> matrices. Because the execution of lines 1–2 and of lines 6–7 each take at least unit time, as does the multiplication in line 5, inspection of the procedure yields the recurrence</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, <em>i</em>, <em>j</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>if</strong> <em>i</em> == <em>j</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>return</strong> 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><em>m</em>[<em>i</em>, <em>j</em>] = ∞</td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><strong>for</strong> <em>k</em> = <em>i</em> <strong>to</strong> <em>j</em> − 1</td>
</tr>
<tr>
<td class="td1" rowspan="3"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><em>q</em> = R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, <em>i</em>, <em>k</em>)</p></td>
</tr>
<tr>
<td class="td1"><p class="p3">+ R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, <em>k</em> + 1, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1"><p class="p3">+ <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>q</em> &lt; <em>m</em>[<em>i</em>, <em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p3"><em>m</em>[<em>i</em>, <em>j</em>] = <em>q</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><strong>return</strong> <em>m</em>[<em>i</em>, <em>j</em>]</td>
</tr>
</table>
</div>
<p class="eql"><img alt="art" src="images/Art_P466.jpg"/></p>
<p class="noindent">Noting that for <em>i</em> = 1, 2, …, <em>n</em> − 1, each term <em>T</em>(<em>i</em>) appears once as <em>T</em>(<em>k</em>) and once as <em>T</em>(<em>n</em> − <em>k</em>), and collecting the <em>n</em> − 1 1s in the summation together with the 1 out front, we can rewrite the recurrence as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P467.jpg"/></p>
<p>Let’s prove that <em>T</em>(<em>n</em>) = Ω(2<em><sup>n</sup></em>) using the substitution method. Specifically, we’ll show that <em>T</em>(<em>n</em>) ≥ 2<sup><em>n</em>−1</sup> for all <em>n</em> ≥ 1. For the base case <em>n</em> = 1, the summation is empty, and we get <em>T</em>(1) ≥ 1 = 2<sup>0</sup>. Inductively, for <em>n</em> ≥ 2 we have</p>
<p class="eql"><img alt="art" src="images/Art_P468.jpg"/></p>
<a id="p390"/>
<p class="noindent">which completes the proof. Thus, the total amount of work performed by the call R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, 1, <em>n</em>) is at least exponential in <em>n</em>.</p>
<p>Compare this top-down, recursive algorithm (without memoization) with the bottom-up dynamic-programming algorithm. The latter is more efficient because it takes advantage of the overlapping-subproblems property. Matrix-chain multiplication has only Θ(<em>n</em><sup>2</sup>) distinct subproblems, and the dynamic-programming algorithm solves each exactly once. The recursive algorithm, on the other hand, must solve each subproblem every time it reappears in the recursion tree. Whenever a recursion tree for the natural recursive solution to a problem contains the same subproblem repeatedly, and the total number of distinct subproblems is small, dynamic programming can improve efficiency, sometimes dramatically.</p>
<p class="level4"><strong>Reconstructing an optimal solution</strong></p>
<p class="noindent">As a practical matter, you’ll often want to store in a separate table which choice you made in each subproblem so that you do not have to reconstruct this information from the table of costs.</p>
<p>For matrix-chain multiplication, the table <em>s</em>[<em>i</em>, <em>j</em>] saves a significant amount of work when we need to reconstruct an optimal solution. Suppose that the M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> procedure on page 378 did not maintain the <em>s</em>[<em>i</em>, <em>j</em>] table, so that it filled in only the table <em>m</em>[<em>i</em>, <em>j</em>] containing optimal subproblem costs. The procedure chooses from among <em>j</em> − <em>i</em> possibilities when determining which subproblems to use in an optimal solution to parenthesizing <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, and <em>j</em> − <em>i</em> is not a constant. Therefore, it would take Θ(<em>j</em> −<em>i</em>) = <em>ω</em>(1) time to reconstruct which subproblems it chose for a solution to a given problem. Because M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> stores in <em>s</em>[<em>i</em>, <em>j</em>] the index of the matrix at which it split the product <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em>, the P<small>RINT</small>-O<small>PTIMAL</small>-P<small>ARENS</small> procedure on page 381 can look up each choice in <em>O</em>(1) time.</p>
<p class="level4"><strong>Memoization</strong></p>
<p class="noindent">As we saw for the rod-cutting problem, there is an alternative approach to dynamic programming that often offers the efficiency of the bottom-up dynamic-programming approach while maintaining a top-down strategy. The idea is to <strong><em><span class="blue1">memoize</span></em></strong> the natural, but inefficient, recursive algorithm. As in the bottom-up approach, you maintain a table with subproblem solutions, but the control structure for filling in the table is more like the recursive algorithm.</p>
<p>A memoized recursive algorithm maintains an entry in a table for the solution to each subproblem. Each table entry initially contains a special value to indicate that the entry has yet to be filled in. When the subproblem is first encountered as the recursive algorithm unfolds, its solution is computed and then stored in the table. <a id="p391"/>Each subsequent encounter of this subproblem simply looks up the value stored in the table and returns it.<sup><a epub:type="footnote" href="#footnote_7" id="footnote_ref_7">7</a></sup></p>
<p>The procedure M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small> is a memoized version of the procedure R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small> on page 389. Note where it resembles the memoized top-down method on page 369 for the rod-cutting problem.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small>(<em>p</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1">let <em>m</em>[1 : <em>n</em>, 1 : <em>n</em>] be a new table</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = <em>i</em> <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p3"><em>m</em>[<em>i</em>, <em>j</em>] = ∞</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><strong>return</strong> L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, 1, <em>n</em>)</td>
</tr>
</table>
<p class="box-headinga">L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, <em>i</em>, <em>j</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>if</strong> <em>m</em>[<em>i</em>, <em>j</em>] &lt; ∞</td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>return</strong> <em>m</em>[<em>i</em>, <em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><strong>if</strong> <em>i</em> == <em>j</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>m</em>[<em>i</em>, <em>j</em>] = 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><strong>else for</strong> <em>k</em> = <em>i</em> <strong>to</strong> <em>j</em> − 1</td>
</tr>
<tr>
<td class="td1" rowspan="2"><span class="x-small">6</span></td>
<td class="td1"><p class="p3"><em>q</em> = L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, <em>i</em>, <em>k</em>)</p></td>
</tr>
<tr>
<td class="td1"><p class="p4">+ L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, <em>k</em> + 1, <em>j</em>) + <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p3"><strong>if</strong> <em>q</em> &lt; <em>m</em>[<em>i</em>, <em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><p class="p4"><em>m</em>[<em>i</em>, <em>j</em>] = <em>q</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">9</span></td>
<td class="td1"><strong>return</strong> <em>m</em>[<em>i</em>, <em>j</em>]</td>
</tr>
</table>
</div>
<p>The M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small> procedure, like the bottom-up M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> procedure on page 378, maintains a table <em>m</em>[1 : <em>n</em>, 1 : <em>n</em>] of computed values of <em>m</em>[<em>i</em>, <em>j</em>], the minimum number of scalar multiplications needed to compute the matrix <em>A</em><sub><em>i</em>:<em>j</em></sub>. Each table entry initially contains the value ∞ to indicate that the entry has yet to be filled in. Upon calling L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, <em>i</em>, <em>j</em>), if line 1 finds that <em>m</em>[<em>i</em>, <em>j</em>] &lt; ∞, then the procedure simply returns the previously computed cost <em>m</em>[<em>i</em>, <em>j</em>] in line 2. Otherwise, the cost is computed as in R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>, stored in <em>m</em>[<em>i</em>, <em>j</em>], and returned. Thus, L<small>OOKUP</small>-C<small>HAIN</small>(<em>m</em>, <em>p</em>, <em>i</em>, <em>j</em>) always returns the value of <em>m</em>[<em>i</em>, <em>j</em>], but it computes it only upon the first call of L<small>OOKUP</small>-C<small>HAIN</small> with these specific values of <em>i</em> and <em>j</em>. <a id="p392"/><a href="chapter014.xhtml#Fig_14-7">Figure 14.7</a> illustrates how M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small> saves time compared with R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>. Subtrees shaded blue represent values that are looked up rather than recomputed.</p>
<p>Like the bottom-up procedure M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>, the memoized procedure M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small> runs in <em>O</em>(<em>n</em><sup>3</sup>) time. To begin with, line 4 of M<small>EMOIZED</small>-M<small>ATRIX</small>-C<small>HAIN</small> executes Θ(<em>n</em><sup>2</sup>) times, which dominates the running time outside of the call to L<small>OOKUP</small>-C<small>HAIN</small> in line 5. We can categorize the calls of L<small>OOKUP</small>-C<small>HAIN</small> into two types:</p>
<ol class="olnoindent" epub:type="list">
<li>calls in which <em>m</em>[<em>i</em>, <em>j</em>] = ∞, so that lines 3–9 execute, and</li>
<li class="litop">calls in which <em>m</em>[<em>i</em>, <em>j</em>] &lt; ∞, so that L<small>OOKUP</small>-C<small>HAIN</small> simply returns in line 2.</li></ol>
<p class="noindent">There are Θ(<em>n</em><sup>2</sup>) calls of the first type, one per table entry. All calls of the second type are made as recursive calls by calls of the first type. Whenever a given call of L<small>OOKUP</small>-C<small>HAIN</small> makes recursive calls, it makes <em>O</em>(<em>n</em>) of them. Therefore, there are <em>O</em>(<em>n</em><sup>3</sup>) calls of the second type in all. Each call of the second type takes <em>O</em>(1) time, and each call of the first type takes <em>O</em>(<em>n</em>) time plus the time spent in its recursive calls. The total time, therefore, is <em>O</em>(<em>n</em><sup>3</sup>). Memoization thus turns an Ω(2<em><sup>n</sup></em>)-time algorithm into an <em>O</em>(<em>n</em><sup>3</sup>)-time algorithm.</p>
<p>We have seen how to solve the matrix-chain multiplication problem by either a top-down, memoized dynamic-programming algorithm or a bottom-up dynamic-programming algorithm in <em>O</em>(<em>n</em><sup>3</sup>) time. Both the bottom-up and memoized methods take advantage of the overlapping-subproblems property. There are only Θ(<em>n</em><sup>2</sup>) distinct subproblems in total, and either of these methods computes the solution to each subproblem only once. Without memoization, the natural recursive algorithm runs in exponential time, since solved subproblems are repeatedly solved.</p>
<p>In general practice, if all subproblems must be solved at least once, a bottom-up dynamic-programming algorithm usually outperforms the corresponding top-down memoized algorithm by a constant factor, because the bottom-up algorithm has no overhead for recursion and less overhead for maintaining the table. Moreover, for some problems you can exploit the regular pattern of table accesses in the dynamic-programming algorithm to reduce time or space requirements even further. On the other hand, in certain situations, some of the subproblems in the subproblem space might not need to be solved at all. In that case, the memoized solution has the advantage of solving only those subproblems that are definitely required.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>14.3-1</em></strong></p>
<p class="noindent">Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing <a id="p393"/>the product and computing the number of multiplications for each, or running R<small>ECURSIVE</small>-M<small>ATRIX</small>-C<small>HAIN</small>? Justify your answer.</p>
<p class="level3"><strong><em>14.3-2</em></strong></p>
<p class="noindent">Draw the recursion tree for the M<small>ERGE</small>-S<small>ORT</small> procedure from <a href="chapter002.xhtml#Sec_2.3.1">Section 2.3.1</a> on an array of 16 elements. Explain why memoization fails to speed up a good divide-and-conquer algorithm such as M<small>ERGE</small>-S<small>ORT</small>.</p>
<p class="level3"><strong><em>14.3-3</em></strong></p>
<p class="noindent">Consider the antithetical variant of the matrix-chain multiplication problem where the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure?</p>
<p class="level3"><strong><em>14.3-4</em></strong></p>
<p class="noindent">As stated, in dynamic programming, you first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that she does not always need to solve all the subproblems in order to find an optimal solution. She suggests that she can find an optimal solution to the matrix-chain multiplication problem by always choosing the matrix <em>A<sub>k</sub></em> at which to split the subproduct <em>A<sub>i</sub>A</em><sub><em>i</em>+1</sub> <span class="font1">⋯</span> <em>A<sub>j</sub></em> (by selecting <em>k</em> to minimize the quantity <em>p</em><sub><em>i</em>−1</sub> <em>p<sub>k</sub> p<sub>j</sub></em>) before solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution.</p>
<p class="level3"><strong><em>14.3-5</em></strong></p>
<p class="noindent">Suppose that the rod-cutting problem of <a href="chapter014.xhtml#Sec_14.1">Section 14.1</a> also had a limit <em>l<sub>i</sub></em> on the number of pieces of length <em>i</em> allowed to be produced, for <em>i</em> = 1, 2, …, <em>n</em>. Show that the optimal-substructure property described in <a href="chapter014.xhtml#Sec_14.1">Section 14.1</a> no longer holds.</p>
</section>
<p class="line1"/>
<section title="14.4 Longest common subsequence">
<a id="Sec_14.4"/>
<p class="level1" id="h1-84"><a href="toc.xhtml#Rh1-84"><strong>14.4    Longest common subsequence</strong></a></p>
<p class="noindent">Biological applications often need to compare the DNA of two (or more) different organisms. A strand of DNA consists of a string of molecules called <strong><em><span class="blue1">bases</span></em></strong>, where the possible bases are adenine, cytosine, guanine, and thymine. Representing each of these bases by its initial letter, we can express a strand of DNA as a string over the 4-element set {<span class="courierfont">A</span>, <span class="courierfont">C</span>, <span class="courierfont">G</span>, <span class="courierfont">T</span>}. (See <a href="appendix003.xhtml#Sec_C.1">Section C.1</a> for the definition of a string.) For example, the DNA of one organism may be <em>S</em><sub>1</sub> = <span class="courierfont">ACCGGTCGAGTGCGCGGAAGCCGGCCGAA</span>, and the DNA of another organism may be <em>S</em><sub>2</sub> = <span class="courierfont">GTCGTTCGGAATGCCGTTGCTCTGTAAA</span>. One reason to compare <a id="p394"/>two strands of DNA is to determine how “similar” the two strands are, as some measure of how closely related the two organisms are. We can, and do, define similarity in many different ways. For example, we can say that two DNA strands are similar if one is a substring of the other. (<a href="chapter032.xhtml">Chapter 32</a> explores algorithms to solve this problem.) In our example, neither <em>S</em><sub>1</sub> nor <em>S</em><sub>2</sub> is a substring of the other. Alternatively, we could say that two strands are similar if the number of changes needed to turn one into the other is small. (Problem 14-5 looks at this notion.) Yet another way to measure the similarity of strands <em>S</em><sub>1</sub> and <em>S</em><sub>2</sub> is by finding a third strand <em>S</em><sub>3</sub> in which the bases in <em>S</em><sub>3</sub> appear in each of <em>S</em><sub>1</sub> and <em>S</em><sub>2</sub>. These bases must appear in the same order, but not necessarily consecutively. The longer the strand <em>S</em><sub>3</sub> we can find, the more similar <em>S</em><sub>1</sub> and <em>S</em><sub>2</sub> are. In our example, the longest strand <em>S</em><sub>3</sub> is <span class="courierfont">GTCGTCGGAAGCCGGCCGAA</span>.</p>
<p>We formalize this last notion of similarity as the longest-common-subsequence problem. A subsequence of a given sequence is just the given sequence with 0 or more elements left out. Formally, given a sequence <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span>, another sequence <em>Z</em> = <span class="font1">〈</span><em>z</em><sub>1</sub>, <em>z</em><sub>2</sub>, …, <em>z<sub>k</sub></em><span class="font1">〉</span> is a <strong><em><span class="blue1">subsequence</span></em></strong> of <em>X</em> if there exists a strictly increasing sequence <span class="font1">〈</span><em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, …, <em>i<sub>k</sub></em><span class="font1">〉</span> of indices of <em>X</em> such that for all <em>j</em> = 1, 2, …, <em>k</em>, we have <img alt="art" src="images/Art_P469.jpg"/>. For example, <em>Z</em> = <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>D</em>, <em>B</em><span class="font1">〉</span> is a subsequence of <em>X</em> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span> with corresponding index sequence <span class="font1">〈</span>2, 3, 5, 7<span class="font1">〉</span>.</p>
<p>Given two sequences <em>X</em> and <em>Y</em>, we say that a sequence <em>Z</em> is a <strong><em><span class="blue1">common subsequence</span></em></strong> of <em>X</em> and <em>Y</em> if <em>Z</em> is a subsequence of both <em>X</em> and <em>Y</em>. For example, if <em>X</em> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>B</em>, <em>D</em>, <em>C</em>, <em>A</em>, <em>B</em>, <em>A</em><span class="font1">〉</span>, the sequence <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>A</em><span class="font1">〉</span> is a common subsequence of both <em>X</em> and <em>Y</em>. The sequence <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>A</em><span class="font1">〉</span> is not a <em>longest</em> common subsequence (<strong><em><span class="blue1">LCS</span></em></strong>) of <em>X</em> and <em>Y</em>, however, since it has length 3 and the sequence <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>B</em>, <em>A</em><span class="font1">〉</span>, which is also common to both sequences <em>X</em> and <em>Y</em>, has length 4. The sequence <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>B</em>, <em>A</em><span class="font1">〉</span> is an LCS of <em>X</em> and <em>Y</em>, as is the sequence <span class="font1">〈</span><em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span>, since <em>X</em> and <em>Y</em> have no common subsequence of length 5 or greater.</p>
<p>In the <strong><em><span class="blue1">longest-common-subsequence problem</span></em></strong>, the input is two sequences <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span>, and the goal is to find a maximum-length common subsequence of <em>X</em> and <em>Y</em>. This section shows how to efficiently solve the LCS problem using dynamic programming.</p>
<p class="level4"><strong>Step 1: Characterizing a longest common subsequence</strong></p>
<p class="noindent">You can solve the LCS problem with a brute-force approach: enumerate all subsequences of <em>X</em> and check each subsequence to see whether it is also a subsequence of <em>Y</em>, keeping track of the longest subsequence you find. Each subsequence of <em>X</em> corresponds to a subset of the indices {1, 2, …, <em>m</em>} of <em>X</em>. Because <em>X</em> has 2<em><sup>m</sup></em> subsequences, this approach requires exponential time, making it impractical for long sequences.</p>
<a id="p395"/>
<p>The LCS problem has an optimal-substructure property, however, as the following theorem shows. As we’ll see, the natural classes of subproblems correspond to pairs of “prefixes” of the two input sequences. To be precise, given a sequence <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span>, we define the <em>i</em>th <strong><em><span class="blue1">prefix</span></em></strong> of <em>X</em>, for <em>i</em> = 0, 1, …, <em>m</em>, as <em>X<sub>i</sub></em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>i</sub></em><span class="font1">〉</span>. For example, if <em>X</em> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span>, then <em>X</em><sub>4</sub> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em><span class="font1">〉</span> and <em>X</em><sub>0</sub> is the empty sequence.</p>
<p class="theo"><strong><em>Theorem 14.1 (Optimal substructure of an LCS)</em></strong></p>
<p class="noindent">Let <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span> be sequences, and let <em>Z</em> = <span class="font1">〈</span><em>z</em><sub>1</sub>, <em>z</em><sub>2</sub>, …, <em>z<sub>k</sub></em><span class="font1">〉</span> be any LCS of <em>X</em> and <em>Y</em>.</p>
<ol class="olnoindent" epub:type="list">
<li>If <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em>, then <em>z<sub>k</sub></em> = <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em> and <em>Z</em><sub><em>k</em>−1</sub> is an LCS of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em><sub><em>n</em>−1</sub>.</li>
<li class="litop">If <em>x<sub>m</sub></em> ≠ <em>y<sub>n</sub></em> and <em>z<sub>k</sub></em> ≠ <em>x<sub>m</sub></em>, then <em>Z</em> is an LCS of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em>.</li>
<li class="litop">If <em>x<sub>m</sub></em> ≠ <em>y<sub>n</sub></em> and <em>z<sub>k</sub></em> ≠ <em>y<sub>n</sub></em>, then <em>Z</em> is an LCS of <em>X</em> and <em>Y</em><sub><em>n</em>−1</sub>.</li></ol>
<p class="proof"><strong><em>Proof</em></strong>   (1) If <em>z<sub>k</sub></em> ≠ <em>x<sub>m</sub></em>, then we could append <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em> to <em>Z</em> to obtain a common subsequence of <em>X</em> and <em>Y</em> of length <em>k</em> + 1, contradicting the supposition that <em>Z</em> is a <em>longest</em> common subsequence of <em>X</em> and <em>Y</em>. Thus, we must have <em>z<sub>k</sub></em> = <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em>. Now, the prefix <em>Z</em><sub><em>k</em>−1</sub> is a length-(<em>k</em> − 1) common subsequence of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em><sub><em>n</em>−1</sub>. We wish to show that it is an LCS. Suppose for the purpose of contradiction that there exists a common subsequence <em>W</em> of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em><sub><em>n</em>−1</sub> with length greater than <em>k</em> − 1. Then, appending <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em> to <em>W</em> produces a common subsequence of <em>X</em> and <em>Y</em> whose length is greater than <em>k</em>, which is a contradiction.</p>
<p>(2) If <em>z<sub>k</sub></em> ≠ <em>x<sub>m</sub></em>, then <em>Z</em> is a common subsequence of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em>. If there were a common subsequence <em>W</em> of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em> with length greater than <em>k</em>, then <em>W</em> would also be a common subsequence of <em>X<sub>m</sub></em> and <em>Y</em>, contradicting the assumption that <em>Z</em> is an LCS of <em>X</em> and <em>Y</em>.</p>
<p>(3) The proof is symmetric to (2).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The way that Theorem 14.1 characterizes longest common subsequences says that an LCS of two sequences contains within it an LCS of prefixes of the two sequences. Thus, the LCS problem has an optimal-substructure property. A recursive solution also has the overlapping-subproblems property, as we’ll see in a moment.</p>
<p class="level4"><strong>Step 2: A recursive solution</strong></p>
<p class="noindent">Theorem 14.1 implies that you should examine either one or two subproblems when finding an LCS of <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span>. If <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em>, you need to find an LCS of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em><sub><em>n</em>−1</sub>. Appending <em>x<sub>m</sub></em> = <em>y<sub>n</sub></em> to this LCS yields an LCS of <em>X</em> and <em>Y</em>. If <em>x<sub>m</sub></em> ≠ <em>y<sub>n</sub></em>, then you have to solve two subproblems: finding an LCS of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em> and finding an LCS of <em>X</em> and <em>Y</em><sub><em>n</em>−1</sub>. <a id="p396"/>Whichever of these two LCSs is longer is an LCS of <em>X</em> and <em>Y</em>. Because these cases exhaust all possibilities, one of the optimal subproblem solutions must appear within an LCS of <em>X</em> and <em>Y</em>.</p>
<p>The LCS problem has the overlapping-subproblems property. Here’s how. To find an LCS of <em>X</em> and <em>Y</em>, you might need to find the LCSs of <em>X</em> and <em>Y</em><sub><em>n</em>−1</sub> and of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em>. But each of these subproblems has the subsubproblem of finding an LCS of <em>X</em><sub><em>m</em>−1</sub> and <em>Y</em><sub><em>n</em>−1</sub>. Many other subproblems share subsubproblems.</p>
<p>As in the matrix-chain multiplication problem, solving the LCS problem recursively involves establishing a recurrence for the value of an optimal solution. Let’s define <em>c</em>[<em>i</em>, <em>j</em>] to be the length of an LCS of the sequences <em>X<sub>i</sub></em> and <em>Y<sub>j</sub></em>. If either <em>i</em> = 0 or <em>j</em> = 0, one of the sequences has length 0, and so the LCS has length 0. The optimal substructure of the LCS problem gives the recursive formula</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P470.jpg"/></p>
<p>In this recursive formulation, a condition in the problem restricts which subproblems to consider. When <em>x<sub>i</sub></em> = <em>y<sub>j</sub></em>, you can and should consider the subproblem of finding an LCS of <em>X</em><sub><em>i</em>−1</sub> and <em>Y</em><sub><em>j</em>−1</sub>. Otherwise, you instead consider the two subproblems of finding an LCS of <em>X<sub>i</sub></em> and <em>Y</em><sub><em>j</em>−1</sub> and of <em>X</em><sub><em>i</em>−1</sub> and <em>Y<sub>j</sub></em>. In the previous dynamic-programming algorithms we have examined—for rod cutting and matrix-chain multiplication—we didn’t rule out any subproblems due to conditions in the problem. Finding an LCS is not the only dynamic-programming algorithm that rules out subproblems based on conditions in the problem. For example, the edit-distance problem (see Problem 14-5) has this characteristic.</p>
<p class="level4"><strong>Step 3: Computing the length of an LCS</strong></p>
<p class="noindent">Based on equation (14.9), you could write an exponential-time recursive algorithm to compute the length of an LCS of two sequences. Since the LCS problem has only Θ(<em>mn</em>) distinct subproblems (computing <em>c</em>[<em>i</em>, <em>j</em>] for 0 ≤ <em>i</em> ≤ <em>m</em> and 0 ≤ <em>j</em> ≤ <em>n</em>), dynamic programming can compute the solutions bottom up.</p>
<p>The procedure LCS-L<small>ENGTH</small> on the next page takes two sequences <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>m</em></sub><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span> as inputs, along with their lengths. It stores the <em>c</em>[<em>i</em>, <em>j</em>] values in a table <em>c</em>[0 : <em>m</em>, 0 : <em>n</em>], and it computes the entries in <strong><em><span class="blue1">row-major</span></em></strong> order. That is, the procedure fills in the first row of <em>c</em> from left to right, then the second row, and so on. The procedure also maintains the table <em>b</em>[1 : <em>m</em>, 1 : <em>n</em>] to help in constructing an optimal solution. Intuitively, <em>b</em>[<em>i</em>, <em>j</em>] points to the table entry corresponding to the optimal subproblem solution chosen when computing <em>c</em>[<em>i</em>, <em>j</em>]. The procedure returns the <em>b</em> and <em>c</em> tables, where <em>c</em>[<em>m</em>, <em>n</em>] contains the length of an LCS of <em>X</em> and <em>Y</em>. <a href="chapter014.xhtml#Fig_14-8">Figure 14.8</a> shows the tables produced by LCS-L<small>ENGTH</small> on the <a id="p397"/>sequences <em>X</em> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>B</em>, <em>D</em>, <em>C</em>, <em>A</em>, <em>B</em>, <em>A</em><span class="font1">〉</span>. The running time of the procedure is Θ(<em>mn</em>), since each table entry takes Θ(1) time to compute.</p>
<div class="pull-quote1">
<p class="box-heading">LCS-L<small>ENGTH</small>(<em>X</em>, <em>Y</em>, <em>m</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="2">let <em>b</em>[1 : <em>m</em>, 1 : <em>n</em>] and <em>c</em>[0 : <em>m</em>, 0 : <em>n</em>] be new tables</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>m</em></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="p2"><em>c</em>[<em>i</em>, 0] = 0</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><strong>for</strong> <em>j</em> = 0 <strong>to</strong> <em>n</em></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2"><em>c</em>[0, <em>j</em>] = 0</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>m</em></td>
<td class="td1"><span class="red"><strong>//</strong> compute table entries in row-major order</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2">
<p class="p2"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1" colspan="2">
<p class="p3"><strong>if</strong> <em>x<sub>i</sub></em> == <em>y<sub>j</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>c</em>[<em>i</em>, <em>j</em>] = <em>c</em>[<em>i</em> − 1, <em>j</em> − 1] + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>b</em>[<em>i</em>, <em>j</em>] = “<span class="font1">↖</span>”</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1" colspan="2">
<p class="p3"><strong>elseif</strong> <em>c</em>[<em>i</em> − 1, <em>j</em>] ≥ <em>c</em>[<em>i</em>, <em>j</em> − 1]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>c</em>[<em>i</em>, <em>j</em>] = <em>c</em>[<em>i</em> − 1, <em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>b</em>[<em>i</em>, <em>j</em>] = “↑”</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="2">
<p class="p3"><strong>else</strong> <em>c</em>[<em>i</em>, <em>j</em>] = <em>c</em>[<em>i</em>, <em>j</em> − 1]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1" colspan="2">
<p class="p4"><em>b</em>[<em>i</em>, <em>j</em>] = “←”</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">16</span></td>
<td class="td1" colspan="2"><strong>return</strong> <em>c</em> and <em>b</em></td>
</tr>
<tr>
<td class="td1" colspan="2"><p class="box-headinga">P<small>RINT</small>-LCS(<em>b</em>, <em>X</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="2"><strong>if</strong> <em>i</em> == 0 or <em>j</em> == 0</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="p2"><strong>return</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> the LCS has length 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1" colspan="2"><strong>if</strong> <em>b</em>[<em>i</em>, <em>j</em>] == “<span class="font1">↖</span>”</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1" colspan="2">
<p class="p2">P<small>RINT</small>-LCS(<em>b</em>, <em>X</em>, <em>i</em> − 1, <em>j</em> − 1)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2">print <em>x<sub>i</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> same as <em>y<sub>j</sub></em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1" colspan="2"><strong>elseif</strong> <em>b</em>[<em>i</em>, <em>j</em>] == “↑”</td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2">
<p class="p2">P<small>RINT</small>-LCS(<em>b</em>, <em>X</em>, <em>i</em> − 1, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1" colspan="2"><strong>else</strong> P<small>RINT</small>-LCS(<em>b</em>, <em>X</em>, <em>i</em>, <em>j</em> − 1)</td>
</tr>
</table>
</div>
<p class="level4"><strong>Step 4: Constructing an LCS</strong></p>
<p class="noindent">With the <em>b</em> table returned by LCS-L<small>ENGTH</small>, you can quickly construct an LCS of <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span>. Begin at <em>b</em>[<em>m</em>, <em>n</em>] and trace through the table by following the arrows. Each “<span class="font1">↖</span>” encountered in an entry <em>b</em>[<em>i</em>, <em>j</em>] implies that <em>x<sub>i</sub></em> = <em>y<sub>j</sub></em> is an element of the LCS that LCS-L<small>ENGTH</small> found. This method gives you the elements of this LCS in reverse order. The recursive procedure P<small>RINT</small>-LCS prints out an LCS of <em>X</em> and <em>Y</em> in the proper, forward order.</p>
<a id="p398"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-8"><img alt="art" src="images/Art_P471.jpg"/></p>
<p class="caption"><strong>Figure 14.8</strong> The <em>c</em> and <em>b</em> tables computed by LCS-L<small>ENGTH</small> on the sequences <em>X</em> = <span class="font1">〈</span><em>A</em>, <em>B</em>, <em>C</em>, <em>B</em>, <em>D</em>, <em>A</em>, <em>B</em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>B</em>, <em>D</em>, <em>C</em>, <em>A</em>, <em>B</em>, <em>A</em><span class="font1">〉</span>. The square in row <em>i</em> and column <em>j</em> contains the value of <em>c</em>[<em>i</em>, <em>j</em>] and the appropriate arrow for the value of <em>b</em>[<em>i</em>, <em>j</em>]. The entry 4 in <em>c</em>[7, 6]—the lower right-hand corner of the table—is the length of an LCS <span class="font1">〈</span><em>B</em>, <em>C</em>, <em>B</em>, <em>A</em><span class="font1">〉</span> of <em>X</em> and <em>Y</em>. For <em>i</em>, <em>j</em> &gt; 0, entry <em>c</em>[<em>i</em>, <em>j</em>] depends only on whether <em>x<sub>i</sub></em> = <em>y<sub>j</sub></em> and the values in entries <em>c</em>[<em>i</em> − 1, <em>j</em>], <em>c</em>[<em>i</em>, <em>j</em> − 1], and <em>c</em>[<em>i</em> − 1, <em>j</em> − 1], which are computed before <em>c</em>[<em>i</em>, <em>j</em>]. To reconstruct the elements of an LCS, follow the <em>b</em>[<em>i</em>, <em>j</em>] arrows from the lower right-hand corner, as shown by the sequence shaded blue. Each “<span class="font1">↖</span>” on the shaded-blue sequence corresponds to an entry (highlighted) for which <em>x<sub>i</sub></em> = <em>y<sub>j</sub></em> is a member of an LCS.</p>
</div>
<p class="noindent">The initial call is P<small>RINT</small>-LCS(<em>b</em>, <em>X</em>, <em>m</em>, <em>n</em>). For the <em>b</em> table in <a href="chapter014.xhtml#Fig_14-8">Figure 14.8</a>, this procedure prints <em>BCBA</em>. The procedure takes <em>O</em>(<em>m</em> + <em>n</em>) time, since it decrements at least one of <em>i</em> and <em>j</em> in each recursive call.</p>
<p class="level4"><strong>Improving the code</strong></p>
<p class="noindent">Once you have developed an algorithm, you will often find that you can improve on the time or space it uses. Some changes can simplify the code and improve constant factors but otherwise yield no asymptotic improvement in performance. Others can yield substantial asymptotic savings in time and space.</p>
<p>In the LCS algorithm, for example, you can eliminate the <em>b</em> table altogether. Each <em>c</em>[<em>i</em>, <em>j</em>] entry depends on only three other <em>c</em> table entries: <em>c</em>[<em>i</em> − 1, <em>j</em> − 1], <em>c</em>[<em>i</em> − 1, <em>j</em>], and <em>c</em>[<em>i</em>, <em>j</em> − 1]. Given the value of <em>c</em>[<em>i</em>, <em>j</em>], you can determine in <em>O</em>(1) time which of these three values was used to compute <em>c</em>[<em>i</em>, <em>j</em>], without inspecting table <em>b</em>. Thus, you can reconstruct an LCS in <em>O</em>(<em>m</em>+<em>n</em>) time using a procedure similar to P<small>RINT</small>-LCS. (Exercise 14.4-2 asks you to give the pseudocode.) Although this method saves Θ(<em>mn</em>) space, the auxiliary space requirement for computing <a id="p399"/>an LCS does not asymptotically decrease, since the <em>c</em> table takes Θ(<em>mn</em>) space anyway.</p>
<p>You can, however, reduce the asymptotic space requirements for LCS-L<small>ENGTH</small>, since it needs only two rows of table <em>c</em> at a time: the row being computed and the previous row. (In fact, as Exercise 14.4-4 asks you to show, you can use only slightly more than the space for one row of <em>c</em> to compute the length of an LCS.) This improvement works if you need only the length of an LCS. If you need to reconstruct the elements of an LCS, the smaller table does not keep enough information to retrace the algorithm’s steps in <em>O</em>(<em>m</em> + <em>n</em>) time.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>14.4-1</em></strong></p>
<p class="noindent">Determine an LCS of <span class="font1">〈</span>1, 0, 0, 1, 0, 1, 0, 1<span class="font1">〉</span> and <span class="font1">〈</span>0, 1, 0, 1, 1, 0, 1, 1, 0<span class="font1">〉</span>.</p>
<p class="level3"><strong><em>14.4-2</em></strong></p>
<p class="noindent">Give pseudocode to reconstruct an LCS from the completed <em>c</em> table and the original sequences <em>X</em> = <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x<sub>m</sub></em><span class="font1">〉</span> and <em>Y</em> = <span class="font1">〈</span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y<sub>n</sub></em><span class="font1">〉</span> in <em>O</em>(<em>m</em> + <em>n</em>) time, without using the <em>b</em> table.</p>
<p class="level3"><strong><em>14.4-3</em></strong></p>
<p class="noindent">Give a memoized version of LCS-L<small>ENGTH</small> that runs in <em>O</em>(<em>mn</em>) time.</p>
<p class="level3"><strong><em>14.4-4</em></strong></p>
<p class="noindent">Show how to compute the length of an LCS using only 2 · min {<em>m</em>, <em>n</em>} entries in the <em>c</em> table plus <em>O</em>(1) additional space. Then show how to do the same thing, but using min {<em>m</em>, <em>n</em>} entries plus <em>O</em>(1) additional space.</p>
<p class="level3"><strong><em>14.4-5</em></strong></p>
<p class="noindent">Give an <em>O</em>(<em>n</em><sup>2</sup>)-time algorithm to find the longest monotonically increasing subsequence of a sequence of <em>n</em> numbers.</p>
<p class="level3"><span class="font1">★</span> <strong><em>14.4-6</em></strong></p>
<p class="noindent">Give an <em>O</em>(<em>n</em> lg <em>n</em>)-time algorithm to find the longest monotonically increasing subsequence of a sequence of <em>n</em> numbers. (<em>Hint:</em> The last element of a candidate subsequence of length <em>i</em> is at least as large as the last element of a candidate subsequence of length <em>i</em> −1. Maintain candidate subsequences by linking them through the input sequence.)</p>
<a id="p400"/>
</section>
<p class="line1"/>
<section title="14.5 Optimal binary search trees">
<a id="Sec_14.5"/>
<p class="level1" id="h1-85"><a href="toc.xhtml#Rh1-85"><strong>14.5    Optimal binary search trees</strong></a></p>
<p class="noindent">Suppose that you are designing a program to translate text from English to Latvian. For each occurrence of each English word in the text, you need to look up its Latvian equivalent. You can perform these lookup operations by building a binary search tree with <em>n</em> English words as keys and their Latvian equivalents as satellite data. Because you will search the tree for each individual word in the text, you want the total time spent searching to be as low as possible. You can ensure an <em>O</em>(lg <em>n</em>) search time per occurrence by using a red-black tree or any other balanced binary search tree. Words appear with different frequencies, however, and a frequently used word such as <em>the</em> can end up appearing far from the root while a rarely used word such as <em>naumachia</em> appears near the root. Such an organization would slow down the translation, since the number of nodes visited when searching for a key in a binary search tree equals 1 plus the depth of the node containing the key. You want words that occur frequently in the text to be placed nearer the root.<sup><a epub:type="footnote" href="#footnote_8" id="footnote_ref_8">8</a></sup> Moreover, some words in the text might have no Latvian translation,<sup><a epub:type="footnote" href="#footnote_9" id="footnote_ref_9">9</a></sup> and such words would not appear in the binary search tree at all. How can you organize a binary search tree so as to minimize the number of nodes visited in all searches, given that you know how often each word occurs?</p>
<p>What you need is an <strong><em><span class="blue1">optimal binary search tree</span></em></strong>. Formally, given a sequence <em>K</em> = <span class="font1">〈</span><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>n</sub></em><span class="font1">〉</span> of <em>n</em> distinct keys such that <em>k</em><sub>1</sub> &lt; <em>k</em><sub>2</sub> &lt; … &lt; <em>k<sub>n</sub></em>, build a binary search tree containing them. For each key <em>k<sub>i</sub></em>, you are given the probability <em>p<sub>i</sub></em> that any given search is for key <em>k<sub>i</sub></em>. Since some searches may be for values not in <em>K</em>, you also have <em>n</em> + 1 “dummy” keys <em>d</em><sub>0</sub>, <em>d</em><sub>1</sub>, <em>d</em><sub>2</sub>, …, <em>d<sub>n</sub></em> representing those values. In particular, <em>d</em><sub>0</sub> represents all values less than <em>k</em><sub>1</sub>, <em>d<sub>n</sub></em> represents all values greater than <em>k<sub>n</sub></em>, and for <em>i</em> = 1, 2, …, <em>n</em> − 1, the dummy key <em>d<sub>i</sub></em> represents all values between <em>k<sub>i</sub></em> and <em>k</em><sub><em>i</em>+1</sub>. For each dummy key <em>d<sub>i</sub></em>, you have the probability <em>q<sub>i</sub></em> that a search corresponds to <em>d<sub>i</sub></em>. <a href="chapter014.xhtml#Fig_14-9">Figure 14.9</a> shows two binary search trees for a set of <em>n</em> = 5 keys. Each key <em>k<sub>i</sub></em> is an internal node, and each dummy key <em>d<sub>i</sub></em> is a leaf. Since every search is either successful (finding some key <em>k<sub>i</sub></em>) or unsuccessful (finding some dummy key <em>d<sub>i</sub></em>), we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P472.jpg"/></p>
<a id="p401"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-9"><img alt="art" src="images/Art_P473.jpg"/></p>
<p class="caption"><strong>Figure 14.9</strong> Two binary search trees for a set of <em>n</em> = 5 keys with the following probabilities:</p>
<table class="table2s">
<tr>
<td class="th2"><p class="center"><em>i</em></p></td>
<td class="th1"><p class="center">0</p></td>
<td class="th1"><p class="center">1</p></td>
<td class="th1"><p class="center">2</p></td>
<td class="th1"><p class="center">3</p></td>
<td class="th1"><p class="center">4</p></td>
<td class="th1"><p class="center">5</p></td>
</tr>
<tr>
<td class="td1r"><em>p<sub>i</sub></em></td>
<td class="td1"/>
<td class="td1"><p class="center">0.15</p></td>
<td class="td1"><p class="center">0.10</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.10</p></td>
<td class="td1"><p class="center">0.20</p></td>
</tr>
<tr>
<td class="td1r"><em>q<sub>i</sub></em></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.10</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.10</p></td>
</tr>
</table>
<p class="caption1"><strong>(a)</strong> A binary search tree with expected search cost 2.80. <strong>(b)</strong> A binary search tree with expected search cost 2.75. This tree is optimal.</p>
</div>
<p>Knowing the probabilities of searches for each key and each dummy key allows us to determine the expected cost of a search in a given binary search tree <em>T</em>. Let us assume that the actual cost of a search equals the number of nodes examined, which is the depth of the node found by the search in <em>T</em>, plus 1. Then the expected cost of a search in <em>T</em> is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P474.jpg"/></p>
<a id="p402"/>
<p class="noindent">where depth<em><sub>T</sub></em> denotes a node’s depth in the tree <em>T</em>. The last equation follows from equation (14.10). <a href="chapter014.xhtml#Fig_14-9">Figure 14.9</a> shows how to calculate the expected search cost node by node.</p>
<p>For a given set of probabilities, your goal is to construct a binary search tree whose expected search cost is smallest. We call such a tree an <strong><em><span class="blue1">optimal binary search tree</span></em></strong>. <a href="chapter014.xhtml#Fig_14-9">Figure 14.9(a)</a> shows one binary search tree, with expected cost 2.80, for the probabilities given in the figure caption. Part (b) of the figure displays an optimal binary search tree, with expected cost 2.75. This example demonstrates that an optimal binary search tree is not necessarily a tree whose overall height is smallest. Nor does an optimal binary search tree always have the key with the greatest probability at the root. Here, key <em>k</em><sub>5</sub> has the greatest search probability of any key, yet the root of the optimal binary search tree shown is <em>k</em><sub>2</sub>. (The lowest expected cost of any binary search tree with <em>k</em><sub>5</sub> at the root is 2.85.)</p>
<p>As with matrix-chain multiplication, exhaustive checking of all possibilities fails to yield an efficient algorithm. You can label the nodes of any <em>n</em>-node binary tree with the keys <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k<sub>n</sub></em> to construct a binary search tree, and then add in the dummy keys as leaves. In Problem 12-4 on page 329, we saw that the number of binary trees with <em>n</em> nodes is Ω(4<sup><em>n</em></sup>/<em>n</em><sup>3/2</sup>). Thus you would need to examine an exponential number of binary search trees to perform an exhaustive search. We’ll see how to solve this problem more efficiently with dynamic programming.</p>
<p class="level4"><strong>Step 1: The structure of an optimal binary search tree</strong></p>
<p class="noindent">To characterize the optimal substructure of optimal binary search trees, we start with an observation about subtrees. Consider any subtree of a binary search tree. It must contain keys in a contiguous range <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, for some 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>. In addition, a subtree that contains keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em> must also have as its leaves the dummy keys <em>d</em><sub><em>i</em>−1</sub>, …, <em>d<sub>j</sub></em>.</p>
<p>Now we can state the optimal substructure: if an optimal binary search tree <em>T</em> has a subtree <em>T</em>′ containing keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, then this subtree <em>T</em>′ must be optimal as well for the subproblem with keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em> and dummy keys <em>d</em><sub><em>i</em>−1</sub>, …, <em>d<sub>j</sub></em>. The usual cut-and-paste argument applies. If there were a subtree <em>T</em>″ whose expected cost is lower than that of <em>T</em>′, then cutting <em>T</em>′ out of <em>T</em> and pasting in <em>T</em>″ would result in a binary search tree of lower expected cost than <em>T</em>, thus contradicting the optimality of <em>T</em>.</p>
<p>With the optimal substructure in hand, here is how to construct an optimal solution to the problem from optimal solutions to subproblems. Given keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, one of these keys, say <em>k<sub>r</sub></em> (<em>i</em> ≤ <em>r</em> ≤ <em>j</em>), is the root of an optimal subtree containing these keys. The left subtree of the root <em>k<sub>r</sub></em> contains the keys <em>k<sub>i</sub></em>, …, <em>k</em><sub><em>r</em>−1</sub> (and dummy keys <em>d</em><sub><em>i</em>−1</sub>, …, <em>d</em><sub><em>r</em>−1</sub>), and the right subtree contains the keys <em>k</em><sub><em>r</em>+1</sub>, …, <em>k<sub>j</sub></em> (and dummy keys <em>d<sub>r</sub></em>, …, <em>d<sub>j</sub></em>). As long as you examine all candidate roots <em>k<sub>r</sub></em>, <a id="p403"/>where <em>i</em> ≤ <em>r</em> ≤ <em>j</em>, and you determine all optimal binary search trees containing <em>k<sub>i</sub></em>, …, <em>k</em><sub><em>r</em>−1</sub> and those containing <em>k</em><sub><em>r</em>+1</sub>, …, <em>k<sub>j</sub></em>, you are guaranteed to find an optimal binary search tree.</p>
<p>There is one technical detail worth understanding about “empty” subtrees. Suppose that in a subtree with keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, you select <em>k<sub>i</sub></em> as the root. By the above argument, <em>k<sub>i</sub></em>’s left subtree contains the keys <em>k<sub>i</sub></em>, …, <em>k</em><sub><em>i</em>−1</sub>: no keys at all. Bear in mind, however, that subtrees also contain dummy keys. We adopt the convention that a subtree containing keys <em>k<sub>i</sub></em>, …, <em>k</em><sub><em>i</em>−1</sub> has no actual keys but does contain the single dummy key <em>d</em><sub><em>i</em>−1</sub>. Symmetrically, if you select <em>k<sub>j</sub></em> as the root, then <em>k<sub>j</sub></em>’s right subtree contains the keys <em>k</em><sub><em>j</em>+1</sub>, …, <em>k<sub>j</sub></em>. This right subtree contains no actual keys, but it does contain the dummy key <em>d<sub>j</sub></em>.</p>
<p class="level4"><strong>Step 2: A recursive solution</strong></p>
<p class="noindent">To define the value of an optimal solution recursively, the subproblem domain is finding an optimal binary search tree containing the keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, where <em>i</em> ≥ 1, <em>j</em> ≤ <em>n</em>, and <em>j</em> ≥ <em>i</em> − 1. (When <em>j</em> = <em>i</em> − 1, there is just the dummy key <em>d</em><sub><em>i</em>−1</sub>, but no actual keys.) Let <em>e</em>[<em>i</em>, <em>j</em>] denote the expected cost of searching an optimal binary search tree containing the keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>. Your goal is to compute <em>e</em>[1, <em>n</em>], the expected cost of searching an optimal binary search tree for all the actual and dummy keys.</p>
<p>The easy case occurs when <em>j</em> = <em>i</em> − 1. Then the subproblem consists of just the dummy key <em>d</em><sub><em>i</em>−1</sub>. The expected search cost is <em>e</em>[<em>i</em>, <em>i</em> − 1] = <em>q</em><sub><em>i</em>−1</sub>.</p>
<p>When <em>j</em> ≥ <em>i</em>, you need to select a root <em>k<sub>r</sub></em> from among <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em> and then make an optimal binary search tree with keys <em>k<sub>i</sub></em>, …, <em>k</em><sub><em>r</em>−1</sub> as its left subtree and an optimal binary search tree with keys <em>k</em><sub><em>r</em>+1</sub>, …, <em>k<sub>j</sub></em> as its right subtree. What happens to the expected search cost of a subtree when it becomes a subtree of a node? The depth of each node in the subtree increases by 1. By equation (14.11), the expected search cost of this subtree increases by the sum of all the probabilities in the subtree. For a subtree with keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, denote this sum of probabilities as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P475.jpg"/></p>
<p class="noindent">Thus, if <em>k<sub>r</sub></em> is the root of an optimal subtree containing keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>, we have</p>
<p class="eql"><em>e</em>[<em>i</em>, <em>j</em>] = <em>p<sub>r</sub></em> + (<em>e</em>[<em>i</em>, <em>r</em> − 1] + <em>w</em>(<em>i</em>, <em>r</em> − 1)) + (<em>e</em>[<em>r</em> + 1, <em>j</em>] + <em>w</em>(<em>r</em> + 1, <em>j</em>)).</p>
<p class="noindent">Noting that</p>
<p class="eql"><em>w</em>(<em>i</em>, <em>j</em>) = <em>w</em>(<em>i</em>, <em>r</em> − 1) + <em>p<sub>r</sub></em> + <em>w</em>(<em>r</em> + 1, <em>j</em>),</p>
<p class="noindent">we rewrite <em>e</em>[<em>i</em>, <em>j</em>] as</p>
<a id="p404"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P476.jpg"/></p>
<p>The recursive equation (14.13) assumes that you know which node <em>k<sub>r</sub></em> to use as the root. Of course, you choose the root that gives the lowest expected search cost, giving the final recursive formulation:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P477.jpg"/></p>
<p>The <em>e</em>[<em>i</em>, <em>j</em>] values give the expected search costs in optimal binary search trees. To help keep track of the structure of optimal binary search trees, define <em>root</em>[<em>i</em>, <em>j</em>], for 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>, to be the index <em>r</em> for which <em>k<sub>r</sub></em> is the root of an optimal binary search tree containing keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>. Although we’ll see how to compute the values of <em>root</em>[<em>i</em>, <em>j</em>], the construction of an optimal binary search tree from these values is left as Exercise 14.5-1.</p>
<p class="level4"><strong>Step 3: Computing the expected search cost of an optimal binary search tree</strong></p>
<p class="noindent">At this point, you may have noticed some similarities between our characterizations of optimal binary search trees and matrix-chain multiplication. For both problem domains, the subproblems consist of contiguous index subranges. A direct, recursive implementation of equation (14.14) would be just as inefficient as a direct, recursive matrix-chain multiplication algorithm. Instead, you can store the <em>e</em>[<em>i</em>, <em>j</em>] values in a table <em>e</em>[1 : <em>n</em> + 1, 0 : <em>n</em>]. The first index needs to run to <em>n</em> + 1 rather than <em>n</em> because in order to have a subtree containing only the dummy key <em>d<sub>n</sub></em>, you need to compute and store <em>e</em>[<em>n</em> + 1, <em>n</em>]. The second index needs to start from 0 because in order to have a subtree containing only the dummy key <em>d</em><sub>0</sub>, you need to compute and store <em>e</em>[1, 0]. Only the entries <em>e</em>[<em>i</em>, <em>j</em>] for which <em>j</em> ≥ <em>i</em> − 1 are filled in. The table <em>root</em>[<em>i</em>, <em>j</em>] records the root of the subtree containing keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em> and uses only the entries for which 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>.</p>
<p>One other table makes the dynamic-programming algorithm a little faster. Instead of computing the value of <em>w</em>(<em>i</em>, <em>j</em>) from scratch every time you compute <em>e</em>[<em>i</em>, <em>j</em>], which would take Θ(<em>j</em> − <em>i</em>) additions, store these values in a table <em>w</em>[1 : <em>n</em> + 1, 0 : <em>n</em>]. For the base case, compute <em>w</em>[<em>i</em>, <em>i</em> − 1] = <em>q</em><sub><em>i</em>−1</sub> for 1 ≤ <em>i</em> ≤ <em>n</em> + 1. For <em>j</em> ≥ <em>i</em>, compute</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P478.jpg"/></p>
<p class="noindent">Thus, you can compute the Θ(<em>n</em><sup>2</sup>) values of <em>w</em>[<em>i</em>, <em>j</em>] in Θ(1) time each.</p>
<p>The O<small>PTIMAL</small>-BST procedure on the next page takes as inputs the probabilities <em>p</em><sub>1</sub>, …, <em>p<sub>n</sub></em> and <em>q</em><sub>0</sub>, …, <em>q<sub>n</sub></em> and the size <em>n</em>, and it returns the tables <em>e</em> and <em>root</em>. From the description above and the similarity to the M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> procedure <a id="p405"/>in <a href="chapter014.xhtml#Sec_14.2">Section 14.2</a>, you should find the operation of this procedure to be fairly straightforward. The <strong>for</strong> loop of lines 2–4 initializes the values of <em>e</em>[<em>i</em>, <em>i</em> − 1]and <em>w</em>[<em>i</em>, <em>i</em> − 1]. Then the <strong>for</strong> loop of lines 5–14 uses the recurrences (14.14) and (14.15) to compute <em>e</em>[<em>i</em>, <em>j</em>] and <em>w</em>[<em>i</em>, <em>j</em>] for all 1 ≤ <em>i</em> ≤ <em>j</em> ≤ <em>n</em>. In the first iteration, when <em>l</em> = 1, the loop computes <em>e</em>[<em>i</em>, <em>i</em>] and <em>w</em>[<em>i</em>, <em>i</em>] for <em>i</em> = 1, 2, …, <em>n</em>. The second iteration, with <em>l</em> = 2, computes <em>e</em>[<em>i</em>, <em>i</em> + 1] and <em>w</em>[<em>i</em>, <em>i</em> + 1] for <em>i</em> = 1, 2, …, <em>n</em> − 1, and so on. The innermost <strong>for</strong> loop, in lines 10–14, tries each candidate index <em>r</em> to determine which key <em>k<sub>r</sub></em> to use as the root of an optimal binary search tree containing keys <em>k<sub>i</sub></em>, …, <em>k<sub>j</sub></em>. This <strong>for</strong> loop saves the current value of the index <em>r</em> in <em>root</em>[<em>i</em>, <em>j</em>] whenever it finds a better key to use as the root.</p>
<div class="pull-quote1">
<p class="box-heading">O<small>PTIMAL</small>-BST(<em>p</em>, <em>q</em>, <em>n</em>)</p>
<table class="table1a1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="3">let <em>e</em>[1 : <em>n</em> + 1, 0 : <em>n</em>], <em>w</em>[1 : <em>n</em> + 1, 0 : <em>n</em>],</td>
</tr>
<tr>
<td class="td1"/>
<td class="td1" colspan="3"><p class="p3">and <em>root</em>[1 : <em>n</em>, 1 : <em>n</em>] be new tables</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> + 1</td>
<td class="td1" colspan="2"><span class="red"><strong>//</strong> base cases</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="p2"><em>e</em>[<em>i</em>, <em>i</em> − 1] = <em>q</em><sub><em>i</em>−1</sub></p></td>
<td class="td1" colspan="2"><span class="red"><strong>//</strong> equation (14.14)</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1" colspan="3"><p class="p2"><em>w</em>[<em>i</em>, <em>i</em> − 1] = <em>q</em><sub><em>i</em>−1</sub></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1" colspan="3"><strong>for</strong> <em>l</em> = 1 <strong>to</strong> <em>n</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1" colspan="3"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> − <em>l</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="3"><p class="p3"><em>j</em> = <em>i</em> + <em>l</em> − 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1" colspan="3"><p class="p3"><em>e</em>[<em>i</em>, <em>j</em>] = ∞</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2"><p class="p3"><em>w</em>[<em>i</em>, <em>j</em>] = <em>w</em>[<em>i</em>, <em>j</em> − 1] + <em>p<sub>j</sub></em> + <em>q<sub>j</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> equation (14.15)</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1" colspan="2"><p class="p3"><strong>for</strong> <em>r</em> = <em>i</em> <strong>to</strong> <em>j</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> try all possible roots <em>r</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1" colspan="3"><p class="p4"><em>t</em> = <em>e</em>[<em>i</em>, <em>r</em> − 1] + <em>e</em>[<em>r</em> + 1, <em>j</em>] + <em>w</em>[<em>i</em>, <em>j</em>] <span class="red"><strong>//</strong> equation (14.14)</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2"><p class="p4"><strong>if</strong> <em>t</em> &lt; <em>e</em>[<em>i</em>, <em>j</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> new minimum?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="3"><p class="p5"><em>e</em>[<em>i</em>, <em>j</em>] = <em>t</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="3"><p class="p5"><em>root</em>[<em>i</em>, <em>j</em>] = <em>r</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1" colspan="3"><strong>return</strong> <em>e</em> and <em>root</em></td>
</tr>
</table>
</div>
<p><a href="chapter014.xhtml#Fig_14-10">Figure 14.10</a> shows the tables <em>e</em>[<em>i</em>, <em>j</em>], <em>w</em>[<em>i</em>, <em>j</em>], and <em>root</em>[<em>i</em>, <em>j</em>] computed by the procedure O<small>PTIMAL</small>-BST on the key distribution shown in <a href="chapter014.xhtml#Fig_14-9">Figure 14.9</a>. As in the matrix-chain multiplication example of <a href="chapter014.xhtml#Fig_14-5">Figure 14.5</a>, the tables are rotated to make the diagonals run horizontally. O<small>PTIMAL</small>-BST computes the rows from bottom to top and from left to right within each row.</p>
<p>The O<small>PTIMAL</small>-BST procedure takes Θ(<em>n</em><sup>3</sup>) time, just like M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>. Its running time is <em>O</em>(<em>n</em><sup>3</sup>), since its <strong>for</strong> loops are nested three deep and each loop index takes on at most <em>n</em> values. The loop indices in O<small>PTIMAL</small>-BST do not have exactly the same bounds as those in M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>, but they are within at most 1 in all directions. Thus, like M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small>, the O<small>PTIMAL</small>-BST procedure takes Ω(<em>n</em><sup>3</sup>) time.</p>
<a id="p406"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-10"><img alt="art" class="width100" src="images/Art_P479.jpg"/></p>
<p class="caption"><strong>Figure 14.10</strong> The tables <em>e</em>[<em>i</em>, <em>j</em>], <em>w</em>[<em>i</em>, <em>j</em>], and <em>root</em>[<em>i</em>, <em>j</em>] computed by O<small>PTIMAL</small>-BST on the key distribution shown in <a href="chapter014.xhtml#Fig_14-9">Figure 14.9</a>. The tables are rotated so that the diagonals run horizontally.</p>
</div>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>14.5-1</em></strong></p>
<p class="noindent">Write pseudocode for the procedure C<small>ONSTRUCT</small>-O<small>PTIMAL</small>-BST(<em>root, n</em>) which, given the table <em>root</em>[1 : <em>n</em>, 1 : <em>n</em>], outputs the structure of an optimal binary search tree. For the example in <a href="chapter014.xhtml#Fig_14-10">Figure 14.10</a>, your procedure should print out the structure</p>
<div class="pull-quote">
<p class="noindent"><em>k</em><sub>2</sub> is the root</p>
<p class="noindent"><em>k</em><sub>1</sub> is the left child of <em>k</em><sub>2</sub></p>
<p class="noindent"><em>d</em><sub>0</sub> is the left child of <em>k</em><sub>1</sub></p>
<p class="noindent"><em>d</em><sub>1</sub> is the right child of <em>k</em><sub>1</sub></p>
<p class="noindent"><em>k</em><sub>5</sub> is the right child of <em>k</em><sub>2</sub></p>
<p class="noindent"><em>k</em><sub>4</sub> is the left child of <em>k</em><sub>5</sub></p>
<p class="noindent"><em>k</em><sub>3</sub> is the left child of <em>k</em><sub>4</sub></p>
<p class="noindent"><em>d</em><sub>2</sub> is the left child of <em>k</em><sub>3</sub></p>
<p class="noindent"><em>d</em><sub>3</sub> is the right child of <em>k</em><sub>3</sub></p>
<p class="noindent"><em>d</em><sub>4</sub> is the right child of <em>k</em><sub>4</sub></p>
<p class="noindent"><em>d</em><sub>5</sub> is the right child of <em>k</em><sub>5</sub></p>
</div>
<p class="noindent">corresponding to the optimal binary search tree shown in <a href="chapter014.xhtml#Fig_14-9">Figure 14.9(b)</a>.</p>
<a id="p407"/>
<p class="level3"><strong><em>14.5-2</em></strong></p>
<p class="noindent">Determine the cost and structure of an optimal binary search tree for a set of <em>n</em> = 7 keys with the following probabilities:</p>
<table class="table1s">
<tr>
<td class="th2"><p class="center"><em>i</em></p></td>
<td class="th1"><p class="center">0</p></td>
<td class="th1"><p class="center">1</p></td>
<td class="th1"><p class="center">2</p></td>
<td class="th1"><p class="center">3</p></td>
<td class="th1"><p class="center">4</p></td>
<td class="th1"><p class="center">5</p></td>
<td class="th1"><p class="center">6</p></td>
<td class="th1"><p class="center">7</p></td>
</tr>
<tr>
<td class="td1r"><p class="center"><em>p<sub>i</sub></em></p></td>
<td class="td1"/>
<td class="td1"><p class="center">0.04</p></td>
<td class="td1"><p class="center">0.06</p></td>
<td class="td1"><p class="center">0.08</p></td>
<td class="td1"><p class="center">0.02</p></td>
<td class="td1"><p class="center">0.10</p></td>
<td class="td1"><p class="center">0.12</p></td>
<td class="td1"><p class="center">0.14</p></td>
</tr>
<tr>
<td class="td1r"><p class="center"><em>q<sub>i</sub></em></p></td>
<td class="td1"><p class="center">0.06</p></td>
<td class="td1"><p class="center">0.06</p></td>
<td class="td1"><p class="center">0.06</p></td>
<td class="td1"><p class="center">0.06</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.05</p></td>
<td class="td1"><p class="center">0.05</p></td>
</tr>
</table>
<p class="level3"><strong><em>14.5-3</em></strong></p>
<p class="noindent">Suppose that instead of maintaining the table <em>w</em>[<em>i</em>, <em>j</em>], you computed the value of <em>w</em>(<em>i</em>, <em>j</em>) directly from equation (14.12) in line 9 of O<small>PTIMAL</small>-BST and used this computed value in line 11. How would this change affect the asymptotic running time of O<small>PTIMAL</small>-BST?</p>
<p class="level3"><span class="font1">★</span> <strong><em>14.5-4</em></strong></p>
<p class="noindent">Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_264">264</a>] has shown that there are always roots of optimal subtrees such that <em>root</em>[<em>i</em>, <em>j</em> − 1] ≤ <em>root</em>[<em>i</em>, <em>j</em>] ≤ <em>root</em>[<em>i</em> + 1, <em>j</em>] for all 1 ≤ <em>i</em> &lt; <em>j</em> ≤ <em>n</em>. Use this fact to modify the O<small>PTIMAL</small>-BST procedure to run in Θ(<em>n</em><sup>2</sup>) time.</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-86"><strong>Problems</strong></p>
<section title="14-1 Longest simple path in a directed acyclic graph">
<p class="level2"><strong><em>14-1     Longest simple path in a directed acyclic graph</em></strong></p>
<p class="noindent">You are given a directed acyclic graph <em>G</em> = (<em>V</em>, <em>E</em>) with real-valued edge weights and two distinguished vertices <em>s</em> and <em>t</em>. The <strong><em><span class="blue1">weight</span></em></strong> of a path is the sum of the weights of the edges in the path. Describe a dynamic-programming approach for finding a longest weighted simple path from <em>s</em> to <em>t</em>. What is the running time of your algorithm?</p>
</section>
<section title="14-2 Longest palindrome subsequence">
<p class="level2"><strong><em>14-2     Longest palindrome subsequence</em></strong></p>
<p class="noindent">A <strong><em><span class="blue1">palindrome</span></em></strong> is a nonempty string over some alphabet that reads the same forward and backward. Examples of palindromes are all strings of length 1, <span class="courierfont">civic</span>, <span class="courierfont">racecar</span>, and <span class="courierfont">aibohphobia</span> (fear of palindromes).</p>
<p>Give an efficient algorithm to find the longest palindrome that is a subsequence of a given input string. For example, given the input <span class="courierfont">character</span>, your algorithm should return <span class="courierfont">carac</span>. What is the running time of your algorithm?</p>
</section>
<section title="14-3 Bitonic euclidean traveling-salesperson problem">
<p class="level2"><strong><em>14-3     Bitonic euclidean traveling-salesperson problem</em></strong></p>
<p class="noindent">In the <strong><em><span class="blue1">euclidean traveling-salesperson problem</span></em></strong>, you are given a set of <em>n</em> points in the plane, and your goal is to find the shortest closed tour that connects all <em>n</em> points.</p>
<a id="p408"/>
<div class="divimage">
<p class="fig-imga" id="Fig_14-11"><img alt="art" src="images/Art_P480.jpg"/></p>
<p class="caption"><strong>Figure 14.11</strong> Seven points in the plane, shown on a unit grid. <strong>(a)</strong> The shortest closed tour, with length approximately 24.89. This tour is not bitonic. <strong>(b)</strong> The shortest bitonic tour for the same set of points. Its length is approximately 25.58.</p>
</div>
<p class="noindent"><a href="chapter014.xhtml#Fig_14-11">Figure 14.11(a)</a> shows the solution to a 7-point problem. The general problem is NP-hard, and its solution is therefore believed to require more than polynomial time (see <a href="chapter034.xhtml">Chapter 34</a>).</p>
<p>J. L. Bentley has suggested simplifying the problem by considering only <strong><em><span class="blue1">bitonic tours</span></em></strong>, that is, tours that start at the leftmost point, go strictly rightward to the rightmost point, and then go strictly leftward back to the starting point. <a href="chapter014.xhtml#Fig_14-11">Figure 14.11(b)</a> shows the shortest bitonic tour of the same 7 points. In this case, a polynomial-time algorithm is possible.</p>
<p>Describe an <em>O</em>(<em>n</em><sup>2</sup>)-time algorithm for determining an optimal bitonic tour. You may assume that no two points have the same <em>x</em>-coordinate and that all operations on real numbers take unit time. (<em>Hint:</em> Scan left to right, maintaining optimal possibilities for the two parts of the tour.)</p>
</section>
<section title="14-4 Printing neatly">
<p class="level2"><strong><em>14-4     Printing neatly</em></strong></p>
<p class="noindent">Consider the problem of neatly printing a paragraph with a monospaced font (all characters having the same width). The input text is a sequence of <em>n</em> words of lengths <em>l</em><sub>1</sub>, <em>l</em><sub>2</sub>, …, <em>l<sub>n</sub></em>, measured in characters, which are to be printed neatly on a number of lines that hold a maximum of <em>M</em> characters each. No word exceeds the line length, so that <em>l<sub>i</sub></em> ≤ <em>M</em> for <em>i</em> = 1, 2, …, <em>n</em>. The criterion of “neatness” is as follows. If a given line contains words <em>i</em> through <em>j</em>, where <em>i</em> ≤ <em>j</em>, and exactly one space appears between words, then the number of extra space characters at the end of the line is <img alt="art" src="images/Art_P481.jpg"/>, which must be nonnegative so that the words fit on the line. The goal is to minimize the sum, over all lines except the last, of the cubes of the numbers of extra space characters at the ends of lines. Give a dynamic-programming algorithm to print a paragraph of <em>n</em> words neatly. Analyze the running time and space requirements of your algorithm.</p>
<a id="p409"/>
</section>
<section title="14-5 Edit distance">
<p class="level2"><strong><em>14-5     Edit distance</em></strong></p>
<p class="noindent">In order to transform a source string of text <em>x</em>[1 : <em>m</em>] to a target string <em>y</em>[1 : <em>n</em>], you can perform various transformation operations. The goal is, given <em>x</em> and <em>y</em>, to produce a series of transformations that changes <em>x</em> to <em>y</em>. An array <em>z</em>—assumed to be large enough to hold all the characters it needs—holds the intermediate results. Initially, <em>z</em> is empty, and at termination, you should have <em>z</em>[<em>j</em>] = <em>y</em>[<em>j</em>] for <em>j</em> = 1, 2, …, <em>n</em>. The procedure for solving this problem maintains current indices <em>i</em> into <em>x</em> and <em>j</em> into <em>z</em>, and the operations are allowed to alter <em>z</em> and these indices. Initially, <em>i</em> = <em>j</em> = 1. Every character in <em>x</em> must be examined during the transformation, which means that at the end of the sequence of transformation operations, <em>i</em> = <em>m</em> + 1.</p>
<p>You may choose from among six transformation operations, each of which has a constant cost that depends on the operation:</p>
<p class="para-hang1"><strong>Copy</strong> a character from <em>x</em> to <em>z</em> by setting <em>z</em>[<em>j</em>] = <em>x</em>[<em>i</em>] and then incrementing both <em>i</em> and <em>j</em>. This operation examines <em>x</em>[<em>i</em>] and has cost <em>Q<sub>C</sub></em>.</p>
<p class="para-hang1"><strong>Replace</strong> a character from <em>x</em> by another character <em>c</em>, by setting <em>z</em>[<em>j</em>] = <em>c</em>, and then incrementing both <em>i</em> and <em>j</em>. This operation examines <em>x</em>[<em>i</em>] and has cost <em>Q<sub>R</sub></em>.</p>
<p class="para-hang1"><strong>Delete</strong> a character from <em>x</em> by incrementing <em>i</em> but leaving <em>j</em> alone. This operation examines <em>x</em>[<em>i</em>] and has cost <em>Q<sub>D</sub></em>.</p>
<p class="para-hang1"><strong>Insert</strong> the character <em>c</em> into <em>z</em> by setting <em>z</em>[<em>j</em>] = <em>c</em> and then incrementing <em>j</em>, but leaving <em>i</em> alone. This operation examines no characters of <em>x</em> and has cost <em>Q<sub>I</sub></em>.</p>
<p class="para-hang1"><strong>Twiddle</strong> (i.e., exchange) the next two characters by copying them from <em>x</em> to <em>z</em> but in the opposite order: setting <em>z</em>[<em>j</em>] = <em>x</em>[<em>i</em> + 1] and <em>z</em>[<em>j</em> + 1] = <em>x</em>[<em>i</em>], and then setting <em>i</em> = <em>i</em> + 2 and <em>j</em> = <em>j</em> + 2. This operation examines <em>x</em>[<em>i</em>] and <em>x</em>[<em>i</em> + 1] and has cost <em>Q<sub>T</sub></em>.</p>
<p class="para-hang1"><strong>Kill</strong> the remainder of <em>x</em> by setting <em>i</em> = <em>m</em> + 1. This operation examines all characters in <em>x</em> that have not yet been examined. This operation, if performed, must be the final operation. It has cost <em>Q<sub>K</sub></em>.</p>
<p class="space-break"><a href="chapter014.xhtml#Fig_14-12">Figure 14.12</a> gives one way to transform the source string <span class="courierfont">algorithm</span> to the target string <span class="courierfont">altruistic</span>. Several other sequences of transformation operations can transform <span class="courierfont">algorithm</span> to <span class="courierfont">altruistic</span>.</p>
<p>Assume that <em>Q<sub>C</sub></em> &lt; <em>Q<sub>D</sub></em> + <em>Q<sub>I</sub></em> and <em>Q<sub>R</sub></em> &lt; <em>Q<sub>D</sub></em> + <em>Q<sub>I</sub></em>, since otherwise, the copy and replace operations would not be used. The cost of a given sequence of transformation operations is the sum of the costs of the individual operations in the sequence. For the sequence above, the cost of transforming <span class="courierfont">algorithm</span> to <span class="courierfont">altruistic</span> is 3<em>Q<sub>C</sub></em> + <em>Q<sub>R</sub></em> + <em>Q<sub>D</sub></em> + 4<em>Q<sub>I</sub></em> + <em>Q<sub>T</sub></em> + <em>Q<sub>K</sub></em>.</p>
<p class="nl"><strong><em>a.</em></strong> Given two sequences <em>x</em>[1 : <em>m</em>] and <em>y</em>[1 : <em>n</em>] and the costs of the transformation operations, the <strong><em><span class="blue1">edit distance</span></em></strong> from <em>x</em> to <em>y</em> is the cost of the least expensive operation <a id="p410"/>sequence that transforms <em>x</em> to <em>y</em>. Describe a dynamic-programming algorithm that finds the edit distance from <em>x</em>[1 : <em>m</em>] to <em>y</em>[1 : <em>n</em>] and prints an optimal operation sequence. Analyze the running time and space requirements of your algorithm.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_14-12"><img alt="art" src="images/Art_P482.jpg"/></p>
<p class="caption"><strong>Figure 14.12</strong> A sequence of operations that transforms the source <span class="courierfont">algorithm</span> to the target string <span class="courierfont">altruistic</span>. The underlined characters are <em>x</em>[<em>i</em>] and <em>z</em>[<em>j</em>] after the operation.</p>
</div>
<p class="noindent">The edit-distance problem generalizes the problem of aligning two DNA sequences (see, for example, Setubal and Meidanis [<a epub:type="noteref" href="bibliography001.xhtml#endnote_405">405</a>, <a href="chapter003.xhtml#Sec_3.2">Section 3.2</a>]). There are several methods for measuring the similarity of two DNA sequences by aligning them. One such method to align two sequences <em>x</em> and <em>y</em> consists of inserting spaces at arbitrary locations in the two sequences (including at either end) so that the resulting sequences <em>x</em>′ and <em>y</em>′ have the same length but do not have a space in the same position (i.e., for no position <em>j</em> are both <em>x</em>′[<em>j</em>] and <em>y</em>′[<em>j</em>] a space). Then we assign a “score” to each position. Position <em>j</em> receives a score as follows:</p>
<ul class="ulnoindent" epub:type="list">
<li>+1 if <em>x</em>′[<em>j</em>] = <em>y</em>′[<em>j</em>] and neither is a space,</li>
<li class="litop">−1 if <em>x</em>′[<em>j</em>] ≠ <em>y</em>′[<em>j</em>] and neither is a space,</li>
<li class="litop">−2 if either <em>x</em>′[<em>j</em>] or <em>y</em>′[<em>j</em>] is a space.</li></ul>
<p class="noindent">The score for the alignment is the sum of the scores of the individual positions. For example, given the sequences <em>x</em> = <span class="courierfont">GATCGGCAT</span> and <em>y</em> = <span class="courierfont">CAATGTGAATC</span>, one alignment is</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="courierfont">G ATCG GCAT</span></p>
<p class="noindent"><span class="courierfont">CAAT GTGAATC</span></p>
<p class="noindent"><span class="courierfont">-*++*+*+-++*</span></p></td>
</tr>
</table>
<a id="p411"/>
<p class="noindent">A <span class="courierfont">+</span> under a position indicates a score of +1 for that position, a <span class="courierfont">-</span> indicates a score of −1, and a <span class="courierfont">*</span> indicates a score of −2, so that this alignment has a total score of 6 · 1 − 2 · 1 − 4 · 2 = −4.</p>
<p class="nl"><strong><em>b.</em></strong> Explain how to cast the problem of finding an optimal alignment as an edit-distance problem using a subset of the transformation operations copy, replace, delete, insert, twiddle, and kill.</p>
</section>
<section title="14-6 Planning a company party">
<p class="level2"><strong><em>14-6     Planning a company party</em></strong></p>
<p class="noindent">Professor Blutarsky is consulting for the president of a corporation that is planning a company party. The company has a hierarchical structure, that is, the supervisor relation forms a tree rooted at the president. The human resources department has ranked each employee with a conviviality rating, which is a real number. In order to make the party fun for all attendees, the president does not want both an employee and his or her immediate supervisor to attend.</p>
<p>Professor Blutarsky is given the tree that describes the structure of the corporation, using the left-child, right-sibling representation described in <a href="chapter010.xhtml#Sec_10.3">Section 10.3</a>. Each node of the tree holds, in addition to the pointers, the name of an employee and that employee’s conviviality ranking. Describe an algorithm to make up a guest list that maximizes the sum of the conviviality ratings of the guests. Analyze the running time of your algorithm.</p>
</section>
<section title="14-7 Viterbi algorithm">
<p class="level2"><strong><em>14-7     Viterbi algorithm</em></strong></p>
<p class="noindent">Dynamic programming on a directed graph can play a part in speech recognition. A directed graph <em>G</em> = (<em>V</em>, <em>E</em>) with labeled edges forms a formal model of a person speaking a restricted language. Each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em> is labeled with a sound <em>σ</em>(<em>u</em>, <em>v</em>) from a finite set Σ of sounds. Each directed path in the graph starting from a distinguished vertex <em>v</em><sub>0</sub> ∈ <em>V</em> corresponds to a possible sequence of sounds produced by the model, with the label of a path being the concatenation of the labels of the edges on that path.</p>
<p class="nl"><strong><em>a.</em></strong> Describe an efficient algorithm that, given an edge-labeled directed graph <em>G</em> with distinguished vertex <em>v</em><sub>0</sub> and a sequence <em>s</em> = <span class="font1">〈</span><em>σ</em><sub>1</sub>, <em>σ</em><sub>2</sub>, …, <em>σ<sub>k</sub></em><span class="font1">〉</span> of sounds from Σ, returns a path in <em>G</em> that begins at <em>v</em><sub>0</sub> and has <em>s</em> as its label, if any such path exists. Otherwise, the algorithm should return <small>NO</small>-<small>SUCH</small>-<small>PATH</small>. Analyze the running time of your algorithm. (<em>Hint:</em> You may find concepts from <a href="chapter020.xhtml">Chapter 20</a> useful.)</p>
<p class="noindent1-top">Now suppose that every edge (<em>u</em>, <em>v</em>) ∈ <em>E</em> has an associated nonnegative probability <em>p</em>(<em>u</em>, <em>v</em>) of being traversed, so that the corresponding sound is produced. The sum of the probabilities of the edges leaving any vertex equals 1. The probability of a path is defined to be the product of the probabilities of its edges. Think of <a id="p412"/>the probability of a path beginning at vertex <em>v</em><sub>0</sub> as the probability that a “random walk” beginning at <em>v</em><sub>0</sub> follows the specified path, where the edge leaving a vertex <em>u</em> is taken randomly, according to the probabilities of the available edges leaving <em>u</em>.</p>
<p class="nl"><strong><em>b.</em></strong> Extend your answer to part (a) so that if a path is returned, it is a <em>most probable path</em> starting at vertex <em>v</em><sub>0</sub> and having label <em>s</em>. Analyze the running time of your algorithm.</p>
</section>
<section title="14-8 Image compression by seam carving">
<p class="level2"><strong><em>14-8     Image compression by seam carving</em></strong></p>
<p class="noindent">Suppose that you are given a color picture consisting of an <em>m</em>×<em>n</em> array <em>A</em>[1 : <em>m</em>, 1 : <em>n</em>] of pixels, where each pixel specifies a triple of red, green, and blue (RGB) intensities. You want to compress this picture slightly, by removing one pixel from each of the <em>m</em> rows, so that the whole picture becomes one pixel narrower. To avoid incongruous visual effects, however, the pixels removed in two adjacent rows must lie in either the same column or adjacent columns. In this way, the pixels removed form a “seam” from the top row to the bottom row, where successive pixels in the seam are adjacent vertically or diagonally.</p>
<p class="nl"><strong><em>a.</em></strong> Show that the number of such possible seams grows at least exponentially in <em>m</em>, assuming that <em>n</em> &gt; 1.</p>
<p class="nl"><strong><em>b.</em></strong> Suppose now that along with each pixel <em>A</em>[<em>i</em>, <em>j</em>], you are given a real-valued disruption measure <em>d</em>[<em>i</em>, <em>j</em>], indicating how disruptive it would be to remove pixel <em>A</em>[<em>i</em>, <em>j</em>]. Intuitively, the lower a pixel’s disruption measure, the more similar the pixel is to its neighbors. Define the disruption measure of a seam as the sum of the disruption measures of its pixels.</p>
<p class="nl-parat">Give an algorithm to find a seam with the lowest disruption measure. How efficient is your algorithm?</p>
</section>
<section title="14-9 Breaking a string">
<p class="level2"><strong><em>14-9     Breaking a string</em></strong></p>
<p class="noindent">A certain string-processing programming language allows you to break a string into two pieces. Because this operation copies the string, it costs <em>n</em> time units to break a string of <em>n</em> characters into two pieces. Suppose that you want to break a string into many pieces. The order in which the breaks occur can affect the total amount of time used. For example, suppose that you want to break a 20-character string after characters 2, 8, and 10 (numbering the characters in ascending order from the left-hand end, starting from 1). If you program the breaks to occur in left-to-right order, then the first break costs 20 time units, the <a id="p413"/>second break costs 18 time units (breaking the string from characters 3 to 20 at character 8), and the third break costs 12 time units, totaling 50 time units. If you program the breaks to occur in right-to-left order, however, then the first break costs 20 time units, the second break costs 10 time units, and the third break costs 8 time units, totaling 38 time units. In yet another order, you could break first at 8 (costing 20), then break the left piece at 2 (costing another 8), and finally the right piece at 10 (costing 12), for a total cost of 40.</p>
<p>Design an algorithm that, given the numbers of characters after which to break, determines a least-cost way to sequence those breaks. More formally, given an array <em>L</em>[1 : <em>m</em>] containing the break points for a string of <em>n</em> characters, compute the lowest cost for a sequence of breaks, along with a sequence of breaks that achieves this cost.</p>
</section>
<section title="14-10 Planning an investment strategy">
<p class="level2"><strong><em>14-10     Planning an investment strategy</em></strong></p>
<p class="noindent">Your knowledge of algorithms helps you obtain an exciting job with a hot startup, along with a $10,000 signing bonus. You decide to invest this money with the goal of maximizing your return at the end of 10 years. You decide to use your investment manager, G. I. Luvcache, to manage your signing bonus. The company that Luvcache works with requires you to observe the following rules. It offers <em>n</em> different investments, numbered 1 through <em>n</em>. In each year <em>j</em>, investment <em>i</em> provides a return rate of <em>r<sub>ij</sub></em>. In other words, if you invest <em>d</em> dollars in investment <em>i</em> in year <em>j</em>, then at the end of year <em>j</em>, you have <em>dr<sub>ij</sub></em> dollars. The return rates are guaranteed, that is, you are given all the return rates for the next 10 years for each investment. You make investment decisions only once per year. At the end of each year, you can leave the money made in the previous year in the same investments, or you can shift money to other investments, by either shifting money between existing investments or moving money to a new investment. If you do not move your money between two consecutive years, you pay a fee of <em>f</em><sub>1</sub> dollars, whereas if you switch your money, you pay a fee of <em>f</em><sub>2</sub> dollars, where <em>f</em><sub>2</sub> &gt; <em>f</em><sub>1</sub>. You pay the fee once per year at the end of the year, and it is the same amount, <em>f</em><sub>2</sub>, whether you move money in and out of only one investment, or in and out of many investments.</p>
<p class="nl"><strong><em>a.</em></strong> The problem, as stated, allows you to invest your money in multiple investments in each year. Prove that there exists an optimal investment strategy that, in each year, puts all the money into a single investment. (Recall that an optimal investment strategy maximizes the amount of money after 10 years and is not concerned with any other objectives, such as minimizing risk.)</p>
<p class="nl"><strong><em>b.</em></strong> Prove that the problem of planning your optimal investment strategy exhibits optimal substructure.</p>
<p class="nl"><strong><em>c.</em></strong> Design an algorithm that plans your optimal investment strategy. What is the running time of your algorithm?</p>
<a id="p414"/>
<p class="nl"><strong><em>d.</em></strong> Suppose that Luvcache’s company imposes the additional restriction that, at any point, you can have no more than $15,000 in any one investment. Show that the problem of maximizing your income at the end of 10 years no longer exhibits optimal substructure.</p>
</section>
<section title="14-11 Inventory planning">
<p class="level2"><strong><em>14-11     Inventory planning</em></strong></p>
<p class="noindent">The Rinky Dink Company makes machines that resurface ice rinks. The demand for such products varies from month to month, and so the company needs to develop a strategy to plan its manufacturing given the fluctuating, but predictable, demand. The company wishes to design a plan for the next <em>n</em> months. For each month <em>i</em>, the company knows the demand <em>d<sub>i</sub></em>, that is, the number of machines that it will sell. Let <img alt="art" src="images/Art_P483.jpg"/> be the total demand over the next <em>n</em> months. The company keeps a full-time staff who provide labor to manufacture up to <em>m</em> machines per month. If the company needs to make more than <em>m</em> machines in a given month, it can hire additional, part-time labor, at a cost that works out to <em>c</em> dollars per machine. Furthermore, if the company is holding any unsold machines at the end of a month, it must pay inventory costs. The company can hold up to <em>D</em> machines, with the cost for holding <em>j</em> machines given as a function <em>h</em>(<em>j</em>) for <em>j</em> = 1, 2, …, <em>D</em> that monotonically increases with <em>j</em>.</p>
<p>Give an algorithm that calculates a plan for the company that minimizes its costs while fulfilling all the demand. The running time should be polynomial in <em>n</em> and <em>D</em>.</p>
</section>
<section title="14-12 Signing free-agent baseball players">
<p class="level2"><strong><em>14-12     Signing free-agent baseball players</em></strong></p>
<p class="noindent">Suppose that you are the general manager for a major-league baseball team. During the off-season, you need to sign some free-agent players for your team. The team owner has given you a budget of $<em>X</em> to spend on free agents. You are allowed to spend less than $<em>X</em>, but the owner will fire you if you spend any more than $<em>X</em>.</p>
<p>You are considering <em>N</em> different positions, and for each position, <em>P</em> free-agent players who play that position are available.<sup><a epub:type="footnote" href="#footnote_10" id="footnote_ref_10">10</a></sup> Because you do not want to overload your roster with too many players at any position, for each position you may sign at most one free agent who plays that position. (If you do not sign any players at a particular position, then you plan to stick with the players you already have at that position.)</p>
<a id="p415"/>
<p>To determine how valuable a player is going to be, you decide to use a sabermetric statistic<sup><a epub:type="footnote" href="#footnote_11" id="footnote_ref_11">11</a></sup> known as “WAR,” or “wins above replacement.” A player with a higher WAR is more valuable than a player with a lower WAR. It is not necessarily more expensive to sign a player with a higher WAR than a player with a lower WAR, because factors other than a player’s value determine how much it costs to sign them.</p>
<p>For each available free-agent player <em>p</em>, you have three pieces of information:</p>
<ul class="ulnoindent" epub:type="list">
<li>the player’s position,</li>
<li class="litop"><em>p.cost</em>, the amount of money it costs to sign the player, and</li>
<li class="litop"><em>p.war</em>, the player’s WAR.</li></ul>
<p>Devise an algorithm that maximizes the total WAR of the players you sign while spending no more than $<em>X</em>. You may assume that each player signs for a multiple of $100,000. Your algorithm should output the total WAR of the players you sign, the total amount of money you spend, and a list of which players you sign. Analyze the running time and space requirement of your algorithm.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-87"><strong>Chapter notes</strong></p>
<p class="noindent">Bellman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_44">44</a>] began the systematic study of dynamic programming in 1955, publishing a book about it in 1957. The word “programming,” both here and in linear programming, refers to using a tabular solution method. Although optimization techniques incorporating elements of dynamic programming were known earlier, Bellman provided the area with a solid mathematical basis.</p>
<p>Galil and Park [<a epub:type="noteref" href="bibliography001.xhtml#endnote_172">172</a>] classify dynamic-programming algorithms according to the size of the table and the number of other table entries each entry depends on. They call a dynamic-programming algorithm <em>tD</em>/<em>eD</em> if its table size is <em>O</em>(<em>n<sup>t</sup></em>) and each entry depends on <em>O</em>(<em>n<sup>e</sup></em>) other entries. For example, the matrix-chain multiplication algorithm in <a href="chapter014.xhtml#Sec_14.2">Section 14.2</a> is 2<em>D</em>/1<em>D</em>, and the longest-common-subsequence algorithm in <a href="chapter014.xhtml#Sec_14.4">Section 14.4</a> is 2<em>D</em>/0<em>D</em>.</p>
<p>The M<small>ATRIX</small>-C<small>HAIN</small>-O<small>RDER</small> algorithm on page 378 is by Muraoka and Kuck [<a epub:type="noteref" href="bibliography001.xhtml#endnote_339">339</a>]. Hu and Shing [<a epub:type="noteref" href="bibliography001.xhtml#endnote_230">230</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_231">231</a>] give an <em>O</em>(<em>n</em> lg <em>n</em>)-time algorithm for the matrix-chain multiplication problem.</p>
<p>The <em>O</em>(<em>mn</em>)-time algorithm for the longest-common-subsequence problem appears to be a folk algorithm. Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_95">95</a>] posed the question of whether subquadratic <a id="p416"/>algorithms for the LCS problem exist. Masek and Paterson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_316">316</a>] answered this question in the affirmative by giving an algorithm that runs in <em>O</em>(<em>mn</em>/lg <em>n</em>) time, where <em>n</em> ≤ <em>m</em> and the sequences are drawn from a set of bounded size. For the special case in which no element appears more than once in an input sequence, Szymanski [<a epub:type="noteref" href="bibliography001.xhtml#endnote_425">425</a>] shows how to solve the problem in <em>O</em>((<em>n</em> + <em>m</em>) lg(<em>n</em> + <em>m</em>)) time. Many of these results extend to the problem of computing string edit distances (Problem 14-5).</p>
<p>An early paper on variable-length binary encodings by Gilbert and Moore [<a epub:type="noteref" href="bibliography001.xhtml#endnote_181">181</a>], which had applications to constructing optimal binary search trees for the case in which all probabilities <em>p<sub>i</sub></em> are 0, contains an <em>O</em>(<em>n</em><sup>3</sup>)-time algorithm. Aho, Hopcroft, and Ullman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_5">5</a>] present the algorithm from <a href="chapter014.xhtml#Sec_14.5">Section 14.5</a>. Splay trees [<a epub:type="noteref" href="bibliography001.xhtml#endnote_418">418</a>], which modify the tree in response to the search queries, come within a constant factor of the optimal bounds without being initialized with the frequencies. Exercise 14.5-4 is due to Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_264">264</a>]. Hu and Tucker [<a epub:type="noteref" href="bibliography001.xhtml#endnote_232">232</a>] devised an algorithm for the case in which all probabilities <em>p<sub>i</sub></em> are 0 that uses <em>O</em>(<em>n</em><sup>2</sup>) time and <em>O</em>(<em>n</em>) space. Subsequently, Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_261">261</a>] reduced the time to <em>O</em>(<em>n</em> lg <em>n</em>).</p>
<p>Problem 14-8 is due to Avidan and Shamir [<a epub:type="noteref" href="bibliography001.xhtml#endnote_30">30</a>], who have posted on the web a wonderful video illustrating this image-compression technique.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> If pieces are required to be cut in order of monotonically increasing size, there are fewer ways to consider. For <em>n</em> = 4, only 5 such ways are possible: parts (a), (b), (c), (e), and (h) in <a href="chapter014.xhtml#Fig_14-2">Figure 14.2</a>. The number of ways is called the <strong><em><span class="blue1">partition function</span></em></strong>, which is approximately equal to <img alt="art" src="images/Art_P484.jpg"/>. This quantity is less than 2<sup><em>n</em>−1</sup>, but still much greater than any polynomial in <em>n</em>. We won’t pursue this line of inquiry further, however.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> The technical term “memoization” is not a misspelling of “memorization.” The word “memoization” comes from “memo,” since the technique consists of recording a value to be looked up later.</p>
<p class="footnote1" id="footnote_3"><a href="#footnote_ref_3"><sup>3</sup></a> None of the three methods from <a href="chapter004.xhtml#Sec_4.1">Sections 4.1</a> and <a href="chapter004.xhtml#Sec_4.2">Section 4.2</a> can be used directly, because they apply only to square matrices.</p>
<p class="footnote1" id="footnote_4"><a href="#footnote_ref_4"><sup>4</sup></a> The <img alt="art" src="images/subsupn2.jpg"/> term counts all pairs in which <em>i</em> &lt; <em>j</em>. Because <em>i</em> and <em>j</em> may be equal, we need to add in the <em>n</em> term.</p>
<p class="footnote1" id="footnote_5"><a href="#footnote_ref_5"><sup>5</sup></a> We use the term “unweighted” to distinguish this problem from that of finding shortest paths with weighted edges, which we shall see in <a href="chapter022.xhtml">Chapters 22</a> and <a href="chapter023.xhtml">23</a>. You can use the breadth-first search technique of <a href="chapter020.xhtml">Chapter 20</a> to solve the unweighted problem.</p>
<p class="footnote1" id="footnote_6"><a href="#footnote_ref_6"><sup>6</sup></a> It may seem strange that dynamic programming relies on subproblems being both independent and overlapping. Although these requirements may sound contradictory, they describe two different notions, rather than two points on the same axis. Two subproblems of the same problem are independent if they do not share resources. Two subproblems are overlapping if they are really the same subproblem that occurs as a subproblem of different problems.</p>
<p class="footnote1" id="footnote_7"><a href="#footnote_ref_7"><sup>7</sup></a> This approach presupposes that you know the set of all possible subproblem parameters and that you have established the relationship between table positions and subproblems. Another, more general, approach is to memoize by using hashing with the subproblem parameters as keys.</p>
<p class="footnote1" id="footnote_8"><a href="#footnote_ref_8"><sup>8</sup></a> If the subject of the text is ancient Rome, you might want <em>naumachia</em> to appear near the root.</p>
<p class="footnote1" id="footnote_9"><a href="#footnote_ref_9"><sup>9</sup></a> Yes, <em>naumachia</em> has a Latvian counterpart: <em>nomačija</em>.</p>
<p class="footnote1" id="footnote_10"><a href="#footnote_ref_10"><sup>10</sup></a> Although there are nine positions on a baseball team, <em>N</em> is not necessarily equal to 9 because some general managers have particular ways of thinking about positions. For example, a general manager might consider right-handed pitchers and left-handed pitchers to be separate “positions,” as well as starting pitchers, long relief pitchers (relief pitchers who can pitch several innings), and short relief pitchers (relief pitchers who normally pitch at most only one inning).</p>
<p class="footnote1" id="footnote_11"><a href="#footnote_ref_11"><sup>11</sup></a> <strong><em><span class="blue1">Sabermetrics</span></em></strong> is the application of statistical analysis to baseball records. It provides several ways to compare the relative values of individual players.</p>
</section>
</section>
</div>
</body>
</html>