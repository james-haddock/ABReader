<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p791"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="27 Online Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-27"><strong><span class="blue1">27        Online Algorithms</span></strong></a></p>
<p class="noindent">Most problems described in this book have assumed that the entire input was available before the algorithm executes. In many situations, however, the input becomes available not in advance, but only as the algorithm executes. This idea was implicit in much of the discussion of data structures in <a href="part003.xhtml">Part III</a>. The reason that you want to design, for example, a data structure that can handle <em>n</em> I<small>NSERT</small>, D<small>ELETE</small>, and S<small>EARCH</small> operations in <em>O</em>(lg <em>n</em>) time per operation is most likely because you are going to receive <em>n</em> such operation requests without knowing in advance what operations will be coming. This idea was also implicit in amortized analysis in <a href="chapter016.xhtml">Chapter 16</a>, where we saw how to maintain a table that can grow or shrink in response to a sequence of insertion and deletion operations, yet with a constant amortized cost per operation.</p>
<p>An <strong><em><span class="blue1">online algorithm</span></em></strong> receives its input progressively over time, rather than having the entire input available at the start, as in an <strong><em><span class="blue1">offline algorithm</span></em></strong>. Online algorithms pertain to many situations in which information arrives gradually. A stock trader must make decisions today, without knowing what the prices will be tomorrow, yet wants to achieve good returns. A computer system must schedule arriving jobs without knowing what work will need to be done in the future. A store must decide when to order more inventory without knowing what the future demand will be. A driver for a ride-hailing service must decide whether to pick up a fare without knowing who will request rides in the future. In each of these situations, and many more, algorithmic decisions must be made without knowledge of the future.</p>
<p>There are several approaches for dealing with unknown future inputs. One approach is to form a probabilistic model of future inputs and design an algorithm that assumes future inputs conform to the model. This technique is common, for example, in the field of queuing theory, and it is also related to machine learning. Of course, you might not be able to develop a workable probabilistic model, or even if you can, some inputs might not conform to it. This chapter takes a different <a id="p792"/>approach. Instead of assuming anything about the future input, we employ a conservative strategy of limiting how poor a solution any input can entail.</p>
<p>This chapter, therefore, adopts a worst-case approach, designing online algorithms that guarantee the quality of the solution for all possible future inputs. We’ll analyze online algorithms by comparing the solution produced by the online algorithm with a solution produced by an optimal algorithm that knows the future inputs, and taking a worst-case ratio over all possible instances. We call this methodology <strong><em><span class="blue1">competitive analysis</span></em></strong>. We’ll use a similar approach when we study approximation algorithms in <a href="chapter035.xhtml">Chapter 35</a>, where we’ll compare the solution returned by an algorithm that might be suboptimal with the value of the optimal solution, and determine a worst-case ratio over all possible instances.</p>
<p>We start with a “toy” problem: deciding between whether to take the elevator or the stairs. This problem will introduce the basic methodology of thinking about online algorithms and how to analyze them via competitive analysis. We will then look at two problems that use competitive analysis. The first is how to maintain a search list so that the access time is not too large, and the second is about strategies for deciding which cache blocks to evict from a cache or other kind of fast computer memory.</p>
<p class="line1"/>
<section title="27.1 Waiting for an elevator">
<a id="Sec_27.1"/>
<p class="level1" id="h1-157"><a href="toc.xhtml#Rh1-157"><strong>27.1    Waiting for an elevator</strong></a></p>
<p class="noindent">Our first example of an online algorithm models a problem that you likely have encountered yourself: whether you should wait for an elevator to arrive or just take the stairs. Suppose that you enter a building and wish to visit an office that is <em>k</em> floors up. You have two choices: walk up the stairs or take the elevator. Let’s assume, for convenience, that you can climb the stairs at the rate of one floor per minute. The elevator travels much faster than you can climb the stairs: it can ascend all <em>k</em> floors in just one minute. Your dilemma is that you do not know how long it will take for the elevator to arrive at the ground floor and pick you up. Should you take the elevator or the stairs? How do you decide?</p>
<p>Let’s analyze the problem. Taking the stairs takes <em>k</em> minutes, no matter what. Suppose you know that the elevator takes at most <em>B</em> − 1 minutes to arrive for some value of <em>B</em> that is considerably higher than <em>k</em>. (The elevator could be going up when you call for it and then stop at several floors on its way down.) To keep things simple, let’s also assume that the number of minutes for the elevator to arrive is an integer. Therefore, waiting for the elevator and taking it <em>k</em> floors up takes anywhere from one minute (if the elevator is already at the ground floor) to (<em>B</em> − 1) + 1 = <em>B</em> minutes (the worst case). Although you know <em>B</em> and <em>k</em>, you don’t know how long the elevator will take to arrive this time. You can use competitive <a id="p793"/>analysis to inform your decision regarding whether to take the stairs or elevator. In the spirit of competitive analysis, you want to be sure that, no matter what the future brings (i.e., how long the elevator takes to arrive), you will not wait much longer than a seer who knows when the elevator will arrive.</p>
<p>Let us first consider what the seer would do. If the seer knows that the elevator is going to arrive in at most <em>k</em> − 1 minutes, the seer waits for the elevator, and otherwise, the seer takes the stairs. Letting <em>m</em> denote the number of minutes it takes for the elevator to arrive at the ground floor, we can express the time that the seer spends as the function</p>
<p class="eqr"><img alt="art" src="images/Art_P842.jpg"/></p>
<p>We typically evaluate online algorithms by their <strong><em><span class="blue1">competitive ratio</span></em></strong>. Let <em><span class="font1">U</span></em> denote the set (universe) of all possible inputs, and consider some input <em>I</em> ∈ <em><span class="font1">U</span></em>. For a minimization problem, such as the stairs-versus-elevator problem, if an online algorithm <em>A</em> produces a solution with value <em>A</em>(<em>I</em>) on input <em>I</em> and the solution from an algorithm <em>F</em> that knows the future has value <em>F</em>(<em>I</em>) on the same input, then the competitive ratio of algorithm <em>A</em> is</p>
<p class="eql">max {<em>A</em>(<em>I</em>)/<em>F</em>(<em>I</em>) : <em>I</em> ∈ <em><span class="font1">U</span></em>}.</p>
<p class="noindent">If an online algorithm has a competitive ratio of <em>c</em>, we say that it is <strong><em><span class="blue1">c-competitive</span></em></strong>. The competitive ratio is always at least 1, so that we want an online algorithm with a competitive ratio as close to 1 as possible.</p>
<p>In the stairs-versus-elevator problem, the only input is the time for the elevator to arrive. Algorithm <em>F</em> knows this information, but an online algorithm has to make a decision without knowing when the elevator will arrive. Consider the algorithm “always take the stairs,” which always takes exactly <em>k</em> minutes. Using equation (27.1), the competitive ratio is</p>
<p class="eqr"><img alt="art" src="images/Art_P843.jpg"/></p>
<p class="noindent">Enumerating the terms in equation (27.2) gives the competitive ratio as</p>
<p class="eql"><img alt="art" src="images/Art_P844.jpg"/></p>
<p class="noindent">so that the competitive ratio is <em>k</em>. The maximum is achieved when the elevator arrives immediately. In this case, taking the stairs requires <em>k</em> minutes, but the optimal solution takes just 1 minute.</p>
<p>Now let’s consider the opposite approach: “always take the elevator.” If it takes <em>m</em> minutes for the elevator to arrive at the ground floor, then this algorithm will always take <em>m</em> + 1 minutes. Thus the competitive ratio becomes</p>
<p class="eql">max {(<em>m</em> + 1)/<em>t</em>(<em>m</em>) : 0 ≤ <em>m</em> ≤ <em>B</em> − 1},</p>
<a id="p794"/>
<p class="noindent">which we can again enumerate as</p>
<p class="eql"><img alt="art" src="images/Art_P845.jpg"/></p>
<p class="noindent">Now the maximum is achieved when the elevator takes <em>B</em> − 1 minutes to arrive, compared with the optimal approach of taking the stairs, which requires <em>k</em> minutes.</p>
<p>Hence, the algorithm “always take the stairs” has competitive ratio <em>k</em>, and the algorithm “always take the elevator” has competitive ratio <em>B</em>/<em>k</em>. Because we prefer the algorithm with smaller competitive ratio, if <em>k</em> = 10 and <em>B</em> = 300, we prefer “always take the stairs,” with competitive ratio 10, over “always take the elevator,” with competitive ratio 30. Taking the stairs is not always better, or necessarily more often better. It’s just that taking the stairs guards better against the worst-case future.</p>
<p>These two approaches of always taking the stairs and always taking the elevator are extreme solutions, however. Instead, you can “hedge your bets” and guard even better against a worst-case future. In particular, you can wait for the elevator for a while, and then if it doesn’t arrive, take the stairs. How long is “a while”? Let’s say that “a while” is <em>k</em> minutes. Then the time <em>h</em>(<em>m</em>) required by this hedging strategy, as a function of the number <em>m</em> of minutes before the elevator arrives, is</p>
<p class="eql"><img alt="art" src="images/Art_P846.jpg"/></p>
<p class="noindent">In the second case, <em>h</em>(<em>m</em>) = 2<em>k</em> because you wait for <em>k</em> minutes and then climb the stairs for <em>k</em> minutes. The competitive ratio is now</p>
<p class="eql">max {<em>h</em>(<em>m</em>)/<em>t</em>(<em>m</em>) : 0 ≤ <em>m</em> ≤ <em>B</em> − 1}.</p>
<p class="noindent">Enumerating this ratio yields</p>
<p class="eql"><img alt="art" src="images/Art_P847.jpg"/></p>
<p class="noindent">The competitive ratio is now <em>independent</em> of <em>k</em> and <em>B</em>.</p>
<p>This example illustrates a common philosophy in online algorithms: we want an algorithm that guards against any possible worst case. Initially, waiting for the elevator guards against the case when the elevator arrives quickly, but eventually switching to the stairs guards against the case when the elevator takes a long time to arrive.</p>
<a id="p795"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>27.1-1</em></strong></p>
<p class="noindent">Suppose that when hedging your bets, you wait for <em>p</em> minutes, instead of for <em>k</em> minutes, before taking the stairs. What is the competitive ratio as a function of <em>p</em> and <em>k</em>? How should you choose <em>p</em> to minimize the competitive ratio?</p>
<p class="level3"><strong><em>27.1-2</em></strong></p>
<p class="noindent">Imagine that you decide to take up downhill skiing. Suppose that a pair of skis costs <em>r</em> dollars to rent for a day and <em>b</em> dollars to buy, where <em>b</em> &gt; <em>r</em>. If you knew in advance how many days you would ever ski, your decision whether to rent or buy would be easy. If you’ll ski for at least <span class="font1">⌈</span><em>b</em>/<em>r</em><span class="font1">⌉</span> days, then you should buy skis, and otherwise you should rent. This strategy minimizes the total that you ever spend. In reality, you don’t know in advance how many days you’ll eventually ski. Even after you have skied several times, you still don’t know how many more times you’ll ever ski. Yet you don’t want to waste your money. Give and analyze an algorithm that has a competitive ratio of 2, that is, an algorithm guaranteeing that, no matter how many times you ski, you never spend more than twice what you would have spent if you knew from the outset how many times you’ll ski.</p>
<p class="level3"><strong><em>27.1-3</em></strong></p>
<p class="noindent">In “concentration solitaire,” a game for one person, you have <em>n</em> pairs of matching cards. The backs of the cards are all the same, but the fronts contain pictures of animals. One pair has pictures of aardvarks, one pair has pictures of bears, one pair has pictures of camels, and so on. At the start of the game, the cards are all placed face down. In each round, you can turn two cards face up to reveal their pictures. If the pictures match, then you remove that pair from the game. If they don’t match, then you turn both of them over, hiding their pictures once again. The game ends when you have removed all <em>n</em> pairs, and your score is how many rounds you needed to do so. Suppose that you can remember the picture on every card that you have seen. Give an algorithm to play concentration solitaire that has a competitive ratio of 2.</p>
</section>
<p class="line1"/>
<section title="27.2 Maintaining a search list">
<a id="Sec_27.2"/>
<p class="level1" id="h1-158"><a href="toc.xhtml#Rh1-158"><strong>27.2    Maintaining a search list</strong></a></p>
<p class="noindent">The next example of an online algorithm pertains to maintaining the order of elements in a linked list, as in <a href="chapter010.xhtml#Sec_10.2">Section 10.2</a>. This problem often arises in practice for hash tables when collisions are resolved by chaining (see <a href="chapter011.xhtml#Sec_11.2">Section 11.2</a>), since each slot contains a linked list. Reordering the linked list of elements in each slot of the hash table can boost the performance of searches measurably.</p>
<a id="p796"/>
<p>The list-maintenance problem can be set up as follows. You are given a list <em>L</em> of <em>n</em> elements {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>}. We’ll assume that the list is doubly linked, although the algorithms and analysis work just as well for singly linked lists. Denote the position of element <em>x<sub>i</sub></em> in the list <em>L</em> by <em>r<sub>L</sub></em>(<em>x<sub>i</sub></em>), where 1 ≤ <em>r<sub>L</sub></em>(<em>x<sub>i</sub></em>) ≤ <em>n</em>. Calling L<small>IST</small>-S<small>EARCH</small>(<em>L</em>, <em>x<sub>i</sub></em>) on page 260 thus takes Θ(<em>r<sub>L</sub></em>(<em>x<sub>i</sub></em>)) time.</p>
<p>If you know in advance something about the distribution of search requests, then it makes sense to arrange the list ahead of time to put the more frequently searched elements closer to the front, which minimizes the total cost (see Exercise 27.2-1). If instead you don’t know anything about the search sequence, then no matter how you arrange the list, it is possible that every search is for whatever element appears at the tail of the list. The total searching time would then be Θ(<em>nm</em>), where <em>m</em> is the number of searches.</p>
<p>If you notice patterns in the access sequence or you observe differences in the frequencies in which elements are accessed, then you might want to rearrange the list as you perform searches. For example, if you discover that every search is for a particular element, you could move that element to the front of the list. In general, you could rearrange the list after each call to L<small>IST</small>-S<small>EARCH</small>. But how would you do so without knowing the future? After all, no matter how you move elements around, every search could be for the last element.</p>
<p>But it turns out that some search sequences are “easier” than others. Rather than just evaluate performance on the worst-case sequence, let’s compare a reorganization scheme with whatever an optimal offline algorithm would do if it knew the search sequence in advance. That way, if the sequence is fundamentally hard, the optimal offline algorithm will also find it hard, but if the sequence is easy, we can hope to do reasonably well.</p>
<p>To ease analysis, we’ll drop the asymptotic notation and say that the cost is just <em>i</em> to search for the <em>i</em>th element in the list. Let’s also assume that the only way to reorder the elements in the list is by swapping two adjacent elements in the list. Because the list is doubly linked, each swap incurs a cost of 1. Thus, for example, a search for the sixth element followed by moving it forward two places (entailing two swaps) incurs a total cost 8. The goal is to minimize the total cost of calls to L<small>IST</small>-S<small>EARCH</small> plus the total number of swaps performed.</p>
<p>The online algorithm that we’ll explore is M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>(<em>L</em>, <em>x</em>). This procedure first searches for <em>x</em> in the doubly linked list <em>L</em>, and then it moves <em>x</em> to the front of the list.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> If <em>x</em> is located at position <em>r</em> = <em>r<sub>L</sub></em>(<em>x</em>) before the call, M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> swaps <em>x</em> with the element in position <em>r</em> − 1, then with the element in position <em>r</em> − 2, <a id="p797"/>and so on, until it finally swaps <em>x</em> with the element in position 1. Thus if the call M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>(<em>L</em>, 8) executes on the list <em>L</em> = <span class="font1">〈</span>5, 3, 12, 4, 8, 9, 22<span class="font1">〉</span>, the list becomes <span class="font1">〈</span>8, 5, 3, 12, 4, 9, 22<span class="font1">〉</span>. The call M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>(<em>L</em>, <em>k</em>) costs 2<em>r<sub>L</sub></em>(<em>k</em>) − 1: it costs <em>r<sub>L</sub></em>(<em>k</em>) to search for <em>k</em>, and it costs 1 for each of the <em>r<sub>L</sub></em>(<em>k</em>) − 1 swaps that move <em>k</em> to the front of the list.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_27-1"><img alt="art" src="images/Art_P848.jpg"/></p>
<p class="caption"><strong>Figure 27.1</strong> The costs incurred by the procedures F<small>ORESEE</small> and M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> when searching for the elements 5, 3, 4, and 4, starting with the list <em>L</em> = <span class="font1">〈</span>1, 2, 3, 4, 5<span class="font1">〉</span>. If F<small>ORESEE</small> instead moved 3 to the front after the search for 5, the cumulative cost would not change, nor would the cumulative cost change if 4 moved to the second position after the search for 5.</p>
</div>
<p>We’ll see that M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> has a competitive ratio of 4. Let’s think about what this means. M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> performs a series of operations on a doubly linked list, accumulating cost. For comparison, suppose that there is an algorithm F<small>ORESEE</small> that knows the future. Like M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>, it also searches the list and moves elements around, but after each call it optimally rearranges the list for the future. (There may be more than one optimal order.) Thus F<small>ORESEE</small> and M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> maintain different lists of the same elements.</p>
<p>Consider the example shown in <a href="chapter027.xhtml#Fig_27-1">Figure 27.1</a>. Starting with the list <span class="font1">〈</span>1, 2, 3, 4, 5<span class="font1">〉</span>, four searches occur, for the elements 5, 3, 4, and 4. The hypothetical procedure F<small>ORESEE</small>, after searching for 3, moves 4 to the front of the list, knowing that a search for 4 is imminent. It thus incurs a swap cost of 3 upon its second call, after which no further swap costs accrue. M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> incurs swap costs in each step, moving the found element to the front. In this example, M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> has a higher cost in each step, but that is not necessarily always the case.</p>
<p>The key to proving the competitive bound is to show that at any point, the total cost of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> is not much higher than that of F<small>ORESEE</small>. Surprisingly, we can determine a bound on the costs incurred by M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> relative to F<small>ORESEE</small> even though M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> cannot see the future.</p>
<p>If we compare any particular step, M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> and F<small>ORESEE</small> may be operating on very different lists and do very different things. If we focus on the search for 4 above, we observe that F<small>ORESEE</small> actually moves it to the front of the list early, paying to move the element to the front before it is accessed. To capture this concept, <a id="p798"/>we use the idea of an <strong><em><span class="blue1">inversion</span></em></strong>: a pair of elements, say <em>a</em> and <em>b</em>, in which <em>a</em> appears before <em>b</em> in one list, but <em>b</em> appears before <em>a</em> in another list. For two lists <em>L</em> and <em>L</em>′, let <em>I</em>(<em>L</em>, <em>L</em>′), called the <strong><em><span class="blue1">inversion count</span></em></strong>, denote the number of inversions between the two lists, that is, the number of pairs of elements whose order differs in the two lists. For example, with lists <em>L</em> = <span class="font1">〈</span>5,3,1,4,2<span class="font1">〉</span> and <em>L</em>′ = <span class="font1">〈</span>3,1,2,4,5<span class="font1">〉</span>, then out of the <img alt="art" src="images/Art_P849.jpg"/> pairs, exactly five of them—(1, 5), (2, 4), (2, 5), (3, 5), (4, 5)—are inversions, since these pairs, and only these pairs, appear in different orders in the two lists. Thus the inversion count is <em>I</em>(<em>L</em>, <em>L</em>′) = 5.</p>
<p>In order to analyze the algorithm, we define the following notation. Let <img alt="art" src="images/Art_P850.jpg"/> be the list maintained by M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> immediately after the <em>i</em>th search, and similarly, let <img alt="art" src="images/Art_P851.jpg"/> be F<small>ORESEE</small>’s list immediately after the <em>i</em>th search. Let <img alt="art" src="images/Art_P852.jpg"/> and <img alt="art" src="images/Art_P853.jpg"/> be the costs incurred by M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> and F<small>ORESEE</small> on their <em>i</em>th calls, respectively. We don’t know how many swaps F<small>ORESEE</small> performs in its <em>i</em>th call, but we’ll denote that number by <em>t<sub>i</sub></em>. Therefore, if the <em>i</em>th operation is a search for element <em>x</em>, then</p>
<p class="eqr"><img alt="art" src="images/Art_P854.jpg"/></p>
<p>In order to compare these costs more carefully, let’s break down the elements into subsets, depending on their positions in the two lists before the <em>i</em>th search, relative to the element <em>x</em> being searched for in the <em>i</em>th search. We define three sets:</p>
<p class="eql"><em>BB</em> = {elements before <em>x</em> in both <img alt="art" src="images/LM.jpg"/> and <img alt="art" src="images/LF.jpg"/>},</p>
<p class="eql"><em>BA</em> = {elements before <em>x</em> in <img alt="art" src="images/LM.jpg"/> but after <em>x</em> in <img alt="art" src="images/LF.jpg"/>},</p>
<p class="eql"><em>AB</em> = {elements after <em>x</em> in <img alt="art" src="images/LM.jpg"/> but before <em>x</em> in <img alt="art" src="images/LF.jpg"/>}.</p>
<p class="noindent">We can now relate the position of element <em>x</em> in <img alt="art" src="images/LF.jpg"/> and <img alt="art" src="images/LM.jpg"/> to the sizes of these sets:</p>
<p class="eqr"><img alt="art" src="images/Art_P855.jpg"/></p>
<p>When a swap occurs in one of the lists, it changes the relative positions of the two elements involved, which in turn changes the inversion count. Suppose that elements <em>x</em> and <em>y</em> are swapped in some list. Then the only possible difference in the inversion count between this list and <em>any</em> other list depends on whether (<em>x</em>, <em>y</em>) is an inversion. In fact, the inversion count of (<em>x</em>, <em>y</em>) with respect to any other list <em>must</em> change. If (<em>x</em>, <em>y</em>) is an inversion before the swap, it no longer is afterward, and vice versa. Therefore, if two consecutive elements <em>x</em> and <em>y</em> swap positions in a list <em>L</em>, then for any other list <em>L</em>′, the value of the inversion count <em>I</em>(<em>L</em>, <em>L</em>′) either increases by 1 or decreases by 1.</p>
<a id="p799"/>
<p>As we compare M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> and F<small>ORESEE</small> searching and modifying their lists, we’ll think about M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> executing on its list for the <em>i</em>th time and then F<small>ORESEE</small> executing on its list for the <em>i</em>th time. After M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> has executed for the <em>i</em>th time and before F<small>ORESEE</small> has executed for the <em>i</em>th time, we’ll compare <img alt="art" src="images/Art_P856.jpg"/> (the inversion count immediately before the <em>i</em>th call of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>) with <img alt="art" src="images/Art_P857.jpg"/> (the inversion count after the <em>i</em>th call of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> but before the <em>i</em>th call of F<small>ORESEE</small>). We’ll concern ourselves later with what F<small>ORESEE</small> does.</p>
<p>Let us analyze what happens to the inversion count after executing the <em>i</em>th call of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>, and suppose that it searches for element <em>x</em>. More precisely, we’ll compute <img alt="art" src="images/Art_P858.jpg"/>, the change in the inversion count, which gives a rough idea of how much M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>’s list becomes more or less like F<small>ORESEE</small>’s list. After searching, M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> performs a series of swaps with each of the elements on the list <img alt="art" src="images/LM.jpg"/> that precedes <em>x</em>. Using the notation above, the number of such swaps is |<em>BB</em>| + |<em>BA</em>|. Bearing in mind that the list <img alt="art" src="images/LF.jpg"/> has yet to be changed by the <em>i</em>th call of F<small>ORESEE</small>, let’s see how the inversion count changes.</p>
<p>Consider a swap with an element <em>y</em> ∈ <em>BB</em>. Before the swap, <em>y</em> precedes <em>x</em> in both <img alt="art" src="images/LM.jpg"/> and <img alt="art" src="images/LF.jpg"/>. After the swap, <em>x</em> precedes <em>y</em> in <img alt="art" src="images/Art_P859.jpg"/>, and <img alt="art" src="images/LF.jpg"/> does not change. Therefore, the inversion count increases by 1 for each element in <em>BB</em>. Now consider a swap with an element <em>z</em> ∈ <em>BA</em>. Before the swap, <em>z</em> precedes <em>x</em> in <img alt="art" src="images/LM.jpg"/> but <em>x</em> precedes <em>z</em> in <img alt="art" src="images/LF.jpg"/>. After the swap, <em>x</em> precedes <em>z</em> in both lists. Therefore, the inversion count decreases by 1 for each element in <em>BA</em>. Thus altogether, the inversion count increases by</p>
<p class="eqr"><img alt="art" src="images/Art_P860.jpg"/></p>
<p>We have laid the groundwork needed to analyze M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>.</p>
<p class="theo"><strong><em>Theorem 27.1</em></strong></p>
<p class="noindent">Algorithm M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> has a competitive ratio of 4.</p>
<p class="prof"><strong><em>Proof</em></strong>   The proof uses a potential function, as described in <a href="chapter016.xhtml">Chapter 16</a> on amortized analysis. The value Φ<em><sub>i</sub></em> of the potential function after the <em>i</em>th calls of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> and F<small>ORESEE</small> depends on the inversion count:</p>
<p class="eql"><img alt="art" src="images/Art_P861.jpg"/></p>
<p class="noindent">(Intuitively, the factor of 2 embodies the notion that each inversion represents a cost of 2 for M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> relative to F<small>ORESEE</small>: 1 for searching and 1 for swapping.) By equation (27.7), after the <em>i</em>th call of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>, but before the <em>i</em>th call of F<small>ORESEE</small>, the potential increases by 2(|<em>BB</em>| − |<em>BA</em>|). Since the inversion count of the two lists is nonnegative, we have Φ<em><sub>i</sub></em> ≥ 0 for all <em>i</em> ≥ 0. <a id="p800"/>Assuming that M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> and F<small>ORESEE</small> start with the same list, the initial potential Φ<sub>0</sub> is 0, so that Φ<em><sub>i</sub></em> ≥ Φ<sub>0</sub> for all <em>i</em>.</p>
<p>Drawing from equation (16.2) on page 456, the amortized cost <img alt="art" src="images/Art_P862.jpg"/> of the <em>i</em>th M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> operation is</p>
<p class="eql"><img alt="art" src="images/Art_P863.jpg"/></p>
<p class="noindent">where <img alt="art" src="images/Art_P864.jpg"/>, the actual cost of the <em>i</em>th M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> operation, is given by equation (27.3):</p>
<p class="eql"><img alt="art" src="images/Art_P865.jpg"/></p>
<p class="noindent">Now, let’s consider the potential change Φ<em><sub>i</sub></em> − Φ<sub><em>i</em>−1</sub>. Since both <em>L<sup>M</sup></em> and <em>L<sup>F</sup></em> change, let’s consider the changes to one list at a time. Recall that when M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> moves element <em>x</em> to the front, it increases the potential by exactly 2(|<em>BB</em>| − |<em>BA</em>|). We now consider how the optimal algorithm F<small>ORESEE</small> changes its list <em>L<sup>F</sup></em>: it performs <em>t<sub>i</sub></em> swaps. Each swap performed by F<small>ORESEE</small> either increases or decreases the potential by 2, and thus the increase in potential by F<small>ORESEE</small> in the <em>i</em>th call can be at most 2<em>t<sub>i</sub></em>. We therefore have</p>
<p class="eqr"><img alt="art" src="images/Art_P866.jpg"/></p>
<p>We now finish the proof as in <a href="chapter016.xhtml">Chapter 16</a> by showing that the total amortized cost provides an upper bound on the total actual cost, because the initial potential function is 0 and the potential function is always nonnegative. By equation (16.3) on page 456, for any sequence of <em>m</em> M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> operations, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P867.jpg"/></p>
<a id="p801"/>
<p class="noindent">Therefore, we have</p>
<p class="eql"><img alt="art" src="images/Art_P868.jpg"/></p>
<p class="noindent">Thus the total cost of the <em>m</em> M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> operations is at most 4 times the total cost of the <em>m</em> F<small>ORESEE</small> operations, so M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> is 4-competitive.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Isn’t it amazing that we can compare M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> with the optimal algorithm F<small>ORESEE</small> when we have no idea of the swaps that F<small>ORESEE</small> makes? We were able to relate the performance of M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> to the optimal algorithm by capturing how particular properties (swaps in this case) must evolve relative to the optimal algorithm, without actually knowing the optimal algorithm.</p>
<p>The online algorithm M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> has a competitive ratio of 4: on any input sequence, it incurs a cost at most 4 times that of any other algorithm. On a particular input sequence, it could cost much less than 4 times the optimal algorithm, perhaps even matching the optimal algorithm.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>27.2-1</em></strong></p>
<p class="noindent">You are given a set <em>S</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>} of <em>n</em> elements, and you wish to make a static list <em>L</em> (no rearranging once the list is created) containing the elements of <em>S</em> that is good for searching. Suppose that you have a probability distribution, where <em>p</em>(<em>x<sub>i</sub></em>) is the probability that a given search searches for element <em>x<sub>i</sub></em>. Argue that the expected cost for <em>m</em> searches is</p>
<p class="eql"><img alt="art" src="images/Art_P869.jpg"/></p>
<p class="noindent">Prove that this sum is minimized when the elements of <em>L</em> are sorted in decreasing order with respect to <em>p</em>(<em>x<sub>i</sub></em>).</p>
<p class="level3"><strong><em>27.2-2</em></strong></p>
<p class="noindent">Professor Carnac claims that since F<small>ORESEE</small> is an optimal algorithm that knows the future, then at each step it must incur no more cost than M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>. Either prove that Professor Carnac is correct or provide a counterexample.</p>
<a id="p802"/>
<p class="level3"><strong><em>27.2-3</em></strong></p>
<p class="noindent">Another way to maintain a linked list for efficient searching is for each element to maintain a <strong><em><span class="blue1">frequency count</span></em></strong>: the number of times that the element has been searched for. The idea is to rearrange list elements after searches so that the list is always sorted by decreasing frequency count, from largest to smallest. Either show that this algorithm is <em>O</em>(1)-competitive, or prove that it is not.</p>
<p class="level3"><strong><em>27.2-4</em></strong></p>
<p class="noindent">The model in this section charged a cost of 1 for each swap. We can consider an alternative cost model in which, after accessing <em>x</em>, you can move <em>x</em> anywhere earlier in the list, and there is no cost for doing so. The only cost is the cost of the actual accesses. Show that M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> is 2-competitive in this cost model, assuming that the number requests is sufficiently large. (<em>Hint:</em> Use the potential function <img alt="art" src="images/Art_P870.jpg"/>.)</p>
</section>
<p class="line1"/>
<section title="27.3 Online caching">
<a id="Sec_27.3"/>
<p class="level1" id="h1-159"><a href="toc.xhtml#Rh1-159"><strong>27.3    Online caching</strong></a></p>
<p class="noindent">In <a href="chapter015.xhtml#Sec_15.4">Section 15.4</a>, we studied the caching problem, in which <strong><em><span class="blue1">blocks</span></em></strong> of data from the main memory of a computer are stored in the <strong><em><span class="blue1">cache</span></em></strong>: a small but faster memory. In that section, we studied the offline version of the problem, in which we assumed that we knew the sequence of memory requests in advance, and we designed an algorithm to minimize the number of cache misses. In almost all computer systems, caching is, in fact, an online problem. We do not generally know the series of cache requests in advance; they are presented to the algorithm only as the requests for blocks are actually made. To gain a better understanding of this more realistic scenario, we analyze online algorithms for caching. We will first see that all deterministic online algorithms for caching have a lower bound of Ω(<em>k</em>) for the competitive ratio, where <em>k</em> is the size of the cache. We will then present an algorithm with a competitive ratio of Θ(<em>n</em>), where the input size is <em>n</em>, and one with a competitive ratio of <em>O</em>(<em>k</em>), which matches the lower bound. We will end by showing how to use randomization to design an algorithm with a much better competitive ratio of Θ(lg <em>k</em>). We will also discuss the assumptions that underlie randomized online algorithms, via the notion of an adversary, such as we saw in <a href="chapter011.xhtml">Chapter 11</a> and will see in <a href="chapter031.xhtml">Chapter 31</a>.</p>
<p>You can find the terminology used to describe the caching problem in <a href="chapter015.xhtml#Sec_15.4">Section 15.4</a>, which you might wish to review before proceeding.</p>
<a id="p803"/>
<section title="27.3.1 Deterministic caching algorithms">
<p class="level2" id="Sec_27.3.1"><strong>27.3.1    Deterministic caching algorithms</strong></p>
<p class="noindent">In the caching problem, the input comprises a sequence of <em>n</em> memory requests, for data in blocks <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>n</sub></em>, in that order. The blocks requested are not necessarily distinct: each block may appear multiple times within the request sequence. After block <em>b<sub>i</sub></em> is requested, it resides in a cache that can hold up to <em>k</em> blocks, where <em>k</em> is a fixed cache size. We assume that <em>n</em> &gt; <em>k</em>, since otherwise we are assured that the cache can hold all the requested blocks at once. When a block <em>b<sub>i</sub></em> is requested, if it is already in the cache, then a <strong><em><span class="blue1">cache hit</span></em></strong> occurs and the cache remains unchanged. If <em>b<sub>i</sub></em> is not in the cache, then a <strong><em><span class="blue1">cache miss</span></em></strong> occurs. If the cache contains fewer than <em>k</em> blocks upon a cache miss, block <em>b<sub>i</sub></em> is placed into the cache, which now contains one block more than before. If a cache miss occurs with an already full cache, however, some block must be evicted from the cache before <em>b<sub>i</sub></em> can enter. Thus, a caching algorithm must decide which block to evict from the cache upon a cache miss when the cache is full. The goal is to minimize the number of cache misses over the entire request sequence. The caching algorithms considered in this chapter differ only in which block they decide to evict upon a cache miss. We do not consider abilities such as prefetching, in which a block is brought into the cache before an upcoming request in order to avert a future cache miss.</p>
<p>There are many online caching policies to determine which block to evict, including the following:</p>
<ul class="ulnoindent" epub:type="list">
<li>First-in, first-out (FIFO): evict the block that has been in the cache the longest time.</li>
<li class="litop">Last-in, first-out (LIFO): evict the block that has been in the cache the shortest time.</li>
<li class="litop">Least Recently Used (LRU): evict the block whose last use is furthest in the past.</li>
<li class="litop">Least Frequently Used (LFU): evict the block that has been accessed the fewest times, breaking ties by choosing the block that has been in the cache the longest.</li></ul>
<p>To analyze these algorithms, we assume that the cache starts out empty, so that no evictions occur during the first <em>k</em> requests. We wish to compare the performance of an online algorithm to an optimal offline algorithm that knows the future requests. As we will soon see, all these deterministic online algorithms have a lower bound of Ω(<em>k</em>) for their competitive ratio. Some deterministic algorithms also have a competitive ratio with an <em>O</em>(<em>k</em>) upper bound, but some other deterministic algorithms are considerably worse, having a competitive ratio of Θ(<em>n</em>/<em>k</em>).</p>
<p>We now proceed to analyze the LIFO and LRU policies. In addition to assuming that <em>n</em> &gt; <em>k</em>, we will assume that at least <em>k</em> distinct blocks are requested. Otherwise, the cache never fills up and no blocks are evicted, so that all algorithms exhibit the same behavior. We begin by showing that LIFO has a large competitive ratio.</p>
<a id="p804"/>
<p class="theo"><strong><em>Theorem 27.2</em></strong></p>
<p class="noindent">LIFO has a competitive ratio of Θ(<em>n</em>/<em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>.</p>
<p class="prof"><strong><em>Proof</em></strong>   We first show a lower bound of Ω(<em>n</em>/<em>k</em>). Suppose that the input consists of <em>k</em> + 1 blocks, numbered 1, 2, … , <em>k</em> + 1, and the request sequence is</p>
<p class="eql">1, 2, 3, 4, … , <em>k</em>, <em>k</em> + 1, <em>k</em>, <em>k</em> + 1, <em>k</em>, <em>k</em> + 1, … ,</p>
<p class="noindent">where after the initial 1, 2, … , <em>k</em>, <em>k</em> + 1, the remainder of the sequence alternates between <em>k</em> and <em>k</em> + 1, with a total of <em>n</em> requests. The sequence ends on block <em>k</em> if <em>n</em> and <em>k</em> are either both even or both odd, and otherwise, the sequence ends on block <em>k</em>+1. That is, <em>b<sub>i</sub></em> = <em>i</em> for <em>i</em> = 1, 2, … <em>k</em>−1, <em>b<sub>i</sub></em> = <em>k</em>+1 for <em>i</em> = <em>k</em>+1, <em>k</em>+3, … and <em>b<sub>i</sub></em> = <em>k</em> for <em>i</em> = <em>k</em>, <em>k</em> + 2, …. How many blocks does LIFO evict? After the first <em>k</em> requests (which are considered to be cache misses), the cache is filled with blocks 1, 2, … , <em>k</em>. The (<em>k</em> + 1)st request, which is for block <em>k</em> + 1, causes block <em>k</em> to be evicted. The (<em>k</em> + 2)nd request, which is for block <em>k</em>, forces block <em>k</em> + 1 to be evicted, since that block was just placed into the cache. This behavior continues, alternately evicting blocks <em>k</em> and <em>k</em>+1 for the remaining requests. LIFO, therefore, suffers a cache miss on every one of the <em>n</em> requests.</p>
<p>The optimal offline algorithm knows the entire sequence of requests in advance. Upon the first request of block <em>k</em> + 1, it just evicts any block except block <em>k</em>, and then it never evicts another block. Thus, the optimal offline algorithm evicts only once. Since the first <em>k</em> requests are considered cache misses, the total number of cache misses is <em>k</em> + 1. The competitive ratio, therefore, is <em>n</em>/(<em>k</em> + 1), or Ω(<em>n</em>/<em>k</em>).</p>
<p>For the upper bound, observe that on any input of size <em>n</em>, any caching algorithm incurs at most <em>n</em> cache misses. Because the input contains at least <em>k</em> distinct blocks, any caching algorithm, including the optimal offline algorithm, must incur at least <em>k</em> cache misses. Therefore, LIFO has a competitive ratio of <em>O</em>(<em>n</em>/<em>k</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We call such a competitive ratio <strong><em><span class="blue1">unbounded</span></em></strong>, because it grows with the input size. Exercise 27.3-2 asks you to show that LFU also has an unbounded competitive ratio.</p>
<p>FIFO and LRU have a much better competitive ratio of Θ(<em>k</em>). There is a big difference between competitive ratios of Θ(<em>n</em>/<em>k</em>) and Θ(<em>k</em>). The cache size <em>k</em> is independent of the input sequence and does not grow as more requests arrive over time. A competitive ratio that depends on <em>n</em>, on the other hand, does grow with the size of the input sequence and thus can get quite large. It is preferable to use an algorithm with a competitive ratio that does not grow with the input sequence’s size, when possible.</p>
<p>We now show that LRU has a competitive ratio of Θ(<em>k</em>), first showing the upper bound.</p>
<a id="p805"/>
<p class="theo"><strong><em>Theorem 27.3</em></strong></p>
<p class="noindent">LRU has a competitive ratio of <em>O</em>(<em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>.</p>
<p class="prof"><strong><em>Proof</em></strong>   To analyze LRU, we will divide the sequence of requests into <strong><em><span class="blue1">epochs</span></em></strong>. Epoch 1 begins with the first request. Epoch <em>i</em>, for <em>i</em> &gt; 1, begins upon encountering the (<em>k</em> + 1)st distinct request since the beginning of epoch <em>i</em> − 1. Consider the following example of requests with <em>k</em> = 3:</p>
<p class="eqr"><img alt="art" src="images/Art_P871.jpg"/></p>
<p class="noindent">The first <em>k</em> = 3 distinct requests are for blocks 1, 2 and 5, so epoch 2 begins with the first request for block 4. In epoch 2, the first 3 distinct requests are for blocks 4, 1, and 2. Requests for these blocks recur until the request for block 3, and with this request epoch 3 begins. Thus, this example has four epochs:</p>
<p class="eqr"><img alt="art" src="images/Art_P872.jpg"/></p>
<p>Now we consider the behavior of LRU. In each epoch, the first time a request for a particular block appears, it may cause a cache miss, but subsequent requests for that block within the epoch cannot cause a cache miss, since the block is now one of the <em>k</em> most recently used. For example, in epoch 2, the first request for block 4 causes a cache miss, but the subsequent requests for block 4 do not. (Exercise 27.3-1 asks you to show the contents of the cache after each request.) In epoch 3, requests for blocks 3 and 5 cause cache misses, but the request for block 4 does not, because it was recently accessed in epoch 2. Since only the first request for a block within an epoch can cause a cache miss and the cache holds <em>k</em> blocks, each epoch incurs at most <em>k</em> cache misses.</p>
<p>Now consider the behavior of the optimal algorithm. The first request in each epoch must cause a cache miss, even for an optimal algorithm. The miss occurs because, by the definition of an epoch, there <em>must</em> have been <em>k</em> other blocks accessed since the last access to this block.</p>
<p>Since, for each epoch, the optimal algorithm incurs at least one miss and LRU incurs at most <em>k</em>, the competitive ratio is at most <em>k</em>/1 = <em>O</em>(<em>k</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Exercise 27.3-3 asks you to show that FIFO also has a competitive ratio of <em>O</em>(<em>k</em>).</p>
<p>We could show lower bounds of Ω(<em>k</em>) on LRU and FIFO, but in fact, we can make a much stronger statement: <em>any</em> deterministic online caching algorithm must have a competitive ratio of Ω(<em>k</em>). The proof relies on an adversary who knows the online algorithm being used and can tailor the future requests to cause the online algorithm to incur more cache misses than the optimal offline algorithm.</p>
<p>Consider a scenario in which the cache has size <em>k</em> and the set of possible blocks to request is {1, 2, … , <em>k</em> + 1}. The first <em>k</em> requests are for blocks 1, 2, … , <em>k</em>, so <a id="p806"/>that both the adversary and the deterministic online algorithm place these blocks into the cache. The next request is for block <em>k</em> + 1. In order to make room in the cache for block <em>k</em> + 1, the online algorithm evicts some block <em>b</em><sub>1</sub> from the cache. The adversary, knowing that the online algorithm has just evicted block <em>b</em><sub>1</sub>, makes the next request be for <em>b</em><sub>1</sub>, so that the online algorithm must evict some other block <em>b</em><sub>2</sub> to clear room in the cache for <em>b</em><sub>1</sub>. As you might have guessed, the adversary makes the next request be for block <em>b</em><sub>2</sub>, so that the online algorithm evicts some other block <em>b</em><sub>3</sub> to make room for <em>b</em><sub>2</sub>. The online algorithm and the adversary continue in this manner. The online algorithm incurs a cache miss on every request and therefore incurs <em>n</em> cache misses over the <em>n</em> requests.</p>
<p>Now let’s consider an optimal offline algorithm, which knows the future. As discussed in <a href="chapter015.xhtml#Sec_15.4">Section 15.4</a>, this algorithm is known as furthest-in-future, and it always evicts the block whose next request is furthest in the future. Since there are only <em>k</em> + 1 unique blocks, when furthest-in-future evicts a block, we know that it will not be accessed during at least the next <em>k</em> requests. Thus, after the first <em>k</em> cache misses, the optimal algorithm incurs a cache miss at most once every <em>k</em> requests. Therefore, the number of cache misses over <em>n</em> requests is at most <em>k</em> + <em>n</em>/<em>k</em>.</p>
<p>Since the deterministic online algorithm incurs <em>n</em> cache misses and the optimal offline algorithm incurs at most <em>k</em> + <em>n</em>/<em>k</em> cache misses, the competitive ratio is at least</p>
<p class="eql"><img alt="art" src="images/Art_P873.jpg"/></p>
<p class="noindent">For <em>n</em> ≥ <em>k</em><sup>2</sup>, the above expression is at least</p>
<p class="eql"><img alt="art" src="images/Art_P874.jpg"/></p>
<p class="noindent">Thus, for sufficiently long request sequences, we have shown the following:</p>
<p class="theo"><strong><em>Theorem 27.4</em></strong></p>
<p class="noindent">Any deterministic online algorithm for caching with a cache size of <em>k</em> has competitive ratio Ω(<em>k</em>).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Although we can analyze the common caching strategies from the point of view of competitive analysis, the results are somewhat unsatisfying. Yes, we can distinguish between algorithms with a competitive ratio of Θ(<em>k</em>) and those with unbounded competitive ratios. In the end, however, all of these competitive ratios are rather high. The online algorithms we have seen so far are deterministic, and it is this property that the adversary is able to exploit.</p>
<a id="p807"/>
</section>
<section title="27.3.2 Randomized caching algorithms">
<p class="level2" id="Sec_27.3.2"><strong>27.3.2    Randomized caching algorithms</strong></p>
<p class="noindent">If we don’t limit ourselves to deterministic online algorithms, we can use randomization to develop an online caching algorithm with a significantly smaller competitive ratio. Before describing the algorithm, let’s discuss randomization in online algorithms in general. Recall that we analyze online algorithms with respect to an adversary who knows the online algorithm and can design requests knowing the decisions made by the online algorithm. With randomization, we must ask whether the adversary also knows the random choices made by the online algorithm. An adversary who does not know the random choices is <strong><em><span class="blue1">oblivious</span></em></strong>, and an adversary who knows the random choices is <strong><em><span class="blue1">nonoblivious</span></em></strong>. Ideally, we prefer to design algorithms against a nonoblivious adversary, as this adversary is stronger than an oblivious one. Unfortunately, a nonoblivious adversary mitigates much of the power of randomness, as an adversary who knows the outcome of random choices typically can act as if the online algorithm is deterministic. The oblivious adversary, on the other hand, does not know the random choices of the online algorithm, and that is the adversary we typically use.</p>
<p>As a simple illustration of the difference between an oblivious and nonoblivious adversary, imagine that you are flipping a fair coin <em>n</em> times, and the adversary wants to know how many heads you flipped. A nonoblivious adversary knows, after each flip, whether the coin came up heads or tails, and hence knows how many heads you flipped. An oblivious adversary, on the other hand, knows only that you are flipping a fair coin <em>n</em> times. The oblivious adversary, therefore, can reason that the number of heads follows a binomial distribution, so that the expected number of heads is <em>n</em>/2 (by equation (C.41) on page 1199) and the variance is <em>n</em>/4 (by equation (C.44) on page 1200). But the oblivious adversary has no way of knowing exactly how many heads you actually flipped.</p>
<p>Let’s return to caching. We’ll start with a deterministic algorithm and then randomize it. The algorithm we’ll use is an approximation of LRU called M<small>ARKING</small>. Rather than “least recently used,” think of M<small>ARKING</small> as simply “recently used.” M<small>ARKING</small> maintains a 1-bit attribute <em>mark</em> for each block in the cache. Initially, all blocks in the cache are unmarked. When a block is requested, if it is already in the cache, it is marked. If the request is a cache miss, M<small>ARKING</small> checks to see whether there are any unmarked blocks in the cache. If all blocks are marked, then they are all changed to unmarked. Now, regardless of whether all blocks in the cache were marked when the request occurred, there is at least one unmarked block in the cache, and so an arbitrary unmarked block is evicted, and the requested block is placed into the cache and marked.</p>
<p>How should the block to evict from among the unmarked blocks in the cache be chosen? The procedure R<small>ANDOMIZED</small>-M<small>ARKING</small> on the next page shows the <a id="p808"/>process when the block is chosen randomly. The procedure takes as input a block <em>b</em> being requested.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ANDOMIZED</small>-M<small>ARKING</small>(<em>b</em>)</p>
<table class="table1">
<tr>
<td class="td1"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>if</strong> block <em>b</em> resides in the cache,</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><em>b.mark</em> = 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="noindent"><strong>else</strong></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><strong>if</strong> all blocks <em>b</em>′ in the cache have <em>b</em>′.<em>mark</em> = 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p3">unmark all blocks <em>b</em>′ in the cache, setting <em>b</em>′.<em>mark</em> = 0</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p2">select an unmarked block <em>u</em> with <em>u</em>.<em>mark</em> = 0 uniformly at random</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p2">evict block <em>u</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="p2">place block <em>b</em> into the cache</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">9</span></p></td>
<td class="td1"><p class="p2"><em>b</em>.<em>mark</em> = 1</p></td>
</tr>
</table>
</div>
<p>For the purpose of analysis, we say that a new epoch begins immediately after each time line 5 executes. An epoch starts with no marked blocks in the cache. The first time a block is requested during an epoch, the number of marked blocks increases by 1, and any subsequent requests to that block do not change the number of marked blocks. Therefore, the number of marked blocks monotonically increases within an epoch. Under this view, epochs are the same as in the proof of Theorem 27.3: with a cache that holds <em>k</em> blocks, an epoch comprises requests for <em>k</em> distinct blocks (possibly fewer for the final epoch), and the next epoch begins upon a request for a block not in those <em>k</em>.</p>
<p>Because we are going to analyze a randomized algorithm, we will compute the expected competitive ratio. Recall that for an input <em>I</em>, we denote the solution value of an online algorithm <em>A</em> by <em>A</em>(<em>I</em>) and the solution value of an optimal algorithm <em>F</em> by <em>F</em>(<em>I</em>). Online algorithm <em>A</em> has an <strong><em><span class="blue1">expected competitive ratio</span></em></strong> <em>c</em> if for all inputs <em>I</em>, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P875.jpg"/></p>
<p class="noindent">where the expectation is taken over the random choices made by <em>A</em>.</p>
<p>Although the deterministic M<small>ARKING</small> algorithm has a competitive ratio of Θ(<em>k</em>) (Theorem 27.4 provides the lower bound and see Exercise 27.3-4 for the upper bound), R<small>ANDOMIZED</small>-M<small>ARKING</small> has a much smaller expected competitive ratio, namely <em>O</em>(lg <em>k</em>). The key to the improved competitive ratio is that the adversary cannot always make a request for a block that is not in the cache, since an oblivious adversary does not know which blocks are in the cache.</p>
<a id="p809"/>
<p class="theo"><strong><em>Theorem 27.5</em></strong></p>
<p class="noindent">R<small>ANDOMIZED</small>-M<small>ARKING</small> has an expected competitive ratio of <em>O</em>(lg <em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>, against an oblivious adversary.</p>
<p class="space-break">Before proving Theorem 27.5, we prove a basic probabilistic fact.</p>
<p class="lem"><strong><em>Lemma 27.6</em></strong></p>
<p class="noindent">Suppose that a bag contains <em>x</em> + <em>y</em> balls: <em>x</em> − 1 blue balls, <em>y</em> white balls, and 1 red ball. You repeatedly choose a ball at random and remove it from the bag until you have chosen a total of <em>m</em> balls that are either blue or red, where <em>m</em> ≤ <em>x</em>. You set aside each white ball you choose. Then, one of the balls chosen is the red ball with probability <em>m</em>/<em>x</em>.</p>
<p class="prof"><strong><em>Proof</em></strong>   Choosing a white ball does not affect how many blue or red balls are chosen in any way. Therefore, we can continue the analysis as if there were no white balls and the bag contains just <em>x</em> − 1 blue balls and 1 red ball.</p>
<p>Let <em>A</em> be the event that the red ball is not chosen, and let <em>A<sub>i</sub></em> be the event that the <em>i</em>th draw does not choose the red ball. By equation (C.22) on page 1190, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P876.jpg"/></p>
<p class="noindent">The probability Pr{<em>A</em><sub>1</sub>} that the first ball is blue equals (<em>x</em> − 1)/<em>x</em>, since initially there are <em>x</em> − 1 blue balls and 1 red ball. More generally, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P877.jpg"/></p>
<p class="noindent">since the <em>i</em>th draw is from <em>x</em> − <em>i</em> blue balls and 1 red ball. Equations (27.13) and (27.14) give</p>
<p class="eqr"><img alt="art" src="images/Art_P878.jpg"/></p>
<p class="noindent">The right-hand side of equation (27.15) is a telescoping product, similar to the telescoping series in equation (A.12) on page 1143. The numerator of one term equals the denominator of the next, so that everything except the first denominator and last numerator cancel, and we obtain Pr{<em>A</em>} = (<em>x</em> − <em>m</em>)/<em>x</em>. Since we actually want to compute Pr{<em><span class="font1">Ā</span></em>} = 1 − Pr{<em>A</em>}, that is, the probability that the red ball <em>is</em> chosen, we get Pr{<em><span class="font1">Ā</span></em>} = 1 − (<em>x</em> − <em>m</em>)/<em>x</em> = <em>m</em>/<em>x</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Now we can prove Theorem 27.5.</p>
<a id="p810"/>
<p class="prof"><strong><em>Proof</em></strong>   We’ll analyze R<small>ANDOMIZED</small>-M<small>ARKING</small> one epoch at a time. Within epoch <em>i</em>, any request for a block <em>b</em> that is not the first request for block <em>b</em> in epoch <em>i</em> must result in a cache hit, since after the first request in epoch <em>i</em>, block <em>b</em> resides in the cache and is marked, so that it cannot be evicted during the epoch. Therefore, since we are counting cache misses, we’ll consider only the first request for each block within each epoch, disregarding all other requests.</p>
<p>We can classify the requests in an epoch as either old or new. If block <em>b</em> resides in the cache at the start of epoch <em>i</em>, each request for block <em>b</em> during epoch <em>i</em> is an <strong><em><span class="blue1">old request</span></em></strong>. Old requests in epoch <em>i</em> are for blocks requested in epoch <em>i</em> − 1. If a request in epoch <em>i</em> is not old, it is a <strong><em><span class="blue1">new request</span></em></strong>, and it is for a block not requested in epoch <em>i</em> − 1. All requests in epoch 1 are new. For example, let’s look again at the request sequence in example (27.11):</p>
<table class="table2">
<tr>
<td class="td1">1, 2, 1, 5</td>
<td class="td1">4, 4, 1, 2, 4, 2</td>
<td class="td1">3, 4, 5</td>
<td class="td1">2, 2, 1, 2, 2.</td>
</tr>
</table>
<p class="noindent">Since we can disregard all requests for a block within an epoch other than the first request, to analyze the cache behavior, we can view this request sequence as just</p>
<table class="table2">
<tr>
<td class="td1">1, 2, 5</td>
<td class="td1">4, 1, 2</td>
<td class="td1">3, 4, 5</td>
<td class="td1">2, 1.</td>
</tr>
</table>
<p class="noindent">All three requests in epoch 1 are new. In epoch 2, the requests for blocks 1 and 2 are old, but the request for block 4 is new. In epoch 3, the request for block 4 is old, and the requests for blocks 3 and 5 are new. Both requests in epoch 4 are new.</p>
<p>Within an epoch, each new request must cause a cache miss since, by definition, the block is not already in the cache. An old request, on the other hand, may or may not cause a cache miss. The old block is in the cache at the beginning of the epoch, but other requests might cause it to be evicted. Returning to our example, in epoch 2, the request for block 4 must cause a cache miss, as this request is new. The request for block 1, which is old, may or may not cause a cache miss. If block 1 was evicted when block 4 was requested, then a cache miss occurs and block 1 must be brought back into the cache. If instead block 1 was not evicted when block 4 was requested, then the request for block 1 results in a cache hit. The request for block 2 could incur a cache miss under two scenarios. One is if block 2 was evicted when block 4 was requested. The other is if block 1 was evicted when block 4 was requested, and then block 2 was evicted when block 1 was requested. We see that, within an epoch, each ensuing old request has an increasing chance of causing a cache miss.</p>
<p>Because we consider only the first request for each block within an epoch, we assume that each epoch contains exactly <em>k</em> requests, and each request within an epoch is for a unique block. (The last epoch might contain fewer than <em>k</em> requests. If it does, just add dummy requests to fill it out to <em>k</em> requests.) In epoch <em>i</em>, denote the number of new requests by <em>r<sub>i</sub></em> ≥ 1 (an epoch must contain at least one new <a id="p811"/>request), so that the number of old requests is <em>k</em> − <em>r<sub>i</sub></em>. As mentioned above, a new request always incurs a cache miss.</p>
<p>Let us now focus on an arbitrary epoch <em>i</em> to obtain a bound on the expected number of cache misses within that epoch. In particular, let’s think about the <em>j</em>th old request within the epoch, where 1 ≤ <em>j</em> &lt; <em>k</em>. Denote by <em>b<sub>ij</sub></em> the block requested in the <em>j</em>th old request of epoch <em>i</em>, and denote by <em>n<sub>ij</sub></em> and <em>o<sub>ij</sub></em> the number of new and old requests, respectively, that occur within epoch <em>i</em> but before the <em>j</em>th old request. Because <em>j</em> − 1 old requests occur before the <em>j</em>th old request, we have <em>o<sub>ij</sub></em> = <em>j</em> − 1. We will show that the probability of a cache miss upon the <em>j</em> th old request is <em>n<sub>ij</sub></em>/(<em>k</em> − <em>o<sub>ij</sub></em>), or <em>n<sub>ij</sub></em>/(<em>k</em> − <em>j</em> + 1).</p>
<p>Start by considering the first old request, for block <em>b</em><sub><em>i</em>,1</sub>. What is the probability that this request causes a cache miss? It causes a cache miss precisely when one of the <em>n</em><sub><em>i</em>,1</sub> previous requests resulted in <em>b</em><sub><em>i</em>,1</sub> being evicted. We can determine the probability that <em>b</em><sub><em>i</em>,1</sub> was chosen for eviction by using Lemma 27.6: consider the <em>k</em> blocks in the cache to be <em>k</em> balls, with block <em>b</em><sub><em>i</em>,1</sub> as the red ball, the other <em>k</em> − 1 blocks as the <em>k</em> − 1 blue balls, and no white balls. Each of the <em>n</em><sub><em>i</em>,1</sub> requests chooses a block to evict with equal probability, corresponding to drawing balls <em>n</em><sub><em>i</em>,1</sub> times. Thus, we can apply Lemma 27.6 with <em>x</em> = <em>k</em>, <em>y</em> = 0, and <em>m</em> = <em>n</em><sub><em>i</em>,1</sub>, deriving the probability of a cache miss upon the first old request as <em>n</em><sub><em>i</em>,1</sub>/<em>k</em>, which equals <em>n<sub>ij</sub></em>/(<em>k</em> − <em>j</em> + 1) since <em>j</em> = 1.</p>
<p>In order to determine the probability of a cache miss for subsequent old requests, we’ll need an additional observation. Let’s consider the second old request, which is for block <em>b</em><sub><em>i</em>,2</sub>. This request causes a cache miss precisely when one of the previous requests evicts <em>b</em><sub><em>i</em>,2</sub>. Let’s consider two cases, based on the request for <em>b</em><sub><em>i</em>,1</sub>. In the first case, suppose that the request for <em>b</em><sub><em>i</em>,1</sub> did not cause an eviction, because <em>b</em><sub><em>i</em>,1</sub> was already in the cache. Then, the only way that <em>b</em><sub><em>i</em>,2</sub> could have been evicted is by one of the <em>n</em><sub><em>i</em>,2</sub> new requests that precedes it. What is the probability that this eviction happens? There are <em>n</em><sub><em>i</em>,2</sub> chances for <em>b</em><sub><em>i</em>,2</sub> to be evicted, but we also know that there is one block in the cache, namely <em>b</em><sub><em>i</em>,1</sub>, that is not evicted. Thus, we can again apply Lemma 27.6, but with <em>b</em><sub><em>i</em>,1</sub> as the white ball, <em>b</em><sub><em>i</em>,2</sub> as the red ball, the remaining blocks as the blue balls, and drawing balls <em>n</em><sub><em>i</em>,2</sub> times. Applying Lemma 27.6, with <em>x</em> = <em>k</em> − 1, <em>y</em> = 1, and <em>m</em> = <em>n</em><sub><em>i</em>,2</sub>, we find that the probability of a cache miss is <em>n</em><sub><em>i</em>,2</sub>/(<em>k</em> − 1). In the second case, the request for <em>b</em><sub><em>i</em>,1</sub> does cause an eviction, which can happen only if one of the new requests preceding the request for <em>b</em><sub><em>i</em>,1</sub> evicts <em>b</em><sub><em>i</em>,1</sub>. Then, the request for <em>b</em><sub><em>i</em>,1</sub> brings <em>b</em><sub><em>i</em>,1</sub> back into the cache and evicts some other block. In this case, we know that of the new requests, one of them did not result in <em>b</em><sub><em>i</em>,2</sub> being evicted, since <em>b</em><sub><em>i</em>,1</sub> was evicted. Therefore, <em>n</em><sub><em>i</em>,2</sub> − 1 new requests could evict <em>b</em><sub><em>i</em>,2</sub>, as could the request for <em>b</em><sub><em>i</em>,1</sub>, so that the number of requests that could evict <em>b</em><sub><em>i</em>,2</sub> is <em>n</em><sub><em>i</em>,2</sub>. Each such request evicts a block chosen from among <em>k</em> − 1 blocks, since the request that resulted in evicting <em>b</em><sub><em>i</em>,1</sub> did not also cause <em>b</em><sub><em>i</em>,2</sub> to be evicted. Therefore, we can apply Lemma 27.6, with <em>x</em> = <em>k</em> − 1, <a id="p812"/><em>y</em> = 1, and <em>m</em> = <em>n</em><sub><em>i</em>,2</sub>, and get that the probability of a miss is <em>n</em><sub><em>i</em>,2</sub>/(<em>k</em> − 1). In both cases the probability is the same, and it equals <em>n<sub>ij</sub></em>/(<em>k</em> − <em>j</em> + 1) since <em>j</em> = 2.</p>
<p>More generally, <em>o<sub>ij</sub></em> old requests occur before the <em>j</em>th old request. Each of these prior old requests either caused an eviction or did not. For those that caused an eviction, it is because they were evicted by a previous request, and for those that did not cause an eviction, it is because they were not evicted by any previous request. In either case, we can decrease the number of blocks that the random process is choosing from by 1 for each old request, and thus <em>o<sub>ij</sub></em> requests cannot cause <em>b<sub>ij</sub></em> to be evicted. Therefore, we can use Lemma 27.6 to determine the probability that <em>b<sub>ij</sub></em> was evicted by a previous request, with <em>x</em> = <em>k</em> − <em>o<sub>ij</sub></em>, <em>y</em> = <em>o<sub>ij</sub></em> and <em>m</em> = <em>n<sub>ij</sub></em>. Thus, we have proven our claim that the probability of a cache miss on the <em>j</em>th request for an old block is <em>n<sub>ij</sub></em>/(<em>k</em> − <em>o<sub>ij</sub></em>), or <em>n<sub>ij</sub></em>/(<em>k</em> − <em>j</em> + 1). Since <em>n<sub>ij</sub></em> ≤ <em>r<sub>i</sub></em> (recall that <em>r<sub>i</sub></em> is the number of new requests during epoch <em>i</em>), we have an upper bound of <em>r<sub>i</sub></em>/(<em>k</em> − <em>j</em> + 1) on the probability that the <em>j</em>th old request incurs a cache miss.</p>
<p>We can now compute the expected number of misses during epoch <em>i</em> using indicator random variables, as introduced in <a href="chapter005.xhtml#Sec_5.2">Section 5.2</a>. We define indicator random variables</p>
<p class="eql"><em>Y</em><sub><em>ij</em></sub> = I{the <em>j</em>th old request in epoch <em>i</em> incurs a cache miss},</p>
<p class="eql"><em>Z</em><sub><em>ij</em></sub> = I{the <em>j</em> th new request in epoch <em>i</em> incurs a cache miss}.</p>
<p class="noindent">We have <em>Z<sub>ij</sub></em> = 1 for <em>j</em> = 1, 2, … , <em>r<sub>i</sub></em>, since every new request results in a cache miss. Let <em>X<sub>i</sub></em> be the random variable denoting the number of cache misses during epoch <em>i</em>, so that</p>
<p class="eql"><img alt="art" src="images/Art_P879.jpg"/></p>
<p class="noindent">and so</p>
<p class="eql"><img alt="art" src="images/Art_P880.jpg"/></p>
<a id="p813"/>
<p class="noindent">where <em>H<sub>k</sub></em> is the <em>k</em>th harmonic number.</p>
<p>To compute the expected total number of cache misses, we sum over all epochs. Let <em>p</em> denote the number of epochs and <em>X</em> be the random variable denoting the number of cache misses. Then, we have <img alt="art" src="images/Art_P881.jpg"/>, so that</p>
<p class="eqr"><img alt="art" src="images/Art_P882.jpg"/></p>
<p>To complete the analysis, we need to understand the behavior of the optimal offline algorithm. It could make a completely different set of decisions from those made by R<small>ANDOMIZED</small>-M<small>ARKING</small>, and at any point its cache may look nothing like the cache of the randomized algorithm. Yet, we want to relate the number of cache misses of the optimal offline algorithm to the value in inequality (27.17), in order to have a competitive ratio that does not depend on <img alt="art" src="images/Art_P883.jpg"/>. Focusing on individual epochs won’t suffice. At the beginning of any epoch, the offline algorithm might have loaded the cache with exactly the blocks that will be requested in that epoch. Therefore, we cannot take any one epoch in isolation and claim that an offline algorithm must suffer any cache misses during that epoch.</p>
<p>If we consider two consecutive epochs, however, we can better analyze the optimal offline algorithm. Consider two consecutive epochs, <em>i</em> −1 and <em>i</em>. Each contains <em>k</em> requests for <em>k</em> different blocks. (Recall our assumption that all requests are first requests in an epoch.) Epoch <em>i</em> contains <em>r<sub>i</sub></em> requests for new blocks, that is, blocks that were not requested during epoch <em>i</em> − 1. Therefore, the number of distinct requests during epochs <em>i</em>−1 and <em>i</em> is exactly <em>k</em>+<em>r<sub>i</sub></em>. No matter what the cache contents were at the beginning of epoch <em>i</em> − 1, after <em>k</em> + <em>r<sub>i</sub></em> distinct requests, there must be at least <em>r<sub>i</sub></em> cache misses. There could be more, but there is no way to have fewer. Letting <em>m</em><sub><em>i</em></sub> denote the number of cache misses of the offline algorithm during epoch <em>i</em>, we have just argued that</p>
<p class="eqr"><img alt="art" src="images/Art_P884.jpg"/></p>
<a id="p814"/>
<p class="noindent">The total number of cache misses of the offline algorithm is</p>
<p class="eql"><img alt="art" src="images/Art_P885.jpg"/></p>
<p class="noindent">The justification <em>m</em><sub>1</sub> = <em>r</em><sub>1</sub> for the last equality follows because, by our assumptions, the cache starts out empty and every request incurs a cache miss in the first epoch, even for the optimal offline adversary.</p>
<p>To conclude the analysis, because we have an upper bound of <img alt="art" src="images/Art_P886.jpg"/> on the expected number of cache misses for R<small>ANDOMIZED</small>-M<small>ARKING</small> and a lower bound of <img alt="art" src="images/Art_P887.jpg"/> on the number of cache misses for the optimal offline algorithm, the expected competitive ratio is at most</p>
<p class="eql"><img alt="art" src="images/Art_P888.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>27.3-1</em></strong></p>
<p class="noindent">For the cache sequence (27.10), show the contents of the cache after each request and count the number of cache misses. How many misses does each epoch incur?</p>
<p class="level3"><strong><em>27.3-2</em></strong></p>
<p class="noindent">Show that LFU has a competitive ratio of Θ(<em>n</em>/<em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>.</p>
<p class="level3"><strong><em>27.3-3</em></strong></p>
<p class="noindent">Show that FIFO has a competitive ratio of <em>O</em>(<em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>.</p>
<a id="p815"/>
<p class="level3"><strong><em>27.3-4</em></strong></p>
<p class="noindent">Show that the deterministic M<small>ARKING</small> algorithm has a competitive ratio of <em>O</em>(<em>k</em>) for the online caching problem with <em>n</em> requests and a cache of size <em>k</em>.</p>
<p class="level3"><strong><em>27.3-5</em></strong></p>
<p class="noindent">Theorem 27.4 shows that any deterministic online algorithm for caching has a competitive ratio of Ω(<em>k</em>), where <em>k</em> is the cache size. One way in which an algorithm might be able to perform better is to have some ability to know what the next few requests will be. We say that an algorithm is <strong><em><span class="blue1">l-lookahead</span></em></strong> if it has the ability to look ahead at the next <em>l</em> requests. Prove that for every constant <em>l</em> ≥ 0 and every cache size <em>k</em> ≥1, every deterministic <em>l</em>-lookahead algorithm has competitive ratio Ω(<em>k</em>).</p>
</section>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-160"><strong>Problems</strong></p>
<section title="27-1 Cow-path problem">
<p class="level2"><strong><em>27-1     Cow-path problem</em></strong></p>
<p class="noindent">The Appalachian Trail (AT) is a marked hiking trail in the eastern United States extending between Springer Mountain in Georgia and Mount Katahdin in Maine. The trail is about 2,190 miles long. You decide that you are going to hike the AT from Georgia to Maine and back. You plan to learn more about algorithms while on the trail, and so you bring along your copy of <em>Introduction to Algorithms</em> in your backpack.<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> You have already read through this chapter before starting out. Because the beauty of the trail distracts you, you forget about reading this book until you have reached Maine and hiked halfway back to Georgia. At that point, you decide that you have already seen the trail and want to continue reading the rest of the book, starting with <a href="chapter028.xhtml">Chapter 28</a>. Unfortunately, you find that the book is no longer in your pack. You must have left it somewhere along the trail, but you don’t know where. It could be anywhere between Georgia and Maine. You want to find the book, but now that you have learned something about online algorithms, you want your algorithm for finding it to have a good competitive ratio. That is, no matter where the book is, if its distance from you is <em>x</em> miles away, you would like to be sure that you do not walk more than <em>cx</em> miles to find it, for some constant <em>c</em>. You do not know <em>x</em>, though you may assume that <em>x</em> ≥ 1.<sup><a epub:type="footnote" href="#footnote_3" id="footnote_ref_3">3</a></sup></p>
<a id="p816"/>
<p>What algorithm should you use, and what constant <em>c</em> can you prove bounds the total distance <em>cx</em> that you would have to walk? Your algorithm should work for a trail of any length, not just the 2,190-mile-long AT.</p>
</section>
<section title="27-2 Online scheduling to minimize average completion time">
<p class="level2"><strong><em>27-2     Online scheduling to minimize average completion time</em></strong></p>
<p class="noindent">Problem 15-2 discusses scheduling to minimize average completion time on one machine, without release times and preemption and with release times and preemption. Now you will develop an online algorithm for nonpreemptively scheduling a set of tasks with release times. Suppose you are given a set <em>S</em> = {<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em>} of tasks, where task <em>a<sub>i</sub></em> has <strong><em><span class="blue1">release time</span></em></strong> <em>r<sub>i</sub></em>, before which it cannot start, and requires <em>p<sub>i</sub></em> units of processing time to complete once it has started. You have one computer on which to run the tasks. Tasks cannot be <strong><em><span class="blue1">preempted</span></em></strong>, which is to say that once started, a task must run to completion without interruption. (See Problem 15-2 on page 446 for a more detailed description of this problem.) Given a schedule, let <em>C<sub>i</sub></em> be the <strong><em><span class="blue1">completion time</span></em></strong> of task <em>a<sub>i</sub></em>, that is, the time at which task <em>a<sub>i</sub></em> completes processing. Your goal is to find a schedule that minimizes the average completion time, that is, to minimize <img alt="art" src="images/Art_P889.jpg"/>.</p>
<p>In the online version of this problem, you learn about task <em>i</em> only when it arrives at its release time <em>r<sub>i</sub></em>, and at that point, you know its processing time <em>p<sub>i</sub></em>. The offline version of this problem is NP-hard (see <a href="chapter034.xhtml">Chapter 34</a>), but you will develop a 2-competitive online algorithm.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Show that, if there are release times, scheduling by shortest processing time (when the machine becomes idle, start the already released task with the smallest processing time that has not yet run) is not <em>d</em>-competitive for any constant <em>d</em>.</p>
<p class="space-break">In order to develop an online algorithm, consider the preemptive version of this problem, which is discussed in Problem 15-2(b). One way to schedule is to run the tasks according to the shortest remaining processing time (SRPT) order. That is, at any point, the machine is running the available task with the smallest amount of remaining processing time.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Explain how to run SRPT as an online algorithm.</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Suppose that you run SRPT and obtain completion times <img alt="art" src="images/Art_P890.jpg"/>. Show that</p>
<p class="nl-1list-dp1"><img alt="art" src="images/Art_P891.jpg"/></p>
<p class="nl-1list-dp1">where the <img alt="art" src="images/Art_P892.jpg"/> are the completion times in an optimal nonpreemptive schedule.</p>
<a id="p817"/>
<p class="noindent1-top">Consider the (offline) algorithm C<small>OMPLETION</small>-T<small>IME</small>-S<small>CHEDULE</small>.</p>
<div class="pull-quote1">
<p class="box-heading">C<small>OMPLETION</small>-T<small>IME</small>-S<small>CHEDULE</small>(<em>S</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="tdph">compute an optimal schedule for the preemptive version of the problem</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="tdph">renumber the tasks so that the completion times in the optimal preemptive schedule are ordered by their completion times <img alt="art" src="images/Art_P893.jpg"/> in SRPT order</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="tdph">greedily schedule the tasks nonpreemptively in the renumbered order <em>a</em><sub>1</sub>, … , <em>a<sub>n</sub></em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="tdph">let <em>C</em><sub>1</sub>, … , <em>C<sub>n</sub></em> be the completion times of renumbered tasks <em>a</em><sub>1</sub>, … , <em>a<sub>n</sub></em> in this nonpreemptive schedule</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="tdph"><strong>return</strong> <em>C</em><sub>1</sub>, … , <em>C<sub>n</sub></em></p></td>
</tr>
</table>
</div>
<p class="nl-1list-d"><strong><em>d.</em></strong> Prove that <img alt="art" src="images/Art_P894.jpg"/> for <em>i</em> = 1, … , <em>n</em>.</p>
<p class="nl-1list-d"><strong><em>e.</em></strong> Prove that <img alt="art" src="images/Art_P895.jpg"/> for <em>i</em> = 1, … , <em>n</em>.</p>
<p class="nl-1list-d"><strong><em>f.</em></strong> Algorithm C<small>OMPLETION</small>-T<small>IME</small>-S<small>CHEDULE</small> is an offline algorithm. Explain how to modify it to produce an online algorithm.</p>
<p class="nl-1list-d"><strong><em>g.</em></strong> Combine parts (c)–(f) to show that the online version of C<small>OMPLETION</small>-T<small>IME</small>-S<small>CHEDULE</small> is 2-competitive.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-161"><strong>Chapter notes</strong></p>
<p class="noindent">Online algorithms are widely used in many domains. Some good overviews include the textbook by Borodin and El-Yaniv [<a epub:type="noteref" href="bibliography001.xhtml#endnote_68">68</a>], the collection of surveys edited by Fiat and Woeginger [<a epub:type="noteref" href="bibliography001.xhtml#endnote_142">142</a>], and the survey by Albers [<a epub:type="noteref" href="bibliography001.xhtml#endnote_14">14</a>].</p>
<p>The move-to-front heuristic from <a href="chapter027.xhtml#Sec_27.2">Section 27.2</a> was analyzed by Sleator and Tarjan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_416">416</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_417">417</a>] as part of their early work on amortized analysis. This rule works quite well in practice.</p>
<p>Competitive analysis of online caching also originated with Sleator and Tarjan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_417">417</a>]. The randomized marking algorithm was proposed and analyzed by Fiat et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_141">141</a>]. Young [<a epub:type="noteref" href="bibliography001.xhtml#endnote_464">464</a>] surveys online caching and paging algorithms, and Buchbinder and Naor [<a epub:type="noteref" href="bibliography001.xhtml#endnote_76">76</a>] survey primal-dual online algorithms.</p>
<p>Specific types of online algorithms are described using other names. <strong><em><span class="blue1">Dynamic graph algorithms</span></em></strong> are online algorithms on graphs, where at each step a vertex or edge undergoes modification. Typically a vertex or edge is either inserted or <a id="p818"/>deleted, or some associated property, such as edge weight, changes. Some graph problems need to be solved again after each change to the graph, and a good dynamic graph algorithm will not need to solve from scratch. For example, edges are inserted and deleted, and after each change to the graph, the minimum spanning tree is recomputed. Exercise 21.2-8 asks such a question. Similar questions can be asked for other graph algorithms, such as shortest paths, connectivity, or matching. The first paper in this field is credited to Even and Shiloach [<a epub:type="noteref" href="bibliography001.xhtml#endnote_138">138</a>], who study how to maintain a shortest-path tree as edges are being deleted from a graph. Since then hundreds of papers have been published. Demetrescu et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_110">110</a>] survey early developments in dynamic graph algorithms.</p>
<p>For massive data sets, the input data might be too large to store. <strong><em><span class="blue1">Streaming algorithms</span></em></strong> model this situation by requiring the memory used by an algorithm to be significantly smaller than the input size. For example, you may have a graph with <em>n</em> vertices and <em>m</em> edges with <em>m</em> <span class="font1">≫</span> <em>n</em>, but the memory allowed may be only <em>O</em>(<em>n</em>). Or you may have <em>n</em> numbers, but the memory allowed may only be <em>O</em>(lg <em>n</em>) or <img alt="art" src="images/Art_P896.jpg"/>. A streaming algorithm is measured by the number of passes made over the data in addition to the running time of the algorithm. McGregor [<a epub:type="noteref" href="bibliography001.xhtml#endnote_322">322</a>] surveys streaming algorithms for graphs and Muthukrishnan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_341">341</a>] surveys general streaming algorithms.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> The path-compression heuristic in <a href="chapter019.xhtml#Sec_19.3">Section 19.3</a> resembles M<small>OVE</small>-T<small>O</small>-F<small>RONT</small>, although it would be more accurately expressed as “move-to-next-to-front.” Unlike M<small>OVE</small>-T<small>O</small>-F<small>RONT</small> in a doubly linked list, path compression can relocate multiple elements to become “next-to-front.”</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> This book is heavy. We do not recommend that you carry it on a long hike.</p>
<p class="footnote1" id="footnote_3"><a href="#footnote_ref_3"><sup>3</sup></a> In case you’re wondering what this problem has to do with cows, some papers about it frame the problem as a cow looking for a field in which to graze.</p>
</section>
</section>
</div>
</body>
</html>