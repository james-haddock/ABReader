<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p76"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="4 Divide-and-Conquer">
<p class="chapter-title"><a href="toc.xhtml#chap-4"><strong><span class="blue1">4          Divide-and-Conquer</span></strong></a></p>
<p class="noindent">The divide-and-conquer method is a powerful strategy for designing asymptotically efficient algorithms. We saw an example of divide-and-conquer in <a href="chapter002.xhtml#Sec_2.3.1">Section 2.3.1</a> when learning about merge sort. In this chapter, we’ll explore applications of the divide-and-conquer method and acquire valuable mathematical tools that you can use to solve the recurrences that arise when analyzing divide-and-conquer algorithms.</p>
<p>Recall that for divide-and-conquer, you solve a given problem (instance) recursively. If the problem is small enough—the <strong><em><span class="blue1">base case</span></em></strong>—you just solve it directly without recursing. Otherwise—the <strong><em><span class="blue1">recursive case</span></em></strong>—you perform three characteristic steps:</p>
<p class="para-hang1"><strong>Divide</strong> the problem into one or more subproblems that are smaller instances of the same problem.</p>
<p class="para-hang1"><strong>Conquer</strong> the subproblems by solving them recursively.</p>
<p class="para-hang1"><strong>Combine</strong> the subproblem solutions to form a solution to the original problem.</p>
<p class="noindent1-top">A divide-and-conquer algorithm breaks down a large problem into smaller subproblems, which themselves may be broken down into even smaller subproblems, and so forth. The recursion <strong><em><span class="blue1">bottoms out</span></em></strong> when it reaches a base case and the subproblem is small enough to solve directly without further recursing.</p>
<p class="level4"><strong>Recurrences</strong></p>
<p class="noindent">To analyze recursive divide-and-conquer algorithms, we’ll need some mathematical tools. A <strong><em><span class="blue1">recurrence</span></em></strong> is an equation that describes a function in terms of its value on other, typically smaller, arguments. Recurrences go hand in hand with the divide-and-conquer method because they give us a natural way to characterize the running times of recursive algorithms mathematically. You saw an example of a recurrence in <a href="chapter002.xhtml#Sec_2.3.2">Section 2.3.2</a> when we analyzed the worst-case running time of merge sort.</p>
<a id="p77"/>
<p>For the divide-and-conquer matrix-multiplication algorithms presented in <a href="chapter004.xhtml#Sec_4.1">Sections 4.1</a> and <a href="chapter004.xhtml#Sec_4.2">4.2</a>, we’ll derive recurrences that describe their worst-case running times. To understand why these two divide-and-conquer algorithms perform the way they do, you’ll need to learn how to solve the recurrences that describe their running times. <a href="chapter004.xhtml#Sec_4.3">Sections 4.3</a>–<a href="chapter004.xhtml#Sec_4.7">4.7</a> teach several methods for solving recurrences. These sections also explore the mathematics behind recurrences, which can give you stronger intuition for designing your own divide-and-conquer algorithms.</p>
<p>We want to get to the algorithms as soon as possible. So, let’s just cover a few recurrence basics now, and then we’ll look more deeply at recurrences, especially how to solve them, after we see the matrix-multiplication examples.</p>
<p>The general form of a recurrence is an equation or inequality that describes a function over the integers or reals using the function itself. It contains two or more cases, depending on the argument. If a case involves the recursive invocation of the function on different (usually smaller) inputs, it is a <strong><em><span class="blue1">recursive case</span></em></strong>. If a case does not involve a recursive invocation, it is a <strong><em><span class="blue1">base case</span></em></strong>. There may be zero, one, or many functions that satisfy the statement of the recurrence. The recurrence is <strong><em><span class="blue1">well defined</span></em></strong> if there is at least one function that satisfies it, and <strong><em><span class="blue1">ill defined</span></em></strong> otherwise.</p>
<p class="level4"><strong>Algorithmic recurrences</strong></p>
<p class="noindent">We’ll be particularly interested in recurrences that describe the running times of divide-and-conquer algorithms. A recurrence <em>T</em> (<em>n</em>) is <strong><em><span class="blue1">algorithmic</span></em></strong> if, for every sufficiently large <strong><em><span class="blue1">threshold</span></em></strong> constant <em>n</em><sub>0</sub> &gt; 0, the following two properties hold:</p>
<p class="nl-top">1. For all <em>n</em> &lt; <em>n</em><sub>0</sub>, we have <em>T</em> (<em>n</em>) = Θ(1).</p>
<p class="nl">2. For all <em>n</em> ≥ <em>n</em><sub>0</sub>, every path of recursion terminates in a defined base case within a finite number of recursive invocations.</p>
<p class="noindent1-top">Similar to how we sometimes abuse asymptotic notation (see page 60), when a function is not defined for all arguments, we understand that this definition is constrained to values of <em>n</em> for which <em>T</em> (<em>n</em>) is defined.</p>
<p>Why would a recurrence <em>T</em> (<em>n</em>) that represents a (correct) divide-and-conquer algorithm’s worst-case running time satisfy these properties for all sufficiently large threshold constants? The first property says that there exist constants <em>c</em><sub>1</sub>, <em>c</em><sub>2</sub> such that 0 &lt; <em>c</em><sub>1</sub> ≤ <em>T</em> (<em>n</em>) ≤ <em>c</em><sub>2</sub> for <em>n</em> &lt; <em>n</em><sub>0</sub>. For every legal input, the algorithm must output the solution to the problem it’s solving in finite time (see <a href="chapter001.xhtml#Sec_1.1">Section 1.1</a>). Thus we can let <em>c</em><sub>1</sub> be the minimum amount of time to call and return from a procedure, which must be positive, because machine instructions need to be executed to invoke a procedure. The running time of the algorithm may not be defined for some values of <em>n</em> if there are no legal inputs of that size, but it must be defined for at least one, or else the “algorithm” doesn’t solve any problem. Thus we can let <em>c</em><sub>2</sub> be the algorithm’s maximum running time on any input of size <em>n</em> &lt; <em>n</em><sub>0</sub>, where <em>n</em><sub>0</sub> is <a id="p78"/>sufficiently large that the algorithm solves at least one problem of size less than <em>n</em><sub>0</sub>. The maximum is well defined, since there are at most a finite number of inputs of size less than <em>n</em><sub>0</sub>, and there is at least one if <em>n</em><sub>0</sub> is sufficiently large. Consequently, <em>T</em> (<em>n</em>) satisfies the first property. If the second property fails to hold for <em>T</em> (<em>n</em>), then the algorithm isn’t correct, because it would end up in an infinite recursive loop or otherwise fail to compute a solution. Thus, it stands to reason that a recurrence for the worst-case running time of a correct divide-and-conquer algorithm would be algorithmic.</p>
<p class="level4"><strong>Conventions for recurrences</strong></p>
<p class="noindent">We adopt the following convention:</p>
<div class="pull-quote">
<p class="pq-noindent"><em>Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic.</em></p>
</div>
<p class="noindent">That means you’re free to pick any sufficiently large threshold constant <em>n</em><sub>0</sub> for the range of base cases where <em>T</em> (<em>n</em>) = Θ(1). Interestingly, the asymptotic solutions of most algorithmic recurrences you’re likely to see when analyzing algorithms don’t depend on the choice of threshold constant, as long as it’s large enough to make the recurrence well defined.</p>
<p>Asymptotic solutions of algorithmic divide-and-conquer recurrences also don’t tend to change when we drop any floors or ceilings in a recurrence defined on the integers to convert it to a recurrence defined on the reals. <a href="chapter004.xhtml#Sec_4.7">Section 4.7</a> gives a sufficient condition for ignoring floors and ceilings that applies to most of the divide-and-conquer recurrences you’re likely to see. Consequently, we’ll frequently state algorithmic recurrences without floors and ceilings. Doing so generally simplifies the statement of the recurrences, as well as any math that we do with them.</p>
<p>You may sometimes see recurrences that are not equations, but rather inequalities, such as <em>T</em> (<em>n</em>) ≤ 2<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>). Because such a recurrence states only an upper bound on <em>T</em> (<em>n</em>), we express its solution using <em>O</em>-notation rather than Θ-notation. Similarly, if the inequality is reversed to <em>T</em> (<em>n</em>) ≥ 2<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>), then, because the recurrence gives only a lower bound on <em>T</em> (<em>n</em>), we use Ω-notation in its solution.</p>
<p class="level4"><strong>Divide-and-conquer and recurrences</strong></p>
<p class="noindent">This chapter illustrates the divide-and-conquer method by presenting and using recurrences to analyze two divide-and-conquer algorithms for multiplying <em>n</em> × <em>n</em> matrices. <a href="chapter004.xhtml#Sec_4.1">Section 4.1</a> presents a simple divide-and-conquer algorithm that solves a matrix-multiplication problem of size <em>n</em> by breaking it into four subproblems of size <em>n</em>/2, which it then solves recursively. The running time of the algorithm can be characterized by the recurrence</p>
<a id="p79"/>
<p class="eq"><em>T</em> (<em>n</em>) = 8<em>T</em> (<em>n</em>/2) + Θ(1),</p>
<p class="noindent">which turns out to have the solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>3</sup>). Although this divide-and-conquer algorithm is no faster than the straightforward method that uses a triply nested loop, it leads to an asymptotically faster divide-and-conquer algorithm due to V. Strassen, which we’ll explore in <a href="chapter004.xhtml#Sec_4.2">Section 4.2</a>. Strassen’s remarkable algorithm divides a problem of size <em>n</em> into seven subproblems of size <em>n</em>/2 which it solves recursively. The running time of Strassen’s algorithm can be described by the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = 7<em>T</em> (<em>n</em>/2) + Θ(<em>n</em><sup>2</sup>),</p>
<p class="noindent">which has the solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>lg 7</sup>) = <em>O</em>(<em>n</em><sup>2.81</sup>). Strassen’s algorithm beats the straightforward looping method asymptotically.</p>
<p>These two divide-and-conquer algorithms both break a problem of size <em>n</em> into several subproblems of size <em>n</em>/2. Although it is common when using divide-and-conquer for all the subproblems to have the same size, that isn’t always the case. Sometimes it’s productive to divide a problem of size <em>n</em> into subproblems of different sizes, and then the recurrence describing the running time reflects the irregularity. For example, consider a divide-and-conquer algorithm that divides a problem of size <em>n</em> into one subproblem of size <em>n</em>/3 and another of size 2<em>n</em>/3, taking Θ(<em>n</em>) time to divide the problem and combine the solutions to the subproblems. Then the algorithm’s running time can be described by the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/3) + <em>T</em> (2<em>n</em>/3) + Θ(<em>n</em>),</p>
<p class="noindent">which turns out to have solution <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>). We’ll even see an algorithm in <a href="chapter009.xhtml">Chapter 9</a> that solves a problem of size <em>n</em> by recursively solving a subproblem of size <em>n</em>/5 and another of size 7<em>n</em>/10, taking Θ(<em>n</em>) time for the divide and combine steps. Its performance satisfies the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/5) + <em>T</em> (7<em>n</em>/10) + Θ(<em>n</em>),</p>
<p class="noindent">which has solution <em>T</em> (<em>n</em>) = Θ(<em>n</em>).</p>
<p>Although divide-and-conquer algorithms usually create subproblems with sizes a constant fraction of the original problem size, that’s not always the case. For example, a recursive version of linear search (see Exercise 2.1-4) creates just one subproblem, with one element less than the original problem. Each recursive call takes constant time plus the time to recursively solve a subproblem with one less element, leading to the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> – 1) + Θ(1),</p>
<p class="noindent">which has solution <em>T</em> (<em>n</em>) = Θ(<em>n</em>). Nevertheless, the vast majority of efficient divide-and-conquer algorithms solve subproblems that are a constant fraction of the size of the original problem, which is where we’ll focus our efforts.</p>
<a id="p80"/>
<p class="level4"><strong>Solving recurrences</strong></p>
<p class="noindent">After learning about divide-and-conquer algorithms for matrix multiplication in <a href="chapter004.xhtml#Sec_4.1">Sections 4.1</a> and <a href="chapter004.xhtml#Sec_4.2">4.2</a>, we’ll explore several mathematical tools for solving recurrences—that is, for obtaining asymptotic Θ-, <em>O</em>-, or Ω-bounds on their solutions. We want simple-to-use tools that can handle the most commonly occurring situations. But we also want general tools that work, perhaps with a little more effort, for less common cases. This chapter offers four methods for solving recurrences:</p>
<ul class="ulnoindent" epub:type="list">
<li>In the <strong><em><span class="blue1">substitution method</span></em></strong> (<a href="chapter004.xhtml#Sec_4.3">Section 4.3</a>), you guess the form of a bound and then use mathematical induction to prove your guess correct and solve for constants. This method is perhaps the most robust method for solving recurrences, but it also requires you to make a good guess and to produce an inductive proof.</li>
<li class="litop">The <strong><em><span class="blue1">recursion-tree method</span></em></strong> (<a href="chapter004.xhtml#Sec_4.4">Section 4.4</a>) models the recurrence as a tree whose nodes represent the costs incurred at various levels of the recursion. To solve the recurrence, you determine the costs at each level and add them up, perhaps using techniques for bounding summations from <a href="appendix001.xhtml#Sec_A.2">Section A.2</a>. Even if you don’t use this method to formally prove a bound, it can be helpful in guessing the form of the bound for use in the substitution method.</li>
<li class="litop">The <strong><em><span class="blue1">master method</span></em></strong> (<a href="chapter004.xhtml#Sec_4.5">Sections 4.5</a> and <a href="chapter004.xhtml#Sec_4.6">4.6</a>) is the easiest method, when it applies. It provides bounds for recurrences of the form
<p class="eql"><em>T</em> (<em>n</em>) = <em>aT</em> (<em>n</em>/<em>b</em>) + <em>f</em> (<em>n</em>),</p>
<p class="noindent">where <em>a</em> &gt; 0 and <em>b</em> &gt; 1 are constants and <em>f</em> (<em>n</em>) is a given “driving” function. This type of recurrence tends to arise more frequently in the study of algorithms than any other. It characterizes a divide-and-conquer algorithm that creates <em>a</em> subproblems, each of which is 1/<em>b</em> times the size of the original problem, using <em>f</em> (<em>n</em>) time for the divide and combine steps. To apply the master method, you need to memorize three cases, but once you do, you can easily determine asymptotic bounds on running times for many divide-and-conquer algorithms.</p>
</li>
<li class="litop">The <strong><em><span class="blue1">Akra-Bazzi method</span></em></strong> (<a href="chapter004.xhtml#Sec_4.7">Section 4.7</a>) is a general method for solving divide-and-conquer recurrences. Although it involves calculus, it can be used to attack more complicated recurrences than those addressed by the master method.</li></ul>
<p class="line1"/>
<section title="4.1 Multiplying square matrices">
<a id="Sec_4.1"/>
<p class="level1" id="h1-16"><a href="toc.xhtml#Rh1-16"><strong>4.1      Multiplying square matrices</strong></a></p>
<p class="noindent">We can use the divide-and-conquer method to multiply square matrices. If you’ve seen matrices before, then you probably know how to multiply them. (Otherwise, <a id="p81"/>you should read <a href="appendix004.xhtml#Sec_D.1">Section D.1</a>.) Let <em>A</em> = (<em>a<sub>ik</sub></em>) and <em>B</em> = (<em>b<sub>jk</sub></em>) be square <em>n</em> × <em>n</em> matrices. The matrix product <em>C</em> = <em>A</em> · <em>B</em> is also an <em>n</em> × <em>n</em> matrix, where for <em>i</em>, <em>j</em> = 1, 2, … , <em>n</em>, the (<em>i</em>, <em>j</em>) entry of <em>C</em> is given by</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P100.jpg"/></p>
<p class="noindent">Generally, we’ll assume that the matrices are <strong><em><span class="blue1">dense</span></em></strong>, meaning that most of the <em>n</em><sup>2</sup> entries are not 0, as opposed to <strong><em><span class="blue1">sparse</span></em></strong>, where most of the <em>n</em><sup>2</sup> entries are 0 and the nonzero entries can be stored more compactly than in an <em>n</em> × <em>n</em> array.</p>
<p>Computing the matrix <em>C</em> requires computing <em>n</em><sup>2</sup> matrix entries, each of which is the sum of <em>n</em> pairwise products of input elements from <em>A</em> and <em>B</em>. The M<small>ATRIX</small>-M<small>ULTIPLY</small> procedure implements this strategy in a straightforward manner, and it generalizes the problem slightly. It takes as input three <em>n</em> × <em>n</em> matrices <em>A</em>, <em>B</em>, and <em>C</em>, and it adds the matrix product <em>A</em> · <em>B</em> to <em>C</em>, storing the result in <em>C</em>. Thus, it computes <em>C</em> = <em>C</em> + <em>A</em> · <em>B</em>, instead of just <em>C</em> = <em>A</em> · <em>B</em>. If only the product <em>A</em> · <em>B</em> is needed, just initialize all <em>n</em><sup>2</sup> entries of <em>C</em> to 0 before calling the procedure, which takes an additional Θ(<em>n</em><sup>2</sup>) time. We’ll see that the cost of matrix multiplication asymptotically dominates this initialization cost.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>ATRIX</small>-M<small>ULTIPLY</small>(<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> compute entries in each of <em>n</em> rows</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> compute <em>n</em> entries in row <em>i</em></span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p3"><strong>for</strong> <em>k</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p4"><em>c<sub>ij</sub></em> = <em>c<sub>ij</sub></em> + <em>a<sub>ik</sub></em> · <em>b<sub>kj</sub></em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> add in another term of equation (4.1)</span></p></td>
</tr>
</table>
</div>
<p>The pseudocode for M<small>ATRIX</small>-M<small>ULTIPLY</small> works as follows. The <strong>for</strong> loop of lines 1–4 computes the entries of each row <em>i</em>, and within a given row <em>i</em>, the <strong>for</strong> loop of lines 2–4 computes each of the entries <em>c<sub>ij</sub></em> for each column <em>j</em>. Each iteration of the <strong>for</strong> loop of lines 3–4 adds in one more term of equation (4.1).</p>
<p>Because each of the triply nested <strong>for</strong> loops runs for exactly <em>n</em> iterations, and each execution of line 4 takes constant time, the M<small>ATRIX</small>-M<small>ULTIPLY</small> procedure operates in Θ(<em>n</em><sup>3</sup>) time. Even if we add in the Θ(<em>n</em><sup>2</sup>) time for initializing <em>C</em> to 0, the running time is still Θ(<em>n</em><sup>3</sup>).</p>
<p class="level4"><strong>A simple divide-and-conquer algorithm</strong></p>
<p class="noindent">Let’s see how to compute the matrix product <em>A</em> · <em>B</em> using divide-and-conquer. For <em>n</em> &gt; 1, the divide step partitions the <em>n</em> × <em>n</em> matrices into four <em>n</em>/2 × <em>n</em>/2 submatrices. We’ll assume that <em>n</em> is an exact power of 2, so that as the algorithm recurses, we are guaranteed that the submatrix dimensions are integer. (Exercise 4.1-1 asks you <a id="p82"/>to relax this assumption.) As with M<small>ATRIX</small>-M<small>ULTIPLY</small>, we’ll actually compute <em>C</em> = <em>C</em> + <em>A</em> · <em>B</em>. But to simplify the math behind the algorithm, let’s assume that <em>C</em> has been initialized to the zero matrix, so that we are indeed computing <em>C</em> = <em>A</em> · <em>B</em>.</p>
<p>The divide step views each of the <em>n</em> × <em>n</em> matrices <em>A</em>, <em>B</em>, and <em>C</em> as four <em>n</em>/2 × <em>n</em>/2 submatrices:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P101.jpg"/></p>
<p class="noindent">Then we can write the matrix product as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P102.jpg"/></p>
<p class="noindent">which corresponds to the equations</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P103.jpg"/></p>
<p class="noindent">Equations (4.5)–(4.8) involve eight <em>n</em>/2 × <em>n</em>/2 multiplications and four additions of <em>n</em>/2 × <em>n</em>/2 submatrices.</p>
<p>As we look to transform these equations to an algorithm that can be described with pseudocode, or even implemented for real, there are two common approaches for implementing the matrix partitioning.</p>
<p>One strategy is to allocate temporary storage to hold <em>A</em>’s four submatrices <em>A</em><sub>11</sub>, <em>A</em><sub>12</sub>, <em>A</em><sub>21</sub>, and <em>A</em><sub>22</sub> and <em>B</em>’s four submatrices <em>B</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>B</em><sub>21</sub>, and <em>B</em><sub>22</sub>. Then copy each element in <em>A</em> and <em>B</em> to its corresponding location in the appropriate submatrix. After the recursive conquer step, copy the elements in each of <em>C</em>’s four submatrices <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, and <em>C</em><sub>22</sub> to their corresponding locations in <em>C</em>. This approach takes Θ(<em>n</em><sup>2</sup>) time, since 3<em>n</em><sup>2</sup> elements are copied.</p>
<p>The second approach uses index calculations and is faster and more practical. A submatrix can be specified within a matrix by indicating where within the matrix the submatrix lies without touching any matrix elements. Partitioning a matrix (or recursively, a submatrix) only involves arithmetic on this location information, which has constant size independent of the size of the matrix. Changes to the submatrix elements update the original matrix, since they occupy the same storage.</p>
<p>Going forward, we’ll assume that index calculations are used and that partitioning can be performed in Θ(1) time. Exercise 4.1-3 asks you to show that it makes no difference to the overall asymptotic running time of matrix multiplication, however, whether the partitioning of matrices uses the first method of copying or the <a id="p83"/>second method of index calculation. But for other divide-and-conquer matrix calculations, such as matrix addition, it can make a difference, as Exercise 4.1-4 asks you to show.</p>
<p>The procedure M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> uses equations (4.5)–(4.8) to implement a divide-and-conquer strategy for square-matrix multiplication. Like M<small>ATRIX</small>-M<small>ULTIPLY</small>, the procedure M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> computes <em>C</em> = <em>C</em> + <em>A</em> · <em>B</em> since, if necessary, <em>C</em> can be initialized to 0 before the procedure is called in order to compute only <em>C</em> = <em>A</em> · <em>B</em>.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  1</span></p></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>n</em> == 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  2</span></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> Base case.</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  3</span></p></td>
<td class="td1"><p class="p2"><em>c</em><sub>11</sub> = <em>c</em><sub>11</sub> + <em>a</em><sub>11</sub> · <em>b</em><sub>11</sub></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  4</span></p></td>
<td class="td1"><p class="p2"><strong>return</strong></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  5</span></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> Divide.</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  6</span></p></td>
<td class="td1"><p class="para-hang">partition <em>A</em>, <em>B</em>, and <em>C</em> into <em>n</em>/2 × <em>n</em>/2 submatrices</p>
<p class="p2"><em>A</em><sub>11</sub>, <em>A</em><sub>12</sub>, <em>A</em><sub>21</sub>, <em>A</em><sub>22</sub>; <em>B</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>B</em><sub>21</sub>, <em>B</em><sub>22</sub>;<br/>and <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, <em>C</em><sub>22</sub>; respectively</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  7</span></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> Conquer.</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  8</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>11</sub>, <em>B</em><sub>11</sub>, <em>C</em><sub>11</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  9</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>C</em><sub>12</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">10</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>21</sub>, <em>B</em><sub>11</sub>, <em>C</em><sub>21</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">11</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>21</sub>, <em>B</em><sub>12</sub>, <em>C</em><sub>22</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">12</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>12</sub>, <em>B</em><sub>21</sub>, <em>C</em><sub>11</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">13</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>12</sub>, <em>B</em><sub>22</sub>, <em>C</em><sub>12</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">14</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>22</sub>, <em>B</em><sub>21</sub>, <em>C</em><sub>21</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">15</span></p></td>
<td class="td1"><p class="noindent">M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>(<em>A</em><sub>22</sub>, <em>B</em><sub>22</sub>, <em>C</em><sub>22</sub>, <em>n</em>/2)</p></td>
</tr>
</table>
</div>
<p>As we walk through the pseudocode, we’ll derive a recurrence to characterize its running time. Let <em>T</em> (<em>n</em>) be the worst-case time to multiply two <em>n</em> × <em>n</em> matrices using this procedure.</p>
<p>In the base case, when <em>n</em> = 1, line 3 performs just the one scalar multiplication and one addition, which means that <em>T</em> (1) = Θ(1). As is our convention for constant base cases, we can omit this base case in the statement of the recurrence.</p>
<p>The recursive case occurs when <em>n</em> &gt; 1. As discussed, we’ll use index calculations to partition the matrices in line 6, taking Θ(1) time. Lines 8–15 recursively call M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> a total of eight times. The first four recursive calls compute the first terms of equations (4.5)–(4.8), and the subsequent four recursive calls compute and add in the second terms. Each recursive call adds the product of a submatrix of <em>A</em> and a submatrix of <em>B</em> to the appropriate submatrix <a id="p84"/>of <em>C</em> in place, thanks to index calculations. Because each recursive call multiplies two <em>n</em>/2 × <em>n</em>/2 matrices, thereby contributing <em>T</em> (<em>n</em>/2) to the overall running time, the time taken by all eight recursive calls is 8<em>T</em> (<em>n</em>/2). There is no combine step, because the matrix <em>C</em> is updated in place. The total time for the recursive case, therefore, is the sum of the partitioning time and the time for all the recursive calls, or Θ(1) + 8<em>T</em> (<em>n</em>/2).</p>
<p>Thus, omitting the statement of the base case, our recurrence for the running time of M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P104.jpg"/></p>
<p class="noindent">As we’ll see from the master method in <a href="chapter004.xhtml#Sec_4.5">Section 4.5</a>, recurrence (4.9) has the solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>3</sup>), which means that it has the same asymptotic running time as the straightforward M<small>ATRIX</small>-M<small>ULTIPLY</small> procedure.</p>
<p>Why is the Θ(<em>n</em><sup>3</sup>) solution to this recurrence so much larger than the Θ(<em>n</em> lg <em>n</em>) solution to the merge-sort recurrence (2.3) on page 41? After all, the recurrence for merge sort contains a Θ(<em>n</em>) term, whereas the recurrence for recursive matrix multiplication contains only a Θ(1) term.</p>
<p>Let’s think about what the recursion tree for recurrence (4.9) would look like as compared with the recursion tree for merge sort, illustrated in <a href="chapter002.xhtml#Fig_2-5">Figure 2.5</a> on page 43. The factor of 2 in the merge-sort recurrence determines how many children each tree node has, which in turn determines how many terms contribute to the sum at each level of the tree. In comparison, for the recurrence (4.9) for M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, each internal node in the recursion tree has eight children, not two, leading to a “bushier” recursion tree with many more leaves, despite the fact that the internal nodes are each much smaller. Consequently, the solution to recurrence (4.9) grows much more quickly than the solution to recurrence (2.3), which is borne out in the actual solutions: Θ(<em>n</em><sup>3</sup>) versus Θ(<em>n</em> lg <em>n</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="noindent"><em>Note:</em> You may wish to read <a href="chapter004.xhtml#Sec_4.5">Section 4.5</a> before attempting some of these exercises.</p>
<p class="level3"><strong><em>4.1-1</em></strong></p>
<p class="noindent">Generalize M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> to multiply <em>n</em> × <em>n</em> matrices for which <em>n</em> is not necessarily an exact power of 2. Give a recurrence describing its running time. Argue that it runs in Θ(<em>n</em><sup>3</sup>) time in the worst case.</p>
<p class="level3"><strong><em>4.1-2</em></strong></p>
<p class="noindent">How quickly can you multiply a <em>k n</em> × <em>n</em> matrix (<em>k n</em> rows and <em>n</em> columns) by an <em>n</em> × <em>k n</em> matrix, where <em>k</em> ≥ 1, using M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> as a subroutine? Answer the same question for multiplying an <em>n</em> × <em>k n</em> matrix by a <em>k n</em> × <em>n</em> matrix. Which is asymptotically faster, and by how much?</p>
<a id="p85"/>
<p class="level3"><strong><em>4.1-3</em></strong></p>
<p class="noindent">Suppose that instead of partitioning matrices by index calculation in M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, you copy the appropriate elements of <em>A</em>, <em>B</em>, and <em>C</em> into separate <em>n</em>/2 × <em>n</em>/2 submatrices <em>A</em><sub>11</sub>, <em>A</em><sub>12</sub>, <em>A</em><sub>21</sub>, <em>A</em><sub>22</sub>; <em>B</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>B</em><sub>21</sub>, <em>B</em><sub>22</sub>; and <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, <em>C</em><sub>22</sub>, respectively. After the recursive calls, you copy the results from <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, and <em>C</em><sub>22</sub> back into the appropriate places in <em>C</em>. How does recurrence (4.9) change, and what is its solution?</p>
<p class="level3"><strong><em>4.1-4</em></strong></p>
<p class="noindent">Write pseudocode for a divide-and-conquer algorithm MATRIX-ADD-R<small>ECURSIVE</small> that sums two <em>n</em> × <em>n</em> matrices <em>A</em> and <em>B</em> by partitioning each of them into four <em>n</em>/2 × <em>n</em>/2 submatrices and then recursively summing corresponding pairs of submatrices. Assume that matrix partitioning uses Θ(1)-time index calculations. Write a recurrence for the worst-case running time of M<small>ATRIX</small>-A<small>DD</small>-R<small>ECURSIVE</small>, and solve your recurrence. What happens if you use Θ(<em>n</em><sup>2</sup>)-time copying to implement the partitioning instead of index calculations?</p>
</section>
<p class="line1"/>
<section title="4.2 Strassen’s algorithm for matrix multiplication">
<a id="Sec_4.2"/>
<p class="level1" id="h1-17"><a href="toc.xhtml#Rh1-17"><strong>4.2      Strassen’s algorithm for matrix multiplication</strong></a></p>
<p class="noindent">You might find it hard to imagine that any matrix multiplication algorithm could take less than Θ(<em>n</em><sup>3</sup>) time, since the natural definition of matrix multiplication requires <em>n</em><sup>3</sup> scalar multiplications. Indeed, many mathematicians presumed that it was not possible to multiply matrices in <em>o</em>(<em>n</em><sup>3</sup>) time until 1969, when V. Strassen [<a epub:type="noteref" href="bibliography001.xhtml#endnote_424">424</a>] published a remarkable recursive algorithm for multiplying <em>n</em> × <em>n</em> matrices. Strassen’s algorithm runs in Θ(<em>n</em><sup>lg 7</sup>) time. Since lg 7 = 2.8073549 …, Strassen’s algorithm runs in <em>O</em>(<em>n</em><sup>2.81</sup>) time, which is asymptotically better than the Θ(<em>n</em><sup>3</sup>) M<small>ATRIX</small>-M<small>ULTIPLY</small> and M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedures.</p>
<p>The key to Strassen’s method is to use the divide-and-conquer idea from the M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure, but make the recursion tree less bushy. We’ll actually increase the work for each divide and combine step by a constant factor, but the reduction in bushiness will pay off. We won’t reduce the bushiness from the eight-way branching of recurrence (4.9) all the way down to the two-way branching of recurrence (2.3), but we’ll improve it just a little, and that will make a big difference. Instead of performing eight recursive multiplications of <em>n</em>/2 × <em>n</em>/2 matrices, Strassen’s algorithm performs only seven. The cost of eliminating one matrix multiplication is several new additions and subtractions of <em>n</em>/2 × <em>n</em>/2 matrices, but still only a constant number. Rather than saying “additions and subtractions” everywhere, we’ll adopt the common terminology of calling <a id="p86"/>them both “additions” because subtraction is structurally the same computation as addition, except for a change of sign.</p>
<p>To get an inkling how the number of multiplications might be reduced, as well as why reducing the number of multiplications might be desirable for matrix calculations, suppose that you have two numbers <em>x</em> and <em>y</em>, and you want to calculate the quantity <em>x</em><sup>2</sup> – <em>y</em><sup>2</sup>. The straightforward calculation requires two multiplications to square <em>x</em> and <em>y</em>, followed by one subtraction (which you can think of as a “negative addition”). But let’s recall the old algebra trick <em>x</em><sup>2</sup> – <em>y</em><sup>2</sup> = <em>x</em><sup>2</sup> – <em>xy</em> + <em>xy</em> – <em>y</em><sup>2</sup> = <em>x</em>(<em>x</em> – <em>y</em>) + <em>y</em>(<em>x</em> – <em>y</em>) = (<em>x</em> + <em>y</em>)(<em>x</em> – <em>y</em>). Using this formulation of the desired quantity, you could instead compute the sum <em>x</em> + <em>y</em> and the difference <em>x</em> – <em>y</em> and then multiply them, requiring only a single multiplication and two additions. At the cost of an extra addition, only one multiplication is needed to compute an expression that looks as if it requires two. If <em>x</em> and <em>y</em> are scalars, there’s not much difference: both approaches require three scalar operations. If <em>x</em> and <em>y</em> are large matrices, however, the cost of multiplying outweighs the cost of adding, in which case the second method outperforms the first, although not asymptotically.</p>
<p>Strassen’s strategy for reducing the number of matrix multiplications at the expense of more matrix additions is not at all obvious—perhaps the biggest understatement in this book! As with M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, Strassen’s algorithm uses the divide-and-conquer method to compute <em>C</em> = <em>C</em> + <em>A</em> · <em>B</em>, where <em>A</em>, <em>B</em>, and <em>C</em> are all <em>n</em> × <em>n</em> matrices and <em>n</em> is an exact power of 2. Strassen’s algorithm computes the four submatrices <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, and <em>C</em><sub>22</sub> of <em>C</em> from equations (4.5)–(4.8) on page 82 in four steps. We’ll analyze costs as we go along to develop a recurrence <em>T</em> (<em>n</em>) for the overall running time. Let’s see how it works:</p>
<ol class="olnoindent" epub:type="list">
<li>If <em>n</em> = 1, the matrices each contain a single element. Perform a single scalar multiplication and a single scalar addition, as in line 3 of M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, taking Θ(1) time, and return. Otherwise, partition the input matrices <em>A</em> and <em>B</em> and output matrix <em>C</em> into <em>n</em>/2 × <em>n</em>/2 submatrices, as in equation (4.2). This step takes Θ(1) time by index calculation, just as in M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>.</li>
<li class="litop">Create <em>n</em>/2 × <em>n</em>/2 matrices <em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, … , <em>S</em><sub>10</sub>, each of which is the sum or difference of two submatrices from step 1. Create and zero the entries of seven <em>n</em>/2 × <em>n</em>/2 matrices <em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P</em><sub>7</sub> to hold seven <em>n</em>/2 × <em>n</em>/2 matrix products. All 17 matrices can be created, and the <em>P<sub>i</sub></em> initialized, in Θ(<em>n</em><sup>2</sup>) time.</li>
<li class="litop">Using the submatrices from step 1 and the matrices <em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, … , <em>S</em><sub>10</sub> created in step 2, recursively compute each of the seven matrix products <em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P</em><sub>7</sub>, taking 7<em>T</em> (<em>n</em>/2) time.</li>
<li class="litop">Update the four submatrices <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, <em>C</em><sub>22</sub> of the result matrix <em>C</em> by adding or subtracting various <em>P<sub>i</sub></em> matrices, which takes Θ(<em>n</em><sup>2</sup>) time.</li></ol>
<a id="p87"/>
<p>We’ll see the details of steps 2–4 in a moment, but we already have enough information to set up a recurrence for the running time of Strassen’s method. As is common, the base case in step 1 takes Θ(1) time, which we’ll omit when stating the recurrence. When <em>n</em> &gt; 1, steps 1, 2, and 4 take a total of Θ(<em>n</em><sup>2</sup>) time, and step 3 requires seven multiplications of <em>n</em>/2 × <em>n</em>/2 matrices. Hence, we obtain the following recurrence for the running time of Strassen’s algorithm:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P105.jpg"/></p>
<p class="noindent">Compared with M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, we have traded off one recursive submatrix multiplication for a constant number of submatrix additions. Once you understand recurrences and their solutions, you’ll be able to see why this trade-off actually leads to a lower asymptotic running time. By the master method in <a href="chapter004.xhtml#Sec_4.5">Section 4.5</a>, recurrence (4.10) has the solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>lg 7</sup>) = <em>O</em>(<em>n</em><sup>2.81</sup>), beating the Θ(<em>n</em><sup>3</sup>)-time algorithms.</p>
<p>Now, let’s delve into the details. Step 2 creates the following 10 matrices:</p>
<table class="table2b">
<tr>
<td class="td2"><em>S</em><sub>1</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>B</em><sub>12</sub> – <em>B</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>2</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>11</sub> + <em>A</em><sub>12</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>3</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>21</sub> + <em>A</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>4</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>B</em><sub>21</sub> – <em>B</em><sub>11,</sub></td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>5</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>11</sub> + <em>A</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>6</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>B</em><sub>11</sub> + <em>B</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>7</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>12</sub> – <em>A</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>8</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>B</em><sub>21</sub> + <em>B</em><sub>22</sub>,</td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>9</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>11</sub> – <em>A</em><sub>21,</sub></td>
</tr>
<tr>
<td class="td2"><em>S</em><sub>10</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>B</em><sub>11</sub> + <em>B</em><sub>12</sub>.</td>
</tr>
</table>
<p class="noindent">This step adds or subtracts <em>n</em>/2 × <em>n</em>/2 matrices 10 times, taking Θ(<em>n</em><sup>2</sup>) time.</p>
<p>Step 3 recursively multiplies <em>n</em>/2 × <em>n</em>/2 matrices 7 times to compute the following <em>n</em>/2 × <em>n</em>/2 matrices, each of which is the sum or difference of products of <em>A</em> and <em>B</em> submatrices:</p>
<table class="table2b">
<tr>
<td class="td2"><em>P</em><sub>1</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>11</sub> · <em>S</em><sub>1</sub></td>
<td class="td2">(= <em>A</em><sub>11</sub> · <em>B</em><sub>12</sub> – <em>A</em><sub>11</sub> · <em>B</em><sub>22</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>2</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>S</em><sub>2</sub> · <em>B</em><sub>22</sub></td>
<td class="td2">(= <em>A</em><sub>11</sub> · <em>B</em><sub>22</sub> + <em>A</em><sub>12</sub> · <em>B</em><sub>22</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>3</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>S</em><sub>3</sub> · <em>B</em><sub>11</sub></td>
<td class="td2">(= <em>A</em><sub>21</sub> · <em>B</em><sub>11</sub> + <em>A</em><sub>22</sub> · <em>B</em><sub>11</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>4</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>A</em><sub>22</sub> · <em>S</em><sub>4</sub></td>
<td class="td2">(= <em>A</em><sub>22</sub> · <em>B</em><sub>21</sub> – <em>A</em><sub>22</sub> · <em>B</em><sub>11</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>5</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>S</em><sub>5</sub> · <em>S</em><sub>6</sub></td>
<td class="td2">(= <em>A</em><sub>11</sub> · <em>B</em><sub>11</sub> + <em>A</em><sub>11</sub> · <em>B</em><sub>22</sub> + <em>A</em><sub>22</sub> · <em>B</em><sub>11</sub> + <em>A</em><sub>22</sub> · <em>B</em><sub>22</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>6</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>S</em><sub>7</sub> · <em>S</em><sub>8</sub></td>
<td class="td2">(= <em>A</em><sub>12</sub> · <em>B</em><sub>21</sub> + <em>A</em><sub>12</sub> · <em>B</em><sub>22</sub> – <em>A</em><sub>22</sub> · <em>B</em><sub>21</sub> – <em>A</em><sub>22</sub> · <em>B</em><sub>22</sub>),</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>7</sub></td>
<td class="td2m">=</td>
<td class="td2"><em>S</em><sub>9</sub> · <em>S</em><sub>10</sub></td>
<td class="td2">(= <em>A</em><sub>11</sub> · <em>B</em><sub>11</sub> + <em>A</em><sub>11</sub> · <em>B</em><sub>12</sub> – <em>A</em><sub>21</sub> · <em>B</em><sub>11</sub> – <em>A</em><sub>21</sub> · <em>B</em><sub>12</sub>).</td>
</tr>
</table>
<a id="p88"/>
<p class="noindent">The only multiplications that the algorithm performs are those in the middle column of these equations. The right-hand column just shows what these products equal in terms of the original submatrices created in step 1, but the terms are never explicitly calculated by the algorithm.</p>
<p>Step 4 adds to and subtracts from the four <em>n</em>/2 × <em>n</em>/2 submatrices of the product <em>C</em> the various <em>P<sub>i</sub></em> matrices created in step 3. We start with</p>
<p class="eql"><em>C</em><sub>11</sub> = <em>C</em><sub>11</sub> + <em>P</em><sub>5</sub> + <em>P</em><sub>4</sub> – <em>P</em><sub>2</sub> + <em>P</em><sub>6</sub>.</p>
<p class="noindent">Expanding the calculation on the right-hand side, with the expansion of each <em>P<sub>i</sub></em> on its own line and vertically aligning terms that cancel out, we see that the update to <em>C</em><sub>11</sub> equals</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P106.jpg"/></p>
<p class="noindent">which corresponds to equation (4.5). Similarly, setting</p>
<p class="eql"><em>C</em><sub>12</sub> = <em>C</em><sub>12</sub> + <em>P</em><sub>1</sub> + <em>P</em><sub>2</sub></p>
<p class="noindent">means that the update to <em>C</em><sub>12</sub> equals</p>
<p class="eql"><img alt="art" src="images/Art_P107.jpg"/></p>
<p class="noindent">corresponding to equation (4.6). Setting</p>
<p class="eql"><em>C</em><sub>21</sub> = <em>C</em><sub>21</sub> + <em>P</em><sub>3</sub> + <em>P</em><sub>4</sub></p>
<p class="noindent">means that the update to <em>C</em><sub>21</sub> equals</p>
<p class="eql"><img alt="art" src="images/Art_P108.jpg"/></p>
<p class="noindent">corresponding to equation (4.7). Finally, setting</p>
<p class="eql"><em>C</em><sub>22</sub> = <em>C</em><sub>22</sub> + <em>P</em><sub>5</sub> + <em>P</em><sub>1</sub> – <em>P</em><sub>3</sub> – <em>P</em><sub>7</sub></p>
<p class="noindent">means that the update to <em>C</em><sub>22</sub> equals</p>
<a id="p89"/>
<p class="eql"><img alt="art" class="width100" src="images/Art_P109.jpg"/></p>
<p class="noindent">which corresponds to equation (4.8). Altogether, since we add or subtract <em>n</em>/2×<em>n</em>/2 matrices 12 times in step 4, this step indeed takes Θ(<em>n</em><sup>2</sup>) time.</p>
<p>We can see that Strassen’s remarkable algorithm, comprising steps 1–4, produces the correct matrix product using 7 submatrix multiplications and 18 submatrix additions. We can also see that recurrence (4.10) characterizes its running time. Since <a href="chapter004.xhtml#Sec_4.5">Section 4.5</a> shows that this recurrence has the solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>lg 7</sup>) = <em>o</em>(<em>n</em><sup>3</sup>), Strassen’s method asymptotically beats the Θ(<em>n</em><sup>3</sup>) M<small>ATRIX</small>-M<small>ULTIPLY</small> and M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedures.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="noindent"><em>Note:</em> You may wish to read <a href="chapter004.xhtml#Sec_4.5">Section 4.5</a> before attempting some of these exercises.</p>
<p class="level3"><strong><em>4.2-1</em></strong></p>
<p class="noindent">Use Strassen’s algorithm to compute the matrix product</p>
<p class="eql"><img alt="art" src="images/Art_P110.jpg"/></p>
<p class="noindent">Show your work.</p>
<p class="level3"><strong><em>4.2-2</em></strong></p>
<p class="noindent">Write pseudocode for Strassen’s algorithm.</p>
<p class="level3"><strong><em>4.2-3</em></strong></p>
<p class="noindent">What is the largest <em>k</em> such that if you can multiply 3 × 3 matrices using <em>k</em> multiplications (not assuming commutativity of multiplication), then you can multiply <em>n</em> × <em>n</em> matrices in <em>o</em>(<em>n</em><sup>lg 7</sup>) time? What is the running time of this algorithm?</p>
<p class="level3"><strong><em>4.2-4</em></strong></p>
<p class="noindent">V. Pan discovered a way of multiplying 68 × 68 matrices using 132,464 multiplications, a way of multiplying 70 × 70 matrices using 143,640 multiplications, and a way of multiplying 72 × 72 matrices using 155,424 multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare with Strassen’s algorithm?</p>
<a id="p90"/>
<p class="level3"><strong><em>4.2-5</em></strong></p>
<p class="noindent">Show how to multiply the complex numbers <em>a</em> + <em>bi</em> and <em>c</em> + <em>d i</em> using only three multiplications of real numbers. The algorithm should take <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em> as input and produce the real component <em>ac</em> – <em>bd</em> and the imaginary component <em>ad</em> + <em>bc</em> separately.</p>
<p class="level3"><strong><em>4.2-6</em></strong></p>
<p class="noindent">Suppose that you have a Θ(<em>n<sup>α</sup></em>)-time algorithm for squaring <em>n</em> × <em>n</em> matrices, where <em>α</em> ≥ 2. Show how to use that algorithm to multiply two different <em>n</em> × <em>n</em> matrices in Θ(<em>n<sup>α</sup></em>) time.</p>
</section>
<p class="line1"/>
<section title="4.3 The substitution method for solving recurrences">
<a id="Sec_4.3"/>
<p class="level1" id="h1-18"><a href="toc.xhtml#Rh1-18"><strong>4.3      The substitution method for solving recurrences</strong></a></p>
<p class="noindent">Now that you have seen how recurrences characterize the running times of divide-and-conquer algorithms, let’s learn how to solve them. We start in this section with the <strong><em><span class="blue1">substitution method</span></em></strong>, which is the most general of the four methods in this chapter. The substitution method comprises two steps:</p>
<ol class="olnoindent" epub:type="list">
<li>Guess the form of the solution using symbolic constants.</li>
<li class="litop">Use mathematical induction to show that the solution works, and find the constants.</li></ol>
<p class="noindent">To apply the inductive hypothesis, you substitute the guessed solution for the function on smaller values—hence the name “substitution method.” This method is powerful, but you must guess the form of the answer. Although generating a good guess might seem difficult, a little practice can quickly improve your intuition.</p>
<p>You can use the substitution method to establish either an upper or a lower bound on a recurrence. It’s usually best not to try to do both at the same time. That is, rather than trying to prove a Θ-bound directly, first prove an <em>O</em>-bound, and then prove an Ω-bound. Together, they give you a Θ-bound (Theorem 3.1 on page 56).</p>
<p>As an example of the substitution method, let’s determine an asymptotic upper bound on the recurrence:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P111.jpg"/></p>
<p class="noindent">This recurrence is similar to recurrence (2.3) on page 41 for merge sort, except for the floor function, which ensures that <em>T</em> (<em>n</em>) is defined over the integers. Let’s guess that the asymptotic upper bound is the same—<em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>)—and use the substitution method to prove it.</p>
<p>We’ll adopt the inductive hypothesis that <em>T</em> (<em>n</em>) ≤ <em>c n</em> lg <em>n</em> for all <em>n</em> ≥ <em>n</em><sub>0</sub>, where we’ll choose the specific constants <em>c</em> &gt; 0 and <em>n</em><sub>0</sub> &gt; 0 later, after we see what <a id="p91"/>constraints they need to obey. If we can establish this inductive hypothesis, we can conclude that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>). It would be dangerous to use <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>) as the inductive hypothesis because the constants matter, as we’ll see in a moment in our discussion of pitfalls.</p>
<p>Assume by induction that this bound holds for all numbers at least as big as <em>n</em><sub>0</sub> and less than <em>n</em>. In particular, therefore, if <em>n</em> ≥ 2<em>n</em><sub>0</sub>, it holds for <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>, yielding <em>T</em> (<span class="font1">⌊</span><em>n</em>/2 <span class="font1">⌋</span>) ≤ <em>c</em> <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> lg(<span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>). Substituting into recurrence (4.11)—hence the name “substitution” method—yields</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">2(<em>c</em> <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> lg(<span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>)) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2">2(<em>c</em>(<em>n</em>/2) lg(<em>n</em>/2)) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cn</em> lg(<em>n</em>/2) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cn</em> lg <em>n</em> – <em>cn</em> lg 2 + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cn</em> lg <em>n</em> – <em>cn</em> + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>cn</em> lg <em>n</em>,</td>
</tr>
</table>
<p class="noindent">where the last step holds if we constrain the constants <em>n</em><sub>0</sub> and <em>c</em> to be sufficiently large that for <em>n</em> ≥ 2<em>n</em><sub>0</sub>, the quantity <em>cn</em> dominates the anonymous function hidden by the Θ(<em>n</em>) term.</p>
<p>We’ve shown that the inductive hypothesis holds for the inductive case, but we also need to prove that the inductive hypothesis holds for the base cases of the induction, that is, that <em>T</em> (<em>n</em>) ≤ <em>cn</em> lg <em>n</em> when <em>n</em><sub>0</sub> ≤ <em>n</em> &lt; 2<em>n</em><sub>0</sub>. As long as <em>n</em><sub>0</sub> &gt; 1 (a new constraint on <em>n</em><sub>0</sub>), we have lg <em>n</em> &gt; 0, which implies that <em>n</em> lg <em>n</em> &gt; 0. So let’s pick <em>n</em><sub>0</sub> = 2. Since the base case of recurrence (4.11) is not stated explicitly, by our convention, <em>T</em> (<em>n</em>) is algorithmic, which means that <em>T</em> (2) and <em>T</em> (3) are constant (as they should be if they describe the worst-case running time of any real program on inputs of size 2 or 3). Picking <em>c</em> = max {<em>T</em> (2), <em>T</em> (3)} yields <em>T</em> (2) ≤ <em>c</em> &lt; (2 lg 2)<em>c</em> and <em>T</em> (3) ≤ <em>c</em> &lt; (3 lg 3)<em>c</em>, establishing the inductive hypothesis for the base cases.</p>
<p>Thus, we have <em>T</em> (<em>n</em>) ≤ <em>cn</em> lg <em>n</em> for all <em>n</em> ≥ 2, which implies that the solution to recurrence (4.11) is <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>).</p>
<p>In the algorithms literature, people rarely carry out their substitution proofs to this level of detail, especially in their treatment of base cases. The reason is that for most algorithmic divide-and-conquer recurrences, the base cases are all handled in pretty much the same way. You ground the induction on a range of values from a convenient positive constant <em>n</em><sub>0</sub> up to some constant <img alt="art" src="images/Art_P112.jpg"/> such that for <img alt="art" src="images/Art_P113.jpg"/>, the recurrence always bottoms out in a constant-sized base case between <em>n</em><sub>0</sub> and <img alt="art" src="images/Art_P114.jpg"/>. (This example used <img alt="art" src="images/Art_P115.jpg"/>.) Then, it’s usually apparent, without spelling out the details, that with a suitably large choice of the leading constant (such as <em>c</em> for this example), the inductive hypothesis can be made to hold for all the values in the range from <em>n</em><sub>0</sub> to <img alt="art" src="images/Art_P116.jpg"/>.</p>
<a id="p92"/>
<p class="level4"><strong>Making a good guess</strong></p>
<p class="noindent">Unfortunately, there is no general way to correctly guess the tightest asymptotic solution to an arbitrary recurrence. Making a good guess takes experience and, occasionally, creativity. Fortunately, learning some recurrence-solving heuristics, as well as playing around with recurrences to gain experience, can help you become a good guesser. You can also use recursion trees, which we’ll see in <a href="chapter004.xhtml#Sec_4.4">Section 4.4</a>, to help generate good guesses.</p>
<p>If a recurrence is similar to one you’ve seen before, then guessing a similar solution is reasonable. As an example, consider the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2 + 17) + Θ(<em>n</em>),</p>
<p class="noindent">defined on the reals. This recurrence looks somewhat like the merge-sort recurrence (2.3), but it’s more complicated because of the added “17” in the argument to <em>T</em> on the right-hand side. Intuitively, however, this additional term shouldn’t substantially affect the solution to the recurrence. When <em>n</em> is large, the relative difference between <em>n</em>/2 and <em>n</em>/2 + 17 is not that large: both cut <em>n</em> nearly in half. Consequently, it makes sense to guess that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>), which you can verify is correct using the substitution method (see Exercise 4.3-1).</p>
<p>Another way to make a good guess is to determine loose upper and lower bounds on the recurrence and then reduce your range of uncertainty. For example, you might start with a lower bound of <em>T</em> (<em>n</em>) = Ω(<em>n</em>) for recurrence (4.11), since the recurrence includes the term Θ(<em>n</em>), and you can prove an initial upper bound of <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>2</sup>). Then split your time between trying to lower the upper bound and trying to raise the lower bound until you converge on the correct, asymptotically tight solution, which in this case is <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).</p>
<p class="level4"><strong>A trick of the trade: subtracting a low-order term</strong></p>
<p class="noindent">Sometimes, you might correctly guess a tight asymptotic bound on the solution of a recurrence, but somehow the math fails to work out in the induction proof. The problem frequently turns out to be that the inductive assumption is not strong enough. The trick to resolving this problem is to revise your guess by <em>subtracting</em> a lower-order term when you hit such a snag. The math then often goes through.</p>
<p>Consider the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P117.jpg"/></p>
<p class="noindent">defined on the reals. Let’s guess that the solution is <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>) and try to show that <em>T</em> (<em>n</em>) ≤ <em>cn</em> for <em>n</em> ≥ <em>n</em><sub>0</sub>, where we choose the constants <em>c</em>, <em>n</em><sub>0</sub> &gt; 0 suitably. Substituting our guess into the recurrence, we obtain</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">2(<em>c</em>(<em>n</em>/2)) + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cn</em> + Θ(1),</td>
</tr>
</table>
<a id="p93"/>
<p class="noindent">which, unfortunately, does not imply that <em>T</em> (<em>n</em>) ≤ <em>cn</em> for <em>any</em> choice of <em>c</em>. We might be tempted to try a larger guess, say <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>2</sup>). Although this larger guess works, it provides only a loose upper bound. It turns out that our original guess of <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>) is correct and tight. In order to show that it is correct, however, we must strengthen our inductive hypothesis.</p>
<p>Intuitively, our guess is nearly right: we are off only by Θ(1), a lower-order term. Nevertheless, mathematical induction requires us to prove the <em>exact</em> form of the inductive hypothesis. Let’s try our trick of subtracting a lower-order term from our previous guess: <em>T</em> (<em>n</em>) ≤ <em>cn</em> – <em>d</em>, where <em>d</em> ≥ 0 is a constant. We now have</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">2(<em>c</em>(<em>n</em>/2) – <em>d</em>) + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cn</em> – 2<em>d</em> + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>cn</em> – <em>d</em> – (<em>d</em> – Θ(1))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>cn</em> – <em>d</em></td>
</tr>
</table>
<p class="noindent">as long as we choose <em>d</em> to be larger than the anonymous upper-bound constant hidden by the Θ-notation. Subtracting a lower-order term works! Of course, we must not forget to handle the base case, which is to choose the constant <em>c</em> large enough that <em>cn</em> – <em>d</em> dominates the implicit base cases.</p>
<p>You might find the idea of subtracting a lower-order term to be counterintuitive. After all, if the math doesn’t work out, shouldn’t you increase your guess? Not necessarily! When the recurrence contains more than one recursive invocation (recurrence (4.12) contains two), if you add a lower-order term to the guess, then you end up adding it once for each of the recursive invocations. Doing so takes you even further away from the inductive hypothesis. On the other hand, if you subtract a lower-order term from the guess, then you get to subtract it once for each of the recursive invocations. In the above example, we subtracted the constant <em>d</em> twice because the coefficient of <em>T</em> (<em>n</em>/2) is 2. We ended up with the inequality <em>T</em> (<em>n</em>) ≤ <em>cn</em> – <em>d</em> – (<em>d</em> – Θ(1)), and we readily found a suitable value for <em>d</em>.</p>
<p class="level4"><strong>Avoiding pitfalls</strong></p>
<p class="noindent">Avoid using asymptotic notation in the inductive hypothesis for the substitution method because it’s error prone. For example, for recurrence (4.11), we can falsely “prove” that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>) if we unwisely adopt <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>) as our inductive hypothesis:</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2" colspan="2">2 · <em>O</em>(<span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2" colspan="2">2 · <em>O</em>(<em>n</em>) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>O</em>(<em>n</em>).</td>
<td class="td2"><p class="p3"><img alt="art" src="images/arrow.jpg"/> <span class="red"><em>wrong</em>!</span></p></td>
</tr>
</table>
<a id="p94"/>
<p class="noindent">The problem with this reasoning is that the constant hidden by the <em>O</em>-notation changes. We can expose the fallacy by repeating the “proof” using an explicit constant. For the inductive hypothesis, assume that <em>T</em> (<em>n</em>) ≤ <em>cn</em> for all <em>n</em> ≥ <em>n</em><sub>0</sub>, where <em>c</em>, <em>n</em><sub>0</sub> &gt; 0 are constants. Repeating the first two steps in the inequality chain yields</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">2(<em>c</em> <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>cn</em> + Θ(<em>n</em>).</td>
</tr>
</table>
<p class="noindent">Now, indeed <em>cn</em> + Θ(<em>n</em>) = <em>O</em>(<em>n</em>), but the constant hidden by the <em>O</em>-notation must be larger than <em>c</em> because the anonymous function hidden by the Θ(<em>n</em>) is asymptotically positive. We cannot take the third step to conclude that <em>cn</em> + Θ(<em>n</em>) ≤ <em>cn</em>, thus exposing the fallacy.</p>
<p>When using the substitution method, or more generally mathematical induction, you must be careful that the constants hidden by any asymptotic notation are the same constants throughout the proof. Consequently, it’s best to avoid asymptotic notation in your inductive hypothesis and to name constants explicitly.</p>
<p>Here’s another fallacious use of the substitution method to show that the solution to recurrence (4.11) is <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>). We guess <em>T</em> (<em>n</em>) ≤ <em>cn</em> and then argue</p>
<table class="table2b">
<tr>
<td class="td2"><p class="noindent"><em>T</em> (<em>n</em>)</p></td>
<td class="td2"><p class="center">≤</p></td>
<td class="td2" colspan="2"><p class="noindent">2(<em>c</em> <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>) + Θ(<em>n</em>)</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center">≤</p></td>
<td class="td2" colspan="2"><p class="noindent"><em>cn</em> + Θ(<em>n</em>)</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>O</em>(<em>n</em>),</p></td>
<td class="td2"><p class="p3"><img alt="art" src="images/arrow.jpg"/> <span class="red"><em>wrong</em>!</span></p></td>
</tr>
</table>
<p class="noindent">since <em>c</em> is a positive constant. The mistake stems from the difference between our goal—to prove that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>)—and our inductive hypothesis—to prove that <em>T</em> (<em>n</em>) ≤ <em>cn</em>. When using the substitution method, or in any inductive proof, you must prove the <em>exact</em> statement of the inductive hypothesis. In this case, we must explicitly prove that <em>T</em> (<em>n</em>) ≤ <em>cn</em> to show that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>4.3-1</em></strong></p>
<p class="noindent">Use the substitution method to show that each of the following recurrences defined on the reals has the asymptotic solution specified:</p>
<p class="nl"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> – 1) + <em>n</em> has solution <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>2</sup>).</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/2) + Θ(1) has solution <em>T</em> (<em>n</em>) = <em>O</em>(lg <em>n</em>).</p>
<p class="nl"><strong><em>c.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + <em>n</em> has solution <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2 + 17) + <em>n</em> has solution <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>).</p>
<p class="nl"><strong><em>e.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/3) + Θ(<em>n</em>) has solution <em>T</em> (<em>n</em>) = Θ(<em>n</em>).</p>
<p class="nl"><strong><em>f.</em></strong> <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>) has solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>2</sup>).</p>
<a id="p95"/>
<p class="level3"><strong><em>4.3-2</em></strong></p>
<p class="noindent">The solution to the recurrence <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/2)+<em>n</em> turns out to be <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>2</sup>). Show that a substitution proof with the assumption <em>T</em> (<em>n</em>) ≤ <em>cn</em><sup>2</sup> fails. Then show how to subtract a lower-order term to make a substitution proof work.</p>
<p class="level3"><strong><em>4.3-3</em></strong></p>
<p class="noindent">The recurrence <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em> – 1) + 1 has the solution <em>T</em> (<em>n</em>) = <em>O</em>(2<em><sup>n</sup></em>). Show that a substitution proof fails with the assumption <em>T</em> (<em>n</em>) ≤ <em>c</em> 2<em><sup>n</sup></em>, where <em>c</em> &gt; 0 is constant. Then show how to subtract a lower-order term to make a substitution proof work.</p>
</section>
<p class="line1"/>
<section title="4.4 The recursion-tree method for solving recurrences">
<a id="Sec_4.4"/>
<p class="level1" id="h1-19"><a href="toc.xhtml#Rh1-19"><strong>4.4      The recursion-tree method for solving recurrences</strong></a></p>
<p class="noindent">Although you can use the substitution method to prove that a solution to a recurrence is correct, you might have trouble coming up with a good guess. Drawing out a recursion tree, as we did in our analysis of the merge-sort recurrence in <a href="chapter002.xhtml#Sec_2.3.2">Section 2.3.2</a>, can help. In a <strong><em><span class="blue1">recursion tree</span></em></strong>, each node represents the cost of a single subproblem somewhere in the set of recursive function invocations. You typically sum the costs within each level of the tree to obtain the per-level costs, and then you sum all the per-level costs to determine the total cost of all levels of the recursion. Sometimes, however, adding up the total cost takes more creativity.</p>
<p>A recursion tree is best used to generate intuition for a good guess, which you can then verify by the substitution method. If you are meticulous when drawing out a recursion tree and summing the costs, however, you can use a recursion tree as a direct proof of a solution to a recurrence. But if you use it only to generate a good guess, you can often tolerate a small amount of “sloppiness,” which can simplify the math. When you verify your guess with the substitution method later on, your math should be precise. This section demonstrates how you can use recursion trees to solve recurrences, generate good guesses, and gain intuition for recurrences.</p>
<p class="level4"><strong>An illustrative example</strong></p>
<p class="noindent">Let’s see how a recursion tree can provide a good guess for an upper-bound solution to the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P118.jpg"/></p>
<p class="noindent"><a href="chapter004.xhtml#Fig_4-1">Figure 4.1</a> shows how to derive the recursion tree for <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/4) + <em>cn</em><sup>2</sup>, where the constant <em>c</em> &gt; 0 is the upper-bound constant in the Θ(<em>n</em><sup>2</sup>) term. Part (a) of the figure shows <em>T</em> (<em>n</em>), which part (b) expands into an equivalent tree representing the recurrence. The <em>cn</em><sup>2</sup> term at the root represents the cost at the top level of recursion, and the three subtrees of the root represent the costs incurred by the subproblems of size <em>n</em>/4. Part (c) shows this process carried one step further by expanding each node with cost <em>T</em> (<em>n</em>/4) from part (b). The cost for each of the three children of the root is <em>c</em>(<em>n</em>/4)<sup>2</sup>. We continue expanding each node in the tree by breaking it into its constituent parts as determined by the recurrence.</p>
<a id="p96"/>
<div class="divimage">
<p class="fig-imga" id="Fig_4-1"><img alt="art" class="width100" src="images/Art_P119.jpg"/></p>
<p class="caption"><strong>Figure 4.1</strong> Constructing a recursion tree for the recurrence <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/4) + <em>cn</em><sup>2</sup>. Part <strong>(a)</strong> shows <em>T</em> (<em>n</em>), which progressively expands in <strong>(b)–(d)</strong> to form the recursion tree. The fully expanded tree in <strong>(d)</strong> has height log<sub>4</sub> <em>n</em>.</p>
</div>
<a id="p97"/>
<p>Because subproblem sizes decrease by a factor of 4 every time we go down one level, the recursion must eventually bottom out in a base case where <em>n</em> &lt; <em>n</em><sub>0</sub>. By convention, the base case is <em>T</em> (<em>n</em>) = Θ(1) for <em>n</em> &lt; <em>n</em><sub>0</sub>, where <em>n</em><sub>0</sub> &gt; 0 is any threshold constant sufficiently large that the recurrence is well defined. For the purpose of intuition, however, let’s simplify the math a little. Let’s assume that <em>n</em> is an exact power of 4 and that the base case is <em>T</em> (1) = Θ(1). As it turns out, these assumptions don’t affect the asymptotic solution.</p>
<p>What’s the height of the recursion tree? The subproblem size for a node at depth <em>i</em> is <em>n</em>/4<em><sup>i</sup></em>. As we descend the tree from the root, the subproblem size hits <em>n</em> = 1 when <em>n</em>/4<em><sup>i</sup></em> = 1 or, equivalently, when <em>i</em> = log<sub>4</sub> <em>n</em>. Thus, the tree has internal nodes at depths 0, 1, 2, … , log<sub>4</sub> <em>n</em> – 1 and leaves at depth log<sub>4</sub> <em>n</em>.</p>
<p>Part (d) of <a href="chapter004.xhtml#Fig_4-1">Figure 4.1</a> shows the cost at each level of the tree. Each level has three times as many nodes as the level above, and so the number of nodes at depth <em>i</em> is 3<em><sup>i</sup></em>. Because subproblem sizes reduce by a factor of 4 for each level further from the root, each internal node at depth <em>i</em> = 0, 1, 2, … , log<sub>4</sub> <em>n</em> – 1 has a cost of <em>c</em>(<em>n</em>/4<em><sup>i</sup></em>)<sup>2</sup>. Multiplying, we see that the total cost of all nodes at a given depth <em>i</em> is 3<em><sup>i</sup>c</em>(<em>n</em>/4<em><sup>i</sup></em>)<sup>2</sup> = (3/16)<em><sup>i</sup>cn</em><sup>2</sup>. The bottom level, at depth log<sub>4</sub> <em>n</em>, contains <img alt="art" src="images/Art_P120.jpg"/> leaves (using equation (3.21) on page 66). Each leaf contributes Θ(1), leading to a total leaf cost of <img alt="art" src="images/Art_P121.jpg"/>.</p>
<p>Now we add up the costs over all levels to determine the cost for the entire tree:</p>
<p class="eql"><img alt="art" src="images/Art_P122.jpg"/></p>
<p class="noindent">We’ve derived the guess of <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>2</sup>) for the original recurrence. In this example, the coefficients of <em>cn</em><sup>2</sup> form a decreasing geometric series. By equation (A.7), the sum of these coefficients is bounded from above by the constant 16/13. Since <a id="p98"/>the root’s contribution to the total cost is <em>cn</em><sup>2</sup>, the cost of the root dominates the total cost of the tree.</p>
<p>In fact, if <em>O</em>(<em>n</em><sup>2</sup>) is indeed an upper bound for the recurrence (as we’ll verify in a moment), then it must be a tight bound. Why? The first recursive call contributes a cost of Θ(<em>n</em><sup>2</sup>), and so Ω(<em>n</em><sup>2</sup>) must be a lower bound for the recurrence.</p>
<p>Let’s now use the substitution method to verify that our guess is correct, namely, that <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>2</sup>) is an upper bound for the recurrence <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/4)+Θ(<em>n</em><sup>2</sup>). We want to show that <em>T</em> (<em>n</em>) ≤ <em>dn</em><sup>2</sup> for some constant <em>d</em> &gt; 0. Using the same constant <em>c</em> &gt; 0 as before, we have</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">3<em>T</em> (<em>n</em>/4) + <em>cn</em><sup>2</sup></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2">3<em>d</em>(<em>n</em>/4)<sup>2</sup> + <em>cn</em><sup>2</sup></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><img alt="art" src="images/Art_P123.jpg"/></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>dn</em><sup>2</sup>,</td>
</tr>
</table>
<p class="noindent">where the last step holds if we choose <em>d</em> ≥ (16/13)<em>c</em>.</p>
<p>For the base case of the induction, let <em>n</em><sub>0</sub> &gt; 0 be a sufficiently large threshold constant that the recurrence is well defined when <em>T</em> (<em>n</em>) = Θ(1) for <em>n</em> &lt; <em>n</em><sub>0</sub>. We can pick <em>d</em> large enough that <em>d</em> dominates the constant hidden by the Θ, in which case <em>dn</em><sup>2</sup> ≥ <em>d</em> ≥ <em>T</em> (<em>n</em>) for 1 ≤ <em>n</em> &lt; <em>n</em><sub>0</sub>, completing the proof of the base case.</p>
<p>The substitution proof we just saw involves two named constants, <em>c</em> and <em>d</em>. We named <em>c</em> and used it to stand for the upper-bound constant hidden and guaranteed to exist by the Θ-notation. We cannot pick <em>c</em> arbitrarily—it’s given to us—although, for any such <em>c</em>, any constant <em>c</em><sup>′</sup> ≥ <em>c</em> also suffices. We also named <em>d</em>, but we were free to choose any value for it that fit our needs. In this example, the value of <em>d</em> happened to depend on the value of <em>c</em>, which is fine, since <em>d</em> is constant if <em>c</em> is constant.</p>
<p class="level4"><strong>An irregular example</strong></p>
<p class="noindent">Let’s find an asymptotic upper bound for another, more irregular, example. <a href="chapter004.xhtml#Fig_4-2">Figure 4.2</a> shows the recursion tree for the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P124.jpg"/></p>
<p class="noindent">This recursion tree is unbalanced, with different root-to-leaf paths having different lengths. Going left at any node produces a subproblem of one-third the size, and going right produces a subproblem of two-thirds the size. Let <em>n</em><sub>0</sub> &gt; 0 be the implicit threshold constant such that <em>T</em> (<em>n</em>) = Θ(1) for 0 &lt; <em>n</em> &lt; <em>n</em><sub>0</sub>, and let <em>c</em> represent the upper-bound constant hidden by the Θ(<em>n</em>) term for <em>n</em> ≥ <em>n</em><sub>0</sub>. There are actually two <em>n</em><sub>0</sub> constants here—one for the threshold in the recurrence, and the other for the threshold in the Θ-notation, so we’ll let <em>n</em><sub>0</sub> be the larger of the two constants.</p>
<a id="p99"/>
<div class="divimage">
<p class="fig-imga" id="Fig_4-2"><img alt="art" class="width100" src="images/Art_P125.jpg"/></p>
<p class="caption"><strong>Figure 4.2</strong> A recursion tree for the recurrence <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/3) + <em>T</em> (2<em>n</em>/3) + <em>cn</em>.</p>
</div>
<p>The height of the tree runs down the right edge of the tree, corresponding to subproblems of sizes <em>n</em>, (2/3)<em>n</em>, (4/9)<em>n</em>, … , Θ(1) with costs bounded by <em>cn</em>, <em>c</em>(2<em>n</em>/3), <em>c</em>(4<em>n</em>/9), … , Θ(1), respectively. We hit the rightmost leaf when (2/3)<em><sup>h</sup>n</em> &lt; <em>n</em><sub>0</sub> ≤ (2/3)<sup><em>h</em>–1</sup><em>n</em>, which happens when <em>h</em> = <span class="font1">⌊</span>log<sub>3/2</sub>(<em>n</em>/<em>n</em><sub>0</sub>)<span class="font1">⌋</span> + 1 since, applying the floor bounds in equation (3.2) on page 64 with <em>x</em> = log<sub>3/2</sub> (<em>n</em>/<em>n</em><sub>0</sub>), we have (2/3)<em><sup>h</sup>n</em> = (2/3)<sup><span class="font1">⌊</span><em>x</em><span class="font1">⌋</span>+1</sup><em>n</em> &lt; (2/3)<em><sup>x</sup>n</em> = (<em>n</em><sub>0</sub>/<em>n</em>)<em>n</em> = <em>n</em><sub>0</sub> and (2/3)<sup><em>h</em>–1</sup><em>n</em> = (2/3)<sup><span class="font1">⌊</span><em>x</em><span class="font1">⌋</span></sup><em>n</em> &gt; (2/3)<sup><em>x</em></sup><em>n</em> = (<em>n</em><sub>0</sub>/<em>n</em>)<em>n</em> = <em>n</em><sub>0</sub>. Thus, the height of the tree is <em>h</em> = Θ(lg <em>n</em>).</p>
<p>We’re now in a position to understand the upper bound. Let’s postpone dealing with the leaves for a moment. Summing the costs of internal nodes across each level, we have at most <em>cn</em> per level times the Θ(lg <em>n</em>) tree height for a total cost of <em>O</em>(<em>n</em> lg <em>n</em>) for all internal nodes.</p>
<p>It remains to deal with the leaves of the recursion tree, which represent base cases, each costing Θ(1). How many leaves are there? It’s tempting to upper-bound their number by the number of leaves in a complete binary tree of height <em>h</em> = <span class="font1">⌊</span>log<sub>3/2</sub>(<em>n</em>/<em>n</em><sub>0</sub>)<span class="font1">⌋</span> + 1, since the recursion tree is contained within such a complete binary tree. But this approach turns out to give us a poor bound. The complete binary tree has 1 node at the root, 2 nodes at depth 1, and generally 2<em><sup>k</sup></em> nodes at depth <em>k</em>. Since the height is <em>h</em> = <span class="font1">⌊</span>log<sub>3/2</sub> <em>n</em><span class="font1">⌋</span> + 1, there are <a id="p100"/> <img alt="art" src="images/Art_P126.jpg"/> leaves in the complete binary tree, which is an upper bound on the number of leaves in the recursion tree. Because the cost of each leaf is Θ(1), this analysis says that the total cost of all leaves in the recursion tree is <img alt="art" src="images/Art_P127.jpg"/>, which is an asymptotically greater bound than the <em>O</em>(<em>n</em> lg <em>n</em>) cost of all internal nodes. In fact, as we’re about to see, this bound is not tight. The cost of all leaves in the recursion tree is <em>O</em>(<em>n</em>)—asymptotically <em>less</em> than <em>O</em>(<em>n</em> lg <em>n</em>). In other words, the cost of the internal nodes dominates the cost of the leaves, not vice versa.</p>
<p>Rather than analyzing the leaves, we could quit right now and prove by substitution that <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>). This approach works (see Exercise 4.4-3), but it’s instructive to understand how many leaves this recursion tree has. You may see recurrences for which the cost of leaves dominates the cost of internal nodes, and then you’ll be in better shape if you’ve had some experience analyzing the number of leaves.</p>
<p>To figure out how many leaves there really are, let’s write a recurrence <em>L</em>(<em>n</em>) for the number of leaves in the recursion tree for <em>T</em> (<em>n</em>). Since all the leaves in <em>T</em> (<em>n</em>) belong either to the left subtree or the right subtree of the root, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P128.jpg"/></p>
<p class="noindent">This recurrence is similar to recurrence (4.14), but it’s missing the Θ(<em>n</em>) term, and it contains an explicit base case. Because this recurrence omits the Θ(<em>n</em>) term, it is much easier to solve. Let’s apply the substitution method to show that it has solution <em>L</em>(<em>n</em>) = <em>O</em>(<em>n</em>). Using the inductive hypothesis <em>L</em>(<em>n</em>) ≤ <em>dn</em> for some constant <em>d</em> &gt; 0, and assuming that the inductive hypothesis holds for all values less than <em>n</em>, we have</p>
<table class="table2b">
<tr>
<td class="td2"><em>L</em>(<em>n</em>)</td>
<td class="td2m">=</td>
<td class="td2"><em>L</em>(<em>n</em>/3) + <em>L</em>(2<em>n</em>/3)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>dn</em>/3 + 2(<em>dn</em>)/3</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>dn</em>,</td>
</tr>
</table>
<p class="noindent">which holds for any <em>d</em> &gt; 0. We can now choose <em>d</em> large enough to handle the base case <em>L</em>(<em>n</em>) = 1 for 0 &lt; <em>n</em> &lt; <em>n</em><sub>0</sub>, for which <em>d</em> = 1 suffices, thereby completing the substitution method for the upper bound on leaves. (Exercise 4.4-2 asks you to prove that <em>L</em>(<em>n</em>) = Θ(<em>n</em>).)</p>
<p>Returning to recurrence (4.14) for <em>T</em> (<em>n</em>), it now becomes apparent that the total cost of leaves over all levels must be <em>L</em>(<em>n</em>) · Θ(1) = Θ(<em>n</em>). Since we have derived the bound of <em>O</em>(<em>n</em> lg <em>n</em>) on the cost of the internal nodes, it follows that the solution to recurrence (4.14) is <em>T</em> (<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>) + Θ(<em>n</em>) = <em>O</em>(<em>n</em> lg <em>n</em>). (Exercise 4.4-3 asks you to prove that <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).)</p>
<p>It’s wise to verify any bound obtained with a recursion tree by using the substitution method, especially if you’ve made simplifying assumptions. But another <a id="p101"/>strategy altogether is to use more-powerful mathematics, typically in the form of the master method in the next section (which unfortunately doesn’t apply to recurrence (4.14)) or the Akra-Bazzi method (which does, but requires calculus). Even if you use a powerful method, a recursion tree can improve your intuition for what’s going on beneath the heavy math.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>4.4-1</em></strong></p>
<p class="noindent">For each of the following recurrences, sketch its recursion tree, and guess a good asymptotic upper bound on its solution. Then use the substitution method to verify your answer.</p>
<p class="nl"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/2) + <em>n</em><sup>3</sup>.</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/3) + <em>n</em>.</p>
<p class="nl"><strong><em>c.</em></strong> <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/2) + <em>n</em>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em> – 1) + 1.</p>
<p class="level3"><strong><em>4.4-2</em></strong></p>
<p class="noindent">Use the substitution method to prove that recurrence (4.15) has the asymptotic lower bound <em>L</em>(<em>n</em>) = Ω(<em>n</em>). Conclude that <em>L</em>(<em>n</em>) = Θ(<em>n</em>).</p>
<p class="level3"><strong><em>4.4-3</em></strong></p>
<p class="noindent">Use the substitution method to prove that recurrence (4.14) has the solution <em>T</em> (<em>n</em>) = Ω(<em>n</em> lg <em>n</em>). Conclude that <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).</p>
<p class="level3"><strong><em>4.4-4</em></strong></p>
<p class="noindent">Use a recursion tree to justify a good guess for the solution to the recurrence <em>T</em> (<em>n</em>) = <em>T</em> (<em>αn</em>)+<em>T</em> ((1–<em>α</em>)<em>n</em>)+Θ(<em>n</em>), where <em>α</em> is a constant in the range 0 &lt; <em>α</em> &lt; 1.</p>
</section>
<p class="line1"/>
<section title="4.5 The master method for solving recurrences">
<a id="Sec_4.5"/>
<p class="level1" id="h1-20"><a href="toc.xhtml#Rh1-20"><strong>4.5      The master method for solving recurrences</strong></a></p>
<p class="noindent">The master method provides a “cookbook” method for solving algorithmic recurrences of the form</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P129.jpg"/></p>
<p class="noindent">where <em>a</em> &gt; 0 and <em>b</em> &gt; 1 are constants. We call <em>f</em> (<em>n</em>) a <strong><em><span class="blue1">driving function</span></em></strong>, and we call a recurrence of this general form a <strong><em><span class="blue1">master recurrence</span></em></strong>. To use the master method, you need to memorize three cases, but then you’ll be able to solve many master recurrences quite easily.</p>
<a id="p102"/>
<p>A master recurrence describes the running time of a divide-and-conquer algorithm that divides a problem of size <em>n</em> into <em>a</em> subproblems, each of size <em>n</em>/<em>b</em> &lt; <em>n</em>. The algorithm solves the <em>a</em> subproblems recursively, each in <em>T</em> (<em>n</em>/<em>b</em>) time. The driving function <em>f</em> (<em>n</em>) encompasses the cost of dividing the problem before the recursion, as well as the cost of combining the results of the recursive solutions to subproblems. For example, the recurrence arising from Strassen’s algorithm is a master recurrence with <em>a</em> = 7, <em>b</em> = 2, and driving function <em>f</em> (<em>n</em>) = Θ(<em>n</em><sup>2</sup>).</p>
<p>As we have mentioned, in solving a recurrence that describes the running time of an algorithm, one technicality that we’d often prefer to ignore is the requirement that the input size <em>n</em> be an integer. For example, we saw that the running time of merge sort can be described by recurrence (2.3), <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>), on page 41. But if <em>n</em> is an odd number, we really don’t have two problems of exactly half the size. Rather, to ensure that the problem sizes are integers, we round one subproblem down to size <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> and the other up to size <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span>, so the true recurrence is <em>T</em> (<em>n</em>) = <em>T</em> (<span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> + <em>T</em> (<span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>) + Θ(<em>n</em>). But this floors-and-ceilings recurrence is longer to write and messier to deal with than recurrence (2.3), which is defined on the reals. We’d rather not worry about floors and ceilings, if we don’t have to, especially since the two recurrences have the same Θ(<em>n</em> lg <em>n</em>) solution.</p>
<p>The master method allows you to state a master recurrence without floors and ceilings and implicitly infer them. No matter how the arguments are rounded up or down to the nearest integer, the asymptotic bounds that it provides remain the same. Moreover, as we’ll see in <a href="chapter004.xhtml#Sec_4.6">Section 4.6</a>, if you define your master recurrence on the reals, without implicit floors and ceilings, the asymptotic bounds still don’t change. Thus you can ignore floors and ceilings for master recurrences. <a href="chapter004.xhtml#Sec_4.7">Section 4.7</a> gives sufficient conditions for ignoring floors and ceilings in more general divide-and-conquer recurrences.</p>
<p class="level4"><strong>The master theorem</strong></p>
<p class="noindent">The master method depends upon the following theorem.</p>
<p class="theo"><strong><em>Theorem 4.1 (Master theorem)</em></strong></p>
<p class="noindent">Let <em>a</em> &gt; 0 and <em>b</em> &gt; 1 be constants, and let <em>f</em> (<em>n</em>) be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence <em>T</em> (<em>n</em>) on <em>n</em> ∈ <span class="struck">N</span> by</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P130.jpg"/></p>
<p class="noindent">where <em>aT</em> (<em>n</em>/<em>b</em>) actually means <em>a</em><sup>′</sup><em>T</em> (<span class="font1">⌊</span><em>n</em>/<em>b</em><span class="font1">⌋</span>) + <em>a</em><sup>″</sup><em>T</em> (<span class="font1">⌈</span><em>n</em>/<em>b</em><span class="font1">⌉</span>) for some constants <em>a</em><sup>′</sup> ≥ 0 and <em>a</em><sup>″</sup> ≥ 0 satisfying <em>a</em> = <em>a</em><sup>′</sup> + <em>a</em><sup>″</sup>. Then the asymptotic behavior of <em>T</em> (<em>n</em>) can be characterized as follows:</p>
<a id="p103"/>
<ol class="olnoindent" epub:type="list">
<li>If there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P131.jpg"/>, then <img alt="art" src="images/Art_P132.jpg"/>.</li>
<li class="litop">If there exists a constant <em>k</em> ≥ 0 such that <img alt="art" src="images/Art_P133.jpg"/>, then <img alt="art" src="images/Art_P134.jpg"/>.</li>
<li class="litop">If there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P135.jpg"/>, and if <em>f</em> (<em>n</em>) additionally satisfies the <strong><em><span class="blue1">regularity condition</span></em></strong> <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) for some constant <em>c</em> &lt; 1 and all sufficiently large <em>n</em>, then <em>T</em> (<em>n</em>) = Θ(<em>f</em> (<em>n</em>)).</li></ol>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Before applying the master theorem to some examples, let’s spend a few moments to understand broadly what it says. The function <img alt="art" src="images/Art_P136.jpg"/> is called the <strong><em><span class="blue1">watershed function</span></em></strong>. In each of the three cases, we compare the driving function <em>f</em> (<em>n</em>) to the watershed function <img alt="art" src="images/Art_P137.jpg"/>. Intuitively, if the watershed function grows asymptotically faster than the driving function, then case 1 applies. Case 2 applies if the two functions grow at nearly the same asymptotic rate. Case 3 is the “opposite” of case 1, where the driving function grows asymptotically faster than the watershed function. But the technical details matter.</p>
<p>In case 1, not only must the watershed function grow asymptotically faster than the driving function, it must grow <em>polynomially</em> faster. That is, the watershed function <img alt="art" src="images/Art_P138.jpg"/> must be asymptotically larger than the driving function <em>f</em> (<em>n</em>) by at least a factor of Θ(<em>n<sup><span class="font1">ϵ</span></sup></em>) for some constant <em><span class="font1">ϵ</span></em> &gt; 0. The master theorem then says that the solution is <img alt="art" src="images/Art_P139.jpg"/>. In this case, if we look at the recursion tree for the recurrence, the cost per level grows at least geometrically from root to leaves, and the total cost of leaves dominates the total cost of the internal nodes.</p>
<p>In case 2, the watershed and driving functions grow at nearly the same asymptotic rate. But more specifically, the driving function grows faster than the watershed function by a factor of Θ(lg<em><sup>k</sup> n</em>), where <em>k</em> ≥ 0. The master theorem says that we tack on an extra lg <em>n</em> factor to <em>f</em> (<em>n</em>), yielding the solution <img alt="art" src="images/Art_P140.jpg"/>. In this case, each level of the recursion tree costs approximately the same—<img alt="art" src="images/Art_P141.jpg"/>—and there are Θ(lg <em>n</em>) levels. In practice, the most common situation for case 2 occurs when <em>k</em> = 0, in which case the watershed and driving functions have the same asymptotic growth, and the solution is <img alt="art" src="images/Art_P142.jpg"/>.</p>
<p>Case 3 mirrors case 1. Not only must the driving function grow asymptotically faster than the watershed function, it must grow <em>polynomially</em> faster. That is, the driving function <em>f</em> (<em>n</em>) must be asymptotically larger than the watershed function <img alt="art" src="images/Art_P143.jpg"/> by at least a factor of Θ(<em>n<sup><span class="font1">ϵ</span></sup></em>) for some constant <em><span class="font1">ϵ</span></em> &gt; 0. Moreover, the driving function must satisfy the regularity condition that <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>). This condition is satisfied by most of the polynomially bounded functions that you’re likely to encounter when applying case 3. The regularity condition might not be satisfied <a id="p104"/>if the driving function grows slowly in local areas, yet relatively quickly overall. (Exercise 4.5-5 gives an example of such a function.) For case 3, the master theorem says that the solution is <em>T</em> (<em>n</em>) = Θ(<em>f</em> (<em>n</em>)). If we look at the recursion tree, the cost per level drops at least geometrically from the root to the leaves, and the root cost dominates the cost of all other nodes.</p>
<p>It’s worth looking again at the requirement that there be polynomial separation between the watershed function and the driving function for either case 1 or case 3 to apply. The separation doesn’t need to be much, but it must be there, and it must grow polynomially. For example, for the recurrence <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/2) + <em>n</em><sup>1.99</sup> (admittedly not a recurrence you’re likely to see when analyzing an algorithm), the watershed function is <img alt="art" src="images/Art_P144.jpg"/>. Hence the driving function <em>f</em> (<em>n</em>) = <em>n</em><sup>1.99</sup> is polynomially smaller by a factor of <em>n</em><sup>0.01</sup>. Thus case 1 applies with <em><span class="font1">ϵ</span></em> = 0.01.</p>
<p class="level4"><strong>Using the master method</strong></p>
<p class="noindent">To use the master method, you determine which case (if any) of the master theorem applies and write down the answer.</p>
<p>As a first example, consider the recurrence <em>T</em> (<em>n</em>) = 9<em>T</em> (<em>n</em>/3) + <em>n</em>. For this recurrence, we have <em>a</em> = 9 and <em>b</em> = 3, which implies that <img alt="art" src="images/Art_P145.jpg"/>. Since <em>f</em> (<em>n</em>) = <em>n</em> = <em>O</em>(<em>n</em><sup>2–<em><span class="font1">ϵ</span></em></sup>) for any constant <em><span class="font1">ϵ</span></em> ≤ 1, we can apply case 1 of the master theorem to conclude that the solution is <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>2</sup>).</p>
<p>Now consider the recurrence <em>T</em> (<em>n</em>) = <em>T</em> (2<em>n</em>/3) + 1, which has <em>a</em> = 1 and <em>b</em> = 3/2, which means that the watershed function is <img alt="art" src="images/Art_P146.jpg"/>. Case 2 applies since <img alt="art" src="images/Art_P147.jpg"/>. The solution to the recurrence is <em>T</em> (<em>n</em>) = Θ(lg <em>n</em>).</p>
<p>For the recurrence <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/4) + <em>n</em> lg <em>n</em>, we have <em>a</em> = 3 and <em>b</em> = 4, which means that <img alt="art" src="images/Art_P148.jpg"/>. Since <img alt="art" src="images/Art_P149.jpg"/>, where <em><span class="font1">ϵ</span></em> can be as large as approximately 0.2, case 3 applies as long as the regularity condition holds for <em>f</em> (<em>n</em>). It does, because for sufficiently large <em>n</em>, we have that <em>af</em> (<em>n</em>/<em>b</em>) = 3(<em>n</em>/4) lg(<em>n</em>/4) ≤ (3/4)<em>n</em> lg <em>n</em> = <em>cf</em> (<em>n</em>) for <em>c</em> = 3/4. By case 3, the solution to the recurrence is <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).</p>
<p>Next, let’s look at the recurrence <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + <em>n</em> lg <em>n</em>, where we have <em>a</em> = 2, <em>b</em> = 2, and <img alt="art" src="images/Art_P150.jpg"/>. Case 2 applies since <img alt="art" src="images/Art_P151.jpg"/>. We conclude that the solution is <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg<sup>2</sup> <em>n</em>).</p>
<p>We can use the master method to solve the recurrences we saw in <a href="chapter002.xhtml#Sec_2.3.2">Sections 2.3.2</a>, <a href="chapter004.xhtml#Sec_4.1">4.1</a>, and <a href="chapter004.xhtml#Sec_4.2">4.2</a>.</p>
<p>Recurrence (2.3), <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>), on page 41, characterizes the running time of merge sort. Since <em>a</em> = 2 and <em>b</em> = 2, the watershed function is <img alt="art" src="images/Art_P152.jpg"/>. Case 2 applies because <em>f</em> (<em>n</em>) = Θ(<em>n</em>), and the solution is <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).</p>
<a id="p105"/>
<p>Recurrence (4.9), <em>T</em> (<em>n</em>) = 8<em>T</em> (<em>n</em>/2) + Θ(1), on page 84, describes the running time of the simple recursive algorithm for matrix multiplication. We have <em>a</em> = 8 and <em>b</em> = 2, which means that the watershed function is <img alt="art" src="images/Art_P153.jpg"/>. Since <em>n</em><sup>3</sup> is polynomially larger than the driving function <em>f</em> (<em>n</em>) = Θ(1)—indeed, we have <em>f</em> (<em>n</em>) = <em>O</em>(<em>n</em><sup>3–<em><span class="font1">ϵ</span></em></sup>) for any positive <em><span class="font1">ϵ</span></em> &lt; 3—case 1 applies. We conclude that <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>3</sup>).</p>
<p>Finally, recurrence (4.10), <em>T</em> (<em>n</em>) = 7<em>T</em> (<em>n</em>/2) + Θ(<em>n</em><sup>2</sup>), on page 87, arose from the analysis of Strassen’s algorithm for matrix multiplication. For this recurrence, we have <em>a</em> = 7 and <em>b</em> = 2, and the watershed function is <img alt="art" src="images/Art_P154.jpg"/>. Observing that lg 7 = 2.807355 …, we can let <em><span class="font1">ϵ</span></em> = 0.8 and bound the driving function <em>f</em> (<em>n</em>) = Θ(<em>n</em><sup>2</sup>) = <em>O</em>(<em>n</em><sup>lg 7–<em><span class="font1">ϵ</span></em></sup>). Case 1 applies with solution <em>T</em> (<em>n</em>) = Θ(<em>n</em><sup>lg 7</sup>).</p>
<p class="level4"><strong>When the master method doesn’t apply</strong></p>
<p class="noindent">There are situations where you can’t use the master theorem. For example, it can be that the watershed function and the driving function cannot be asymptotically compared. We might have that <img alt="art" src="images/Art_P155.jpg"/> for an infinite number of values of <em>n</em> but also that <img alt="art" src="images/Art_P156.jpg"/> for an infinite number of different values of <em>n</em>. As a practical matter, however, most of the driving functions that arise in the study of algorithms can be meaningfully compared with the watershed function. If you encounter a master recurrence for which that’s not the case, you’ll have to resort to substitution or other methods.</p>
<p>Even when the relative growths of the driving and watershed functions can be compared, the master theorem does not cover all the possibilities. There is a gap between cases 1 and 2 when <img alt="art" src="images/Art_P157.jpg"/>, yet the watershed function does not grow polynomially faster than the driving function. Similarly, there is a gap between cases 2 and 3 when <img alt="art" src="images/Art_P158.jpg"/> and the driving function grows more than polylogarithmically faster than the watershed function, but it does not grow polynomially faster. If the driving function falls into one of these gaps, or if the regularity condition in case 3 fails to hold, you’ll need to use something other than the master method to solve the recurrence.</p>
<p>As an example of a driving function falling into a gap, consider the recurrence <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + <em>n</em>/lg <em>n</em>. Since <em>a</em> = 2 and <em>b</em> = 2, the watershed function is <img alt="art" src="images/Art_P159.jpg"/>. The driving function is <em>n</em>/lg <em>n</em> = <em>o</em>(<em>n</em>), which means that it grows asymptotically more slowly than the watershed function <em>n</em>. But <em>n</em>/lg <em>n</em> grows only <em>logarithmically</em> slower than <em>n</em>, not <em>polynomially</em> slower. More precisely, equation (3.24) on page 67 says that lg <em>n</em> = <em>o</em>(<em>n<sup><span class="font1">ϵ</span></sup></em>) for any constant <em><span class="font1">ϵ</span></em> &gt; 0, which means that 1/lg <em>n</em> = <em>ω</em>(<em>n</em><sup>–<em><span class="font1">ϵ</span></em></sup>) and <img alt="art" src="images/Art_P160.jpg"/>. Thus no constant <em><span class="font1">ϵ</span></em> &gt; 0 exists such that <img alt="art" src="images/Art_P161.jpg"/>, which is required for case 1 to apply. Case 2 fails to apply as well, since <img alt="art" src="images/Art_P162.jpg"/>, where <em>k</em> = –1, but <em>k</em> must be nonnegative for case 2 to apply.</p>
<a id="p106"/>
<p>To solve this kind of recurrence, you must use another method, such as the substitution method (<a href="chapter004.xhtml#Sec_4.3">Section 4.3</a>) or the Akra-Bazzi method (<a href="chapter004.xhtml#Sec_4.7">Section 4.7</a>). (Exercise 4.6-3 asks you to show that the answer is Θ(<em>n</em> lg lg <em>n</em>).) Although the master theorem doesn’t handle this particular recurrence, it does handle the overwhelming majority of recurrences that tend to arise in practice.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>4.5-1</em></strong></p>
<p class="noindent">Use the master method to give tight asymptotic bounds for the following recurrences.</p>
<p class="nl-top"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/4) + 1.</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/4) + <img alt="art" src="images/Art_P163.jpg"/>.</p>
<p class="nl"><strong><em>c.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/4) + <img alt="art" src="images/Art_P164.jpg"/>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/4) + <em>n</em>.</p>
<p class="nl"><strong><em>e.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/4) + <em>n</em><sup>2</sup>.</p>
<p class="level3"><strong><em>4.5-2</em></strong></p>
<p class="noindent">Professor Caesar wants to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen’s algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into <em>n</em>/4 × <em>n</em>/4 submatrices, and the divide and combine steps together will take Θ(<em>n</em><sup>2</sup>) time. Suppose that the professor’s algorithm creates <em>a</em> recursive subproblems of size <em>n</em>/4. What is the largest integer value of <em>a</em> for which his algorithm could possibly run asymptotically faster than Strassen’s?</p>
<p class="level3"><strong><em>4.5-3</em></strong></p>
<p class="noindent">Use the master method to show that the solution to the binary-search recurrence <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/2) + Θ(1) is <em>T</em> (<em>n</em>) = Θ(lg <em>n</em>). (See Exercise 2.3-6 for a description of binary search.)</p>
<p class="level3"><strong><em>4.5-4</em></strong></p>
<p class="noindent">Consider the function <em>f</em> (<em>n</em>) = lg <em>n</em>. Argue that although <em>f</em> (<em>n</em>/2) &lt; <em>f</em> (<em>n</em>), the regularity condition <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) with <em>a</em> = 1 and <em>b</em> = 2 does not hold for any constant <em>c</em> &lt; 1. Argue further that for any <em><span class="font1">ϵ</span></em> &gt; 0, the condition in case 3 that <img alt="art" src="images/Art_P165.jpg"/> does not hold.</p>
<a id="p107"/>
<p class="level3"><strong><em>4.5-5</em></strong></p>
<p class="noindent">Show that for suitable constants <em>a</em>, <em>b</em>, and <em><span class="font1">ϵ</span></em>, the function <em>f</em> (<em>n</em>) = 2<sup><span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span></sup> satisfies all the conditions in case 3 of the master theorem except the regularity condition.</p>
</section>
<p class="line1"/>
<section title="⋆ 4.6 Proof of the continuous master theorem">
<a id="Sec_4.6"/>
<p class="level1-1" id="h1-21"><a href="toc.xhtml#Rh1-21"><span class="font1">★</span> <strong>4.6      Proof of the continuous master theorem</strong></a></p>
<p class="noindent">Proving the master theorem (Theorem 4.1) in its full generality, especially dealing with the knotty technical issue of floors and ceilings, is beyond the scope of this book. This section, however, states and proves a variant of the master theorem, called the <strong><em><span class="blue1">continuous master theorem</span></em></strong><sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> in which the master recurrence (4.17) is defined over sufficiently large positive real numbers. The proof of this version, uncomplicated by floors and ceilings, contains the main ideas needed to understand how master recurrences behave. <a href="chapter004.xhtml#Sec_4.7">Section 4.7</a> discusses floors and ceilings in divide-and-conquer recurrences at greater length, presenting sufficient conditions for them not to affect the asymptotic solutions.</p>
<p>Of course, since you need not understand the proof of the master theorem in order to apply the master method, you may choose to skip this section. But if you wish to study more-advanced algorithms beyond the scope of this textbook, you may appreciate a better understanding of the underlying mathematics, which the proof of the continuous master theorem provides.</p>
<p>Although we usually assume that recurrences are algorithmic and don’t require an explicit statement of a base case, we must be much more careful for proofs that justify the practice. The lemmas and theorem in this section explicitly state the base cases, because the inductive proofs require mathematical grounding. It is common in the world of mathematics to be extraordinarily careful proving theorems that justify acting more casually in practice.</p>
<p>The proof of the continuous master theorem involves two lemmas. Lemma 4.2 uses a slightly simplified master recurrence with a threshold constant of <em>n</em><sub>0</sub> = 1, rather than the more general <em>n</em><sub>0</sub> &gt; 0 threshold constant implied by the unstated base case. The lemma employs a recursion tree to reduce the solution of the simplified master recurrence to that of evaluating a summation. Lemma 4.3 then provides asymptotic bounds for the summation, mirroring the three cases of the master theorem. Finally, the continuous master theorem itself (Theorem 4.4) gives asymptotic bounds for master recurrences, while generalizing to an arbitrary threshold constant <em>n</em><sub>0</sub> &gt; 0 as implied by the unstated base case.</p>
<a id="p108"/>
<p>Some of the proofs use the properties described in Problem 3-5 on pages 72–73 to combine and simplify complicated asymptotic expressions. Although Problem 3-5 addresses only Θ-notation, the properties enumerated there can be extended to <em>O</em>-notation and Ω-notation as well.</p>
<p>Here’s the first lemma.</p>
<p class="lemma"><strong><em>Lemma 4.2</em></strong></p>
<p class="noindent">Let <em>a</em> &gt; 0 and <em>b</em> &gt; 1 be constants, and let <em>f</em> (<em>n</em>) be a function defined over real numbers <em>n</em> ≥ 1. Then the recurrence</p>
<p class="eql"><img alt="art" src="images/Art_P166.jpg"/></p>
<p class="noindent">has solution</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P167.jpg"/></p>
<p class="proof"><strong><em>Proof</em></strong>   Consider the recursion tree in <a href="chapter004.xhtml#Fig_4-3">Figure 4.3</a>. Let’s look first at its internal nodes. The root of the tree has cost <em>f</em> (<em>n</em>), and it has <em>a</em> children, each with cost <em>f</em> (<em>n</em>/<em>b</em>). (It is convenient to think of <em>a</em> as being an integer, especially when visualizing the recursion tree, but the mathematics does not require it.) Each of these children has <em>a</em> children, making <em>a</em><sup>2</sup> nodes at depth 2, and each of the <em>a</em> children has cost <em>f</em> (<em>n</em>/<em>b</em><sup>2</sup>). In general, there are <em>a<sup>j</sup></em> nodes at depth <em>j</em>, and each node has cost <em>f</em> (<em>n</em>/<em>b<sup>j</sup></em>).</p>
<p>Now, let’s move on to understanding the leaves. The tree grows downward until <em>n</em>/<em>b<sup>j</sup></em> becomes less than 1. Thus, the tree has height <span class="font1">⌊</span>log<em><sub>b</sub> n</em><span class="font1">⌋</span> + 1, because <img alt="art" src="images/Art_P168.jpg"/> and <img alt="art" src="images/Art_P169.jpg"/>. Since, as we have observed, the number of nodes at depth <em>j</em> is <em>a<sup>j</sup></em> and all the leaves are at depth <span class="font1">⌊</span>log<em><sub>b</sub></em> <em>n</em><span class="font1">⌋</span> + 1, the tree contains <img alt="art" src="images/Art_P170.jpg"/> leaves. Using the identity (3.21) on page 66, we have <img alt="art" src="images/Art_P171.jpg"/>, since <em>a</em> is constant, and <img alt="art" src="images/Art_P172.jpg"/>. Consequently, the total number of leaves is <img alt="art" src="images/Art_P173.jpg"/>—asymptotically, the watershed function.</p>
<p>We are now in a position to derive equation (4.18) by summing the costs of the nodes at each depth in the tree, as shown in the figure. The first term in the equation is the total costs of the leaves. Since each leaf is at depth <span class="font1">⌊</span>log<sub><em>b</em></sub><em>n</em><span class="font1">⌋</span> + 1 and <img alt="art" src="images/Art_P174.jpg"/>, the base case of the recurrence gives the cost of a leaf: <img alt="art" src="images/Art_P175.jpg"/>. Hence the cost of all <img alt="art" src="images/Art_P176.jpg"/> leaves is <img alt="art" src="images/Art_P177.jpg"/> by Problem 3-5(d). The second term in equation (4.18) is the cost of the internal nodes, which, in the underlying divide-and-conquer algorithm, represents the costs of dividing problems into subproblems and <a id="p109"/>then recombining the subproblems. Since the cost for all the internal nodes at depth <em>j</em> is <em>a<sup>j</sup> f</em> (<em>n</em>/<em>b<sup>j</sup></em>), the total cost of all internal nodes is</p>
<p class="eql"><img alt="art" src="images/Art_P178.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<div class="divimage">
<p class="fig-imga" id="Fig_4-3"><img alt="art" class="width100" src="images/Art_P179.jpg"/></p>
<p class="caption"><strong>Figure 4.3</strong> The recursion tree generated by <em>T</em> (<em>n</em>) = <em>aT</em> (<em>n</em>/<em>b</em>) + <em>f</em> (<em>n</em>). The tree is a complete <em>a</em>-ary tree with <img alt="art" src="images/Art_P180.jpg"/> leaves and height <span class="font1">⌊</span>log<em><sub>b</sub> n</em><span class="font1">⌋</span> + 1. The cost of the nodes at each depth is shown at the right, and their sum is given in equation (4.18).</p>
</div>
<p>As we’ll see, the three cases of the master theorem depend on the distribution of the total cost across levels of the recursion tree:</p>
<p class="para-hang1"><strong>Case 1:</strong> The costs increase geometrically from the root to the leaves, growing by a constant factor with each level.</p>
<p class="para-hang1"><strong>Case 2:</strong> The costs depend on the value of <em>k</em> in the theorem. With <em>k</em> = 0, the costs are equal for each level; with <em>k</em> = 1, the costs grow linearly from the root to the leaves; with <em>k</em> = 2, the growth is quadratic; and in general, the costs grow polynomially in <em>k</em>.</p>
<p class="para-hang1"><strong>Case 3:</strong> The costs decrease geometrically from the root to the leaves, shrinking by a constant factor with each level.</p>
<a id="p110"/>
<p class="space-break">The summation in equation (4.18) describes the cost of the dividing and combining steps in the underlying divide-and-conquer algorithm. The next lemma provides asymptotic bounds on the summation’s growth.</p>
<p class="lemma"><strong><em>Lemma 4.3</em></strong></p>
<p class="noindent">Let <em>a</em> &gt; 0 and <em>b</em> &gt; 1 be constants, and let <em>f</em> (<em>n</em>) be a function defined over real numbers <em>n</em> ≥ 1. Then the asymptotic behavior of the function</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P181.jpg"/></p>
<p class="noindent">defined for <em>n</em> ≥ 1, can be characterized as follows:</p>
<ol class="olnoindent" epub:type="list">
<li>If there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P182.jpg"/>, then <img alt="art" src="images/Art_P183.jpg"/>.</li>
<li class="litop">If there exists a constant <em>k</em> ≥ 0 such that <img alt="art" src="images/Art_P184.jpg"/>, then <img alt="art" src="images/Art_P185.jpg"/>.</li>
<li class="litop">If there exists a constant <em>c</em> in the range 0 &lt; <em>c</em> &lt; 1 such that 0 &lt; <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) for all <em>n</em> ≥ 1, then <em>g</em>(<em>n</em>) = Θ(<em>f</em> (<em>n</em>)).</li></ol>
<p class="proof"><strong><em>Proof</em></strong>   For case 1, we have <img alt="art" src="images/Art_P186.jpg"/>, which implies that <img alt="art" src="images/Art_P187.jpg"/>. Substituting into equation (4.19) yields</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P188.jpg"/></p>
<p class="noindent">the last series being geometric. Since <em>b</em> and <em><span class="font1">ϵ</span></em> are constants, the <em>b<sup><span class="font1">ϵ</span></sup></em> – 1 denominator doesn’t affect the asymptotic growth of <em>g</em>(<em>n</em>), and neither does the –1 in <a id="p111"/>the numerator. Since <img alt="art" src="images/Art_P189.jpg"/>, we obtain <img alt="art" src="images/Art_P190.jpg"/>, thereby proving case 1.</p>
<p>Case 2 assumes that <img alt="art" src="images/Art_P191.jpg"/>, from which we can conclude that <img alt="art" src="images/Art_P192.jpg"/>. Substituting into equation (4.19) and repeatedly applying Problem 3-5(c) yields</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P193.jpg"/></p>
<p class="noindent">The summation within the Θ-notation can be bounded from above as follows:</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P194.jpg"/></p>
<p class="noindent">Exercise 4.6-1 asks you to show that the summation can similarly be bounded from below by <img alt="art" src="images/Art_P195.jpg"/>. Since we have tight upper and lower bounds, the summation is <img alt="art" src="images/Art_P196.jpg"/>, from which we can conclude that <img alt="art" src="images/Art_P197.jpg"/>, thereby completing the proof of case 2.</p>
<a id="p112"/>
<p>For case 3, observe that <em>f</em> (<em>n</em>) appears in the definition (4.19) of <em>g</em>(<em>n</em>) (when <em>j</em> = 0) and that all terms of <em>g</em>(<em>n</em>) are positive. Therefore, we must have <em>g</em>(<em>n</em>) = Ω(<em>f</em> (<em>n</em>)), and it only remains to prove that <em>g</em>(<em>n</em>) = <em>O</em>(<em>f</em> (<em>n</em>)). Performing <em>j</em> iterations of the inequality <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) yields <em>a<sup>j</sup> f</em> (<em>n</em>/<em>b<sup>j</sup></em>) ≤ <em>c<sup>j</sup> f</em> (<em>n</em>). Substituting into equation (4.19), we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P198.jpg"/></p>
<p class="noindent">Thus, we can conclude that <em>g</em>(<em>n</em>) = Θ(<em>f</em> (<em>n</em>)). With case 3 proved, the entire proof of the lemma is complete.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We can now state and prove the continuous master theorem.</p>
<p class="theo"><strong><em>Theorem 4.4 (Continuous master theorem)</em></strong></p>
<p class="noindent">Let <em>a</em> &gt; 0 and <em>b</em> &gt; 1 be constants, and let <em>f</em> (<em>n</em>) be a driving function that is defined and nonnegative on all sufficiently large reals. Define the algorithmic recurrence <em>T</em> (<em>n</em>) on the positive real numbers by</p>
<p class="eql"><em>T</em> (<em>n</em>) = <em>aT</em> (<em>n</em>/<em>b</em>) + <em>f</em> (<em>n</em>).</p>
<p class="noindent">Then the asymptotic behavior of <em>T</em> (<em>n</em>) can be characterized as follows:</p>
<ol class="olnoindent" epub:type="list">
<li>If there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P199.jpg"/>, then <img alt="art" src="images/Art_P200.jpg"/>.</li>
<li class="litop">If there exists a constant <em>k</em> ≥ 0 such that <img alt="art" src="images/Art_P201.jpg"/>, then <img alt="art" src="images/Art_P202.jpg"/>.</li>
<li class="litop">If there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P203.jpg"/>, and if <em>f</em> (<em>n</em>) additionally satisfies the regularity condition <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) for some constant <em>c</em> &lt; 1 and all sufficiently large <em>n</em>, then <em>T</em> (<em>n</em>) = Θ(<em>f</em> (<em>n</em>)).</li></ol>
<p class="proof"><strong><em>Proof</em></strong>   The idea is to bound the summation (4.18) from Lemma 4.2 by applying Lemma 4.3. But we must account for Lemma 4.2 using a base case for 0 &lt; <em>n</em> &lt; 1, <a id="p113"/>whereas this theorem uses an implicit base case for 0 &lt; <em>n</em> &lt; <em>n</em><sub>0</sub>, where <em>n</em><sub>0</sub> &gt; 0 is an arbitrary threshold constant. Since the recurrence is algorithmic, we can assume that <em>f</em> (<em>n</em>) is defined for <em>n</em> ≥ <em>n</em><sub>0</sub>.</p>
<p>For <em>n</em> &gt; 0, let us define two auxiliary functions <em>T</em><sup>′</sup>(<em>n</em>) = <em>T</em> (<em>n</em><sub>0</sub> <em>n</em>) and <em>f</em> <sup>′</sup>(<em>n</em>) = <em>f</em> (<em>n</em><sub>0</sub> <em>n</em>). We have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P204.jpg"/></p>
<p class="noindent">We have obtained a recurrence for <em>T</em> <sup>′</sup>(<em>n</em>) that satisfies the conditions of Lemma 4.2, and by that lemma, the solution is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P205.jpg"/></p>
<p class="noindent">To solve <em>T</em> <sup>′</sup>(<em>n</em>), we first need to bound <em>f</em> <sup>′</sup>(<em>n</em>). Let’s examine the individual cases in the theorem.</p>
<p>The condition for case 1 is <img alt="art" src="images/Art_P206.jpg"/> for some constant <em><span class="font1">ϵ</span></em> &gt; 0. We have</p>
<p class="eql"><img alt="art" src="images/Art_P207.jpg"/></p>
<p class="noindent">since <em>a</em>, <em>b</em>, <em>n</em><sub>0</sub>, and <em><span class="font1">ϵ</span></em> are all constant. The function <em>f</em> <sup>′</sup>(<em>n</em>) satisfies the conditions of case 1 of Lemma 4.3, and the summation in equation (4.18) of Lemma 4.2 evaluates to <img alt="art" src="images/Art_P208.jpg"/>. Because <em>a</em>, <em>b</em> and <em>n</em><sub>0</sub> are all constants, we have</p>
<p class="eql"><img alt="art" src="images/Art_P209.jpg"/></p>
<p class="noindent">thereby completing case 1 of the theorem.</p>
<p>The condition for case 2 is <img alt="art" src="images/Art_P210.jpg"/> for some constant <em>k</em> ≥ 0. We have</p>
<p class="eql"><img alt="art" src="images/Art_P211.jpg"/></p>
<a id="p114"/>
<p class="noindent">Similar to the proof of case 1, the function <em>f</em><sup>′</sup>(<em>n</em>) satisfies the conditions of case 2 of Lemma 4.3. The summation in equation (4.18) of Lemma 4.2 is therefore <img alt="art" src="images/Art_P212.jpg"/>, which implies that</p>
<p class="eql"><img alt="art" src="images/Art_P213.jpg"/></p>
<p class="noindent">which proves case 2 of the theorem.</p>
<p>Finally, the condition for case 3 is <img alt="art" src="images/Art_P214.jpg"/> for some constant <em><span class="font1">ϵ</span></em> &gt; 0 and <em>f</em> (<em>n</em>) additionally satisfies the regularity condition <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub> and some constants <em>c</em> &lt; 1 and <em>n</em><sub>0</sub> &gt; 1. The first part of case 3 is like case 1:</p>
<p class="eql"><img alt="art" src="images/Art_P215.jpg"/></p>
<p class="noindent">Using the definition of <em>f</em> <sup>′</sup>(<em>n</em>) and the fact that <em>n</em><sub>0</sub> <em>n</em> ≥ <em>n</em><sub>0</sub> for all <em>n</em> ≥ 1, we have for <em>n</em> ≥ 1 that</p>
<table class="table2b">
<tr>
<td class="td2"><em>af</em> <sup>′</sup>(<em>n</em>/<em>b</em>)</td>
<td class="td2m">=</td>
<td class="td2"><em>af</em> (<em>n</em><sub>0</sub> <em>n</em>/<em>b</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>cf</em> (<em>n</em><sub>0</sub> <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><em>cf</em> <sup>′</sup>(<em>n</em>).</td>
</tr>
</table>
<p class="noindent">Thus <em>f</em> <sup>′</sup>(<em>n</em>) satisfies the requirements for case 3 of Lemma 4.3, and the summation in equation (4.18) of Lemma 4.2 evaluates to Θ(<em>f</em> <sup>′</sup>(<em>n</em>)), yielding</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2m">=</td>
<td class="td2"><em>T</em> <sup>′</sup>(<em>n</em>/<em>n</em><sub>0</sub>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2"><img alt="art" src="images/Art_P216.jpg"/></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">Θ(<em>f</em> <sup>′</sup>(<em>n</em>/<em>n</em><sub>0</sub>))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">Θ(<em>f</em> (<em>n</em>)),</td>
</tr>
</table>
<p class="noindent">which completes the proof of case 3 of the theorem and thus the whole theorem.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>4.6-1</em></strong></p>
<p class="noindent">Show that <img alt="art" src="images/Art_P217.jpg"/>.</p>
<p class="level3"><span class="font1">★</span> <strong><em>4.6-2</em></strong></p>
<p class="noindent">Show that case 3 of the master theorem is overstated (which is also why case 3 of Lemma 4.3 does not require that <img alt="art" src="images/Art_P218.jpg"/> in the sense that the <a id="p115"/>regularity condition <em>af</em> (<em>n</em>/<em>b</em>) ≤ <em>cf</em> (<em>n</em>) for some constant <em>c</em> &lt; 1 implies that there exists a constant <em><span class="font1">ϵ</span></em> &gt; 0 such that <img alt="art" src="images/Art_P219.jpg"/>.</p>
<p class="level3"><span class="font1">★</span> <strong><em>4.6-3</em></strong></p>
<p class="noindent">For <img alt="art" src="images/Art_P220.jpg"/>, prove that the summation in equation (4.19) has solution <img alt="art" src="images/Art_P221.jpg"/>. Conclude that a master recurrence <em>T</em> (<em>n</em>) using <em>f</em> (<em>n</em>) as its driving function has solution <img alt="art" src="images/Art_P222.jpg"/>.</p>
</section>
<p class="line1"/>
<section title="⋆ 4.7 Akra-Bazzi recurrences">
<a id="Sec_4.7"/>
<p class="level1-1" id="h1-22"><a href="toc.xhtml#Rh1-22"><span class="font1">★</span> <strong>4.7      Akra-Bazzi recurrences</strong></a></p>
<p class="noindent">This section provides an overview of two advanced topics related to divide-and-conquer recurrences. The first deals with technicalities arising from the use of floors and ceilings, and the second discusses the Akra-Bazzi method, which involves a little calculus, for solving complicated divide-and-conquer recurrences.</p>
<p>In particular, we’ll look at the class of algorithmic divide-and-conquer recurrences originally studied by M. Akra and L. Bazzi [<a epub:type="noteref" href="bibliography001.xhtml#endnote_13">13</a>]. These <strong><em><span class="blue1">Akra-Bazzi</span></em></strong> recurrences take the form</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P223.jpg"/></p>
<p class="noindent">where <em>k</em> is a positive integer; all the constants <em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>k</sub></em> ∈ <span class="struck">R</span> are strictly positive; all the constants <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>k</sub></em> ∈ <span class="struck">R</span> are strictly greater than 1; and the driving function <em>f</em> (<em>n</em>) is defined on sufficiently large nonnegative reals and is itself nonnegative.</p>
<p>Akra-Bazzi recurrences generalize the class of recurrences addressed by the master theorem. Whereas master recurrences characterize the running times of divide-and-conquer algorithms that break a problem into equal-sized subproblems (modulo floors and ceilings), Akra-Bazzi recurrences can describe the running time of divide-and-conquer algorithms that break a problem into different-sized subproblems. The master theorem, however, allows you to ignore floors and ceilings, but the Akra-Bazzi method for solving Akra-Bazzi recurrences needs an additional requirement to deal with floors and ceilings.</p>
<p>But before diving into the Akra-Bazzi method itself, let’s understand the limitations involved in ignoring floors and ceilings in Akra-Bazzi recurrences. As you’re aware, algorithms generally deal with integer-sized inputs. The mathematics for recurrences is often easier with real numbers, however, than with integers, where we must cope with floors and ceilings to ensure that terms are well defined. The difference may not seem to be much—especially because that’s often the truth with recurrences—but to be mathematically correct, we must be careful with our <a id="p116"/>assumptions. Since our end goal is to understand algorithms and not the vagaries of mathematical corner cases, we’d like to be casual yet rigorous. How can we treat floors and ceilings casually while still ensuring rigor?</p>
<p>From a mathematical point of view, the difficulty in dealing with floors and ceilings is that some driving functions can be really, really weird. So it’s not okay in general to ignore floors and ceilings in Akra-Bazzi recurrences. Fortunately, most of the driving functions we encounter in the study of algorithms behave nicely, and floors and ceilings don’t make a difference.</p>
<p class="level4"><strong>The polynomial-growth condition</strong></p>
<p class="noindent">If the driving function <em>f</em> (<em>n</em>) in equation (4.22) is well behaved in the following sense, it’s okay to drop floors and ceilings.</p>
<div class="pull-quote">
<p class="pq-noindent">A function <em>f</em> (<em>n</em>) defined on all sufficiently large positive reals satisfies the <strong><em><span class="blue1">polynomial-growth condition</span></em></strong> if there exists a constant <img alt="art" src="images/Art_P224.jpg"/> such that the following holds: for every constant <em><span class="symbolfont">ϕ</span></em> ≥ 1, there exists a constant <em>d</em> &gt; 1 (depending on <em><span class="symbolfont">ϕ</span></em>) such that <em>f</em> (<em>n</em>)/<em>d</em> ≤ <em>f</em> (<em>ψ n</em>) ≤ <em>df</em> (<em>n</em>) for all 1 ≤ <em>ψ</em> ≤ <em><span class="symbolfont">ϕ</span></em> and <img alt="art" src="images/Art_P225.jpg"/>.</p>
</div>
<p class="noindent">This definition may be one of the hardest in this textbook to get your head around. To a first order, it says that <em>f</em> (<em>n</em>) satisfies the property that <em>f</em> (Θ(<em>n</em>)) = Θ(<em>f</em> (<em>n</em>)), although the polynomial-growth condition is actually somewhat stronger (see Exercise 4.7-4). The definition also implies that <em>f</em> (<em>n</em>) is asymptotically positive (see Exercise 4.7-3).</p>
<p>Examples of functions that satisfy the polynomial-growth condition include any function of the form <em>f</em> (<em>n</em>) = Θ(<em>n<sup>α</sup></em> lg<em><sup>β</sup> n</em> lg lg<em><sup>γ</sup>n</em>), where <em>α</em>, <em>β</em>, and <em>γ</em> are constants. Most of the polynomially bounded functions used in this book satisfy the condition. Exponentials and superexponentials do not (see Exercise 4.7-2, for example), and there also exist polynomially bounded functions that do not.</p>
<p class="level4"><strong>Floors and ceilings in “nice” recurrences</strong></p>
<p class="noindent">When the driving function in an Akra-Bazzi recurrence satisfies the polynomial-growth condition, floors and ceilings don’t change the asymptotic behavior of the solution. The following theorem, which is presented without proof, formalizes this notion.</p>
<p class="theo"><strong><em>Theorem 4.5</em></strong></p>
<p class="noindent">Let <em>T</em> (<em>n</em>) be a function defined on the nonnegative reals that satisfies recurrence (4.22), where <em>f</em> (<em>n</em>) satisfies the polynomial-growth condition. Let <em>T</em> <sup>′</sup>(<em>n</em>) be another function defined on the natural numbers also satisfying recurrence (4.22), <a id="p117"/>except that each <em>T</em> (<em>n</em>/<em>b<sub>i</sub></em>) is replaced either with <em>T</em> (<span class="font1">⌈</span><em>n</em>/<em>b<sub>i</sub></em><span class="font1">⌉</span>) or with <em>T</em> (<span class="font1">⌊</span><em>n</em>/<em>b<sub>i</sub></em><span class="font1">⌋</span>). Then we have <em>T</em> <sup>′</sup>(<em>n</em>) = Θ(<em>T</em> (<em>n</em>)).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Floors and ceilings represent a minor perturbation to the arguments in the recursion. By inequality (3.2) on page 64, they perturb an argument by at most 1. But much larger perturbations are tolerable. As long as the driving function <em>f</em> (<em>n</em>) in recurrence (4.22) satisfies the polynomial-growth condition, it turns out that replacing any term <em>T</em> (<em>n</em>/<em>b<sub>i</sub></em>) with <em>T</em> (<em>n</em>/<em>b<sub>i</sub></em> + <em>h<sub>i</sub></em>(<em>n</em>)), where |<em>h<sub>i</sub></em>(<em>n</em>)| = <em>O</em>(<em>n</em>/lg<sup>1+<em><span class="font1">ϵ</span></em></sup> <em>n</em>) for some constant <em><span class="font1">ϵ</span></em> &gt; 0 and sufficiently large <em>n</em>, leaves the asymptotic solution unaffected. Thus, the divide step in a divide-and-conquer algorithm can be moderately coarse without affecting the solution to its running-time recurrence.</p>
<p class="level4"><strong>The Akra-Bazzi method</strong></p>
<p class="noindent">The Akra-Bazzi method, not surprisingly, was developed to solve Akra-Bazzi recurrences (4.22), which by dint of Theorem 4.5, applies in the presence of floors and ceilings or even larger perturbations, as just discussed. The method involves first determining the unique real number <em>p</em> such that <img alt="art" src="images/Art_P226.jpg"/>. Such a <em>p</em> always exists, because when <em>p</em> → –∞, the sum goes to ∞; it decreases as <em>p</em> increases; and when <em>p</em> → ∞, it goes to 0. The Akra-Bazzi method then gives the solution to the recurrence as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P227.jpg"/></p>
<p>As an example, consider the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P228.jpg"/></p>
<p class="noindent">We’ll see the similar recurrence (9.1) on page 240 when we study an algorithm for selecting the <em>i</em>th smallest element from a set of <em>n</em> numbers. This recurrence has the form of equation (4.22), where <em>a</em><sub>1</sub> = <em>a</em><sub>2</sub> = 1, <em>b</em><sub>1</sub> = 5, <em>b</em><sub>2</sub> = 10/7, and <em>f</em> (<em>n</em>) = <em>n</em>. To solve it, the Akra-Bazzi method says that we should determine the unique <em>p</em> satisfying</p>
<p class="eql"><img alt="art" src="images/Art_P229.jpg"/></p>
<p class="noindent">Solving for <em>p</em> is kind of messy—it turns out that <em>p</em> = 0.83978 …—but we can solve the recurrence without actually knowing the exact value for <em>p</em>. Observe that (1/5)<sup>0</sup> + (7/10)<sup>0</sup> = 2 and (1/5)<sup>1</sup> + (7/10)<sup>1</sup> = 9/10, and thus <em>p</em> lies in the range 0 &lt; <em>p</em> &lt; 1. That turns out to be sufficient for the Akra-Bazzi method to give us the solution. We’ll use the fact from calculus that if <em>k</em> ≠ –1, then ∫ <em>x<sup>k</sup>dx</em> = <em>x</em><sup><em>k</em> + 1</sup>/(<em>k</em> + 1), which we’ll apply with <em>k</em> = – <em>p</em> ≠ –1. The Akra-Bazzi <a id="p118"/>solution (4.23) gives us</p>
<p class="eql"><img alt="art" src="images/Art_P230.jpg"/></p>
<p>Although the Akra-Bazzi method is more general than the master theorem, it requires calculus and sometimes a bit more reasoning. You also must ensure that your driving function satisfies the polynomial-growth condition if you want to ignore floors and ceilings, although that’s rarely a problem. When it applies, the master method is much simpler to use, but only when subproblem sizes are more or less equal. They are both good tools for your algorithmic toolkit.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><span class="font1">★</span> <strong><em>4.7-1</em></strong></p>
<p class="noindent">Consider an Akra-Bazzi recurrence <em>T</em> (<em>n</em>) on the reals as given in recurrence (4.22), and define <em>T</em> <sup>′</sup>(<em>n</em>) as</p>
<p class="eql"><img alt="art" src="images/Art_P231.jpg"/></p>
<p class="noindent">where <em>c</em> &gt; 0 is constant. Prove that whatever the implicit initial conditions for <em>T</em> (<em>n</em>) might be, there exist initial conditions for <em>T</em> <sup>′</sup>(<em>n</em>) such that <em>T</em> <sup>′</sup>(<em>n</em>) = <em>cT</em> (<em>n</em>) for all <em>n</em> &gt; 0. Conclude that we can drop the asymptotics on a driving function in any Akra-Bazzi recurrence without affecting its asymptotic solution.</p>
<p class="level3"><strong><em>4.7-2</em></strong></p>
<p class="noindent">Show that <em>f</em> (<em>n</em>) = <em>n</em><sup>2</sup> satisfies the polynomial-growth condition but that <em>f</em> (<em>n</em>) = 2<em><sup>n</sup></em> does not.</p>
<p class="level3"><strong><em>4.7-3</em></strong></p>
<p class="noindent">Let <em>f</em> (<em>n</em>) be a function that satisfies the polynomial-growth condition. Prove that <em>f</em> (<em>n</em>) is asymptotically positive, that is, there exists a constant <em>n</em><sub>0</sub> ≥ 0 such that <em>f</em> (<em>n</em>) ≥ 0 for all <em>n</em> ≥ <em>n</em><sub>0</sub>.</p>
<a id="p119"/>
<p class="level3"><span class="font1">★</span> <strong><em>4.7-4</em></strong></p>
<p class="noindent">Give an example of a function <em>f</em> (<em>n</em>) that does not satisfy the polynomial-growth condition but for which <em>f</em> (Θ(<em>n</em>)) = Θ(<em>f</em> (<em>n</em>)).</p>
<p class="level3"><strong><em>4.7-5</em></strong></p>
<p class="noindent">Use the Akra-Bazzi method to solve the following recurrences.</p>
<p class="nl"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/2) + <em>T</em> (<em>n</em>/3) + <em>T</em> (<em>n</em>/6) + <em>n</em> lg <em>n</em>.</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/3) + 8<em>T</em> (<em>n</em>/4) + <em>n</em><sup>2</sup>/lg <em>n</em>.</p>
<p class="nl"><strong><em>c.</em></strong> <em>T</em> (<em>n</em>) = (2/3)<em>T</em> (<em>n</em>/3) + (1/3)<em>T</em> (2<em>n</em>/3) + lg <em>n</em>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = (1/3)<em>T</em> (<em>n</em>/3) + 1/<em>n</em>.</p>
<p class="nl"><strong><em>e.</em></strong> <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/3) + 3<em>T</em> (2<em>n</em>/3) + <em>n</em><sup>2</sup>.</p>
<p class="level3"><span class="font1">★</span> <strong><em>4.7-6</em></strong></p>
<p class="noindent">Use the Akra-Bazzi method to prove the continuous master theorem.</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-23"><strong>Problems</strong></p>
<section title="4-1 Recurrence examples">
<p class="level2"><strong><em>4-1     Recurrence examples</em></strong></p>
<p class="noindent">Give asymptotically tight upper and lower bounds for <em>T</em> (<em>n</em>) in each of the following algorithmic recurrences. Justify your answers.</p>
<p class="nl"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + <em>n</em><sup>3</sup>.</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (8<em>n</em>/11) + <em>n</em>.</p>
<p class="nl"><strong><em>c.</em></strong> <em>T</em> (<em>n</em>) = 16<em>T</em> (<em>n</em>/4) + <em>n</em><sup>2</sup>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = 4<em>T</em> (<em>n</em>/2) + <em>n</em><sup>2</sup> lg <em>n</em>.</p>
<p class="nl"><strong><em>e.</em></strong> <em>T</em> (<em>n</em>) = 8<em>T</em> (<em>n</em>/3) + <em>n</em><sup>2</sup>.</p>
<p class="nl"><strong><em>f.</em></strong> <em>T</em> (<em>n</em>) = 7<em>T</em> (<em>n</em>/2) + <em>n</em><sup>2</sup> lg <em>n</em>.</p>
<p class="nl"><strong><em>g.</em></strong> <img alt="art" src="images/Art_P232.jpg"/>.</p>
<p class="nl"><strong><em>h.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> –2) + <em>n</em><sup>2</sup>.</p>
<a id="p120"/>
</section>
<section title="4-2 Parameter-passing costs">
<p class="level2"><strong><em>4-2     Parameter-passing costs</em></strong></p>
<p class="noindent">Throughout this book, we assume that parameter passing during procedure calls takes constant time, even if an <em>N</em>-element array is being passed. This assumption is valid in most systems because a pointer to the array is passed, not the array itself. This problem examines the implications of three parameter-passing strategies:</p>
<ol class="olnoindent" epub:type="list">
<li>Arrays are passed by pointer. Time = Θ(1).</li>
<li class="litop">Arrays are passed by copying. Time = Θ(<em>N</em>), where <em>N</em> is the size of the array.</li>
<li class="litop">Arrays are passed by copying only the subrange that might be accessed by the called procedure. Time = Θ(<em>n</em>) if the subarray contains <em>n</em> elements.</li></ol>
<p class="noindent">Consider the following three algorithms:</p>
<p class="nl"><strong><em>a.</em></strong> The recursive binary-search algorithm for finding a number in a sorted array (see Exercise 2.3-6).</p>
<p class="nl"><strong><em>b.</em></strong> The M<small>ERGE</small>-S<small>ORT</small> procedure from <a href="chapter002.xhtml#Sec_2.3.1">Section 2.3.1</a>.</p>
<p class="nl"><strong><em>c.</em></strong> The M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure from <a href="chapter004.xhtml#Sec_4.1">Section 4.1</a>.</p>
<p class="noindent1-top">Give nine recurrences <em>T</em><sub><em>a</em>1</sub>(<em>N, n</em>), <em>T</em><sub><em>a</em>2</sub>(<em>N, n</em>), … , <em>T</em><sub><em>c</em>3</sub>(<em>N, n</em>) for the worst-case running times of each of the three algorithms above when arrays and matrices are passed using each of the three parameter-passing strategies above. Solve your recurrences, giving tight asymptotic bounds.</p>
</section>
<section title="4-3 Solving recurrences with a change of variables">
<p class="level2"><strong><em>4-3     Solving recurrences with a change of variables</em></strong></p>
<p class="noindent">Sometimes, a little algebraic manipulation can make an unknown recurrence similar to one you have seen before. Let’s solve the recurrence</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P233.jpg"/></p>
<p class="noindent">by using the change-of-variables method.</p>
<p class="nl"><strong><em>a.</em></strong> Define <em>m</em> = lg <em>n</em> and <em>S</em>(<em>m</em>) = <em>T</em> (2<em><sup>m</sup></em>). Rewrite recurrence (4.25) in terms of <em>m</em> and <em>S</em>(<em>m</em>).</p>
<p class="nl"><strong><em>b.</em></strong> Solve your recurrence for <em>S</em>(<em>m</em>).</p>
<p class="nl"><strong><em>c.</em></strong> Use your solution for <em>S</em>(<em>m</em>) to conclude that <em>T</em> (<em>n</em>) = Θ(lg <em>n</em> lg lg <em>n</em>).</p>
<p class="nl"><strong><em>d.</em></strong> Sketch the recursion tree for recurrence (4.25), and use it to explain intuitively why the solution is <em>T</em> (<em>n</em>) = Θ(lg <em>n</em> lg lg <em>n</em>).</p>
<p class="noindent1-top">Solve the following recurrences by changing variables:</p>
<a id="p121"/>
<p class="nl"><strong><em>e.</em></strong> <img alt="art" src="images/Art_P234.jpg"/>.</p>
<p class="nl"><strong><em>f.</em></strong> <img alt="art" src="images/Art_P235.jpg"/>.</p>
</section>
<section title="4-4 More recurrence examples">
<p class="level2"><strong><em>4-4     More recurrence examples</em></strong></p>
<p class="noindent">Give asymptotically tight upper and lower bounds for <em>T</em> (<em>n</em>) in each of the following recurrences. Justify your answers.</p>
<p class="nl"><strong><em>a.</em></strong> <em>T</em> (<em>n</em>) = 5<em>T</em> (<em>n</em>/3) + <em>n</em> lg <em>n</em>.</p>
<p class="nl"><strong><em>b.</em></strong> <em>T</em> (<em>n</em>) = 3<em>T</em> (<em>n</em>/3) + <em>n</em>/lg <em>n</em>.</p>
<p class="nl"><strong><em>c.</em></strong> <img alt="art" src="images/Art_P236.jpg"/>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2 –2) + <em>n</em>/2.</p>
<p class="nl"><strong><em>e.</em></strong> <em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + <em>n</em>/lg <em>n</em>.</p>
<p class="nl"><strong><em>f.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em>/2) + <em>T</em> (<em>n</em>/4) + <em>T</em> (<em>n</em>/8) + <em>n</em>.</p>
<p class="nl"><strong><em>g.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> – 1) + 1/<em>n</em>.</p>
<p class="nl"><strong><em>h.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> – 1) + lg <em>n</em>.</p>
<p class="nl"><strong><em>i.</em></strong> <em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> – 2) + 1/lg <em>n</em>.</p>
<p class="nl"><strong><em>j.</em></strong> <img alt="art" src="images/Art_P237.jpg"/>.</p>
</section>
<section title="4-5 Fibonacci numbers">
<p class="level2"><strong><em>4-5     Fibonacci numbers</em></strong></p>
<p class="noindent">This problem develops properties of the Fibonacci numbers, which are defined by recurrence (3.31) on page 69. We’ll explore the technique of generating functions to solve the Fibonacci recurrence. Define the <strong><em><span class="blue1">generating function</span></em></strong> (or <strong><em><span class="blue1">formal power series</span></em></strong>) <span class="script">F</span> as</p>
<p class="eql"><img alt="art" src="images/Art_P238.jpg"/></p>
<p class="noindent">where <em>F<sub>i</sub></em> is the <em>i</em>th Fibonacci number.</p>
<p class="nl"><strong><em>a.</em></strong> Show that <span class="script">F</span> (<em>z</em>) = <em>z</em> + <em>z</em><span class="script">F</span> (<em>z</em>) + <em>z</em><sup>2</sup><span class="script">F</span> (<em>z</em>).</p>
<a id="p122"/>
<p class="nl"><strong><em>b.</em></strong> Show that</p>
<p class="nl-para"><img alt="art" src="images/Art_P239.jpg"/></p>
<p class="nl-para">where <em><span class="symbolfont">ϕ</span></em> is the golden ratio, and <img alt="art" src="images/Art_P240.jpg"/> is its conjugate (see page 69).</p>
<p class="nl"><strong><em>c.</em></strong> Show that</p>
<p class="nl-para"><img alt="art" src="images/Art_P241.jpg"/></p>
<p class="nl-para">You may use without proof the generating-function version of equation (A.7) on page 1142, <img alt="art" src="images/Art_P242.jpg"/>. Because this equation involves a generating function, <em>x</em> is a formal variable, not a real-valued variable, so that you don’t have to worry about convergence of the summation or about the requirement in equation (A.7) that |<em>x</em>| &lt; 1, which doesn’t make sense here.</p>
<p class="nl"><strong><em>d.</em></strong> Use part (c) to prove that <img alt="art" src="images/Art_P243.jpg"/> for <em>i</em> &gt; 0, rounded to the nearest integer. (<em>Hint:</em> Observe that <img alt="art" src="images/Art_P244.jpg"/>.)</p>
<p class="nl"><strong><em>e.</em></strong> Prove that <em>F</em><sub><em>i</em>+2</sub> ≥ <em><span class="symbolfont">ϕ</span><sup>i</sup></em> for <em>i</em> ≥ 0.</p>
</section>
<section title="4-6 Chip testing">
<p class="level2"><strong><em>4-6     Chip testing</em></strong></p>
<p class="noindent">Professor Diogenes has <em>n</em> supposedly identical integrated-circuit chips that in principle are capable of testing each other. The professor’s test jig accommodates two chips at a time. When the jig is loaded, each chip tests the other and reports whether it is good or bad. A good chip always reports accurately whether the other chip is good or bad, but the professor cannot trust the answer of a bad chip. Thus, the four possible outcomes of a test are as follows:</p>
<table class="table2">
<tr>
<td class="td2b">Chip <em>A</em> says</td>
<td class="td2b">Chip <em>B</em> says</td>
<td class="td2b">Conclusion</td>
</tr>
<tr>
<td class="td2"><em>B</em> is good</td>
<td class="td2"><em>A</em> is good</td>
<td class="td2">both are good, or both are bad</td>
</tr>
<tr>
<td class="td2"><em>B</em> is good</td>
<td class="td2"><em>A</em> is bad</td>
<td class="td2">at least one is bad</td>
</tr>
<tr>
<td class="td2"><em>B</em> is bad</td>
<td class="td2"><em>A</em> is good</td>
<td class="td2">at least one is bad</td>
</tr>
<tr>
<td class="td2"><em>B</em> is bad</td>
<td class="td2"><em>A</em> is bad</td>
<td class="td2">at least one is bad</td>
</tr>
</table>
<p class="nl"><strong><em>a.</em></strong> Show that if at least <em>n</em>/2 chips are bad, the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwise test. Assume that the bad chips can conspire to fool the professor.</p>
<a id="p123"/>
<p class="noindent1-top">Now you will design an algorithm to identify which chips are good and which are bad, assuming that more than <em>n</em>/2 of the chips are good. First, you will determine how to identify one good chip.</p>
<p class="nl"><strong><em>b.</em></strong> Show that <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> pairwise tests are sufficient to reduce the problem to one of nearly half the size. That is, show how to use <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> pairwise tests to obtain a set with at most <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> chips that still has the property that more than half of the chips are good.</p>
<p class="nl"><strong><em>c.</em></strong> Show how to apply the solution to part (b) recursively to identify one good chip. Give and solve the recurrence that describes the number of tests needed to identify one good chip.</p>
<p class="noindent1-top">You have now determined how to identify one good chip.</p>
<p class="nl"><strong><em>d.</em></strong> Show how to identify all the good chips with an additional Θ(<em>n</em>) pairwise tests.</p>
</section>
<section title="4-7 Monge arrays">
<p class="level2"><strong><em>4-7     Monge arrays</em></strong></p>
<p class="noindent">An <em>m</em> × <em>n</em> array <em>A</em> of real numbers is a <strong><em><span class="blue1">Monge array</span></em></strong> if for all <em>i</em>, <em>j</em>, <em>k</em>, and <em>l</em> such that 1 ≤ <em>i</em> &lt; <em>k</em> ≤ <em>m</em> and 1 ≤ <em>j</em> &lt; <em>l</em> ≤ <em>n</em>, we have</p>
<p class="eql"><em>A</em>[<em>i</em>, <em>j</em>] + <em>A</em>[<em>k, l</em>] ≤ <em>A</em>[<em>i, l</em>] + <em>A</em>[<em>k</em>, <em>j</em>].</p>
<p class="noindent1-top">In other words, whenever we pick two rows and two columns of a Monge array and consider the four elements at the intersections of the rows and the columns, the sum of the upper-left and lower-right elements is less than or equal to the sum of the lower-left and upper-right elements. For example, the following array is Monge:</p>
<table class="table3a">
<tr>
<td class="td1">10</td>
<td class="td1"><p class="center">17</p></td>
<td class="td1"><p class="center">13</p></td>
<td class="td1"><p class="center">28</p></td>
<td class="td1"><p class="center">23</p></td>
</tr>
<tr>
<td class="td1">17</td>
<td class="td1"><p class="center">22</p></td>
<td class="td1"><p class="center">16</p></td>
<td class="td1"><p class="center">29</p></td>
<td class="td1"><p class="center">23</p></td>
</tr>
<tr>
<td class="td1">24</td>
<td class="td1"><p class="center">28</p></td>
<td class="td1"><p class="center">22</p></td>
<td class="td1"><p class="center">34</p></td>
<td class="td1"><p class="center">24</p></td>
</tr>
<tr>
<td class="td1">11</td>
<td class="td1"><p class="center">13</p></td>
<td class="td1"><p class="center">6</p></td>
<td class="td1"><p class="center">17</p></td>
<td class="td1"><p class="center">7</p></td>
</tr>
<tr>
<td class="td1">45</td>
<td class="td1"><p class="center">44</p></td>
<td class="td1"><p class="center">32</p></td>
<td class="td1"><p class="center">37</p></td>
<td class="td1"><p class="center">23</p></td>
</tr>
<tr>
<td class="td1">36</td>
<td class="td1"><p class="center">33</p></td>
<td class="td1"><p class="center">19</p></td>
<td class="td1"><p class="center">21</p></td>
<td class="td1"><p class="center">6</p></td>
</tr>
<tr>
<td class="td1">75</td>
<td class="td1"><p class="center">66</p></td>
<td class="td1"><p class="center">51</p></td>
<td class="td1"><p class="center">53</p></td>
<td class="td1"><p class="center">34</p></td>
</tr>
</table>
<p class="nl"><strong><em>a.</em></strong> Prove that an array is Monge if and only if for all <em>i</em> = 1, 2, …, <em>m</em> – 1 and <em>j</em> = 1, 2, …, <em>n</em> – 1, we have</p>
<p class="nl-parat"><em>A</em>[<em>i</em>, <em>j</em>] + <em>A</em>[<em>i</em> + 1, <em>j</em> + 1] ≤ <em>A</em>[<em>i</em>, <em>j</em> + 1] + <em>A</em>[<em>i</em> + 1, <em>j</em>].</p>
<p class="nl-para">(<em>Hint:</em> For the “if” part, use induction separately on rows and columns.)</p>
<p class="nl"><strong><em>b.</em></strong> The following array is not Monge. Change one element in order to make it Monge. (<em>Hint:</em> Use part (a).)</p>
<a id="p124"/>
<table class="table3b">
<tr>
<td class="td1">37</td>
<td class="td1"><p class="center">23</p></td>
<td class="td1"><p class="center">22</p></td>
<td class="td1"><p class="center">32</p></td>
</tr>
<tr>
<td class="td1">21</td>
<td class="td1"><p class="center">6</p></td>
<td class="td1"><p class="center">7</p></td>
<td class="td1"><p class="center">10</p></td>
</tr>
<tr>
<td class="td1">53</td>
<td class="td1"><p class="center">34</p></td>
<td class="td1"><p class="center">30</p></td>
<td class="td1"><p class="center">31</p></td>
</tr>
<tr>
<td class="td1">32</td>
<td class="td1"><p class="center">13</p></td>
<td class="td1"><p class="center">9</p></td>
<td class="td1"><p class="center">6</p></td>
</tr>
<tr>
<td class="td1">43</td>
<td class="td1"><p class="center">21</p></td>
<td class="td1"><p class="center">15</p></td>
<td class="td1"><p class="center">8</p></td>
</tr>
</table>
<p class="nl"><strong><em>c.</em></strong> Let <em>f</em> (<em>i</em>) be the index of the column containing the leftmost minimum element of row <em>i</em>. Prove that <em>f</em> (1) ≤ <em>f</em> (2) ≤ <span class="font1">⋯</span> ≤ <em>f</em> (<em>m</em>) for any <em>m</em> × <em>n</em> Monge array.</p>
<p class="nl"><strong><em>d.</em></strong> Here is a description of a divide-and-conquer algorithm that computes the leftmost minimum element in each row of an <em>m</em> × <em>n</em> Monge array <em>A</em>:</p>
<div class="pull-quote">
<p class="pq-noindent">Construct a submatrix <em>A</em><sup>′</sup> of <em>A</em> consisting of the even-numbered rows of <em>A</em>. Recursively determine the leftmost minimum for each row of <em>A</em><sup>′</sup>. Then compute the leftmost minimum in the odd-numbered rows of <em>A</em>.</p>
</div>
<p class="nl-para">Explain how to compute the leftmost minimum in the odd-numbered rows of <em>A</em> (given that the leftmost minimum of the even-numbered rows is known) in <em>O</em>(<em>m</em> + <em>n</em>) time.</p>
<p class="nl"><strong><em>e.</em></strong> Write the recurrence for the running time of the algorithm in part (d). Show that its solution is <em>O</em>(<em>m</em> + <em>n</em> log <em>m</em>).</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-24"><strong>Chapter notes</strong></p>
<p class="noindent">Divide-and-conquer as a technique for designing algorithms dates back at least to 1962 in an article by Karatsuba and Ofman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_242">242</a>], but it might have been used well before then. According to Heideman, Johnson, and Burrus [<a epub:type="noteref" href="bibliography001.xhtml#endnote_211">211</a>], C. F. Gauss devised the first fast Fourier transform algorithm in 1805, and Gauss’s formulation breaks the problem into smaller subproblems whose solutions are combined.</p>
<p>Strassen’s algorithm [<a epub:type="noteref" href="bibliography001.xhtml#endnote_424">424</a>] caused much excitement when it appeared in 1969. Before then, few imagined the possibility of an algorithm asymptotically faster than the basic M<small>ATRIX</small>-M<small>ULTIPLY</small> procedure. Shortly thereafter, S. Winograd reduced the number of submatrix additions from 18 to 15 while still using seven submatrix multiplications. This improvement, which Winograd apparently never published (and which is frequently miscited in the literature), may enhance the practicality of the method, but it does not affect its asymptotic performance. Probert [<a epub:type="noteref" href="bibliography001.xhtml#endnote_368">368</a>] described Winograd’s algorithm and showed that with seven multiplications, 15 additions is the minimum possible.</p>
<p>Strassen’s Θ(<em>n</em><sup>lg 7</sup>) = <em>O</em>(<em>n</em><sup>2.81</sup>) bound for matrix multiplication held until 1987, when Coppersmith and Winograd [<a epub:type="noteref" href="bibliography001.xhtml#endnote_103">103</a>] made a significant advance, improving the <a id="p125"/>bound to <em>O</em>(<em>n</em><sup>2.376</sup>) time with a mathematically sophisticated but wildly impractical algorithm based on tensor products. It took approximately 25 years before the asymptotic upper bound was again improved. In 2012 Vassilevska Williams [<a epub:type="noteref" href="bibliography001.xhtml#endnote_445">445</a>] improved it to <em>O</em>(<em>n</em><sup>2.37287</sup>), and two years later Le Gall [<a epub:type="noteref" href="bibliography001.xhtml#endnote_278">278</a>] achieved <em>O</em>(<em>n</em><sup>2.37286</sup>), both of them using mathematically fascinating but impractical algorithms. The best lower bound to date is just the obvious Ω(<em>n</em><sup>2</sup>) bound (obvious because any algorithm for matrix multiplication must fill in the <em>n</em><sup>2</sup> elements of the product matrix).</p>
<p>The performance of M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> can be improved in practice by coarsening the leaves of the recursion. It also exhibits better cache behavior than M<small>ATRIX</small>-M<small>ULTIPLY</small>, although M<small>ATRIX</small>-M<small>ULTIPLY</small> can be improved by “tiling.” Leiserson et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_293">293</a>] conducted a performance-engineering study of matrix multiplication in which a parallel and vectorized divide-and-conquer algorithm achieved the highest performance. Strassen’s algorithm can be practical for large dense matrices, although large matrices tend to be sparse, and sparse methods can be much faster. When using limited-precision floating-point values, Strassen’s algorithm produces larger numerical errors than the Θ(<em>n</em><sup>3</sup>) algorithms do, although Higham [<a epub:type="noteref" href="bibliography001.xhtml#endnote_215">215</a>] demonstrated that Strassen’s algorithm is amply accurate for some applications.</p>
<p>Recurrences were studied as early as 1202 by Leonardo Bonacci [<a epub:type="noteref" href="bibliography001.xhtml#endnote_66">66</a>], also known as Fibonacci, for whom the Fibonacci numbers are named, although Indian mathematicians had discovered Fibonacci numbers centuries before. The French mathematician De Moivre [<a epub:type="noteref" href="bibliography001.xhtml#endnote_108">108</a>] introduced the method of generating functions with which he studied Fibonacci numbers (see Problem 4-5). Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>] and Liu [<a epub:type="noteref" href="bibliography001.xhtml#endnote_302">302</a>] are good resources for learning the method of generating functions.</p>
<p>Aho, Hopcroft, and Ullman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_5">5</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_6">6</a>] offered one of the first general methods for solving recurrences arising from the analysis of divide-and-conquer algorithms. The master method was adapted from Bentley, Haken, and Saxe [<a epub:type="noteref" href="bibliography001.xhtml#endnote_52">52</a>]. The Akra-Bazzi method is due (unsurprisingly) to Akra and Bazzi [<a epub:type="noteref" href="bibliography001.xhtml#endnote_13">13</a>]. Divide-and-conquer recurrences have been studied by many researchers, including Campbell [<a epub:type="noteref" href="bibliography001.xhtml#endnote_79">79</a>], Graham, Knuth, and Patashnik [<a epub:type="noteref" href="bibliography001.xhtml#endnote_199">199</a>], Kuszmaul and Leiserson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_274">274</a>], Leighton [<a epub:type="noteref" href="bibliography001.xhtml#endnote_287">287</a>], Purdom and Brown [<a epub:type="noteref" href="bibliography001.xhtml#endnote_371">371</a>], Roura [<a epub:type="noteref" href="bibliography001.xhtml#endnote_389">389</a>], Verma [<a epub:type="noteref" href="bibliography001.xhtml#endnote_447">447</a>], and Yap [<a epub:type="noteref" href="bibliography001.xhtml#endnote_462">462</a>].</p>
<p>The issue of floors and ceilings in divide-and-conquer recurrences, including a theorem similar to Theorem 4.5, was studied by Leighton [<a epub:type="noteref" href="bibliography001.xhtml#endnote_287">287</a>]. Leighton proposed a version of the polynomial-growth condition. Campbell [<a epub:type="noteref" href="bibliography001.xhtml#endnote_79">79</a>] removed several limitations in Leighton’s statement of it and showed that there were polynomially bounded functions that do not satisfy Leighton’s condition. Campbell also carefully studied many other technical issues, including the well-definedness of divide-and-conquer recurrences. Kuszmaul and Leiserson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_274">274</a>] provided a proof of Theorem 4.5 that does not involve calculus or other higher math. Both Campbell and Leighton explored the perturbations of arguments beyond simple floors and ceilings.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> This terminology does not mean that either <em>T</em> (<em>n</em>) or <em>f</em> (<em>n</em>) need be continuous, only that the domain of <em>T</em> (<em>n</em>) is the real numbers, as opposed to integers.</p>
</section>
</section>
</div>
</body>
</html>