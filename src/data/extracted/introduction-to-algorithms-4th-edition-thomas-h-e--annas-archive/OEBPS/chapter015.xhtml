<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p417"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="15 Greedy Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-15"><strong><span class="blue1">15        Greedy Algorithms</span></strong></a></p>
<p class="noindent">Algorithms for optimization problems typically go through a sequence of steps, with a set of choices at each step. For many optimization problems, using dynamic programming to determine the best choices is overkill, and simpler, more efficient algorithms will do. A <span class="blue"><strong><em>greedy algorithm</em></strong></span> always makes the choice that looks best at the moment. That is, it makes a locally optimal choice in the hope that this choice leads to a globally optimal solution. This chapter explores optimization problems for which greedy algorithms provide optimal solutions. Before reading this chapter, you should read about dynamic programming in <a href="chapter014.xhtml">Chapter 14</a>, particularly <a href="chapter014.xhtml#Sec_14.3">Section 14.3</a>.</p>
<p>Greedy algorithms do not always yield optimal solutions, but for many problems they do. We first examine, in <a href="chapter015.xhtml#Sec_15.1">Section 15.1</a>, a simple but nontrivial problem, the activity-selection problem, for which a greedy algorithm efficiently computes an optimal solution. We’ll arrive at the greedy algorithm by first considering a dynamic-programming approach and then showing that an optimal solution can result from always making greedy choices. <a href="chapter015.xhtml#Sec_15.2">Section 15.2</a> reviews the basic elements of the greedy approach, giving a direct approach for proving greedy algorithms correct. <a href="chapter015.xhtml#Sec_15.3">Section 15.3</a> presents an important application of greedy techniques: designing data-compression (Huffman) codes. Finally, <a href="chapter015.xhtml#Sec_15.4">Section 15.4</a> shows that in order to decide which blocks to replace when a miss occurs in a cache, the “furthest-in-future” strategy is optimal if the sequence of block accesses is known in advance.</p>
<p>The greedy method is quite powerful and works well for a wide range of problems. Later chapters will present many algorithms that you can view as applications of the greedy method, including minimum-spanning-tree algorithms (<a href="chapter021.xhtml">Chapter 21</a>), Dijkstra’s algorithm for shortest paths from a single source (<a href="chapter022.xhtml#Sec_22.3">Section 22.3</a>), and a greedy set-covering heuristic (<a href="chapter035.xhtml#Sec_35.3">Section 35.3</a>). Minimum-spanning-tree algorithms furnish a classic example of the greedy method. Although you can read this chapter and <a href="chapter021.xhtml">Chapter 21</a> independently of each other, you might find it useful to read them together.</p>
<a id="p418"/>
<p class="line1"/>
<section title="15.1 An activity-selection problem">
<a id="Sec_15.1"/>
<p class="level1" id="h1-88"><a href="toc.xhtml#Rh1-88"><strong>15.1    An activity-selection problem</strong></a></p>
<p class="noindent">Our first example is the problem of scheduling several competing activities that require exclusive use of a common resource, with a goal of selecting a maximum-size set of mutually compatible activities. Imagine that you are in charge of scheduling a conference room. You are presented with a set <em>S</em> = {<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em>} of <em>n</em> proposed <span class="blue"><strong><em>activities</em></strong></span> that wish to reserve the conference room, and the room can serve only one activity at a time. Each activity <em>a<sub>i</sub></em> has a <span class="blue"><strong><em>start time</em></strong></span> <em>s<sub>i</sub></em> and a <span class="blue"><strong><em>finish time</em></strong></span> <em>f<sub>i</sub></em>, where 0 ≤ <em>s<sub>i</sub></em> &lt; <em>f<sub>i</sub></em> &lt; ∞. If selected, activity <em>a<sub>i</sub></em> takes place during the half-open time interval [<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>). Activities <em>a<sub>i</sub></em> and <em>a<sub>j</sub></em> are <span class="blue"><strong><em>compatible</em></strong></span> if the intervals [<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>) and [<em>s<sub>j</sub></em>, <em>f<sub>j</sub></em>) do not overlap. That is, <em>a<sub>i</sub></em> and <em>a<sub>j</sub></em> are compatible if <em>s<sub>i</sub></em> ≥ <em>f<sub>j</sub></em> or <em>s<sub>j</sub></em> ≥ <em>f<sub>i</sub></em>. (Assume that if your staff needs time to change over the room from one activity to the next, the changeover time is built into the intervals.) In the <span class="blue"><strong><em>activity-selection problem</em></strong></span>, your goal is to select a maximum-size subset of mutually compatible activities. Assume that the activities are sorted in monotonically increasing order of finish time:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P485.jpg"/></p>
<p class="noindent">(We’ll see later the advantage that this assumption provides.) For example, consider the set of activities in <a href="chapter015.xhtml#Fig_15-1">Figure 15.1</a>. The subset {<em>a</em><sub>3</sub>, <em>a</em><sub>9</sub>, <em>a</em><sub>11</sub>} consists of mutually compatible activities. It is not a maximum subset, however, since the subset {<em>a</em><sub>1</sub>, <em>a</em><sub>4</sub>, <em>a</em><sub>8</sub>, <em>a</em><sub>11</sub>} is larger. In fact, {<em>a</em><sub>1</sub>, <em>a</em><sub>4</sub>, <em>a</em><sub>8</sub>, <em>a</em><sub>11</sub>} is a largest subset of mutually compatible activities, and another largest subset is {<em>a</em><sub>2</sub>, <em>a</em><sub>4</sub>, <em>a</em><sub>9</sub>, <em>a</em><sub>11</sub>}.</p>
<p>We’ll see how to solve this problem, proceeding in several steps. First we’ll explore a dynamic-programming solution, in which you consider several choices when determining which subproblems to use in an optimal solution. We’ll then observe that you need to consider only one choice—the greedy choice—and that when you make the greedy choice, only one subproblem remains. Based on these observations, we’ll develop a recursive greedy algorithm to solve the activity-selection problem. Finally, we’ll complete the process of developing a greedy solution by converting the recursive algorithm to an iterative one. Although the steps we go through in this section are slightly more involved than is typical when developing a greedy algorithm, they illustrate the relationship between greedy algorithms and dynamic programming.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_15-1"><img alt="art" src="images/Art_P486.jpg"/></p>
<p class="caption"><strong>Figure 15.1</strong> A set {<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a</em><sub>11</sub>} of activities. Activity <em>a<sub>i</sub></em> has start time <em>s<sub>i</sub></em> and finish time <em>f<sub>i</sub></em>.</p>
</div>
<a id="p419"/>
<p class="level4"><strong>The optimal substructure of the activity-selection problem</strong></p>
<p class="noindent">Let’s verify that the activity-selection problem exhibits optimal substructure. Denote by <em>S<sub>ij</sub></em> the set of activities that start after activity <em>a<sub>i</sub></em> finishes and that finish before activity <em>a<sub>j</sub></em> starts. Suppose that you want to find a maximum set of mutually compatible activities in <em>S<sub>ij</sub></em>, and suppose further that such a maximum set is <em>A<sub>ij</sub></em>, which includes some activity <em>a<sub>k</sub></em>. By including <em>a<sub>k</sub></em> in an optimal solution, you are left with two subproblems: finding mutually compatible activities in the set <em>S<sub>ik</sub></em> (activities that start after activity <em>a<sub>i</sub></em> finishes and that finish before activity <em>a<sub>k</sub></em> starts) and finding mutually compatible activities in the set <em>S<sub>kj</sub></em> (activities that start after activity <em>a<sub>k</sub></em> finishes and that finish before activity <em>a<sub>j</sub></em> starts). Let <em>A<sub>ik</sub></em> = <em>A<sub>ij</sub></em> ∩ <em>S<sub>ik</sub></em> and <em>A<sub>kj</sub></em> = <em>A<sub>ij</sub></em> ∩ <em>S<sub>kj</sub></em>, so that <em>A<sub>ik</sub></em> contains the activities in <em>A<sub>ij</sub></em> that finish before <em>a<sub>k</sub></em> starts and <em>A<sub>kj</sub></em> contains the activities in <em>A<sub>ij</sub></em> that start after <em>a<sub>k</sub></em> finishes. Thus, we have <em>A<sub>ij</sub></em> = <em>A<sub>ik</sub></em> ∪ {<em>a<sub>k</sub></em>} ∪ <em>A<sub>kj</sub></em>, and so the maximum-size set <em>A<sub>ij</sub></em> of mutually compatible activities in <em>S<sub>ij</sub></em> consists of |<em>A<sub>ij</sub></em> | = |<em>A<sub>ik</sub></em>| + |<em>A<sub>kj</sub></em> | + 1 activities.</p>
<p>The usual cut-and-paste argument shows that an optimal solution <em>A<sub>ij</sub></em> must also include optimal solutions to the two subproblems for <em>S<sub>ik</sub></em> and <em>S<sub>kj</sub></em>. If you could find a set <img alt="art" src="images/Akj.jpg"/> of mutually compatible activities in <em>S<sub>kj</sub></em> where <img alt="art" src="images/Akj1.jpg"/>, then you could use <img alt="art" src="images/Akj.jpg"/>, rather than <em>A<sub>kj</sub></em>, in a solution to the subproblem for <em>S<sub>ij</sub></em>. You would have constructed a set of <img alt="art" src="images/Akj2.jpg"/> mutually compatible activities, which contradicts the assumption that <em>A<sub>ij</sub></em> is an optimal solution. A symmetric argument applies to the activities in <em>S<sub>ik</sub></em>.</p>
<p>This way of characterizing optimal substructure suggests that you can solve the activity-selection problem by dynamic programming. Let’s denote the size of an optimal solution for the set <em>S<sub>ij</sub></em> by <em>c</em>[<em>i</em>, <em>j</em>]. Then, the dynamic-programming approach gives the recurrence</p>
<p class="eql"><em>c</em>[<em>i</em>, <em>j</em>] = <em>c</em>[<em>i</em>, <em>k</em>] + <em>c</em>[<em>k</em>, <em>j</em>] + 1.</p>
<p class="noindent">Of course, if you do not know that an optimal solution for the set <em>S<sub>ij</sub></em> includes activity <em>a<sub>k</sub></em>, you must examine all activities in <em>S<sub>ij</sub></em> to find which one to choose, so that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P487.jpg"/></p>
<p class="noindent">You can then develop a recursive algorithm and memoize it, or you can work bottom-up and fill in table entries as you go along. But you would be overlooking another important characteristic of the activity-selection problem that you can use to great advantage.</p>
<a id="p420"/>
<p class="level4"><strong>Making the greedy choice</strong></p>
<p class="noindent">What if you could choose an activity to add to an optimal solution without having to first solve all the subproblems? That could save you from having to consider all the choices inherent in recurrence (15.2). In fact, for the activity-selection problem, you need to consider only one choice: the greedy choice.</p>
<p>What is the greedy choice for the activity-selection problem? Intuition suggests that you should choose an activity that leaves the resource available for as many other activities as possible. Of the activities you end up choosing, one of them must be the first one to finish. Intuition says, therefore, choose the activity in <em>S</em> with the earliest finish time, since that leaves the resource available for as many of the activities that follow it as possible. (If more than one activity in <em>S</em> has the earliest finish time, then choose any such activity.) In other words, since the activities are sorted in monotonically increasing order by finish time, the greedy choice is activity <em>a</em><sub>1</sub>. Choosing the first activity to finish is not the only way to think of making a greedy choice for this problem. Exercise 15.1-3 asks you to explore other possibilities.</p>
<p>Once you make the greedy choice, you have only one remaining subproblem to solve: finding activities that start after <em>a</em><sub>1</sub> finishes. Why don’t you have to consider activities that finish before <em>a</em><sub>1</sub> starts? Because <em>s</em><sub>1</sub> &lt; <em>f</em><sub>1</sub>, and because <em>f</em><sub>1</sub> is the earliest finish time of any activity, no activity can have a finish time less than or equal to <em>s</em><sub>1</sub>. Thus, all activities that are compatible with activity <em>a</em><sub>1</sub> must start after <em>a</em><sub>1</sub> finishes.</p>
<p>Furthermore, we have already established that the activity-selection problem exhibits optimal substructure. Let <em>S<sub>k</sub></em> = {<em>a<sub>i</sub></em> ∈ <em>S</em> : <em>s<sub>i</sub></em> ≥ <em>f<sub>k</sub></em>} be the set of activities that start after activity <em>a<sub>k</sub></em> finishes. If you make the greedy choice of activity <em>a</em><sub>1</sub>, then <em>S</em><sub>1</sub> remains as the only subproblem to solve.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> Optimal substructure says that if <em>a</em><sub>1</sub> belongs to an optimal solution, then an optimal solution to the original problem consists of activity <em>a</em><sub>1</sub> and all the activities in an optimal solution to the subproblem <em>S</em><sub>1</sub>.</p>
<p>One big question remains: Is this intuition correct? Is the greedy choice—in which you choose the first activity to finish—always part of some optimal solution? The following theorem shows that it is.</p>
<a id="p421"/>
<p class="theorem"><strong><em>Theorem 15.1</em></strong></p>
<p class="noindent">Consider any nonempty subproblem <em>S<sub>k</sub></em>, and let <em>a<sub>m</sub></em> be an activity in <em>S<sub>k</sub></em> with the earliest finish time. Then <em>a<sub>m</sub></em> is included in some maximum-size subset of mutually compatible activities of <em>S<sub>k</sub></em>.</p>
<p class="proof"><strong><em>Proof</em></strong>   Let <em>A<sub>k</sub></em> be a maximum-size subset of mutually compatible activities in <em>S<sub>k</sub></em>, and let <em>a<sub>j</sub></em> be the activity in <em>A<sub>k</sub></em> with the earliest finish time. If <em>a<sub>j</sub></em> = <em>a<sub>m</sub></em>, we are done, since we have shown that <em>a<sub>m</sub></em> belongs to some maximum-size subset of mutually compatible activities of <em>S<sub>k</sub></em>. If <em>a<sub>j</sub></em> ≠ <em>a<sub>m</sub></em>, let the set <img alt="art" src="images/Art_P488.jpg"/> be <em>A<sub>k</sub></em> but substituting <em>a<sub>m</sub></em> for <em>a<sub>j</sub></em>. The activities in <img alt="art" src="images/Art_P490.jpg"/> are compatible, which follows because the activities in <em>A<sub>k</sub></em> are compatible, <em>a<sub>j</sub></em> is the first activity in <em>A<sub>k</sub></em> to finish, and <em>f<sub>m</sub></em> ≤ <em>f<sub>j</sub></em>. Since <img alt="art" src="images/Art_P491.jpg"/>, we conclude that <img alt="art" src="images/Art_P492.jpg"/> is a maximum-size subset of mutually compatible activities of <em>S<sub>k</sub></em>, and it includes <em>a<sub>m</sub></em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Although you might be able to solve the activity-selection problem with dynamic programming, Theorem 15.1 says that you don’t need to. Instead, you can repeatedly choose the activity that finishes first, keep only the activities compatible with this activity, and repeat until no activities remain. Moreover, because you always choose the activity with the earliest finish time, the finish times of the activities that you choose must strictly increase. You can consider each activity just once overall, in monotonically increasing order of finish times.</p>
<p>An algorithm to solve the activity-selection problem does not need to work bottom-up, like a table-based dynamic-programming algorithm. Instead, it can work top-down, choosing an activity to put into the optimal solution that it constructs and then solving the subproblem of choosing activities from those that are compatible with those already chosen. Greedy algorithms typically have this top-down design: make a choice and then solve a subproblem, rather than the bottom-up technique of solving subproblems before making a choice.</p>
<p class="level4"><strong>A recursive greedy algorithm</strong></p>
<p class="noindent">Now that you know you can bypass the dynamic-programming approach and instead use a top-down, greedy algorithm, let’s see a straightforward, recursive procedure to solve the activity-selection problem. The procedure R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> on the following page takes the start and finish times of the activities, represented as arrays <em>s</em> and <em>f</em>,<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> the index <em>k</em> that defines the subproblem <em>S<sub>k</sub></em> it is to solve, and the size <em>n</em> of the original problem. It returns a maximum-size <a id="p422"/>set of mutually compatible activities in <em>S<sub>k</sub></em>. The procedure assumes that the <em>n</em> input activities are already ordered by monotonically increasing finish time, according to equation (15.1). If not, you can first sort them into this order in <em>O</em>(<em>n</em> lg <em>n</em>) time, breaking ties arbitrarily. In order to start, add the fictitious activity <em>a</em><sub>0</sub> with <em>f</em><sub>0</sub> = 0, so that subproblem <em>S</em><sub>0</sub> is the entire set of activities <em>S</em>. The initial call, which solves the entire problem, is R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, 0, <em>n</em>).</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, <em>k</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>m</em> = <em>k</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><strong>while</strong> <em>m</em> ≤ <em>n</em> and <em>s</em>[<em>m</em>] &lt; <em>f</em> [<em>k</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> find the first activity in <em>S<sub>k</sub></em> to finish</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="p2"><em>m</em> = <em>m</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>if</strong> <em>m</em> ≤ <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong> {<em>a<sub>m</sub></em>} ∪ R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, <em>m</em>, <em>n</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>else return</strong> ∅</p></td>
</tr>
</table>
</div>
<p><a href="chapter015.xhtml#Fig_15-2">Figure 15.2</a> shows how the algorithm operates on the activities in <a href="chapter015.xhtml#Fig_15-1">Figure 15.1</a>. In a given recursive call R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, <em>k</em>, <em>n</em>), the <strong>while</strong> loop of lines 2–3 looks for the first activity in <em>S<sub>k</sub></em> to finish. The loop examines <em>a</em><sub><em>k</em>+1</sub>, <em>a</em><sub><em>k</em>+2</sub>, … , <em>a<sub>n</sub></em>, until it finds the first activity <em>a<sub>m</sub></em> that is compatible with <em>a<sub>k</sub></em>, which means that <em>s<sub>m</sub></em> ≥ <em>f<sub>k</sub></em>. If the loop terminates because it finds such an activity, line 5 returns the union of {<em>a<sub>m</sub></em>} and the maximum-size subset of <em>S<sub>m</sub></em> returned by the recursive call R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, <em>m</em>, <em>n</em>). Alternatively, the loop may terminate because <em>m</em> &gt; <em>n</em>, in which case the procedure has examined all activities in <em>S<sub>k</sub></em> without finding one that is compatible with <em>a<sub>k</sub></em>. In this case, <em>S<sub>k</sub></em> = ∅, and so line 6 returns ∅.</p>
<p>Assuming that the activities have already been sorted by finish times, the running time of the call R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, 0, <em>n</em>) is Θ(<em>n</em>). To see why, observe that over all recursive calls, each activity is examined exactly once in the <strong>while</strong> loop test of line 2. In particular, activity <em>a<sub>i</sub></em> is examined in the last call made in which <em>k</em> &lt; <em>i</em>.</p>
<p class="level4"><strong>An iterative greedy algorithm</strong></p>
<p class="noindent">The recursive procedure can be converted to an iterative one because the procedure R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> is almost “tail recursive” (see Problem 7-5): it ends with a recursive call to itself followed by a union operation. It is usually a straightforward task to transform a tail-recursive procedure to an iterative form. In fact, some compilers for certain programming languages perform this task automatically.</p>
<a id="p423"/>
<div class="divimage">
<p class="fig-imga" id="Fig_15-2"><img alt="art" class="width100" src="images/Art_P493.jpg"/></p>
<p class="caption"><strong>Figure 15.2</strong> The operation of R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> on the 11 activities from <a href="chapter015.xhtml#Fig_15-1">Figure 15.1</a>. Activities considered in each recursive call appear between horizontal lines. The fictitious activity <em>a</em><sub>0</sub> finishes at time 0, and the initial call R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, 0, 11), selects activity <em>a</em><sub>1</sub>. In each recursive call, the activities that have already been selected are blue, and the activity shown in tan is being considered. If the starting time of an activity occurs before the finish time of the most recently added activity (the arrow between them points left), it is rejected. Otherwise (the arrow points directly up or to the right), it is selected. The last recursive call, R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, 11, 11), returns ∅. The resulting set of selected activities is {<em>a</em><sub>1</sub>, <em>a</em><sub>4</sub>, <em>a</em><sub>8</sub>, <em>a</em><sub>11</sub>}.</p>
</div>
<a id="p424"/>
<p>The procedure G<small>REEDY</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> is an iterative version of the procedure R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small>. It, too, assumes that the input activities are ordered by monotonically increasing finish time. It collects selected activities into a set <em>A</em> and returns this set when it is done.</p>
<div class="pull-quote1">
<p class="box-heading">G<small>REEDY</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>A</em> = {<em>a</em><sub>1</sub>}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>k</em> = 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>for</strong> <em>m</em> = 2 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>s</em>[<em>m</em>] ≥ <em>f</em> [<em>k</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> is <em>a<sub>m</sub></em> in <em>S<sub>k</sub></em>?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p3"><em>A</em> = <em>A</em> ∪ {<em>a<sub>m</sub></em>}</p></td>
<td class="td1"><span class="red"><strong>//</strong> yes, so choose it</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p3"><em>k</em> = <em>m</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> and continue from there</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>return</strong> <em>A</em></p></td>
</tr>
</table>
</div>
<p>The procedure works as follows. The variable <em>k</em> indexes the most recent addition to <em>A</em>, corresponding to the activity <em>a<sub>k</sub></em> in the recursive version. Since the procedure considers the activities in order of monotonically increasing finish time, <em>f<sub>k</sub></em> is always the maximum finish time of any activity in <em>A</em>. That is,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P494.jpg"/></p>
<p class="noindent">Lines 1–2 select activity <em>a</em><sub>1</sub>, initialize <em>A</em> to contain just this activity, and initialize <em>k</em> to index this activity. The <strong>for</strong> loop of lines 3–6 finds the earliest activity in <em>S<sub>k</sub></em> to finish. The loop considers each activity <em>a<sub>m</sub></em> in turn and adds <em>a<sub>m</sub></em> to <em>A</em> if it is compatible with all previously selected activities. Such an activity is the earliest in <em>S<sub>k</sub></em> to finish. To see whether activity <em>a<sub>m</sub></em> is compatible with every activity currently in <em>A</em>, it suffices by equation (15.3) to check (in line 4) that its start time <em>s<sub>m</sub></em> is not earlier than the finish time <em>f<sub>k</sub></em> of the activity most recently added to <em>A</em>. If activity <em>a<sub>m</sub></em> is compatible, then lines 5–6 add activity <em>a<sub>m</sub></em> to <em>A</em> and set <em>k</em> to <em>m</em>. The set <em>A</em> returned by the call G<small>REEDY</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>) is precisely the set returned by the initial call R<small>ECURSIVE</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> (<em>s</em>, <em>f</em>, 0, <em>n</em>).</p>
<p>Like the recursive version, G<small>REEDY</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small> schedules a set of <em>n</em> activities in Θ(<em>n</em>) time, assuming that the activities were already sorted initially by their finish times.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>15.1-1</em></strong></p>
<p class="noindent">Give a dynamic-programming algorithm for the activity-selection problem, based on recurrence (15.2). Have your algorithm compute the sizes <em>c</em>[<em>i</em>, <em>j</em>] as defined above and also produce the maximum-size subset of mutually compatible activities. <a id="p425"/>Assume that the inputs have been sorted as in equation (15.1). Compare the running time of your solution to the running time of G<small>REEDY</small>-A<small>CTIVITY</small>-S<small>ELECTOR</small>.</p>
<p class="level3"><strong><em>15.1-2</em></strong></p>
<p class="noindent">Suppose that instead of always selecting the first activity to finish, you instead select the last activity to start that is compatible with all previously selected activities. Describe how this approach is a greedy algorithm, and prove that it yields an optimal solution.</p>
<p class="level3"><strong><em>15.1-3</em></strong></p>
<p class="noindent">Not just any greedy approach to the activity-selection problem produces a maximum-size set of mutually compatible activities. Give an example to show that the approach of selecting the activity of least duration from among those that are compatible with previously selected activities does not work. Do the same for the approaches of always selecting the compatible activity that overlaps the fewest other remaining activities and always selecting the compatible remaining activity with the earliest start time.</p>
<p class="level3"><strong><em>15.1-4</em></strong></p>
<p class="noindent">You are given a set of activities to schedule among a large number of lecture halls, where any activity can take place in any lecture hall. You wish to schedule all the activities using as few lecture halls as possible. Give an efficient greedy algorithm to determine which activity should use which lecture hall.</p>
<p>(This problem is also known as the <span class="blue"><strong><em>interval-graph coloring problem</em></strong></span>. It is modeled by an interval graph whose vertices are the given activities and whose edges connect incompatible activities. The smallest number of colors required to color every vertex so that no two adjacent vertices have the same color corresponds to finding the fewest lecture halls needed to schedule all of the given activities.)</p>
<p class="level3"><strong><em>15.1-5</em></strong></p>
<p class="noindent">Consider a modification to the activity-selection problem in which each activity <em>a<sub>i</sub></em> has, in addition to a start and finish time, a value <em>v<sub>i</sub></em>. The objective is no longer to maximize the number of activities scheduled, but instead to maximize the total value of the activities scheduled. That is, the goal is to choose a set <em>A</em> of compatible activities such that <img alt="art" src="images/Art_P495.jpg"/> is maximized. Give a polynomial-time algorithm for this problem.</p>
<a id="p426"/>
</section>
<p class="line1"/>
<section title="15.2 Elements of the greedy strategy">
<a id="Sec_15.2"/>
<p class="level1" id="h1-89"><a href="toc.xhtml#Rh1-89"><strong>15.2    Elements of the greedy strategy</strong></a></p>
<p class="noindent">A greedy algorithm obtains an optimal solution to a problem by making a sequence of choices. At each decision point, the algorithm makes the choice that seems best at the moment. This heuristic strategy does not always produce an optimal solution, but as in the activity-selection problem, sometimes it does. This section discusses some of the general properties of greedy methods.</p>
<p>The process that we followed in <a href="chapter015.xhtml#Sec_15.1">Section 15.1</a> to develop a greedy algorithm was a bit more involved than is typical. It consisted of the following steps:</p>
<ol class="olnoindent" epub:type="list">
<li>Determine the optimal substructure of the problem.</li>
<li class="litop">Develop a recursive solution. (For the activity-selection problem, we formulated recurrence (15.2), but bypassed developing a recursive algorithm based solely on this recurrence.)</li>
<li class="litop">Show that if you make the greedy choice, then only one subproblem remains.</li>
<li class="litop">Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur in either order.)</li>
<li class="litop">Develop a recursive algorithm that implements the greedy strategy.</li>
<li class="litop">Convert the recursive algorithm to an iterative algorithm.</li></ol>
<p class="noindent">These steps highlighted in great detail the dynamic-programming underpinnings of a greedy algorithm. For example, the first cut at the activity-selection problem defined the subproblems <em>S<sub>ij</sub></em>, where both <em>i</em> and <em>j</em> varied. We then found that if you always make the greedy choice, you can restrict the subproblems to be of the form <em>S<sub>k</sub></em>.</p>
<p>An alternative approach is to fashion optimal substructure with a greedy choice in mind, so that the choice leaves just one subproblem to solve. In the activity-selection problem, start by dropping the second subscript and defining subproblems of the form <em>S<sub>k</sub></em>. Then prove that a greedy choice (the first activity <em>a<sub>m</sub></em> to finish in <em>S<sub>k</sub></em>), combined with an optimal solution to the remaining set <em>S<sub>m</sub></em> of compatible activities, yields an optimal solution to <em>S<sub>k</sub></em>. More generally, you can design greedy algorithms according to the following sequence of steps:</p>
<ol class="olnoindent" epub:type="list">
<li>Cast the optimization problem as one in which you make a choice and are left with one subproblem to solve.</li>
<li class="litop">Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.</li>
<li class="litop">Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if you combine an <a id="p427"/>optimal solution to the subproblem with the greedy choice you have made, you arrive at an optimal solution to the original problem.</li></ol>
<p class="noindent">Later sections of this chapter will use this more direct process. Nevertheless, beneath every greedy algorithm, there is almost always a more cumbersome dynamic-programming solution.</p>
<p>How can you tell whether a greedy algorithm will solve a particular optimization problem? No way works all the time, but the greedy-choice property and optimal substructure are the two key ingredients. If you can demonstrate that the problem has these properties, then you are well on the way to developing a greedy algorithm for it.</p>
<p class="level4"><strong>Greedy-choice property</strong></p>
<p class="noindent">The first key ingredient is the <span class="blue"><strong><em>greedy-choice property</em></strong></span>: you can assemble a globally optimal solution by making locally optimal (greedy) choices. In other words, when you are considering which choice to make, you make the choice that looks best in the current problem, without considering results from subproblems.</p>
<p>Here is where greedy algorithms differ from dynamic programming. In dynamic programming, you make a choice at each step, but the choice usually depends on the solutions to subproblems. Consequently, you typically solve dynamic-programming problems in a bottom-up manner, progressing from smaller subproblems to larger subproblems. (Alternatively, you can solve them top down, but memoizing. Of course, even though the code works top down, you still must solve the subproblems before making a choice.) In a greedy algorithm, you make whatever choice seems best at the moment and then solve the subproblem that remains. The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems. Thus, unlike dynamic programming, which solves the subproblems before making the first choice, a greedy algorithm makes its first choice before solving any subproblems. A dynamic-programming algorithm proceeds bottom up, whereas a greedy strategy usually progresses top down, making one greedy choice after another, reducing each given problem instance to a smaller one.</p>
<p>Of course, you need to prove that a greedy choice at each step yields a globally optimal solution. Typically, as in the case of Theorem 15.1, the proof examines a globally optimal solution to some subproblem. It then shows how to modify the solution to substitute the greedy choice for some other choice, resulting in one similar, but smaller, subproblem.</p>
<p>You can usually make the greedy choice more efficiently than when you have to consider a wider set of choices. For example, in the activity-selection problem, assuming that the activities were already sorted in monotonically increasing order by finish times, each activity needed to be examined just once. By preprocessing <a id="p428"/>the input or by using an appropriate data structure (often a priority queue), you often can make greedy choices quickly, thus yielding an efficient algorithm.</p>
<p class="level4"><strong>Optimal substructure</strong></p>
<p class="noindent">As we saw in <a href="chapter014.xhtml">Chapter 14</a>, a problem exhibits <span class="blue"><strong><em>optimal substructure</em></strong></span> if an optimal solution to the problem contains within it optimal solutions to subproblems. This property is a key ingredient of assessing whether dynamic programming applies, and it’s also essential for greedy algorithms. As an example of optimal substructure, recall how <a href="chapter015.xhtml#Sec_15.1">Section 15.1</a> demonstrated that if an optimal solution to subproblem <em>S<sub>ij</sub></em> includes an activity <em>a<sub>k</sub></em>, then it must also contain optimal solutions to the subproblems <em>S<sub>ik</sub></em> and <em>S<sub>kj</sub></em>. Given this optimal substructure, we argued that if you know which activity to use as <em>a<sub>k</sub></em>, you can construct an optimal solution to <em>S<sub>ij</sub></em> by selecting <em>a<sub>k</sub></em> along with all activities in optimal solutions to the subproblems <em>S<sub>ik</sub></em> and <em>S<sub>kj</sub></em>. This observation of optimal substructure gave rise to the recurrence (15.2) that describes the value of an optimal solution.</p>
<p>You will usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms. As mentioned above, you have the luxury of assuming that you arrived at a subproblem by having made the greedy choice in the original problem. All you really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem. This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step produces an optimal solution.</p>
<p class="level4"><strong>Greedy versus dynamic programming</strong></p>
<p class="noindent">Because both the greedy and dynamic-programming strategies exploit optimal substructure, you might be tempted to generate a dynamic-programming solution to a problem when a greedy solution suffices or, conversely, you might mistakenly think that a greedy solution works when in fact a dynamic-programming solution is required. To illustrate the subtle differences between the two techniques, let’s investigate two variants of a classical optimization problem.</p>
<p>The <span class="blue"><strong><em>0-1 knapsack problem</em></strong></span> is the following. A thief robbing a store wants to take the most valuable load that can be carried in a knapsack capable of carrying at most <em>W</em> pounds of loot. The thief can choose to take any subset of <em>n</em> items in the store. The <em>i</em>th item is worth <em>v<sub>i</sub></em> dollars and weighs <em>w<sub>i</sub></em> pounds, where <em>v<sub>i</sub></em> and <em>w<sub>i</sub></em> are integers. Which items should the thief take? (We call this the 0-1 knapsack problem because for each item, the thief must either take it or leave it behind. The thief cannot take a fractional amount of an item or take an item more than once.)</p>
<a id="p429"/>
<p>In the <span class="blue"><strong><em>fractional knapsack problem</em></strong></span>, the setup is the same, but the thief can take fractions of items, rather than having to make a binary (0-1) choice for each item. You can think of an item in the 0-1 knapsack problem as being like a gold ingot and an item in the fractional knapsack problem as more like gold dust.</p>
<p>Both knapsack problems exhibit the optimal-substructure property. For the 0-1 problem, if the most valuable load weighing at most <em>W</em> pounds includes item <em>j</em>, then the remaining load must be the most valuable load weighing at most <em>W</em> − <em>w<sub>j</sub></em> pounds that the thief can take from the <em>n</em> − 1 original items excluding item <em>j</em>. For the comparable fractional problem, if if the most valuable load weighing at most <em>W</em> pounds includes weight <em>w</em> of item <em>j</em>, then the remaining load must be the most valuable load weighing at most <em>W</em> − <em>w</em> pounds that the thief can take from the <em>n</em> − 1 original items plus <em>w<sub>j</sub></em> − <em>w</em> pounds of item <em>j</em>.</p>
<p>Although the problems are similar, a greedy strategy works to solve the fractional knapsack problem, but not the 0-1 problem. To solve the fractional problem, first compute the value per pound <em>v<sub>i</sub></em>/<em>w<sub>i</sub></em> for each item. Obeying a greedy strategy, the thief begins by taking as much as possible of the item with the greatest value per pound. If the supply of that item is exhausted and the thief can still carry more, then the thief takes as much as possible of the item with the next greatest value per pound, and so forth, until reaching the weight limit <em>W</em>. Thus, by sorting the items by value per pound, the greedy algorithm runs in <em>O</em>(<em>n</em> lg <em>n</em>) time. You are asked to prove that the fractional knapsack problem has the greedy-choice property in Exercise 15.2-1.</p>
<p>To see that this greedy strategy does not work for the 0-1 knapsack problem, consider the problem instance illustrated in <a href="chapter015.xhtml#Fig_15-3">Figure 15.3(a)</a>. This example has three items and a knapsack that can hold 50 pounds. Item 1 weighs 10 pounds and is worth $60. Item 2 weighs 20 pounds and is worth $100. Item 3 weighs 30 pounds and is worth $120. Thus, the value per pound of item 1 is $6 per pound, which is greater than the value per pound of either item 2 ($5 per pound) or item 3 ($4 per pound). The greedy strategy, therefore, would take item 1 first. As you can see from the case analysis in <a href="chapter015.xhtml#Fig_15-3">Figure 15.3(b)</a>, however, the optimal solution takes items 2 and 3, leaving item 1 behind. The two possible solutions that take item 1 are both suboptimal.</p>
<p>For the comparable fractional problem, however, the greedy strategy, which takes item 1 first, does yield an optimal solution, as shown in <a href="chapter015.xhtml#Fig_15-3">Figure 15.3(c)</a>. Taking item 1 doesn’t work in the 0-1 problem, because the thief is unable to fill the knapsack to capacity, and the empty space lowers the effective value per pound of the load. In the 0-1 problem, when you consider whether to include an item in the knapsack, you must compare the solution to the subproblem that includes the item with the solution to the subproblem that excludes the item before you can make the choice. The problem formulated in this way gives rise to many overlapping subproblems—a hallmark of dynamic programming, and indeed, as Exercise 15.2-2 asks you to show, you can use dynamic programming to solve the 0-1 problem.</p>
<a id="p430"/>
<div class="divimage">
<p class="fig-imga" id="Fig_15-3"><img alt="art" class="width100" src="images/Art_P496.jpg"/></p>
<p class="caption"><strong>Figure 15.3</strong> An example showing that the greedy strategy does not work for the 0-1 knapsack problem. <strong>(a)</strong> The thief must select a subset of the three items shown whose weight must not exceed 50 pounds. <strong>(b)</strong> The optimal subset includes items 2 and 3. Any solution with item 1 is suboptimal, even though item 1 has the greatest value per pound. <strong>(c)</strong> For the fractional knapsack problem, taking the items in order of greatest value per pound yields an optimal solution.</p>
</div>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>15.2-1</em></strong></p>
<p class="noindent">Prove that the fractional knapsack problem has the greedy-choice property.</p>
<p class="level3"><strong><em>15.2-2</em></strong></p>
<p class="noindent">Give a dynamic-programming solution to the 0-1 knapsack problem that runs in <em>O</em>(<em>n W</em>) time, where <em>n</em> is the number of items and <em>W</em> is the maximum weight of items that the thief can put in the knapsack.</p>
<p class="level3"><strong><em>15.2-3</em></strong></p>
<p class="noindent">Suppose that in a 0-1 knapsack problem, the order of the items when sorted by increasing weight is the same as their order when sorted by decreasing value. Give an efficient algorithm to find an optimal solution to this variant of the knapsack problem, and argue that your algorithm is correct.</p>
<p class="level3"><strong><em>15.2-4</em></strong></p>
<p class="noindent">Professor Gekko has always dreamed of inline skating across North Dakota. The professor plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the eastern border with Minnesota, to Williston, near the western border with Montana. The professor can carry two liters of water and can skate <em>m</em> miles before <a id="p431"/>running out of water. (Because North Dakota is relatively flat, the professor does not have to worry about drinking water at a greater rate on uphill sections than on flat or downhill sections.) The professor will start in Grand Forks with two full liters of water. The professor has an official North Dakota state map, which shows all the places along U.S. 2 to refill water and the distances between these locations.</p>
<p>The professor’s goal is to minimize the number of water stops along the route across the state. Give an efficient method by which the professor can determine which water stops to make. Prove that your strategy yields an optimal solution, and give its running time.</p>
<p class="level3"><strong><em>15.2-5</em></strong></p>
<p class="noindent">Describe an efficient algorithm that, given a set {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>} of points on the real line, determines the smallest set of unit-length closed intervals that contains all of the given points. Argue that your algorithm is correct.</p>
<p class="level3"><span class="font1">★</span> <strong><em>15.2-6</em></strong></p>
<p class="noindent">Show how to solve the fractional knapsack problem in <em>O</em>(<em>n</em>) time.</p>
<p class="level3"><strong><em>15.2-7</em></strong></p>
<p class="noindent">You are given two sets <em>A</em> and <em>B</em>, each containing <em>n</em> positive integers. You can choose to reorder each set however you like. After reordering, let <em>a<sub>i</sub></em> be the <em>i</em>th element of set <em>A</em>, and let <em>b<sub>i</sub></em> be the <em>i</em>th element of set <em>B</em>. You then receive a payoff of <img alt="art" src="images/Art_P497.jpg"/>. Give an algorithm that maximizes your payoff. Prove that your algorithm maximizes the payoff, and state its running time, omitting the time for reordering the sets.</p>
</section>
<p class="line1"/>
<section title="15.3 Huffman codes">
<a id="Sec_15.3"/>
<p class="level1" id="h1-90"><a href="toc.xhtml#Rh1-90"><strong>15.3    Huffman codes</strong></a></p>
<p class="noindent">Huffman codes compress data well: savings of 20% to 90% are typical, depending on the characteristics of the data being compressed. The data arrive as a sequence of characters. Huffman’s greedy algorithm uses a table giving how often each character occurs (its frequency) to build up an optimal way of representing each character as a binary string.</p>
<p>Suppose that you have a 100,000-character data file that you wish to store compactly and you know that the 6 distinct characters in the file occur with the frequencies given by <a href="chapter015.xhtml#Fig_15-4">Figure 15.4</a>. The character <span class="courierfont">a</span> occurs 45,000 times, the character <span class="courierfont">b</span> occurs 13,000 times, and so on.</p>
<p>You have many options for how to represent such a file of information. Here, we consider the problem of designing a <span class="blue"><strong><em>binary character code</em></strong></span> (or <span class="blue"><strong><em>code</em></strong></span> for short) <a id="p432"/>in which each character is represented by a unique binary string, which we call a <span class="blue"><strong><em>codeword</em></strong></span>. If you use a <span class="blue"><strong><em>fixed-length code</em></strong></span>, you need <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> bits to represent <em>n</em> ≥ 2 characters. For 6 characters, therefore, you need 3 bits: <span class="courierfont">a</span> = 000, <span class="courierfont">b</span> = 001, <span class="courierfont">c</span> = 010, <span class="courierfont">d</span> = 011, <span class="courierfont">e</span> = 100, and <span class="courierfont">f</span> = 101. This method requires 300,000 bits to encode the entire file. Can you do better?</p>
<div class="divimage">
<p class="fig-imga" id="Fig_15-4"><img alt="art" src="images/Art_P498.jpg"/></p>
<p class="caption"><strong>Figure 15.4</strong> A character-coding problem. A data file of 100,000 characters contains only the characters <span class="courierfont">a</span>–<span class="courierfont">f</span>, with the frequencies indicated. With each character represented by a 3-bit codeword, encoding the file requires 300,000 bits. With the variable-length code shown, the encoding requires only 224,000 bits.</p>
</div>
<p>A <span class="blue"><strong><em>variable-length code</em></strong></span> can do considerably better than a fixed-length code. The idea is simple: give frequent characters short codewords and infrequent characters long codewords. <a href="chapter015.xhtml#Fig_15-4">Figure 15.4</a> shows such a code. Here, the 1-bit string 0 represents <span class="courierfont">a</span>, and the 4-bit string 1100 represents <span class="courierfont">f</span>. This code requires</p>
<p class="eql">(45 · 1 + 13 · 3 + 12 · 3 + 16 · 3 + 9 · 4 + 5 · 4) · 1,000 = 224,000 bits</p>
<p class="noindent">to represent the file, a savings of approximately 25%. In fact, this is an optimal character code for this file, as we shall see.</p>
<p class="level4"><strong>Prefix-free codes</strong></p>
<p class="noindent">We consider here only codes in which no codeword is also a prefix of some other codeword. Such codes are called <span class="blue"><strong><em>prefix-free codes</em></strong></span>. Although we won’t prove it here, a prefix-free code can always achieve the optimal data compression among any character code, and so we suffer no loss of generality by restricting our attention to prefix-free codes.</p>
<p>Encoding is always simple for any binary character code: just concatenate the codewords representing each character of the file. For example, with the variable-length prefix-free code of <a href="chapter015.xhtml#Fig_15-4">Figure 15.4</a>, the 4-character file <span class="courierfont">face</span> has the encoding 1100 · 0 · 100 · 1101 = 110001001101, where “·” denotes concatenation.</p>
<p>Prefix-free codes are desirable because they simplify decoding. Since no codeword is a prefix of any other, the codeword that begins an encoded file is unambiguous. You can simply identify the initial codeword, translate it back to the original character, and repeat the decoding process on the remainder of the encoded file. In our example, the string 100011001101 parses uniquely as 100 · 0 · 1100 · 1101, which decodes to <span class="courierfont">cafe</span>.</p>
<a id="p433"/>
<div class="divimage">
<p class="fig-imga" id="Fig_15-5"><img alt="art" src="images/Art_P499.jpg"/></p>
<p class="caption"><strong>Figure 15.5</strong> Trees corresponding to the coding schemes in <a href="chapter015.xhtml#Fig_15-4">Figure 15.4</a>. Each leaf is labeled with a character and its frequency of occurrence. Each internal node is labeled with the sum of the frequencies of the leaves in its subtree. All frequencies are in thousands. <strong>(a)</strong> The tree corresponding to the fixed-length code <span class="courierfont">a</span> = 000, <span class="courierfont">b</span> = 001, <span class="courierfont">c</span> = 010, <span class="courierfont">d</span> = 011, <span class="courierfont">e</span> = 100, <span class="courierfont">f</span> = 101. <strong>(b)</strong> The tree corresponding to the optimal prefix-free code <span class="courierfont">a</span> = 0, <span class="courierfont">b</span> = 101, <span class="courierfont">c</span> = 100, <span class="courierfont">d</span> = 111, <span class="courierfont">e</span> = 1101, <span class="courierfont">f</span> = 1100.</p>
</div>
<p>The decoding process needs a convenient representation for the prefix-free code so that you can easily pick off the initial codeword. A binary tree whose leaves are the given characters provides one such representation. Interpret the binary codeword for a character as the simple path from the root to that character, where 0 means “go to the left child” and 1 means “go to the right child.” <a href="chapter015.xhtml#Fig_15-5">Figure 15.5</a> shows the trees for the two codes of our example. Note that these are not binary search trees, since the leaves need not appear in sorted order and internal nodes do not contain character keys.</p>
<p>An optimal code for a file is always represented by a <em>full</em> binary tree, in which every nonleaf node has two children (see Exercise 15.3-2). The fixed-length code in our example is not optimal since its tree, shown in <a href="chapter015.xhtml#Fig_15-5">Figure 15.5(a)</a>, is not a full binary tree: it contains codewords beginning with 10, but none beginning with 11. Since we can now restrict our attention to full binary trees, we can say that if <em>C</em> is the alphabet from which the characters are drawn and all character frequencies are positive, then the tree for an optimal prefix-free code has exactly |<em>C</em> | leaves, one for each letter of the alphabet, and exactly |<em>C</em> | − 1 internal nodes (see Exercise B.5-3 on page 1175).</p>
<p>Given a tree <em>T</em> corresponding to a prefix-free code, we can compute the number of bits required to encode a file. For each character <em>c</em> in the alphabet <em>C</em>, let the attribute <em>c</em>.<em>freq</em> denote the frequency of <em>c</em> in the file and let <em>d<sub>T</sub></em>(<em>c</em>) denote the depth of <em>c</em>’s leaf in the tree. Note that <em>d<sub>T</sub></em> (<em>c</em>) is also the length of the codeword for character <em>c</em>. The number of bits required to encode a file is thus</p>
<a id="p434"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P500.jpg"/></p>
<p class="noindent">which we define as the <span class="blue"><strong><em>cost</em></strong></span> of the tree <em>T</em>.</p>
<p class="level4"><strong>Constructing a Huffman code</strong></p>
<p class="noindent">Huffman invented a greedy algorithm that constructs an optimal prefix-free code, called a <span class="blue"><strong><em>Huffman code</em></strong></span> in his honor. In line with our observations in <a href="chapter015.xhtml#Sec_15.2">Section 15.2</a>, its proof of correctness relies on the greedy-choice property and optimal substructure. Rather than demonstrating that these properties hold and then developing pseudocode, we present the pseudocode first. Doing so will help clarify how the algorithm makes greedy choices.</p>
<p>The procedure H<small>UFFMAN</small> assumes that <em>C</em> is a set of <em>n</em> characters and that each character <em>c</em> ∈ <em>C</em> is an object with an attribute <em>c</em>.<em>freq</em> giving its frequency. The algorithm builds the tree <em>T</em> corresponding to an optimal code in a bottom-up manner. It begins with a set of |<em>C</em> | leaves and performs a sequence of |<em>C</em> | − 1 “merging” operations to create the final tree. The algorithm uses a min-priority queue <em>Q</em>, keyed on the <em>freq</em> attribute, to identify the two least-frequent objects to merge together. The result of merging two objects is a new object whose frequency is the sum of the frequencies of the two objects that were merged.</p>
<div class="pull-quote1">
<p class="box-heading">H<small>UFFMAN</small>(<em>C</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent"><em>n</em> = |<em>C</em> |</p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="noindent"><em>Q</em> = <em>C</em></p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> − 1</p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="p2">allocate a new node <em>z</em></p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2"><em>x</em> = E<small>XTRACT</small>-M<small>IN</small>(<em>Q</em>)</p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="p2"><em>y</em> = E<small>XTRACT</small>-M<small>IN</small>(<em>Q</em>)</p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1"><p class="p2"><em>z</em>.<em>left</em> = <em>x</em></p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="p2"><em>z</em>.<em>right</em> = <em>y</em></p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1"><p class="p2"><em>z</em>.<em>freq</em> = <em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em></p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><p class="p2">I<small>NSERT</small>(<em>Q</em>, <em>z</em>)</p></td>
<td class="td1"> </td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> E<small>XTRACT</small>-M<small>IN</small>(<em>Q</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> the root of the tree is the only node left</span></td>
</tr>
</table>
</div>
<p class="space-break">For our example, Huffman’s algorithm proceeds as shown in <a href="chapter015.xhtml#Fig_15-6">Figure 15.6</a>. Since the alphabet contains 6 letters, the initial queue size is <em>n</em> = 6, and 5 merge steps build the tree. The final tree represents the optimal prefix-free code. The codeword for a letter is the sequence of edge labels on the simple path from the root to the letter.</p>
<a id="p435"/>
<div class="divimage">
<p class="fig-imga" id="Fig_15-6"><img alt="art" class="width100" src="images/Art_P501.jpg"/></p>
<p class="caption"><strong>Figure 15.6</strong> The steps of Huffman’s algorithm for the frequencies given in <a href="chapter015.xhtml#Fig_15-4">Figure 15.4</a>. Each part shows the contents of the queue sorted into increasing order by frequency. Each step merges the two trees with the lowest frequencies. Leaves are shown as rectangles containing a character and its frequency. Internal nodes are shown as circles containing the sum of the frequencies of their children. An edge connecting an internal node with its children is labeled 0 if it is an edge to a left child and 1 if it is an edge to a right child. The codeword for a letter is the sequence of labels on the edges connecting the root to the leaf for that letter. <strong>(a)</strong> The initial set of <em>n</em> = 6 nodes, one for each letter. <strong>(b)–(e)</strong> Intermediate stages. <strong>(f)</strong> The final tree.</p>
</div>
<p>The H<small>UFFMAN</small> procedure works as follows. Line 2 initializes the min-priority queue <em>Q</em> with the characters in <em>C</em>. The <strong>for</strong> loop in lines 3–10 repeatedly extracts the two nodes <em>x</em> and <em>y</em> of lowest frequency from the queue and replaces them in the queue with a new node <em>z</em> representing their merger. The frequency of <em>z</em> is computed as the sum of the frequencies of <em>x</em> and <em>y</em> in line 9. The node <em>z</em> has <em>x</em> as its left child and <em>y</em> as its right child. (This order is arbitrary. Switching the left and right child of any node yields a different code of the same cost.) After <em>n</em> − 1 mergers, line 11 returns the one node left in the queue, which is the root of the code tree.</p>
<a id="p436"/>
<p>The algorithm produces the same result without the variables <em>x</em> and <em>y</em>, assigning the values returned by the E<small>XTRACT</small>-M<small>IN</small> calls directly to <em>z</em>.<em>left</em> and <em>z</em>.<em>right</em> in lines 7 and 8, and changing line 9 to <em>z</em>.<em>freq</em> = <em>z</em>.<em>left</em>.<em>freq</em>+<em>z</em>.<em>right</em>.<em>freq</em>. We’ll use the node names <em>x</em> and <em>y</em> in the proof of correctness, however, so we leave them in.</p>
<p>The running time of Huffman’s algorithm depends on how the min-priority queue <em>Q</em> is implemented. Let’s assume that it’s implemented as a binary min-heap (see <a href="chapter006.xhtml">Chapter 6</a>). For a set <em>C</em> of <em>n</em> characters, the B<small>UILD</small>-M<small>IN</small>-H<small>EAP</small> procedure discussed in <a href="chapter006.xhtml#Sec_6.3">Section 6.3</a> can initialize <em>Q</em> in line 2 in <em>O</em>(<em>n</em>) time. The <strong>for</strong> loop in lines 3–10 executes exactly <em>n</em> − 1 times, and since each heap operation runs in <em>O</em>(lg <em>n</em>) time, the loop contributes <em>O</em>(<em>n</em> lg <em>n</em>) to the running time. Thus, the total running time of H<small>UFFMAN</small> on a set of <em>n</em> characters is <em>O</em>(<em>n</em> lg <em>n</em>).</p>
<p class="level4"><strong>Correctness of Huffman’s algorithm</strong></p>
<p class="noindent">To prove that the greedy algorithm H<small>UFFMAN</small> is correct, we’ll show that the problem of determining an optimal prefix-free code exhibits the greedy-choice and optimal-substructure properties. The next lemma shows that the greedy-choice property holds.</p>
<p class="lemma"><strong><em>Lemma 15.2 (Optimal prefix-free codes have the greedy-choice property)</em></strong></p>
<p class="noindent">Let <em>C</em> be an alphabet in which each character <em>c</em> ∈ <em>C</em> has frequency <em>c</em>.<em>freq</em>. Let <em>x</em> and <em>y</em> be two characters in <em>C</em> having the lowest frequencies. Then there exists an optimal prefix-free code for <em>C</em> in which the codewords for <em>x</em> and <em>y</em> have the same length and differ only in the last bit.</p>
<p class="proof"><strong><em>Proof</em></strong>   The idea of the proof is to take the tree <em>T</em> representing an arbitrary optimal prefix-free code and modify it to make a tree representing another optimal prefix-free code such that the characters <em>x</em> and <em>y</em> appear as sibling leaves of maximum depth in the new tree. In such a tree, the codewords for <em>x</em> and <em>y</em> have the same length and differ only in the last bit.</p>
<p>Let <em>a</em> and <em>b</em> be any two characters that are sibling leaves of maximum depth in <em>T</em>. Without loss of generality, assume that <em>a</em>.<em>freq</em> ≤ <em>b</em>.<em>freq</em> and <em>x</em>.<em>freq</em> ≤ <em>y</em>.<em>freq</em>. Since <em>x</em>.<em>freq</em> and <em>y</em>.<em>freq</em> are the two lowest leaf frequencies, in order, and <em>a</em>.<em>freq</em> and <em>b</em>.<em>freq</em> are two arbitrary frequencies, in order, we have <em>x</em>.<em>freq</em> ≤ <em>a</em>.<em>freq</em> and <em>y</em>.<em>freq</em> ≤ <em>b</em>.<em>freq</em>.</p>
<p>In the remainder of the proof, it is possible that we could have <em>x</em>.<em>freq</em> = <em>a</em>.<em>freq</em> or <em>y</em>.<em>freq</em> = <em>b</em>.<em>freq</em>, but <em>x</em>.<em>freq</em> = <em>b</em>.<em>freq</em> implies that <em>a</em>.<em>freq</em> = <em>b</em>.<em>freq</em> = <em>x</em>.<em>freq</em> = <em>y</em>.<em>freq</em> (see Exercise 15.3-1), and the lemma would be trivially true. Therefore, assume that <em>x.freq</em> ≠ <em>b.freq</em>, which means that <em>x</em> ≠ <em>b</em>.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_15-7"><img alt="art" class="width100" src="images/Art_P502.jpg"/></p>
<p class="caption"><strong>Figure 15.7</strong> An illustration of the key step in the proof of Lemma 15.2. In the optimal tree <em>T</em>, leaves <em>a</em> and <em>b</em> are two siblings of maximum depth. Leaves <em>x</em> and <em>y</em> are the two characters with the lowest frequencies. They appear in arbitrary positions in <em>T</em>. Assuming that <em>x</em> ≠ <em>b</em>, swapping leaves <em>a</em> and <em>x</em> produces tree <em>T</em>′, and then swapping leaves <em>b</em> and <em>y</em> produces tree <em>T</em> ″. Since each swap does not increase the cost, the resulting tree <em>T</em> ″ is also an optimal tree.</p>
</div>
<p>As <a href="chapter015.xhtml#Fig_15-7">Figure 15.7</a> shows, imagine exchanging the positions in <em>T</em> of <em>a</em> and <em>x</em> to produce a tree <em>T</em>′, and then exchanging the positions in <em>T</em>′ of <em>b</em> and <em>y</em> to produce a <a id="p437"/>tree <em>T</em>″ in which <em>x</em> and <em>y</em> are sibling leaves of maximum depth. (Note that if <em>x</em> = <em>b</em> but <em>y</em> ≠ <em>a</em>, then tree <em>T</em> ″ does not have <em>x</em> and <em>y</em> as sibling leaves of maximum depth. Because we assume that <em>x</em> ≠ <em>b</em>, this situation cannot occur.) By equation (15.4), the difference in cost between <em>T</em> and <em>T</em>′ is</p>
<p class="eql"><img alt="art" src="images/Art_P503.jpg"/></p>
<p class="noindent">because both <em>a</em>.<em>freq</em> − <em>x</em>.<em>freq</em> and <em>d<sub>T</sub></em> (<em>a</em>) − <em>d<sub>T</sub></em> (<em>x</em>) are nonnegative. More specifically, <em>a.freq</em> − <em>x</em>.<em>freq</em> is nonnegative because <em>x</em> is a minimum-frequency leaf, and <em>d<sub>T</sub></em> (<em>a</em>) − <em>d<sub>T</sub></em> (<em>x</em>) is nonnegative because <em>a</em> is a leaf of maximum depth in <em>T</em>. Similarly, exchanging <em>y</em> and <em>b</em> does not increase the cost, and so <em>B</em>(<em>T</em>′) − <em>B</em>(<em>T</em> ″) is nonnegative. Therefore, <em>B</em>(<em>T</em> ″) ≤ <em>B</em>(<em>T</em>′) ≤ <em>B</em>(<em>T</em>), and since <em>T</em> is optimal, we have <em>B</em>(<em>T</em>) ≤ <em>B</em>(<em>T</em> ″), which implies <em>B</em>(<em>T</em> ″) = <em>B</em>(<em>T</em>). Thus, <em>T</em> ″ is an optimal tree in which <em>x</em> and <em>y</em> appear as sibling leaves of maximum depth, from which the lemma follows.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Lemma 15.2 implies that the process of building up an optimal tree by mergers can, without loss of generality, begin with the greedy choice of merging together those two characters of lowest frequency. Why is this a greedy choice? We can view the cost of a single merger as being the sum of the frequencies of the two items being merged. Exercise 15.3-4 shows that the total cost of the tree constructed equals the sum of the costs of its mergers. Of all possible mergers at each step, H<small>UFFMAN</small> chooses the one that incurs the least cost.</p>
<a id="p438"/>
<p>The next lemma shows that the problem of constructing optimal prefix-free codes has the optimal-substructure property.</p>
<p class="lemma"><strong><em>Lemma 15.3 (Optimal prefix-free codes have the optimal-substructure property)</em></strong></p>
<p class="noindent">Let <em>C</em> be a given alphabet with frequency <em>c</em>.<em>freq</em> defined for each character <em>c</em> ∈ <em>C</em>. Let <em>x</em> and <em>y</em> be two characters in <em>C</em> with minimum frequency. Let <em>C</em>′ be the alphabet <em>C</em> with the characters <em>x</em> and <em>y</em> removed and a new character <em>z</em> added, so that <em>C</em>′ = (<em>C</em> − {<em>x</em>, <em>y</em>}) ∪ {<em>z</em>}. Define <em>freq</em> for all characters in <em>C</em>′ with the same values as in <em>C</em>, along with <em>z</em>.<em>freq</em> = <em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em>. Let <em>T</em>′ be any tree representing an optimal prefix-free code for alphabet <em>C</em>′. Then the tree <em>T</em>, obtained from <em>T</em>′ by replacing the leaf node for <em>z</em> with an internal node having <em>x</em> and <em>y</em> as children, represents an optimal prefix-free code for the alphabet <em>C</em>.</p>
<p class="proof"><strong><em>Proof</em></strong>   We first show how to express the cost <em>B</em>(<em>T</em>) of tree <em>T</em> in terms of the cost <em>B</em>(<em>T</em>′) of tree <em>T</em>′, by considering the component costs in equation (15.4). For each character <em>c</em> ∈ <em>C</em> − {<em>x</em>, <em>y</em>}, we have that <em>d<sub>T</sub></em> (<em>c</em>) = <em>d<sub>T′</sub></em> (<em>c</em>), and hence <em>c</em>.<em>freq · d<sub>T</sub></em> (<em>c</em>) = <em>c</em>.<em>freq</em> · <em>d<sub>T′</sub></em> (<em>c</em>). Since <em>d<sub>T</sub></em> (<em>x</em>) = <em>d<sub>T</sub></em> (<em>y</em>) = <em>d<sub>T′</sub></em> (<em>z</em>) + 1, we have</p>
<table class="table2b">
<tr>
<td class="td2"><em>x</em>.<em>freq</em> · <em>d<sub>T</sub></em> (<em>x</em>) + <em>y</em>.<em>freq</em> · <em>d<sub>T</sub></em> (<em>y</em>)</td>
<td class="td2">=</td>
<td class="td2">(<em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em>)(<em>d<sub>T′</sub></em> (<em>z</em>) + 1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>z</em>.<em>freq</em> · <em>d<sub>T′</sub></em>(<em>z</em>)+ (<em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em>),</td>
</tr>
</table>
<p class="noindent">from which we conclude that</p>
<p class="eql"><em>B</em>(<em>T</em>) = <em>B</em>(<em>T</em>′) + <em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em></p>
<p class="noindent">or, equivalently,</p>
<p class="eql"><em>B</em>(<em>T</em>′) = <em>B</em>(<em>T</em>) − <em>x</em>.<em>freq</em> − <em>y</em>.<em>freq</em>.</p>
<p>We now prove the lemma by contradiction. Suppose that <em>T</em> does not represent an optimal prefix-free code for <em>C</em>. Then there exists an optimal tree <em>T</em>″ such that <em>B</em>(<em>T</em>″) &lt; <em>B</em>(<em>T</em>). Without loss of generality (by Lemma 15.2), <em>T</em>″ has <em>x</em> and <em>y</em> as siblings. Let <em>T</em><sup>″′</sup> be the tree <em>T</em>″ with the common parent of <em>x</em> and <em>y</em> replaced by a leaf <em>z</em> with frequency <em>z</em>.<em>freq</em> = <em>x</em>.<em>freq</em> + <em>y</em>.<em>freq</em>. Then</p>
<table class="table2b">
<tr>
<td class="td2"><em>B</em>(<em>T</em><span class="font1">‴</span>)</td>
<td class="td2">=</td>
<td class="td2"><em>B</em>(<em>T</em>″) − <em>x</em>.<em>freq</em> − <em>y</em>.<em>freq</em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">&lt;</td>
<td class="td2"><em>B</em>(<em>T</em>) − <em>x</em>.<em>freq</em> − <em>y</em>.<em>freq</em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>B</em>(<em>T</em>′),</td>
</tr>
</table>
<p class="noindent">yielding a contradiction to the assumption that <em>T</em>′ represents an optimal prefix-free code for <em>C</em>′. Thus, <em>T</em> must represent an optimal prefix-free code for the alphabet <em>C</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="theorem"><strong><em>Theorem 15.4</em></strong></p>
<p class="noindent">Procedure H<small>UFFMAN</small> produces an optimal prefix-free code.</p>
<a id="p439"/>
<p class="proof"><strong><em>Proof</em></strong>   Immediate from Lemmas 15.2 and 15.3.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>15.3-1</em></strong></p>
<p class="noindent">Explain why, in the proof of Lemma 15.2, if <em>x</em>.<em>freq</em> = <em>b</em>.<em>freq</em>, then we must have <em>a</em>.<em>freq</em> = <em>b</em>.<em>freq</em> = <em>x</em>.<em>freq</em> = <em>y</em>.<em>freq</em>.</p>
<p class="level3"><strong><em>15.3-2</em></strong></p>
<p class="noindent">Prove that a non-full binary tree cannot correspond to an optimal prefix-free code.</p>
<p class="level3"><strong><em>15.3-3</em></strong></p>
<p class="noindent">What is an optimal Huffman code for the following set of frequencies, based on the first 8 Fibonacci numbers?</p>
<p class="noindent1-top"><span class="courierfont">a</span>:1 <span class="courierfont">b</span>:1 <span class="courierfont">c</span>:2 <span class="courierfont">d</span>:3 <span class="courierfont">e</span>:5 <span class="courierfont">f</span>:8 <span class="courierfont">g</span>:13 <span class="courierfont">h</span>:21</p>
<p class="noindent1-top">Can you generalize your answer to find the optimal code when the frequencies are the first <em>n</em> Fibonacci numbers?</p>
<p class="level3"><strong><em>15.3-4</em></strong></p>
<p class="noindent">Prove that the total cost <em>B</em>(<em>T</em>) of a full binary tree <em>T</em> for a code equals the sum, over all internal nodes, of the combined frequencies of the two children of the node.</p>
<p class="level3"><strong><em>15.3-5</em></strong></p>
<p class="noindent">Given an optimal prefix-free code on a set <em>C</em> of <em>n</em> characters, you wish to transmit the code itself using as few bits as possible. Show how to represent any optimal prefix-free code on <em>C</em> using only 2<em>n</em> − 1 + <em>n</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> bits. (<em>Hint</em>: Use 2<em>n</em> − 1 bits to specify the structure of the tree, as discovered by a walk of the tree.)</p>
<p class="level3"><strong><em>15.3-6</em></strong></p>
<p class="noindent">Generalize Huffman’s algorithm to ternary codewords (i.e., codewords using the symbols 0, 1, and 2), and prove that it yields optimal ternary codes.</p>
<p class="level3"><strong><em>15.3-7</em></strong></p>
<p class="noindent">A data file contains a sequence of 8-bit characters such that all 256 characters are about equally common: the maximum character frequency is less than twice the minimum character frequency. Prove that Huffman coding in this case is no more efficient than using an ordinary 8-bit fixed-length code.</p>
<p class="level3"><strong><em>15.3-8</em></strong></p>
<p class="noindent">Show that no lossless (invertible) compression scheme can guarantee that for every input file, the corresponding output file is shorter. (<em>Hint</em>: Compare the number of possible files with the number of possible encoded files.)</p>
<a id="p440"/>
</section>
<p class="line1"/>
<section title="15.4 Offline caching">
<a id="Sec_15.4"/>
<p class="level1" id="h1-91"><a href="toc.xhtml#Rh1-91"><strong>15.4    Offline caching</strong></a></p>
<p class="noindent">Computer systems can decrease the time to access data by storing a subset of the main memory in the <span class="blue"><strong><em>cache</em></strong></span>: a small but faster memory. A cache organizes data into <span class="blue"><strong><em>cache blocks</em></strong></span> typically comprising 32, 64, or 128 bytes. You can also think of main memory as a cache for disk-resident data in a virtual-memory system. Here, the blocks are called <span class="blue"><strong><em>pages</em></strong></span>, and 4096 bytes is a typical size.</p>
<p>As a computer program executes, it makes a sequence of memory requests. Say that there are <em>n</em> memory requests, to data in blocks <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>n</sub></em>, in that order. The blocks in the access sequence might not be distinct, and indeed, any given block is usually accessed multiple times. For example, a program that accesses four distinct blocks <em>p</em>, <em>q</em>, <em>r</em>, <em>s</em> might make a sequence of requests to blocks <em>s</em>, <em>q</em>, <em>s</em>, <em>q</em>, <em>q</em>, <em>s</em>, <em>p</em>, <em>p</em>, <em>r</em>, <em>s</em>, <em>s</em>, <em>q</em>, <em>p</em>, <em>r</em>, <em>q</em>. The cache can hold up to some fixed number <em>k</em> of cache blocks. It starts out empty before the first request. Each request causes at most one block to enter the cache and at most one block to be evicted from the cache. Upon a request for block <em>b<sub>i</sub></em>, any one of three scenarios may occur:</p>
<ol class="olnoindent" epub:type="list">
<li>Block <em>b<sub>i</sub></em> is already in the cache, due to a previous request for the same block. The cache remains unchanged. This situation is known as a <span class="blue"><strong><em>cache hit</em></strong></span>.</li>
<li class="litop">Block <em>b<sub>i</sub></em> is not in the cache at that time, but the cache contains fewer than <em>k</em> blocks. In this case, block <em>b<sub>i</sub></em> is placed into the cache, so that the cache contains one more block than it did before the request.</li>
<li class="litop">Block <em>b<sub>i</sub></em> is not in the cache at that time and the cache is full: it contains <em>k</em> blocks. Block <em>b<sub>i</sub></em> is placed into the cache, but before that happens, some other block in the cache must be evicted from the cache in order to make room.</li></ol>
<p>The latter two situations, in which the requested block is not already in the cache, are called <span class="blue"><strong><em>cache misses</em></strong></span>. The goal is to minimize the number of cache misses or, equivalently, to maximize the number of cache hits, over the entire sequence of <em>n</em> requests. A cache miss that occurs while the cache holds fewer than <em>k</em> blocks—that is, as the cache is first being filled up—is known as a <span class="blue"><strong><em>compulsory miss</em></strong></span>, since no prior decision could have kept the requested block in the cache. When a cache miss occurs and the cache is full, ideally the choice of which block to evict should allow for the smallest possible number of cache misses over the entire sequence of future requests.</p>
<p>Typically, caching is an online problem. That is, the computer has to decide which blocks to keep in the cache without knowing the future requests. Here, however, let’s consider the offline version of this problem, in which the computer knows in advance the entire sequence of <em>n</em> requests and the cache size <em>k</em>, with a goal of minimizing the total number of cache misses.</p>
<a id="p441"/>
<p>To solve this offline problem, you can use a greedy strategy called <span class="blue"><strong><em>furthest-in-future</em></strong></span>, which chooses to evict the block in the cache whose next access in the request sequence comes furthest in the future. Intuitively, this strategy makes sense: if you’re not going to need something for a while, why keep it around? We’ll show that the furthest-in-future strategy is indeed optimal by showing that the offline caching problem exhibits optimal substructure and that furthest-in-future has the greedy-choice property.</p>
<p>Now, you might be thinking that since the computer usually doesn’t know the sequence of requests in advance, there is no point in studying the offline problem. Actually, there is. In some situations, you do know the sequence of requests in advance. For example, if you view the main memory as the cache and the full set of data as residing on disk (or a solid-state drive), there are algorithms that plan out the entire set of reads and writes in advance. Furthermore, we can use the number of cache misses produced by an optimal algorithm as a baseline for comparing how well online algorithms perform. We’ll do just that in <a href="chapter027.xhtml#Sec_27.3">Section 27.3</a>.</p>
<p>Offline caching can even model real-world problems. For example, consider a scenario where you know in advance a fixed schedule of <em>n</em> events at known locations. Events may occur at a location multiple times, not necessarily consecutively. You are managing a group of <em>k</em> agents, you need to ensure that you have one agent at each location when an event occurs, and you want to minimize the number of times that agents have to move. Here, the agents are like the blocks, the events are like the requests, and moving an agent is akin to a cache miss.</p>
<p class="level4"><strong>Optimal substructure of offline caching</strong></p>
<p class="noindent">To show that the offline problem exhibits optimal substructure, let’s define the subproblem (<em>C</em>, <em>i</em>) as processing requests for blocks <em>b<sub>i</sub></em>, <em>b</em><sub><em>i</em>+1</sub>, … , <em>b<sub>n</sub></em> with cache configuration <em>C</em> at the time that the request for block <em>b<sub>i</sub></em> occurs, that is, <em>C</em> is a subset of the set of blocks such that |<em>C</em> | ≤ <em>k</em>. A solution to subproblem (<em>C</em>, <em>i</em>) is a sequence of decisions that specifies which block to evict (if any) upon each request for blocks <em>b<sub>i</sub></em>, <em>b</em><sub><em>i</em>+1</sub>, … , <em>b<sub>n</sub></em>. An optimal solution to subproblem (<em>C</em>, <em>i</em>) minimizes the number of cache misses.</p>
<p>Consider an optimal solution <em>S</em> to subproblem (<em>C</em>, <em>i</em>), and let <em>C</em>′ be the contents of the cache after processing the request for block <em>b<sub>i</sub></em> in solution <em>S</em>. Let <em>S</em>′ be the subsolution of <em>S</em> for the resulting subproblem (<em>C</em>′, <em>i</em> + 1). If the request for <em>b<sub>i</sub></em> results in a cache hit, then the cache remains unchanged, so that <em>C</em>′ = <em>C</em>. If the request for block <em>b<sub>i</sub></em> results in a cache miss, then the contents of the cache change, so that <em>C</em>′ ≠ <em>C</em>. We claim that in either case, <em>S</em>′ is an optimal solution to subproblem (<em>C</em>′, <em>i</em> + 1). Why? If <em>S</em>′ is not an optimal solution to subproblem (<em>C</em>′, <em>i</em> + 1), then there exists another solution <em>S</em>″ to subproblem (<em>C</em>′, <em>i</em> + 1) that makes fewer cache misses than <em>S</em>′. Combining <em>S</em>″ with the decision of <em>S</em> at the request for <a id="p442"/>block <em>b<sub>i</sub></em> yields another solution that makes fewer cache misses than <em>S</em>, which contradicts the assumption that <em>S</em> is an optimal solution to subproblem (<em>C</em>, <em>i</em>).</p>
<p>To quantify a recursive solution, we need a little more notation. Let <em>R</em><sub><em>C</em>,<em>i</em></sub> be the set of all cache configurations that can immediately follow configuration <em>C</em> after processing a request for block <em>b<sub>i</sub></em>. If the request results in a cache hit, then the cache remains unchanged, so that <em>R</em><sub><em>C</em>,<em>i</em></sub> = {<em>C</em> }. If the request for <em>b<sub>i</sub></em> results in a cache miss, then there are two possibilities. If the cache is not full (|<em>C</em> | &lt; <em>k</em>), then the cache is filling up and the only choice is to insert <em>b<sub>i</sub></em> into the cache, so that <em>R</em><sub><em>C</em>,<em>i</em></sub>= {<em>C</em> ∪ {<em>b<sub>i</sub></em>}}. If the cache is full (|<em>C</em> | = <em>k</em>) upon a cache miss, then <em>R</em><sub><em>C</em>,<em>i</em></sub> contains <em>k</em> potential configurations: one for each candidate block in <em>C</em> that could be evicted and replaced by block <em>b<sub>i</sub></em>. In this case, <em>R</em><sub><em>C</em>,<em>i</em></sub> = {(<em>C</em> − {<em>x</em>}) ∪ {<em>b<sub>i</sub></em>} : <em>x</em> ∈ <em>C</em> }. For example, if <em>C</em> = {<em>p</em>, <em>q</em>, <em>r</em>}, <em>k</em> = 3, and block <em>s</em> is requested, then <em>R</em><sub><em>C</em>,<em>i</em></sub> = {{<em>p</em>, <em>q</em>, <em>s</em>},{<em>p</em>, <em>r</em>, <em>s</em>},{<em>q</em>, <em>r</em>, <em>s</em>}}.</p>
<p>Let <em>miss</em>(<em>C</em>, <em>i</em>) denote the minimum number of cache misses in a solution for subproblem (<em>C</em>, <em>i</em>). Here is a recurrence for <em>miss</em>(<em>C</em>, <em>i</em>):</p>
<p class="eql"><img alt="art" src="images/Art_P504.jpg"/></p>
<p class="level4"><strong>Greedy-choice property</strong></p>
<p class="noindent">To prove that the furthest-in-future strategy yields an optimal solution, we need to show that optimal offline caching exhibits the greedy-choice property. Combined with the optimal-substructure property, the greedy-choice property will prove that furthest-in-future produces the minimum possible number of cache misses.</p>
<p class="theorem"><strong><em>Theorem 15.5 (Optimal offline caching has the greedy-choice property)</em></strong></p>
<p class="noindent">Consider a subproblem (<em>C</em>, <em>i</em>) when the cache <em>C</em> contains <em>k</em> blocks, so that it is full, and a cache miss occurs. When block <em>b<sub>i</sub></em> is requested, let <em>z</em> = <em>b<sub>m</sub></em> be the block in <em>C</em> whose next access is furthest in the future. (If some block in the cache will never again be referenced, then consider any such block to be block <em>z</em>, and add a dummy request for block <em>z</em> = <em>b<sub>m</sub></em> = <em>b</em><sub><em>n</em>+1</sub>.) Then evicting block <em>z</em> upon a request for block <em>b<sub>i</sub></em> is included in some optimal solution for the subproblem (<em>C</em>, <em>i</em>).</p>
<p class="proof"><strong><em>Proof</em></strong>   Let <em>S</em> be an optimal solution to subproblem (<em>C</em>, <em>i</em>). If <em>S</em> evicts block <em>z</em> upon the request for block <em>b<sub>i</sub></em>, then we are done, since we have shown that some optimal solution includes evicting <em>z</em>.</p>
<p>So now suppose that optimal solution <em>S</em> evicts some other block <em>x</em> when block <em>b<sub>i</sub></em> is requested. We’ll construct another solution <em>S</em>′ to subproblem (<em>C</em>, <em>i</em>) which, upon <a id="p443"/>the request for <em>b<sub>i</sub></em>, evicts block <em>z</em> instead of <em>x</em> and induces no more cache misses than <em>S</em> does, so that <em>S</em>′ is also optimal. Because different solutions may yield different cache configurations, denote by <em>C</em><sub><em>S</em>,<em>j</em></sub> the configuration of the cache under solution <em>S</em> just before the request for some block <em>b<sub>j</sub></em>, and likewise for solution <em>S</em>′ and <em>C</em><sub><em>S</em>′,<em>j</em></sub>. We’ll show how to construct <em>S</em>′ with the following properties:</p>
<ol class="olnoindent" epub:type="list">
<li>For <em>j</em> = <em>i</em> + 1, … , <em>m</em>, let <em>D<sub>j</sub></em> = <em>C</em><sub><em>S</em>,<em>j</em></sub> ∩ <em>C</em><sub><em>S</em>′,<em>j</em></sub>. Then, |<em>D<sub>j</sub></em> | ≥ <em>k</em> − 1, so that the cache configurations <em>C</em><sub><em>S</em>,<em>j</em></sub> and <em>C</em><sub><em>S</em>′,<em>j</em></sub> differ by at most one block. If they differ, then <em>C</em><sub><em>S</em>,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>z</em>} and <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>y</em>} for some block <em>y</em> ≠ <em>z</em>.</li>
<li class="litop">For each request of blocks <em>b<sub>i</sub></em>, … , <em>b</em><sub><em>m</em>−1</sub>, if solution <em>S</em> has a cache hit, then solution <em>S</em>′ also has a cache hit.</li>
<li class="litop">For all <em>j</em> &gt; <em>m</em>, the cache configurations <em>C</em><sub><em>S</em>,<em>j</em></sub> and <em>C</em><sub><em>S</em>′,<em>j</em></sub> are identical.</li>
<li class="litop">Over the sequence of requests for blocks <em>b<sub>i</sub></em>, … , <em>b<sub>m</sub></em>, the number of cache misses produced by solution <em>S</em>′ is at most the number of cache misses produced by solution <em>S</em>.</li></ol>
<p>We’ll prove inductively that these properties hold for each request.</p>
<ol class="olnoindent" epub:type="list">
<li>We proceed by induction on <em>j</em>, for <em>j</em> = <em>i</em> +1, … , <em>m</em>. For the base case, the initial caches <em>C</em><sub><em>S</em>,<em>i</em></sub> and <em>C</em><sub><em>S</em>′,<em>i</em></sub> are identical. Upon the request for block <em>b<sub>i</sub></em>, solution <em>S</em> evicts <em>x</em> and solution <em>S</em>′ evicts <em>z</em>. Thus, cache configurations <em>C</em><sub><em>S</em>,<em>i</em>+1</sub> and <em>C</em><sub><em>S</em>′,<em>i</em>+1</sub> differ by just one block, <em>C</em><sub><em>S</em>,<em>i</em>+1</sub> = <em>D</em><sub><em>i</em>+1</sub> ∪ {<em>z</em>}, <em>C</em><sub><em>S</em>′,<em>i</em>+1</sub> = <em>D</em><sub><em>i</em>+1</sub> ∪ {<em>x</em>}, and <em>x</em> ≠ <em>z</em>.
<p class="ntop1">The inductive step defines how solution <em>S</em>′ behaves upon a request for block <em>b<sub>j</sub></em> for <em>i</em> + 1 ≤ <em>j</em> ≤ <em>m</em> − 1. The inductive hypothesis is that property 1 holds when <em>b<sub>j</sub></em> is requested. Because <em>z</em> = <em>b<sub>m</sub></em> is the block in <em>C</em><sub><em>S</em>,<em>i</em></sub> whose next reference is furthest in the future, we know that <em>b<sub>j</sub></em> ≠ <em>z</em>. We consider several scenarios:</p>
<ul class="ulnoindent1-d" epub:type="list">
<li class="litop">If <em>C</em><sub><em>S</em>,<em>j</em></sub> = <em>C</em><sub><em>S</em>′,<em>j</em></sub> (so that |<em>D<sub>j</sub></em> | = <em>k</em>), then solution <em>S</em>′ makes the same decision upon the request for <em>b<sub>j</sub></em> as <em>S</em> makes, so that <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub>.</li>
<li>If |<em>D<sub>j</sub></em>| = <em>k</em> − 1 and <em>b<sub>j</sub></em> ∈ <em>D<sub>j</sub></em>, then both caches already contain block <em>b<sub>j</sub></em>, and both solutions <em>S</em> and <em>S</em>′ have cache hits. Therefore, <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>,<em>j</em></sub> and <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em></sub>.</li>
<li>If |<em>D</em><sub><em>j</em></sub> | = <em>k</em> − 1 and <em>b<sub>j</sub></em> ∉ <em>D<sub>j</sub></em>, then because <em>C</em><sub><em>S</em>,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>z</em>} and <em>b<sub>j</sub></em> ≠ <em>z</em>, solution <em>S</em> has a cache miss. It evicts either block <em>z</em> or some block <em>w</em> ∈ <em>D<sub>j</sub></em>.
<ul class="ulnoindent1-c" epub:type="list">
<li>If solution <em>S</em> evicts block <em>z</em>, then <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>D<sub>j</sub></em> ∪ {<em>b<sub>j</sub></em>}. There are two cases, depending on whether <em>b<sub>j</sub></em> = <em>y</em>:
<ul class="ulnoindent1-s" epub:type="list">
<li>If <em>b<sub>j</sub></em> = <em>y</em>, then solution <em>S</em>′ has a cache hit, so that <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>b<sub>j</sub></em>}. Thus, <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em> +1</sub>.</li>
<li class="litop">If <em>b<sub>j</sub></em> ≠ <em>y</em>, then solution <em>S</em>′ has a cache miss. It evicts block <em>y</em>, so that <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>D<sub>j</sub></em> ∪ {<em>b<sub>j</sub></em> }, and again <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub>.</li></ul>
<a id="p444"/></li>
<li class="litop">If solution <em>S</em> evicts some block <em>w</em> ∈ <em>D<sub>j</sub></em>, then <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = (<em>D<sub>j</sub></em> − {<em>w</em>}) ∪ {<em>b<sub>j</sub></em>, <em>z</em>}. Once again, there are two cases, depending on whether <em>b<sub>j</sub></em> = <em>y</em>:
<ul class="ulnoindent1-s" epub:type="list">
<li>If <em>b<sub>j</sub></em> = <em>y</em>, then solution <em>S</em>′ has a cache hit, so that <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>b<sub>j</sub></em>}. Since <em>w</em> ∈ <em>D<sub>j</sub></em> and <em>w</em> was not evicted by solution <em>S</em>′, we have <em>w</em> ∈ <em>C</em><sub><em>S</em>′,<em>j</em> +1</sub>. Therefore, <em>w</em> ∉ <em>D</em><sub><em>j</em>+1</sub> and <em>b<sub>j</sub></em> ∈ <em>D</em><sub><em>j</em>+1</sub>, so that <em>D</em><sub><em>j</em>+1</sub> = (<em>D<sub>j</sub></em> − {<em>w</em>}) ∪ {<em>b<sub>j</sub></em> }. Thus, <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>D</em><sub><em>j</em>+1</sub> ∪ {<em>z</em>},<em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>D</em><sub><em>j</em> +1</sub> ∪ {<em>w</em>}, and because <em>w</em> ≠ <em>z</em>, property 1 holds when block <em>b</em><sub><em>j</em>+1</sub> is requested. (In other words, block <em>w</em> replaces block <em>y</em> in property 1.)</li>
<li class="litop">If <em>b<sub>j</sub></em> ≠ <em>y</em>, then solution <em>S</em>′ has a cache miss. It evicts block <em>w</em>, so that <em>C</em><sub><em>S</em>′,<em>j</em> +1</sub> = (<em>D<sub>j</sub></em> − {<em>w</em>}) ∪ {<em>b<sub>j</sub></em>, <em>y</em>}. Therefore, we have that <em>D</em><sub><em>j</em>+1</sub> = (<em>D<sub>j</sub></em> − {<em>w</em>}) ∪ {<em>b<sub>j</sub></em> } and so <em>C</em><sub><em>S</em>,<em>j</em>+1</sub> = <em>D</em><sub><em>j</em>+1</sub> ∪ {<em>z</em>} and <em>C</em><sub><em>S</em>′,<em>j</em>+1</sub> = <em>D</em><sub><em>j</em> +1</sub> ∪ {<em>y</em>}.</li></ul>
</li></ul>
</li></ul>
</li>
<li class="litop">In the above discussion about maintaining property 1, solution <em>S</em> may have a cache hit in only the first two cases, and solution <em>S</em>′ has a cache hit in these cases if and only if <em>S</em> does.</li>
<li class="litop">If <em>C</em><sub><em>S</em>,<em>m</em></sub> = <em>C</em><sub><em>S</em>′,<em>m</em></sub>, then solution <em>S</em>′ makes the same decision upon the request for block <em>z</em> = <em>b<sub>m</sub></em> as <em>S</em> makes, so that <em>C</em><sub><em>S</em>,<em>m</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>m</em>+1</sub>. If <em>C</em><sub><em>S</em>,<em>m</em></sub> ≠ <em>C</em><sub><em>S</em>′,<em>m</em></sub>, then by property 1, <em>C</em><sub><em>S</em>,<em>m</em></sub> = <em>D<sub>m</sub></em>∪{<em>z</em>} and <em>C</em><sub><em>S</em>′,<em>m</em></sub> = <em>D<sub>m</sub></em>∪{<em>y</em>}, where <em>y</em> ≠ <em>z</em>. In this case, solution <em>S</em> has a cache hit, so that <em>C</em><sub><em>S</em>,<em>m</em>+1</sub> = <em>C</em><sub><em>S</em>,<em>m</em></sub> = <em>D<sub>m</sub></em> ∪ {<em>z</em>}. Solution <em>S</em>′ evicts block <em>y</em> and brings in block <em>z</em>, so that <em>C</em><sub><em>S</em>′,<em>m</em>+1</sub> = <em>D<sub>m</sub></em> ∪ {<em>z</em>} = <em>C</em><sub><em>S</em>,<em>m</em>+1</sub>. Thus, regardless of whether or not <em>C</em><sub><em>S</em>,<em>m</em></sub> = <em>C</em><sub><em>S</em>′,<em>m</em></sub>, we have <em>C</em><sub><em>S</em>,<em>m</em>+1</sub> = <em>C</em><sub><em>S</em>′,<em>m</em>+1</sub>, and starting with the request for block <em>b</em><sub><em>m</em>+1</sub>, solution <em>S</em>′ simply makes the same decisions as <em>S</em>.</li>
<li class="litop">By property 2, upon the requests for blocks <em>b<sub>i</sub></em>, … , <em>b</em><sub><em>m</em>−1</sub>, whenever solution <em>S</em> has a cache hit, so does <em>S</em>′. Only the request for block <em>b<sub>m</sub></em> = <em>z</em> remains to be considered. If <em>S</em> has a cache miss upon the request for <em>b<sub>m</sub></em>, then regardless of whether <em>S</em>′ has a cache hit or a cache miss, we are done: <em>S</em>′ has at most the same number of cache misses as <em>S</em>.
<p class="ntop1">So now suppose that <em>S</em> has a cache hit and <em>S</em>′ has a cache miss upon the request for <em>b<sub>m</sub></em>. We’ll show that there exists a request for at least one of blocks <em>b</em><sub><em>i</em>+1</sub>, … , <em>b</em><sub><em>m</em>−1</sub> in which the request results in a cache miss for <em>S</em> and a cache hit for <em>S</em>′, thereby compensating for what happens upon the request for block <em>b<sub>m</sub></em>. The proof is by contradiction. Assume that no request for blocks <em>b</em><sub><em>i</em>+1</sub>, … , <em>b</em><sub><em>m</em>−1</sub> results in a cache miss for <em>S</em> and a cache hit for <em>S</em>′.</p>
<p class="ntop1">We start by observing that once the caches <em>C</em><sub><em>S</em>,<em>j</em></sub> and <em>C</em><sub><em>S</em>′<em>j</em></sub> are equal for some <em>j</em> &gt; <em>i</em>, they remain equal thereafter. Observe also that if <em>b<sub>m</sub></em> ∈ <em>C</em><sub><em>S</em>,<em>m</em></sub> and <em>b<sub>m</sub></em> ∉ <em>C</em><sub><em>S</em>′,<em>m</em></sub>, then <em>C</em><sub><em>S</em>,<em>m</em></sub> ≠ <em>C</em><sub><em>S</em>′,<em>m</em></sub>. Therefore, solution <em>S</em> cannot have evicted block <em>z</em> upon the requests for blocks <em>b<sub>i</sub></em>, … , <em>b</em><sub><em>m</em>−1</sub>, for if it had, then these two <a id="p445"/>cache configurations would be equal. The remaining possibility is that upon each of these requests, we had <em>C</em><sub><em>S</em>,<em>j</em></sub> = <em>D</em><sub><em>j</em></sub> ∪ {<em>z</em>}, <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>y</em>} for some block <em>y</em> ≠ <em>z</em>, and solution <em>S</em> evicted some block <em>w</em> ∈ <em>D<sub>j</sub></em>. Moreover, since none of these requests resulted in a cache miss for <em>S</em> and a cache hit for <em>S</em><sup>′</sup>, the case of <em>b<sub>j</sub></em> = <em>y</em> never occurred. That is, for every request of blocks <em>b</em><sub><em>i</em>+1</sub>, … , <em>b</em><sub><em>m</em>−1</sub>, the requested block <em>b<sub>j</sub></em> was never the block <em>y</em> ∈ <em>C</em><sub><em>S</em>′,<em>j</em></sub> − <em>C</em><sub><em>S</em>,<em>j</em></sub>. In these cases, after processing the request, we had <em>C</em><sub><em>S</em>′,<em>j</em> +1</sub> = <em>D</em><sub><em>j</em> +1</sub> ∪ {<em>y</em>}: the difference between the two caches did not change. Now, let’s go back to the request for block <em>b<sub>i</sub></em>, where afterward, we had <em>C</em><sub><em>S</em>′,<em>i</em>+1</sub> = <em>D</em><sub><em>i</em>+1</sub> ∪ {<em>x</em>}. Because every succeeding request until requesting block <em>b<sub>m</sub></em> did not change the difference between the caches, we had <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>x</em>} for <em>j</em> = <em>i</em> + 1, … , <em>m</em>.</p>
<p class="ntop1">By definition, block <em>z</em> = <em>b<sub>m</sub></em> is requested after block <em>x</em>. That means at least one of blocks <em>b</em><sub><em>i</em>+1</sub>, … , <em>b</em><sub><em>m</em>−1</sub> is block <em>x</em>. But for <em>j</em> = <em>i</em> + 1, … , <em>m</em>, we have <em>x</em> ∈ <em>C</em><sub><em>S</em>′,<em>j</em></sub> and <em>x</em> ∉ <em>C</em><sub><em>S</em>,<em>j</em></sub>, so that at least one of these requests had a cache hit for <em>S</em>′ and a cache miss for <em>S</em>, a contradiction. We conclude that if solution <em>S</em> has a cache hit and solution <em>S</em>′ has a cache miss upon the request for block <em>b<sub>m</sub></em>, then some earlier request had the opposite result, and so solution <em>S</em>′ produces no more cache misses than solution <em>S</em>. Since <em>S</em> is assumed to be optimal, <em>S</em>′ is optimal as well.</p></li></ol>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Along with the optimal-substructure property, Theorem 15.5 tells us that the furthest-in-future strategy yields the minimum number of cache misses.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>15.4-1</em></strong></p>
<p class="noindent">Write pseudocode for a cache manager that uses the furthest-in-future strategy. It should take as input a set <em>C</em> of blocks in the cache, the number of blocks <em>k</em> that the cache can hold, a sequence <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>n</sub></em> of requested blocks, and the index <em>i</em> into the sequence for the block <em>b<sub>i</sub></em> being requested. For each request, it should print out whether a cache hit or cache miss occurs, and for each cache miss, it should also print out which block, if any, is evicted.</p>
<p class="level3"><strong><em>15.4-2</em></strong></p>
<p class="noindent">Real cache managers do not know the future requests, and so they often use the past to decide which block to evict. The <span class="blue"><strong><em>least-recently-used</em></strong></span>, or <span class="blue"><strong><em>LRU</em></strong></span>, strategy evicts the block that, of all blocks currently in the cache, was the least recently requested. (You can think of LRU as “furthest-in-past.”) Give an example of a request sequence in which the LRU strategy is not optimal, by showing that it induces more cache misses than the furthest-in-future strategy does on the same request sequence.</p>
<a id="p446"/>
<p class="level3"><strong><em>15.4-3</em></strong></p>
<p class="noindent">Professor Croesus suggests that in the proof of Theorem 15.5, the last clause in property 1 can change to <em>C</em><sub><em>S</em>′,<em>j</em></sub> = <em>D<sub>j</sub></em> ∪ {<em>x</em>} or, equivalently, require the block <em>y</em> given in property 1 to always be the block <em>x</em> evicted by solution <em>S</em> upon the request for block <em>b<sub>i</sub></em>. Show where the proof breaks down with this requirement.</p>
<p class="level3"><strong><em>15.4-4</em></strong></p>
<p class="noindent">This section has assumed that at most one block is placed into the cache whenever a block is requested. You can imagine, however, a strategy in which multiple blocks may enter the cache upon a single request. Show that for every solution that allows multiple blocks to enter the cache upon each request, there is another solution that brings in only one block upon each request and is at least as good.</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-92"><strong>Problems</strong></p>
<section title="15-1 Coin changing">
<p class="level2"><strong><em>15-1     Coin changing</em></strong></p>
<p class="noindent">Consider the problem of making change for <em>n</em> cents using the smallest number of coins. Assume that each coin’s value is an integer.</p>
<p class="nl"><strong><em>a.</em></strong> Describe a greedy algorithm to make change consisting of quarters, dimes, nickels, and pennies. Prove that your algorithm yields an optimal solution.</p>
<p class="nl"><strong><em>b.</em></strong> Suppose that the available coins are in denominations that are powers of <em>c</em>: the denominations are <em>c</em><sup>0</sup>, <em>c</em><sup>1</sup>, … , <em>c<sup>k</sup></em> for some integers <em>c &gt;</em> 1 and <em>k</em> ≥ 1. Show that the greedy algorithm always yields an optimal solution.</p>
<p class="nl"><strong><em>c.</em></strong> Give a set of coin denominations for which the greedy algorithm does not yield an optimal solution. Your set should include a penny so that there is a solution for every value of <em>n</em>.</p>
<p class="nl"><strong><em>d.</em></strong> Give an <em>O</em>(<em>nk</em>)-time algorithm that makes change for any set of <em>k</em> different coin denominations using the smallest number of coins, assuming that one of the coins is a penny.</p>
</section>
<section title="15-2 Scheduling to minimize average completion time">
<p class="level2"><strong><em>15-2     Scheduling to minimize average completion time</em></strong></p>
<p class="noindent">You are given a set <em>S</em> = {<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em>} of tasks, where task <em>a<sub>i</sub></em> requires <em>p<sub>i</sub></em> units of processing time to complete. Let <em>C<sub>i</sub></em> be the <span class="blue"><strong><em>completion time</em></strong></span> of task <em>a<sub>i</sub></em>, that is, the time at which task <em>a<sub>i</sub></em> completes processing. Your goal is to minimize the average completion time, that is, to minimize <img alt="art" src="images/Art_P505.jpg"/>. For example, suppose that there are two tasks <em>a</em><sub>1</sub> and <em>a</em><sub>2</sub> with <em>p</em><sub>1</sub> = 3 and <em>p</em><sub>2</sub> = 5, and consider the schedule <a id="p447"/>in which <em>a</em><sub>2</sub> runs first, followed by <em>a</em><sub>1</sub>. Then we have <em>C</em><sub>2</sub> = 5, <em>C</em><sub>1</sub> = 8, and the average completion time is (5 + 8)/2 = 6.5. If task <em>a</em><sub>1</sub> runs first, however, then we have <em>C</em><sub>1</sub> = 3, <em>C</em><sub>2</sub> = 8, and the average completion time is (3 + 8)/2 = 5.5.</p>
<p class="nl"><strong><em>a.</em></strong> Give an algorithm that schedules the tasks so as to minimize the average completion time. Each task must run nonpreemptively, that is, once task <em>a<sub>i</sub></em> starts, it must run continuously for <em>p<sub>i</sub></em> units of time until it is done. Prove that your algorithm minimizes the average completion time, and analyze the running time of your algorithm.</p>
<p class="nl"><strong><em>b.</em></strong> Suppose now that the tasks are not all available at once. That is, each task cannot start until its <span class="blue"><strong><em>release time</em></strong></span> <em>b<sub>i</sub></em>. Suppose also that tasks may be <span class="blue"><strong><em>preempted</em></strong></span>, so that a task can be suspended and restarted at a later time. For example, a task <em>a<sub>i</sub></em> with processing time <em>p<sub>i</sub></em> = 6 and release time <em>b<sub>i</sub></em> = 1 might start running at time 1 and be preempted at time 4. It might then resume at time 10 but be preempted at time 11, and it might finally resume at time 13 and complete at time 15. Task <em>a<sub>i</sub></em> has run for a total of 6 time units, but its running time has been divided into three pieces. Give an algorithm that schedules the tasks so as to minimize the average completion time in this new scenario. Prove that your algorithm minimizes the average completion time, and analyze the running time of your algorithm.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-93"><strong>Chapter notes</strong></p>
<p class="noindent">Much more material on greedy algorithms can be found in Lawler [<a epub:type="noteref" href="bibliography001.xhtml#endnote_276">276</a>] and Papadimitriou and Steiglitz [<a epub:type="noteref" href="bibliography001.xhtml#endnote_353">353</a>]. The greedy algorithm first appeared in the combinatorial optimization literature in a 1971 article by Edmonds [<a epub:type="noteref" href="bibliography001.xhtml#endnote_131">131</a>].</p>
<p>The proof of correctness of the greedy algorithm for the activity-selection problem is based on that of Gavril [<a epub:type="noteref" href="bibliography001.xhtml#endnote_179">179</a>].</p>
<p>Huffman codes were invented in 1952 [<a epub:type="noteref" href="bibliography001.xhtml#endnote_233">233</a>]. Lelewer and Hirschberg [<a epub:type="noteref" href="bibliography001.xhtml#endnote_294">294</a>] surveys data-compression techniques known as of 1987.</p>
<p>The furthest-in-future strategy was proposed by Belady [<a epub:type="noteref" href="bibliography001.xhtml#endnote_41">41</a>], who suggested it for virtual-memory systems. Alternative proofs that furthest-in-future is optimal appear in articles by Lee et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_284">284</a>] and Van Roy [<a epub:type="noteref" href="bibliography001.xhtml#endnote_443">443</a>].</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> We sometimes refer to the sets <em>S<sub>k</sub></em> as subproblems rather than as just sets of activities. The context will make it clear whether we are referring to <em>S<sub>k</sub></em> as a set of activities or as a subproblem whose input is that set.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> Because the pseudocode takes <em>s</em> and <em>f</em> as arrays, it indexes into them with square brackets rather than with subscripts.</p>
</section>
</section>
</div>
</body>
</html>