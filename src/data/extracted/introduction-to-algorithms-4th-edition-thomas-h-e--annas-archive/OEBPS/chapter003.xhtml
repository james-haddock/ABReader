<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p49"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="3 Characterizing Running Times">
<p class="chapter-title"><a href="toc.xhtml#chap-3"><strong><span class="blue1">3          Characterizing Running Times</span></strong></a></p>
<p class="noindent">The order of growth of the running time of an algorithm, defined in <a href="chapter002.xhtml">Chapter 2</a>, gives a simple way to characterize the algorithm’s efficiency and also allows us to compare it with alternative algorithms. Once the input size <em>n</em> becomes large enough, merge sort, with its Θ(<em>n</em> lg <em>n</em>) worst-case running time, beats insertion sort, whose worst-case running time is Θ(<em>n</em><sup>2</sup>). Although we can sometimes determine the exact running time of an algorithm, as we did for insertion sort in <a href="chapter002.xhtml">Chapter 2</a>, the extra precision is rarely worth the effort of computing it. For large enough inputs, the multiplicative constants and lower-order terms of an exact running time are dominated by the effects of the input size itself.</p>
<p>When we look at input sizes large enough to make relevant only the order of growth of the running time, we are studying the <strong><em><span class="blue1">asymptotic</span></em></strong> efficiency of algorithms. That is, we are concerned with how the running time of an algorithm increases with the size of the input <em>in the limit</em>, as the size of the input increases without bound. Usually, an algorithm that is asymptotically more efficient is the best choice for all but very small inputs.</p>
<p>This chapter gives several standard methods for simplifying the asymptotic analysis of algorithms. The next section presents informally the three most commonly used types of “asymptotic notation,” of which we have already seen an example in Θ-notation. It also shows one way to use these asymptotic notations to reason about the worst-case running time of insertion sort. Then we look at asymptotic notations more formally and present several notational conventions used throughout this book. The last section reviews the behavior of functions that commonly arise when analyzing algorithms.</p>
<a id="p50"/>
<p class="line1"/>
<section title="3.1 O-notation, Ω-notation, and Θ-notation">
<a id="Sec_3.1"/>
<p class="level1" id="h1-11"><a href="toc.xhtml#Rh1-11"><strong>3.1      <em>O</em>-notation, Ω-notation, and Θ-notation</strong></a></p>
<p class="noindent">When we analyzed the worst-case running time of insertion sort in <a href="chapter002.xhtml">Chapter 2</a>, we started with the complicated expression</p>
<p class="eql"><img alt="art" src="images/Art_P33.jpg"/></p>
<p class="noindent">We then discarded the lower-order terms (<em>c</em><sub>1</sub> + <em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub>/2 – <em>c</em><sub>6</sub>/2 – <em>c</em><sub>7</sub>/2 + <em>c</em><sub>8</sub>)<em>n</em> and <em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub> + <em>c</em><sub>8</sub>, and we also ignored the coefficient <em>c</em><sub>5</sub>/2 + <em>c</em><sub>6</sub>/2 + <em>c</em><sub>7</sub>/2 of <em>n</em><sup>2</sup>. That left just the factor <em>n</em><sup>2</sup>, which we put into Θ-notation as Θ(<em>n</em><sup>2</sup>). We use this style to characterize running times of algorithms: discard the lower-order terms and the coefficient of the leading term, and use a notation that focuses on the rate of growth of the running time.</p>
<p>Θ-notation is not the only such “asymptotic notation.” In this section, we’ll see other forms of asymptotic notation as well. We start with intuitive looks at these notations, revisiting insertion sort to see how we can apply them. In the next section, we’ll see the formal definitions of our asymptotic notations, along with conventions for using them.</p>
<p>Before we get into specifics, bear in mind that the asymptotic notations we’ll see are designed so that they characterize functions in general. It so happens that the functions we are most interested in denote the running times of algorithms. But asymptotic notation can apply to functions that characterize some other aspect of algorithms (the amount of space they use, for example), or even to functions that have nothing whatsoever to do with algorithms.</p>
<p class="level4"><strong><em>O</em>-notation</strong></p>
<p class="noindent"><em>O</em>-notation characterizes an <em>upper bound</em> on the asymptotic behavior of a function. In other words, it says that a function grows <em>no faster</em> than a certain rate, based on the highest-order term. Consider, for example, the function 7<em>n</em><sup>3</sup> + 100<em>n</em><sup>2</sup> – 20<em>n</em> + 6. Its highest-order term is 7<em>n</em><sup>3</sup>, and so we say that this function’s rate of growth is <em>n</em><sup>3</sup>. Because this function grows no faster than <em>n</em><sup>3</sup>, we can write that it is <em>O</em>(<em>n</em><sup>3</sup>). You might be surprised that we can also write that the function 7<em>n</em><sup>3</sup> + 100<em>n</em><sup>2</sup> – 20<em>n</em> + 6 is <em>O</em>(<em>n</em><sup>4</sup>). Why? Because the function grows more slowly than <em>n</em><sup>4</sup>, we are correct in saying that it grows no faster. As you might have guessed, this function is also <em>O</em>(<em>n</em><sup>5</sup>), <em>O</em>(<em>n</em><sup>6</sup>), and so on. More generally, it is <em>O</em>(<em>n<sup>c</sup></em>) for any constant <em>c</em> ≥ 3.</p>
<a id="p51"/>
<p class="level4"><strong>Ω-notation</strong></p>
<p class="noindent">Ω-notation characterizes a <em>lower bound</em> on the asymptotic behavior of a function. In other words, it says that a function grows <em>at least as fast</em> as a certain rate, based — as in <em>O</em>-notation—on the highest-order term. Because the highest-order term in the function 7<em>n</em><sup>3</sup> + 100<em>n</em><sup>2</sup> – 20<em>n</em> + 6 grows at least as fast as <em>n</em><sup>3</sup>, this function is Ω(<em>n</em><sup>3</sup>). This function is also Ω(<em>n</em><sup>2</sup>) and Ω(<em>n</em>). More generally, it is Ω(<em>n<sup>c</sup></em>) for any constant <em>c</em> ≤ 3.</p>
<p class="level4"><strong>Θ-notation</strong></p>
<p class="noindent">Θ-notation characterizes a <em>tight bound</em> on the asymptotic behavior of a function. It says that a function grows <em>precisely</em> at a certain rate, based—once again—on the highest-order term. Put another way, Θ-notation characterizes the rate of growth of the function to within a constant factor from above and to within a constant factor from below. These two constant factors need not be equal.</p>
<p>If you can show that a function is both <em>O</em>(<em>f</em> (<em>n</em>)) and Ω(<em>f</em> (<em>n</em>)) for some function <em>f</em> (<em>n</em>), then you have shown that the function is Θ(<em>f</em> (<em>n</em>)). (The next section states this fact as a theorem.) For example, since the function 7<em>n</em><sup>3</sup> + 100<em>n</em><sup>2</sup> – 20<em>n</em> + 6 is both <em>O</em>(<em>n</em><sup>3</sup>) and Ω(<em>n</em><sup>3</sup>), it is also Θ(<em>n</em><sup>3</sup>).</p>
<p class="level4"><strong>Example: Insertion sort</strong></p>
<p class="noindent">Let’s revisit insertion sort and see how to work with asymptotic notation to characterize its Θ(<em>n</em><sup>2</sup>) worst-case running time without evaluating summations as we did in <a href="chapter002.xhtml">Chapter 2</a>. Here is the I<small>NSERTION</small>-S<small>ORT</small> procedure once again:</p>
<div class="pull-quote1">
<p class="box-heading">I<small>NSERTION</small>-S<small>ORT</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><em>key</em> = <em>A</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><span class="red"><strong>//</strong> Insert <em>A</em>[<em>i</em>] into the sorted subarray <em>A</em>[1 : <em>i</em> – 1].</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><em>j</em> = <em>i</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p2"><strong>while</strong> <em>j</em> &gt; 0 and <em>A</em>[<em>j</em>] &gt; <em>key</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p3"><em>A</em>[<em>j</em> + 1] = <em>A</em>[<em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p3"><em>j</em> = <em>j</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="p2"><em>A</em>[<em>j</em> + 1] = <em>key</em></p></td>
</tr>
</table>
</div>
<p>What can we observe about how the pseudocode operates? The procedure has nested loops. The outer loop is a <strong>for</strong> loop that runs <em>n</em> – 1 times, regardless of the values being sorted. The inner loop is a <strong>while</strong> loop, but the number of iterations it makes depends on the values being sorted. The loop variable <em>j</em> starts at <em>i</em> – 1 <a id="p52"/>and decreases by 1 in each iteration until either it reaches 0 or <em>A</em>[<em>j</em>] ≤ <em>key</em>. For a given value of <em>i</em>, the <strong>while</strong> loop might iterate 0 times, <em>i</em> – 1 times, or anywhere in between. The body of the <strong>while</strong> loop (lines 6–7) takes constant time per iteration of the <strong>while</strong> loop.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_3-1"><img alt="art" src="images/Art_P34.jpg"/></p>
<p class="caption"><strong>Figure 3.1</strong> The Ω(<em>n</em><sup>2</sup>) lower bound for insertion sort. If the first <em>n</em>/3 positions contain the <em>n</em>/3 largest values, each of these values must move through each of the middle <em>n</em>/3 positions, one position at a time, to end up somewhere in the last <em>n</em>/3 positions. Since each of <em>n</em>/3 values moves through at least each of <em>n</em>/3 positions, the time taken in this case is at least proportional to (<em>n</em>/3)(<em>n</em>/3) = <em>n</em><sup>2</sup>/9, or Ω(<em>n</em><sup>2</sup>).</p>
</div>
<p>These observations suffice to deduce an <em>O</em>(<em>n</em><sup>2</sup>) running time for any case of I<small>NSERTION</small>-S<small>ORT</small>, giving us a blanket statement that covers all inputs. The running time is dominated by the inner loop. Because each of the <em>n</em> – 1 iterations of the outer loop causes the inner loop to iterate at most <em>i</em> – 1 times, and because <em>i</em> is at most <em>n</em>, the total number of iterations of the inner loop is at most (<em>n</em> – 1)(<em>n</em> – 1), which is less than <em>n</em><sup>2</sup>. Since each iteration of the inner loop takes constant time, the total time spent in the inner loop is at most a constant times <em>n</em><sup>2</sup>, or <em>O</em>(<em>n</em><sup>2</sup>).</p>
<p>With a little creativity, we can also see that the worst-case running time of I<small>NSERTION</small>-S<small>ORT</small> is Ω(<em>n</em><sup>2</sup>). By saying that the worst-case running time of an algorithm is Ω(<em>n</em><sup>2</sup>), we mean that for every input size <em>n</em> above a certain threshold, there is at least one input of size <em>n</em> for which the algorithm takes at least <em>cn</em><sup>2</sup> time, for some positive constant <em>c</em>. It does not necessarily mean that the algorithm takes at least <em>cn</em><sup>2</sup> time for all inputs.</p>
<p>Let’s now see why the worst-case running time of I<small>NSERTION</small>-S<small>ORT</small> is Ω(<em>n</em><sup>2</sup>). For a value to end up to the right of where it started, it must have been moved in line 6. In fact, for a value to end up <em>k</em> positions to the right of where it started, line 6 must have executed <em>k</em> times. As <a href="chapter003.xhtml#Fig_3-1">Figure 3.1</a> shows, let’s assume that <em>n</em> is a multiple of 3 so that we can divide the array <em>A</em> into groups of <em>n</em>/3 positions. Suppose that in the input to I<small>NSERTION</small>-S<small>ORT</small>, the <em>n</em>/3 largest values occupy the first <em>n</em>/3 array positions <em>A</em>[1 : <em>n</em>/3]. (It does not matter what relative order they have within the first <em>n</em>/3 positions.) Once the array has been sorted, each of these <em>n</em>/3 values ends up somewhere in the last <em>n</em>/3 positions <em>A</em>[2<em>n</em>/3 + 1 : <em>n</em>]. For that to happen, each of these <em>n</em>/3 values must pass through each of the middle <em>n</em>/3 positions <em>A</em>[<em>n</em>/3 + 1 : 2<em>n</em>/3]. Each of these <em>n</em>/3 values passes through these middle <a id="p53"/><em>n</em>/3 positions one position at a time, by at least <em>n</em>/3 executions of line 6. Because at least <em>n</em>/3 values have to pass through at least <em>n</em>/3 positions, the time taken by I<small>NSERTION</small>-S<small>ORT</small> in the worst case is at least proportional to (<em>n</em>/3)(<em>n</em>/3) = <em>n</em><sup>2</sup>/9, which is Ω(<em>n</em><sup>2</sup>).</p>
<p>Because we have shown that I<small>NSERTION</small>-S<small>ORT</small> runs in <em>O</em>(<em>n</em><sup>2</sup>) time in all cases and that there is an input that makes it take Ω(<em>n</em><sup>2</sup>) time, we can conclude that the worst-case running time of I<small>NSERTION</small>-S<small>ORT</small> is Θ(<em>n</em><sup>2</sup>). It does not matter that the constant factors for upper and lower bounds might differ. What matters is that we have characterized the worst-case running time to within constant factors (discounting lower-order terms). This argument does not show that I<small>NSERTION</small>-S<small>ORT</small> runs in Θ(<em>n</em><sup>2</sup>) time in <em>all</em> cases. Indeed, we saw in <a href="chapter002.xhtml">Chapter 2</a> that the best-case running time is Θ(<em>n</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>3.1-1</em></strong></p>
<p class="noindent">Modify the lower-bound argument for insertion sort to handle input sizes that are not necessarily a multiple of 3.</p>
<p class="level3"><strong><em>3.1-2</em></strong></p>
<p class="noindent">Using reasoning similar to what we used for insertion sort, analyze the running time of the selection sort algorithm from Exercise 2.2-2.</p>
<p class="level3"><strong><em>3.1-3</em></strong></p>
<p class="noindent">Suppose that <em>α</em> is a fraction in the range 0 &lt; <em>α</em> &lt; 1. Show how to generalize the lower-bound argument for insertion sort to consider an input in which the <em>αn</em> largest values start in the first <em>αn</em> positions. What additional restriction do you need to put on <em>α</em>? What value of <em>α</em> maximizes the number of times that the <em>αn</em> largest values must pass through each of the middle (1 – 2<em>α</em>)<em>n</em> array positions?</p>
</section>
<p class="line1"/>
<section title="3.2 Asymptotic notation: formal definitions">
<a id="Sec_3.2"/>
<p class="level1" id="h1-12"><a href="toc.xhtml#Rh1-12"><strong>3.2      Asymptotic notation: formal definitions</strong></a></p>
<p class="noindent">Having seen asymptotic notation informally, let’s get more formal. The notations we use to describe the asymptotic running time of an algorithm are defined in terms of functions whose domains are typically the set <span class="struck">N</span> of natural numbers or the set <span class="struck">R</span> of real numbers. Such notations are convenient for describing a running-time function <em>T</em> (<em>n</em>). This section defines the basic asymptotic notations and also introduces some common “proper” notational abuses.</p>
<a id="p54"/>
<div class="divimage">
<p class="fig-imga" id="Fig_3-2"><img alt="art" class="width100" src="images/Art_P35.jpg"/></p>
<p class="caption"><strong>Figure 3.2</strong> Graphic examples of the <em>O</em>, Ω, and Θ notations. In each part, the value of <em>n</em><sub>0</sub> shown is the minimum possible value, but any greater value also works. <strong>(a)</strong> <em>O</em>-notation gives an upper bound for a function to within a constant factor. We write <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) if there are positive constants <em>n</em><sub>0</sub> and <em>c</em> such that at and to the right of <em>n</em><sub>0</sub>, the value of <em>f</em> (<em>n</em>) always lies on or below <em>cg</em>(<em>n</em>). <strong>(b)</strong> Ω-notation gives a lower bound for a function to within a constant factor. We write <em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) if there are positive constants <em>n</em><sub>0</sub> and <em>c</em> such that at and to the right of <em>n</em><sub>0</sub>, the value of <em>f</em> (<em>n</em>) always lies on or above <em>cg</em>(<em>n</em>). <strong>(c)</strong> Θ-notation bounds a function to within constant factors. We write <em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) if there exist positive constants <em>n</em><sub>0</sub>, <em>c</em><sub>1</sub>, and <em>c</em><sub>2</sub> such that at and to the right of <em>n</em><sub>0</sub>, the value of <em>f</em> (<em>n</em>) always lies between <em>c</em><sub>1</sub><em>g</em>(<em>n</em>) and <em>c</em><sub>2</sub><em>g</em>(<em>n</em>) inclusive.</p>
</div>
<p class="level4"><strong><em>O</em>-notation</strong></p>
<p class="noindent">As we saw in <a href="chapter003.xhtml#Sec_3.1">Section 3.1</a>, <em>O</em>-notation describes an <strong><em><span class="blue1">asymptotic upper bound</span></em></strong>. We use <em>O</em>-notation to give an upper bound on a function, to within a constant factor.</p>
<p>Here is the formal definition of <em>O</em>-notation. For a given function <em>g</em>(<em>n</em>), we denote by <em>O</em>(<em>g</em>(<em>n</em>)) (pronounced “big-oh of <em>g</em> of <em>n</em>” or sometimes just “oh of <em>g</em> of <em>n</em>”) the <em>set of functions</em></p>
<table class="table2">
<tr>
<td class="td1"><em>O</em>(<em>g</em>(<em>n</em>)) = {<em>f</em> (<em>n</em>)</td>
<td class="td1"> : </td>
<td class="td1">there exist positive constants <em>c</em> and <em>n</em><sub>0</sub> such that 0 ≤ <em>f</em> (<em>n</em>) ≤ <em>cg</em>(<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup></td>
</tr>
</table>
<p class="noindent">A function <em>f</em> (<em>n</em>) belongs to the set <em>O</em>(<em>g</em>(<em>n</em>)) if there exists a positive constant <em>c</em> such that <em>f</em> (<em>n</em>) ≤ <em>cg</em>(<em>n</em>) for sufficiently large <em>n</em>. <a href="chapter003.xhtml#Fig_3-2">Figure 3.2(a)</a> shows the intuition behind <em>O</em>-notation. For all values <em>n</em> at and to the right of <em>n</em><sub>0</sub>, the value of the function <em>f</em> (<em>n</em>) is on or below <em>cg</em>(<em>n</em>).</p>
<p>The definition of <em>O</em>(<em>g</em>(<em>n</em>)) requires that every function <em>f</em> (<em>n</em>) in the set <em>O</em>(<em>g</em>(<em>n</em>)) be <strong><em><span class="blue1">asymptotically nonnegative</span></em></strong>: <em>f</em> (<em>n</em>) must be nonnegative whenever <em>n</em> is sufficiently large. (An <strong><em><span class="blue1">asymptotically positive</span></em></strong> function is one that is positive for all <a id="p55"/>sufficiently large <em>n</em>.) Consequently, the function <em>g</em>(<em>n</em>) itself must be asymptotically nonnegative, or else the set <em>O</em>(<em>g</em>(<em>n</em>)) is empty. We therefore assume that every function used within <em>O</em>-notation is asymptotically nonnegative. This assumption holds for the other asymptotic notations defined in this chapter as well.</p>
<p>You might be surprised that we define <em>O</em>-notation in terms of sets. Indeed, you might expect that we would write “<em>f</em> (<em>n</em>) ∈ <em>O</em>(<em>g</em>(<em>n</em>))” to indicate that <em>f</em> (<em>n</em>) belongs to the set <em>O</em>(<em>g</em>(<em>n</em>)). Instead, we usually write “<em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>))” and say “<em>f</em> (<em>n</em>) is big-oh of <em>g</em>(<em>n</em>)” to express the same notion. Although it may seem confusing at first to abuse equality in this way, we’ll see later in this section that doing so has its advantages.</p>
<p>Let’s explore an example of how to use the formal definition of <em>O</em>-notation to justify our practice of discarding lower-order terms and ignoring the constant coefficient of the highest-order term. We’ll show that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 = <em>O</em>(<em>n</em><sup>2</sup>), even though the lower-order terms have much larger coefficients than the leading term. We need to find positive constants <em>c</em> and <em>n</em><sub>0</sub> such that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 ≤ <em>cn</em><sup>2</sup> for all <em>n</em> ≥ <em>n</em><sub>0</sub>. Dividing both sides by <em>n</em><sup>2</sup> gives 4 + 100/<em>n</em> + 500/<em>n</em><sup>2</sup> ≤ <em>c</em>. This inequality is satisfied for many choices of <em>c</em> and <em>n</em><sub>0</sub>. For example, if we choose <em>n</em><sub>0</sub> = 1, then this inequality holds for <em>c</em> = 604. If we choose <em>n</em><sub>0</sub> = 10, then <em>c</em> = 19 works, and choosing <em>n</em><sub>0</sub> = 100 allows us to use <em>c</em> = 5.05.</p>
<p>We can also use the formal definition of <em>O</em>-notation to show that the function <em>n</em><sup>3</sup> – 100<em>n</em><sup>2</sup> does not belong to the set <em>O</em>(<em>n</em><sup>2</sup>), even though the coefficient of <em>n</em><sup>2</sup> is a large negative number. If we had <em>n</em><sup>3</sup> – 100<em>n</em><sup>2</sup> = <em>O</em>(<em>n</em><sup>2</sup>), then there would be positive constants <em>c</em> and <em>n</em><sub>0</sub> such that <em>n</em><sup>3</sup> –100<em>n</em><sup>2</sup> ≤ <em>cn</em><sup>2</sup> for all <em>n</em> ≥ <em>n</em><sub>0</sub>. Again, we divide both sides by <em>n</em><sup>2</sup>, giving <em>n</em> – 100 ≤ <em>c</em>. Regardless of what value we choose for the constant <em>c</em>, this inequality does not hold for any value of <em>n</em> &gt; <em>c</em> + 100.</p>
<p class="level4"><strong>Ω-notation</strong></p>
<p class="noindent">Just as <em>O</em>-notation provides an asymptotic <em>upper</em> bound on a function, Ω-notation provides an <strong><em><span class="blue1">asymptotic lower bound</span></em></strong>. For a given function <em>g</em>(<em>n</em>), we denote by Ω(<em>g</em>(<em>n</em>)) (pronounced “big-omega of <em>g</em> of <em>n</em>” or sometimes just “omega of <em>g</em> of <em>n</em>”) the set of functions</p>
<table class="table2">
<tr>
<td class="td1">Ω(<em>g</em>(<em>n</em>)) = {<em>f</em> (<em>n</em>)</td>
<td class="td1"> : </td>
<td class="td1">there exist positive constants <em>c</em> and <em>n</em><sub>0</sub> such that 0 ≤ <em>cg</em>(<em>n</em>) ≤ <em>f</em> (<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="noindent"><a href="chapter003.xhtml#Fig_3-2">Figure 3.2(b)</a> shows the intuition behind Ω-notation. For all values <em>n</em> at or to the right of <em>n</em><sub>0</sub>, the value of <em>f</em> (<em>n</em>) is on or above <em>cg</em>(<em>n</em>).</p>
<p>We’ve already shown that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 = <em>O</em>(<em>n</em><sup>2</sup>). Now let’s show that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 = Ω(<em>n</em><sup>2</sup>). We need to find positive constants <em>c</em> and <em>n</em><sub>0</sub> such that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 ≥ <em>cn</em><sup>2</sup> for all <em>n</em> ≥ <em>n</em><sub>0</sub>. As before, we divide both sides by <em>n</em><sup>2</sup>, <a id="p56"/>giving 4 + 100/<em>n</em> + 500/<em>n</em><sup>2</sup> ≥ <em>c</em>. This inequality holds when <em>n</em><sub>0</sub> is any positive integer and <em>c</em> = 4.</p>
<p>What if we had subtracted the lower-order terms from the 4<em>n</em><sup>2</sup> term instead of adding them? What if we had a small coefficient for the <em>n</em><sup>2</sup> term? The function would still be Ω(<em>n</em><sup>2</sup>). For example, let’s show that <em>n</em><sup>2</sup>/100 – 100<em>n</em> – 500 = Ω(<em>n</em><sup>2</sup>). Dividing by <em>n</em><sup>2</sup> gives 1/100 – 100/<em>n</em> – 500/<em>n</em><sup>2</sup> ≥ <em>c</em>. We can choose any value for <em>n</em><sub>0</sub> that is at least 10,005 and find a positive value for <em>c</em>. For example, when <em>n</em><sub>0</sub> = 10,005, we can choose <em>c</em> = 2.49 × 10<sup>–9</sup>. Yes, that’s a tiny value for <em>c</em>, but it is positive. If we select a larger value for <em>n</em><sub>0</sub>, we can also increase <em>c</em>. For example, if <em>n</em><sub>0</sub> = 100,000, then we can choose <em>c</em> = 0.0089. The higher the value of <em>n</em><sub>0</sub>, the closer to the coefficient 1/100 we can choose <em>c</em>.</p>
<p class="level4"><strong>Θ-notation</strong></p>
<p class="noindent">We use Θ-notation for <strong><em><span class="blue1">asymptotically tight bounds</span></em></strong>. For a given function <em>g</em>(<em>n</em>), we denote by Θ(<em>g</em>(<em>n</em>)) (“theta of <em>g</em> of <em>n</em>”) the set of functions</p>
<table class="table2">
<tr>
<td class="td1">Θ(<em>g</em>(<em>n</em>)) = {<em>f</em> (<em>n</em>)</td>
<td class="td1"> : </td>
<td class="td1">there exist positive constants <em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, and <em>n</em><sub>0</sub> such that 0 ≤ <em>c</em><sub>1</sub><em>g</em>(<em>n</em>) ≤ <em>f</em> (<em>n</em>) ≤ <em>c</em><sub>2</sub><em>g</em>(<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="noindent"><a href="chapter003.xhtml#Fig_3-2">Figure 3.2(c)</a> shows the intuition behind Θ-notation. For all values of <em>n</em> at and to the right of <em>n</em><sub>0</sub>, the value of <em>f</em> (<em>n</em>) lies at or above <em>c</em><sub>1</sub><em>g</em>(<em>n</em>) and at or below <em>c</em><sub>2</sub><em>g</em>(<em>n</em>). In other words, for all <em>n</em> ≥ <em>n</em><sub>0</sub>, the function <em>f</em> (<em>n</em>) is equal to <em>g</em>(<em>n</em>) to within constant factors.</p>
<p>The definitions of <em>O</em>-, Ω-, and Θ-notations lead to the following theorem, whose proof we leave as Exercise 3.2-4.</p>
<p class="theo"><strong><em>Theorem 3.1</em></strong></p>
<p class="noindent">For any two functions <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>), we have <em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) if and only if <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) and <em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>)).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="noindent1-top">We typically apply Theorem 3.1 to prove asymptotically tight bounds from asymptotic upper and lower bounds.</p>
<p class="level4"><strong>Asymptotic notation and running times</strong></p>
<p class="noindent">When you use asymptotic notation to characterize an algorithm’s running time, make sure that the asymptotic notation you use is as precise as possible without overstating which running time it applies to. Here are some examples of using asymptotic notation properly and improperly to characterize running times.</p>
<p>Let’s start with insertion sort. We can correctly say that insertion sort’s worst-case running time is <em>O</em>(<em>n</em><sup>2</sup>), Ω(<em>n</em><sup>2</sup>), and—due to Theorem 3.1—Θ(<em>n</em><sup>2</sup>). Although <a id="p57"/>all three ways to characterize the worst-case running times are correct, the Θ(<em>n</em><sup>2</sup>) bound is the most precise and hence the most preferred. We can also correctly say that insertion sort’s best-case running time is <em>O</em>(<em>n</em>), Ω(<em>n</em>), and Θ(<em>n</em>), again with Θ(<em>n</em>) the most precise and therefore the most preferred.</p>
<p>Here is what we <em>cannot</em> correctly say: insertion sort’s running time is Θ(<em>n</em><sup>2</sup>). That is an overstatement because by omitting “worst-case” from the statement, we’re left with a blanket statement covering all cases. The error here is that insertion sort does not run in Θ(<em>n</em><sup>2</sup>) time in all cases since, as we’ve seen, it runs in Θ(<em>n</em>) time in the best case. We can correctly say that insertion sort’s running time is <em>O</em>(<em>n</em><sup>2</sup>), however, because in all cases, its running time grows no faster than <em>n</em><sup>2</sup>. When we say <em>O</em>(<em>n</em><sup>2</sup>) instead of Θ(<em>n</em><sup>2</sup>), there is no problem in having cases whose running time grows more slowly than <em>n</em><sup>2</sup>. Likewise, we cannot correctly say that insertion sort’s running time is Θ(<em>n</em>), but we can say that its running time is Ω(<em>n</em>).</p>
<p>How about merge sort? Since merge sort runs in Θ(<em>n</em> lg <em>n</em>) time in all cases, we can just say that its running time is Θ(<em>n</em> lg <em>n</em>) without specifying worst-case, best-case, or any other case.</p>
<p>People occasionally conflate <em>O</em>-notation with Θ-notation by mistakenly using <em>O</em>-notation to indicate an asymptotically tight bound. They say things like “an <em>O</em>(<em>n</em> lg <em>n</em>)-time algorithm runs faster than an <em>O</em>(<em>n</em><sup>2</sup>)-time algorithm.” Maybe it does, maybe it doesn’t. Since <em>O</em>-notation denotes only an asymptotic upper bound, that so-called <em>O</em>(<em>n</em><sup>2</sup>)-time algorithm might actually run in Θ(<em>n</em>) time. You should be careful to choose the appropriate asymptotic notation. If you want to indicate an asymptotically tight bound, use Θ-notation.</p>
<p>We typically use asymptotic notation to provide the simplest and most precise bounds possible. For example, if an algorithm has a running time of 3<em>n</em><sup>2</sup> + 20<em>n</em> in all cases, we use asymptotic notation to write that its running time is Θ(<em>n</em><sup>2</sup>). Strictly speaking, we are also correct in writing that the running time is <em>O</em>(<em>n</em><sup>3</sup>) or Θ(3<em>n</em><sup>2</sup> + 20<em>n</em>). Neither of these expressions is as useful as writing Θ(<em>n</em><sup>2</sup>) in this case, however: <em>O</em>(<em>n</em><sup>3</sup>) is less precise than Θ(<em>n</em><sup>2</sup>) if the running time is 3<em>n</em><sup>2</sup> + 20<em>n</em>, and Θ(3<em>n</em><sup>2</sup> + 20<em>n</em>) introduces complexity that obscures the order of growth. By writing the simplest and most precise bound, such as Θ(<em>n</em><sup>2</sup>), we can categorize and compare different algorithms. Throughout the book, you will see asymptotic running times that are almost always based on polynomials and logarithms: functions such as <em>n</em>, <em>n</em> lg<sup>2</sup> <em>n</em>, <em>n</em><sup>2</sup> lg <em>n</em>, or <em>n</em><sup>1/2</sup>. You will also see some other functions, such as exponentials, lg lg <em>n</em>, and lg<sup>*</sup><em>n</em> (see <a href="chapter003.xhtml#Sec_3.3">Section 3.3</a>). It is usually fairly easy to compare the rates of growth of these functions. Problem 3-3 gives you good practice.</p>
<a id="p58"/>
<p class="level4"><strong>Asymptotic notation in equations and inequalities</strong></p>
<p class="noindent">Although we formally define asymptotic notation in terms of sets, we use the equal sign (=) instead of the set membership sign (∈) within formulas. For example, we wrote that 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 = <em>O</em>(<em>n</em><sup>2</sup>). We might also write 2<em>n</em><sup>2</sup> + 3<em>n</em> + 1 = 2<em>n</em><sup>2</sup> + Θ(<em>n</em>). How do we interpret such formulas?</p>
<p>When the asymptotic notation stands alone (that is, not within a larger formula) on the right-hand side of an equation (or inequality), as in 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 = <em>O</em>(<em>n</em><sup>2</sup>), the equal sign means set membership: 4<em>n</em><sup>2</sup> + 100<em>n</em> + 500 ∈ <em>O</em>(<em>n</em><sup>2</sup>). In general, however, when asymptotic notation appears in a formula, we interpret it as standing for some anonymous function that we do not care to name. For example, the formula 2<em>n</em><sup>2</sup> + 3<em>n</em> + 1 = 2<em>n</em><sup>2</sup> + Θ(<em>n</em>) means that 2<em>n</em><sup>2</sup> + 3<em>n</em> + 1 = 2<em>n</em><sup>2</sup> + <em>f</em> (<em>n</em>), where <em>f</em> (<em>n</em>) ∈ Θ(<em>n</em>). In this case, we let <em>f</em> (<em>n</em>) = 3<em>n</em> + 1, which indeed belongs to Θ(<em>n</em>).</p>
<p>Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation. For example, in <a href="chapter002.xhtml">Chapter 2</a> we expressed the worst-case running time of merge sort as the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = 2<em>T</em> (<em>n</em>/2) + Θ(<em>n</em>).</p>
<p class="noindent1-top">If we are interested only in the asymptotic behavior of <em>T</em> (<em>n</em>), there is no point in specifying all the lower-order terms exactly, because they are all understood to be included in the anonymous function denoted by the term Θ(<em>n</em>).</p>
<p>The number of anonymous functions in an expression is understood to be equal to the number of times the asymptotic notation appears. For example, in the expression</p>
<p class="eql"><img alt="art" src="images/Art_P36.jpg"/></p>
<p class="noindent">there is only a single anonymous function (a function of <em>i</em>). This expression is thus <em>not</em> the same as <em>O</em>(1) + <em>O</em>(2) + <span class="font1">⋯</span> + <em>O</em>(<em>n</em>), which doesn’t really have a clean interpretation.</p>
<p>In some cases, asymptotic notation appears on the left-hand side of an equation, as in</p>
<p class="eql">2<em>n</em><sup>2</sup> + Θ(<em>n</em>) = Θ(<em>n</em><sup>2</sup>).</p>
<p class="noindent">Interpret such equations using the following rule: <em>No matter how the anonymous functions are chosen on the left of the equal sign, there is a way to choose the anonymous functions on the right of the equal sign to make the equation valid</em>. Thus, our example means that for <em>any</em> function <em>f</em> (<em>n</em>) ∈ Θ(<em>n</em>), there is <em>some</em> function <em>g</em>(<em>n</em>) ∈ Θ(<em>n</em><sup>2</sup>) such that 2<em>n</em><sup>2</sup> + <em>f</em> (<em>n</em>) = <em>g</em>(<em>n</em>) for all <em>n</em>. In other words, the right-hand side of an equation provides a coarser level of detail than the left-hand side.</p>
<a id="p59"/>
<p>We can chain together a number of such relationships, as in</p>
<table class="table1a">
<tr>
<td class="td1">2<em>n</em><sup>2</sup> + 3<em>n</em> + 1</td>
<td class="td1m"> = </td>
<td class="td1">2<em>n</em><sup>2</sup> + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td1"/>
<td class="td1m"> = </td>
<td class="td1">Θ(<em>n</em><sup>2</sup>).</td>
</tr>
</table>
<p class="noindent">By the rules above, interpret each equation separately. The first equation says that there is <em>some</em> function <em>f</em> (<em>n</em>) ∈ Θ(<em>n</em>) such that 2<em>n</em><sup>2</sup> + 3<em>n</em> + 1 = 2<em>n</em><sup>2</sup> + <em>f</em> (<em>n</em>) for all <em>n</em>. The second equation says that for <em>any</em> function <em>g</em>(<em>n</em>) ∈ Θ(<em>n</em>) (such as the <em>f</em> (<em>n</em>) just mentioned), there is <em>some</em> function <em>h</em>(<em>n</em>) ∈ Θ(<em>n</em><sup>2</sup>) such that 2<em>n</em><sup>2</sup> + <em>g</em>(<em>n</em>) = <em>h</em>(<em>n</em>) for all <em>n</em>. This interpretation implies that 2<em>n</em><sup>2</sup> + 3<em>n</em> + 1 = Θ(<em>n</em><sup>2</sup>), which is what the chaining of equations intuitively says.</p>
<p class="level4"><strong>Proper abuses of asymptotic notation</strong></p>
<p class="noindent">Besides the abuse of equality to mean set membership, which we now see has a precise mathematical interpretation, another abuse of asymptotic notation occurs when the variable tending toward ∞ must be inferred from context. For example, when we say <em>O</em>(<em>g</em>(<em>n</em>)), we can assume that we’re interested in the growth of <em>g</em>(<em>n</em>) as <em>n</em> grows, and if we say <em>O</em>(<em>g</em>(<em>m</em>)) we’re talking about the growth of <em>g</em>(<em>m</em>) as <em>m</em> grows. The free variable in the expression indicates what variable is going to ∞.</p>
<p>The most common situation requiring contextual knowledge of which variable tends to ∞ occurs when the function inside the asymptotic notation is a constant, as in the expression <em>O</em>(1). We cannot infer from the expression which variable is going to ∞, because no variable appears there. The context must disambiguate. For example, if the equation using asymptotic notation is <em>f</em> (<em>n</em>) = <em>O</em>(1), it’s apparent that the variable we’re interested in is <em>n</em>. Knowing from context that the variable of interest is <em>n</em>, however, allows us to make perfect sense of the expression by using the formal definition of <em>O</em>-notation: the expression <em>f</em> (<em>n</em>) = <em>O</em>(1) means that the function <em>f</em> (<em>n</em>) is bounded from above by a constant as <em>n</em> goes to ∞. Technically, it might be less ambiguous if we explicitly indicated the variable tending to ∞ in the asymptotic notation itself, but that would clutter the notation. Instead, we simply ensure that the context makes it clear which variable (or variables) tend to ∞.</p>
<p>When the function inside the asymptotic notation is bounded by a positive constant, as in <em>T</em> (<em>n</em>) = <em>O</em>(1), we often abuse asymptotic notation in yet another way, especially when stating recurrences. We may write something like <em>T</em> (<em>n</em>) = <em>O</em>(1) for <em>n</em> &lt; 3. According to the formal definition of <em>O</em>-notation, this statement is meaningless, because the definition only says that <em>T</em> (<em>n</em>) is bounded above by a positive constant <em>c</em> for <em>n</em> ≥ <em>n</em><sub>0</sub> for some <em>n</em><sub>0</sub> &gt; 0. The value of <em>T</em> (<em>n</em>) for <em>n</em> &lt; <em>n</em><sub>0</sub> need not be so bounded. Thus, in the example <em>T</em> (<em>n</em>) = <em>O</em>(1) for <em>n</em> &lt; 3, we cannot infer any constraint on <em>T</em> (<em>n</em>) when <em>n</em> &lt; 3, because it might be that <em>n</em><sub>0</sub> &gt; 3.</p>
<p>What is conventionally meant when we say <em>T</em> (<em>n</em>) = <em>O</em>(1) for <em>n</em> &lt; 3 is that there exists a positive constant <em>c</em> such that <em>T</em> (<em>n</em>) ≤ <em>c</em> for <em>n</em> &lt; 3. This convention saves <a id="p60"/>us the trouble of naming the bounding constant, allowing it to remain anonymous while we focus on more important variables in an analysis. Similar abuses occur with the other asymptotic notations. For example, <em>T</em> (<em>n</em>) = Θ(1) for <em>n</em> &lt; 3 means that <em>T</em> (<em>n</em>) is bounded above and below by positive constants when <em>n</em> &lt; 3.</p>
<p>Occasionally, the function describing an algorithm’s running time may not be defined for certain input sizes, for example, when an algorithm assumes that the input size is an exact power of 2. We still use asymptotic notation to describe the growth of the running time, understanding that any constraints apply only when the function is defined. For example, suppose that <em>f</em> (<em>n</em>) is defined only on a subset of the natural or nonnegative real numbers. Then <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) means that the bound 0 ≤ <em>T</em> (<em>n</em>) ≤ <em>cg</em>(<em>n</em>) in the definition of <em>O</em>-notation holds for all <em>n</em> ≥ <em>n</em><sub>0</sub> over the domain of <em>f</em> (<em>n</em>), that is, where <em>f</em> (<em>n</em>) is defined. This abuse is rarely pointed out, since what is meant is generally clear from context.</p>
<p>In mathematics, it’s okay — and often desirable — to abuse a notation, as long as we don’t misuse it. If we understand precisely what is meant by the abuse and don’t draw incorrect conclusions, it can simplify our mathematical language, contribute to our higher-level understanding, and help us focus on what really matters.</p>
<p class="level4"><strong><em>o</em>-notation</strong></p>
<p class="noindent">The asymptotic upper bound provided by <em>O</em>-notation may or may not be asymptotically tight. The bound 2<em>n</em><sup>2</sup> = <em>O</em>(<em>n</em><sup>2</sup>) is asymptotically tight, but the bound 2<em>n</em> = <em>O</em>(<em>n</em><sup>2</sup>) is not. We use <em>o</em>-notation to denote an upper bound that is not asymptotically tight. We formally define <em>o</em>(<em>g</em>(<em>n</em>)) (“little-oh of <em>g</em> of <em>n</em>”) as the set</p>
<table class="table1a">
<tr>
<td class="td1"><em>o</em>(<em>g</em>(<em>n</em>)) = {<em>f</em> (<em>n</em>)</td>
<td class="td1"> : </td>
<td class="td1">for any positive constant <em>c</em> &gt; 0, there exists a constant <em>n</em><sub>0</sub> &gt; 0 such that 0 ≤ <em>f</em> (<em>n</em>) &lt; <em>cg</em>(<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="noindent">For example, 2<em>n</em> = <em>o</em>(<em>n</em><sup>2</sup>), but 2<em>n</em><sup>2</sup> ≠ <em>o</em>(<em>n</em><sup>2</sup>).</p>
<p>The definitions of <em>O</em>-notation and <em>o</em>-notation are similar. The main difference is that in <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)), the bound 0 ≤ <em>f</em> (<em>n</em>) ≤ <em>cg</em>(<em>n</em>) holds for <em>some</em> constant <em>c</em> &gt; 0, but in <em>f</em> (<em>n</em>) = <em>o</em>(<em>g</em>(<em>n</em>)), the bound 0 ≤ <em>f</em> (<em>n</em>) &lt; <em>cg</em>(<em>n</em>) holds for <em>all</em> constants <em>c</em> &gt; 0. Intuitively, in <em>o</em>-notation, the function <em>f</em> (<em>n</em>) becomes insignificant relative to <em>g</em>(<em>n</em>) as <em>n</em> gets large:</p>
<p class="eql"><img alt="art" src="images/Art_P37.jpg"/></p>
<p class="noindent">Some authors use this limit as a definition of the <em>o</em>-notation, but the definition in this book also restricts the anonymous functions to be asymptotically nonnegative.</p>
<a id="p61"/>
<p class="level4"><strong><em>ω</em>-notation</strong></p>
<p class="noindent">By analogy, <em>ω</em>-notation is to Ω-notation as <em>o</em>-notation is to <em>O</em>-notation. We use <em>ω</em>-notation to denote a lower bound that is not asymptotically tight. One way to define it is by</p>
<p class="eql"><em>f</em> (<em>n</em>) ∈ <em>ω</em>(<em>g</em>(<em>n</em>)) if and only if <em>g</em>(<em>n</em>) ∈ <em>o</em>(<em>f</em> (<em>n</em>)).</p>
<p class="noindent1-top">Formally, however, we define <em>ω</em>(<em>g</em>(<em>n</em>)) (“little-omega of <em>g</em> of <em>n</em>”) as the set</p>
<table class="table1a">
<tr>
<td class="td1"><em>ω</em>(<em>g</em>(<em>n</em>)) = {<em>f</em> (<em>n</em>)</td>
<td class="td1"> : </td>
<td class="td1">for any positive constant <em>c</em> &gt; 0, there exists a constant <em>n</em><sub>0</sub> &gt; 0 such that 0 ≤ <em>cg</em>(<em>n</em>) &lt; <em>f</em> (<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="noindent">Where the definition of <em>o</em>-notation says that <em>f</em> (<em>n</em>) &lt; <em>cg</em>(<em>n</em>), the definition of <em>ω</em>-notation says the opposite: that <em>cg</em>(<em>n</em>) &lt; <em>f</em> (<em>n</em>). For examples of <em>ω</em>-notation, we have <em>n</em><sup>2</sup>/2 = <em>ω</em>(<em>n</em>), but <em>n</em><sup>2</sup>/2 ≠ <em>ω</em>(<em>n</em><sup>2</sup>). The relation <em>f</em> (<em>n</em>) = <em>ω</em>(<em>g</em>(<em>n</em>)) implies that</p>
<p class="eql"><img alt="art" src="images/Art_P38.jpg"/></p>
<p class="noindent">if the limit exists. That is, <em>f</em> (<em>n</em>) becomes arbitrarily large relative to <em>g</em>(<em>n</em>) as <em>n</em> gets large.</p>
<p class="level4"><strong>Comparing functions</strong></p>
<p class="noindent">Many of the relational properties of real numbers apply to asymptotic comparisons as well. For the following, assume that <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) are asymptotically positive.</p>
<p class="level4"><strong>Transitivity:</strong></p>
<table class="table2a1">
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>))</td>
<td class="td2">and</td>
<td class="td2"><em>g</em>(<em>n</em>) = Θ(<em>h</em>(<em>n</em>))</td>
<td class="td2">imply</td>
<td class="td2"><em>f</em> (<em>n</em>) = Θ(<em>h</em>(<em>n</em>)),</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">and</td>
<td class="td2"><em>g</em>(<em>n</em>) = <em>O</em>(<em>h</em>(<em>n</em>))</td>
<td class="td2">imply</td>
<td class="td2"><em>f</em> (<em>n</em>) = <em>O</em>(<em>h</em>(<em>n</em>)),</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>))</td>
<td class="td2">and</td>
<td class="td2"><em>g</em>(<em>n</em>) = Ω(<em>h</em>(<em>n</em>))</td>
<td class="td2">imply</td>
<td class="td2"><em>f</em> (<em>n</em>) = Ω(<em>h</em>(<em>n</em>)),</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>o</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">and</td>
<td class="td2"><em>g</em>(<em>n</em>) = <em>o</em>(<em>h</em>(<em>n</em>))</td>
<td class="td2">imply</td>
<td class="td2"><em>f</em> (<em>n</em>) = <em>o</em>(<em>h</em>(<em>n</em>)),</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>ω</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">and</td>
<td class="td2"><em>g</em>(<em>n</em>) = <em>ω</em>(<em>h</em>(<em>n</em>))</td>
<td class="td2">imply</td>
<td class="td2"><em>f</em> (<em>n</em>) = <em>ω</em>(<em>h</em>(<em>n</em>)).</td>
</tr>
</table>
<p class="level4"><strong>Reflexivity:</strong></p>
<table class="table2a">
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = Θ(<em>f</em> (<em>n</em>)),</td>
</tr>
<tr>
<td class="td1"><em>f</em> (<em>n</em>) = <em>O</em>(<em>f</em> (<em>n</em>)),</td>
</tr>
<tr>
<td class="td1"><em>f</em> (<em>n</em>) = Ω(<em>f</em> (<em>n</em>)).</td>
</tr>
</table>
<p class="level4"><strong>Symmetry:</strong></p>
<p class="eqi"><em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) if and only if <em>g</em>(<em>n</em>) = Θ(<em>f</em> (<em>n</em>)).</p>
<a id="p62"/>
<p class="level4"><strong>Transpose symmetry:</strong></p>
<table class="table2a">
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">if and only if</td>
<td class="td2"><em>g</em>(<em>n</em>) = Ω(<em>f</em> (<em>n</em>)),</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>o</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">if and only if</td>
<td class="td2"><em>g</em>(<em>n</em>) = <em>ω</em>(<em>f</em> (<em>n</em>)).</td>
</tr>
</table>
<p>Because these properties hold for asymptotic notations, we can draw an analogy between the asymptotic comparison of two functions <em>f</em> and <em>g</em> and the comparison of two real numbers <em>a</em> and <em>b</em>:</p>
<table class="table2b">
<tr>
<td class="td2w"><em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">is like</td>
<td class="td2"><em>a</em> ≤ <em>b</em>,</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>))</td>
<td class="td2">is like</td>
<td class="td2"><em>a</em> ≥ <em>b</em>,</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>))</td>
<td class="td2">is like</td>
<td class="td2"><em>a</em> = <em>b</em>,</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>o</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">is like</td>
<td class="td2"><em>a</em> &lt; <em>b</em>,</td>
</tr>
<tr>
<td class="td2"><em>f</em> (<em>n</em>) = <em>ω</em>(<em>g</em>(<em>n</em>))</td>
<td class="td2">is like</td>
<td class="td2"><em>a</em> &gt; <em>b</em>.</td>
</tr>
</table>
<p class="noindent">We say that <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">asymptotically smaller</span></em></strong> than <em>g</em>(<em>n</em>) if <em>f</em> (<em>n</em>) = <em>o</em>(<em>g</em>(<em>n</em>)), and <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">asymptotically larger</span></em></strong> than <em>g</em>(<em>n</em>) if <em>f</em> (<em>n</em>) = <em>ω</em>(<em>g</em>(<em>n</em>)).</p>
<p>One property of real numbers, however, does not carry over to asymptotic notation:</p>
<p class="para-hang1"><strong>Trichotomy:</strong> For any two real numbers <em>a</em> and <em>b</em>, exactly one of the following must hold: <em>a</em> &lt; <em>b</em>, <em>a</em> = <em>b</em>, or <em>a</em> &gt; <em>b</em>.</p>
<p class="noindent">Although any two real numbers can be compared, not all functions are asymptotically comparable. That is, for two functions <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>), it may be the case that neither <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) nor <em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) holds. For example, we cannot compare the functions <em>n</em> and <em>n</em><sup>1 + sin <em>n</em></sup> using asymptotic notation, since the value of the exponent in <em>n</em><sup>1 + sin <em>n</em></sup> oscillates between 0 and 2, taking on all values in between.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>3.2-1</em></strong></p>
<p class="noindent">Let <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) be asymptotically nonnegative functions. Using the basic definition of Θ-notation, prove that max {<em>f</em> (<em>n</em>), <em>g</em>(<em>n</em>)} = Θ(<em>f</em> (<em>n</em>) + <em>g</em>(<em>n</em>)).</p>
<p class="level3"><strong><em>3.2-2</em></strong></p>
<p class="noindent">Explain why the statement, “The running time of algorithm <em>A</em> is at least <em>O</em>(<em>n</em><sup>2</sup>),” is meaningless.</p>
<p class="level3"><strong><em>3.2-3</em></strong></p>
<p class="noindent">Is 2<sup><em>n</em> + 1</sup> = <em>O</em>(2<em><sup>n</sup></em>)? Is 2<sup>2<em>n</em></sup> = <em>O</em>(2<em><sup>n</sup></em>)?</p>
<p class="level3"><strong><em>3.2-4</em></strong></p>
<p class="noindent">Prove Theorem 3.1.</p>
<a id="p63"/>
<p class="level3"><strong><em>3.2-5</em></strong></p>
<p class="noindent">Prove that the running time of an algorithm is Θ(<em>g</em>(<em>n</em>)) if and only if its worst-case running time is <em>O</em>(<em>g</em>(<em>n</em>)) and its best-case running time is Ω(<em>g</em>(<em>n</em>)).</p>
<p class="level3"><strong><em>3.2-6</em></strong></p>
<p class="noindent">Prove that <em>o</em>(<em>g</em>(<em>n</em>)) ∩ <em>ω</em>(<em>g</em>(<em>n</em>)) is the empty set.</p>
<p class="level3"><strong><em>3.2-7</em></strong></p>
<p class="noindent">We can extend our notation to the case of two parameters <em>n</em> and <em>m</em> that can go to ∞ independently at different rates. For a given function <em>g</em>(<em>n</em>, <em>m</em>), we denote by <em>O</em>(<em>g</em>(<em>n</em>, <em>m</em>)) the set of functions</p>
<table class="table2">
<tr>
<td class="td1"><em>O</em>(<em>g</em>(<em>n</em>, <em>m</em>)) = {<em>f</em> (<em>n</em>, <em>m</em>)</td>
<td class="td1"> : </td>
<td class="td1">there exist positive constants <em>c</em>, <em>n</em><sub>0</sub>, and <em>m</em><sub>0</sub> such that 0 ≤ <em>f</em> (<em>n</em>, <em>m</em>) ≤ <em>cg</em>(<em>n</em>, <em>m</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub> or <em>m</em> ≥ <em>m</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="noindent">Give corresponding definitions for Ω(<em>g</em>(<em>n</em>, <em>m</em>)) and Θ(<em>g</em>(<em>n</em>, <em>m</em>)).</p>
</section>
<p class="line1"/>
<section title="3.3 Standard notations and common functions">
<a id="Sec_3.3"/>
<p class="level1" id="h1-13"><a href="toc.xhtml#Rh1-13"><strong>3.3      Standard notations and common functions</strong></a></p>
<p class="noindent">This section reviews some standard mathematical functions and notations and explores the relationships among them. It also illustrates the use of the asymptotic notations.</p>
<p class="level4"><strong>Monotonicity</strong></p>
<p class="noindent">A function <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">monotonically increasing</span></em></strong> if <em>m</em> ≤ <em>n</em> implies <em>f</em> (<em>m</em>) ≤ <em>f</em> (<em>n</em>). Similarly, it is <strong><em><span class="blue1">monotonically decreasing</span></em></strong> if <em>m</em> ≤ <em>n</em> implies <em>f</em> (<em>m</em>) ≥ <em>f</em> (<em>n</em>). A function <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">strictly increasing</span></em></strong> if <em>m</em> &lt; <em>n</em> implies <em>f</em> (<em>m</em>) &lt; <em>f</em> (<em>n</em>) and <strong><em><span class="blue1">strictly decreasing</span></em></strong> if <em>m</em> &lt; <em>n</em> implies <em>f</em> (<em>m</em>) &gt; <em>f</em> (<em>n</em>).</p>
<p class="level4"><strong>Floors and ceilings</strong></p>
<p class="noindent">For any real number <em>x</em>, we denote the greatest integer less than or equal to <em>x</em> by <span class="font1">⌊</span><em>x</em><span class="font1">⌋</span> (read “the floor of <em>x</em>”) and the least integer greater than or equal to <em>x</em> by <span class="font1">⌈</span><em>x</em><span class="font1">⌉</span> (read “the ceiling of <em>x</em>”). The floor function is monotonically increasing, as is the ceiling function.</p>
<p>Floors and ceilings obey the following properties. For any integer <em>n</em>, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P39.jpg"/></p>
<p class="noindent">For all real <em>x</em>, we have</p>
<a id="p64"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P40.jpg"/></p>
<p class="noindent">We also have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P41.jpg"/></p>
<p class="noindent">or equivalently,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P42.jpg"/></p>
<p class="noindent">For any real number <em>x</em> ≥ 0 and integers <em>a</em>, <em>b</em> &gt; 0, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P43.jpg"/></p>
<p class="noindent">For any integer <em>n</em> and real number <em>x</em>, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P44.jpg"/></p>
<p class="level4"><strong>Modular arithmetic</strong></p>
<p class="noindent">For any integer <em>a</em> and any positive integer <em>n</em>, the value <em>a</em> mod <em>n</em> is the <strong><em><span class="blue1">remainder</span></em></strong> (or <strong><em><span class="blue1">residue</span></em></strong>) of the quotient <em>a</em>/<em>n</em>:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P45.jpg"/></p>
<p class="noindent">It follows that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P46.jpg"/></p>
<p class="noindent">even when <em>a</em> is negative.</p>
<p>Given a well-defined notion of the remainder of one integer when divided by another, it is convenient to provide special notation to indicate equality of remainders. If (<em>a</em> mod <em>n</em>) = (<em>b</em> mod <em>n</em>), we write <em>a</em> = <em>b</em> (mod <em>n</em>) and say that <em>a</em> is <strong><em><span class="blue1">equivalent</span></em></strong> to <em>b</em>, modulo <em>n</em>. In other words, <em>a</em> = <em>b</em> (mod <em>n</em>) if <em>a</em> and <em>b</em> have the same remainder when divided by <em>n</em>. Equivalently, <em>a</em> = <em>b</em> (mod <em>n</em>) if and only if <em>n</em> is a divisor of <em>b</em> – <em>a</em>. We write <em>a</em> ≠ <em>b</em> (mod <em>n</em>) if <em>a</em> is not equivalent to <em>b</em>, modulo <em>n</em>.</p>
<a id="p65"/>
<p class="level4"><strong>Polynomials</strong></p>
<p class="noindent">Given a nonnegative integer <em>d</em>, a <strong><em><span class="blue1">polynomial in n of degree d</span></em></strong> is a function <em>p</em>(<em>n</em>) of the form</p>
<p class="eql"><img alt="art" src="images/Art_P47.jpg"/></p>
<p class="noindent">where the constants <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, … , <em>a<sub>d</sub></em> are the <strong><em><span class="blue1">coefficients</span></em></strong> of the polynomial and <em>a<sub>d</sub></em> ≠ 0. A polynomial is asymptotically positive if and only if <em>a<sub>d</sub></em> &gt; 0. For an asymptotically positive polynomial <em>p</em>(<em>n</em>) of degree <em>d</em>, we have <em>p</em>(<em>n</em>) = Θ(<em>n<sup>d</sup></em>). For any real constant <em>a</em> ≥ 0, the function <em>n<sup>a</sup></em> is monotonically increasing, and for any real constant <em>a</em> ≤ 0, the function <em>n<sup>a</sup></em> is monotonically decreasing. We say that a function <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">polynomially bounded</span></em></strong> if <em>f</em> (<em>n</em>) = <em>O</em>(<em>n<sup>k</sup></em>) for some constant <em>k</em>.</p>
<p class="level4"><strong>Exponentials</strong></p>
<p class="noindent">For all real <em>a</em> &gt; 0, <em>m</em>, and <em>n</em>, we have the following identities:</p>
<table class="table1a">
<tr>
<td class="td2"><p class="right"><em>a</em><sup>0</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">1,</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>a</em><sup>1</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>a</em>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>a</em><sup>–1</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">1/<em>a</em>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">(<em>a<sup>m</sup></em>)<sup><em>n</em></sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>a<sup>mn</sup></em>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">(<em>a<sup>m</sup></em>)<sup><em>n</em></sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">(<em>a<sup>n</sup></em>)<em><sup>m</sup></em>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>a<sup>m</sup>a<sup>n</sup></em></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>a</em><sup><em>m</em>+<em>n</em></sup>.</p></td>
</tr>
</table>
<p class="noindent">For all <em>n</em> and <em>a</em> ≥ 1, the function <em>a<sup>n</sup></em> is monotonically increasing in <em>n</em>. When convenient, we assume that 0<sup>0</sup> = 1.</p>
<p>We can relate the rates of growth of polynomials and exponentials by the following fact. For all real constants <em>a</em> &gt; 1 and <em>b</em>, we have</p>
<p class="eql"><img alt="art" src="images/Art_P48.jpg"/></p>
<p class="noindent">from which we can conclude that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P49.jpg"/></p>
<p class="noindent">Thus, any exponential function with a base strictly greater than 1 grows faster than any polynomial function.</p>
<p>Using <em>e</em> to denote 2.71828 …, the base of the natural-logarithm function, we have for all real <em>x</em>,</p>
<p class="eql"><img alt="art" src="images/Art_P50.jpg"/></p>
<a id="p66"/>
<p class="noindent">where “!” denotes the factorial function defined later in this section. For all real <em>x</em>, we have the inequality</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P51.jpg"/></p>
<p class="noindent">where equality holds only when <em>x</em> = 0. When |<em>x</em>| ≤ 1, we have the approximation</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P52.jpg"/></p>
<p class="noindent">When <em>x</em> → 0, the approximation of <em>e<sup>x</sup></em> by 1 + <em>x</em> is quite good:</p>
<p class="eql"><em>e<sup>x</sup></em> = 1 + <em>x</em> + Θ(<em>x</em><sup>2</sup>).</p>
<p class="noindent">(In this equation, the asymptotic notation is used to describe the limiting behavior as <em>x</em> → 0 rather than as <em>x</em> → ∞.) We have for all <em>x</em>,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P53.jpg"/></p>
<p class="level4"><strong>Logarithms</strong></p>
<p class="noindent">We use the following notations:</p>
<table class="table1a">
<tr>
<td class="td2"><p class="right">lg <em>n</em></p></td>
<td class="td2m">=</td>
<td class="td2">log<sub>2</sub> <em>n</em></td>
<td class="td2">(binary logarithm),</td>
</tr>
<tr>
<td class="td2"><p class="right">ln <em>n</em></p></td>
<td class="td2m">=</td>
<td class="td2">log<sub><em>e</em></sub> <em>n</em></td>
<td class="td2">(natural logarithm),</td>
</tr>
<tr>
<td class="td2"><p class="right">lg<sup><em>k</em></sup> <em>n</em></p></td>
<td class="td2m">=</td>
<td class="td2">(lg <em>n</em>)<em><sup>k</sup></em></td>
<td class="td2">(exponentiation),</td>
</tr>
<tr>
<td class="td2"><p class="right">lg lg <em>n</em></p></td>
<td class="td2m">=</td>
<td class="td2">lg(lg <em>n</em>)</td>
<td class="td2">(composition).</td>
</tr>
</table>
<p class="noindent">We adopt the following notational convention: in the absence of parentheses, <em>a logarithm function applies only to the next term in the formula</em>, so that lg <em>n</em> + 1 means (lg <em>n</em>) + 1 and not lg(<em>n</em> + 1).</p>
<p>For any constant <em>b</em> &gt; 1, the function log<em><sub>b</sub> n</em> is undefined if <em>n</em> ≤ 0, strictly increasing if <em>n</em> &gt; 0, negative if 0 &lt; <em>n</em> &lt; 1, positive if <em>n</em> &gt; 1, and 0 if <em>n</em> = 1. For all real <em>a</em> &gt; 0, <em>b</em> &gt; 0, <em>c</em> &gt; 0, and <em>n</em>, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P54.jpg"/></p>
<p class="noindent">where, in each equation above, logarithm bases are not 1.</p>
<a id="p67"/>
<p>By equation (3.19), changing the base of a logarithm from one constant to another changes the value of the logarithm by only a constant factor. Consequently, we often use the notation “lg <em>n</em>” when we don’t care about constant factors, such as in <em>O</em>-notation. Computer scientists find 2 to be the most natural base for logarithms because so many algorithms and data structures involve splitting a problem into two parts.</p>
<p>There is a simple series expansion for ln(1 + <em>x</em>) when |<em>x</em>| &lt; 1:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P55.jpg"/></p>
<p class="noindent">We also have the following inequalities for <em>x</em> &gt; – 1:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P56.jpg"/></p>
<p class="noindent">where equality holds only for <em>x</em> = 0.</p>
<p>We say that a function <em>f</em> (<em>n</em>) is <strong><em><span class="blue1">polylogarithmically bounded</span></em></strong> if <em>f</em> (<em>n</em>) = <em>O</em>(lg<em><sup>k</sup> n</em>) for some constant <em>k</em>. We can relate the growth of polynomials and polylogarithms by substituting lg <em>n</em> for <em>n</em> and 2<em><sup>a</sup></em> for <em>a</em> in equation (3.13). For all real constants <em>a</em> &gt; 0 and <em>b</em>, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P57.jpg"/></p>
<p class="noindent">Thus, any positive polynomial function grows faster than any polylogarithmic function.</p>
<p class="level4"><strong>Factorials</strong></p>
<p class="noindent">The notation <em>n</em>! (read “<em>n</em> factorial”) is defined for integers <em>n</em> ≥ 0 as</p>
<p class="eql"><img alt="art" src="images/Art_P58.jpg"/></p>
<p class="noindent">Thus, <em>n</em>! = 1 · 2 · 3 <span class="font1">⋯</span> <em>n</em>.</p>
<p>A weak upper bound on the factorial function is <em>n</em>! ≤ <em>n<sup>n</sup></em>, since each of the <em>n</em> terms in the factorial product is at most <em>n</em>. <strong><em><span class="blue1">Stirling’s approximation</span></em></strong>,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P59.jpg"/></p>
<p class="noindent">where <em>e</em> is the base of the natural logarithm, gives us a tighter upper bound, and a lower bound as well. Exercise 3.3-4 asks you to prove the three facts</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P60.jpg"/></p>
<a id="p68"/>
<p class="noindent">where Stirling’s approximation is helpful in proving equation (3.28). The following equation also holds for all <em>n</em> ≥ 1:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P61.jpg"/></p>
<p class="noindent">where</p>
<p class="eql"><img alt="art" src="images/Art_P62.jpg"/></p>
<p class="level4"><strong>Functional iteration</strong></p>
<p class="noindent">We use the notation <em>f</em><sup>(<em>i</em>)</sup> (<em>n</em>) to denote the function <em>f</em> (<em>n</em>) iteratively applied <em>i</em> times to an initial value of <em>n</em>. Formally, let <em>f</em> (<em>n</em>) be a function over the reals. For nonnegative integers <em>i</em>, we recursively define</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P63.jpg"/></p>
<p class="noindent">For example, if <em>f</em> (<em>n</em>) = 2<em>n</em>, then <em>f</em><sup>(<em>i</em>)</sup> (<em>n</em>) = 2<em><sup>i</sup>n</em>.</p>
<p class="level4"><strong>The iterated logarithm function</strong></p>
<p class="noindent">We use the notation lg<sup>*</sup><em>n</em> (read “log star of <em>n</em>”) to denote the iterated logarithm, defined as follows. Let lg<sup>(<em>i</em>)</sup> <em>n</em> be as defined above, with <em>f</em> (<em>n</em>) = lg <em>n</em>. Because the logarithm of a nonpositive number is undefined, lg<em><sup>(i)</sup> n</em> is defined only if lg<sup>(<em>i</em>–1)</sup> <em>n</em> &gt; 0. Be sure to distinguish lg<sup>(<em>i</em>)</sup> <em>n</em> (the logarithm function applied <em>i</em> times in succession, starting with argument <em>n</em>) from lg<em><sup>i</sup> n</em> (the logarithm of <em>n</em> raised to the <em>i</em>th power). Then we define the iterated logarithm function as</p>
<p class="eql">lg<sup>*</sup><em>n</em> = min {<em>i</em> ≥ 0 : lg<sup>(<em>i</em>)</sup> <em>n</em> ≤ 1}.</p>
<p class="noindent">The iterated logarithm is a <em>very</em> slowly growing function:</p>
<table class="table1a">
<tr>
<td class="td2"><p class="right">lg<sup>*</sup> 2</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">1,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">lg<sup>*</sup> 4</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">2,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">lg<sup>*</sup> 16</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">3,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">lg<sup>*</sup> 65536</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">4,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">lg<sup>*</sup> (2<sup>65536</sup>)</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">5.</p></td>
</tr>
</table>
<p class="noindent">Since the number of atoms in the observable universe is estimated to be about 10<sup>80</sup>, which is much less than 2<sup>65536</sup> = 10<sup>65536/lg 10</sup> ≈ 10<sup>19,728</sup>, we rarely encounter an input size <em>n</em> for which lg<sup>*</sup> <em>n</em> &gt; 5.</p>
<a id="p69"/>
<p class="level4"><strong>Fibonacci numbers</strong></p>
<p class="noindent">We define the <strong><em><span class="blue1">Fibonacci numbers</span></em></strong> <em>F<sub>i</sub></em>, for <em>i</em> ≥ 0, as follows:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P64.jpg"/></p>
<p class="noindent">Thus, after the first two, each Fibonacci number is the sum of the two previous ones, yielding the sequence</p>
<p class="eql">0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ….</p>
<p class="noindent">Fibonacci numbers are related to the <strong><em><span class="blue1">golden ratio</span></em></strong> <em><span class="symbolfont">ϕ</span></em> and its conjugate <img alt="art" src="images/phic.jpg"/>, which are the two roots of the equation</p>
<p class="eql"><em>x</em><sup>2</sup> = <em>x</em> + 1.</p>
<p class="noindent">As Exercise 3.3-7 asks you to prove, the golden ratio is given by</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P65.jpg"/></p>
<p class="noindent">and its conjugate, by</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P66.jpg"/></p>
<p class="noindent">Specifically, we have</p>
<p class="eql"><img alt="art" src="images/Art_P67.jpg"/></p>
<p class="noindent">which can be proved by induction (Exercise 3.3-8). Since <img alt="art" src="images/Art_P68.jpg"/>, we have</p>
<p class="eql"><img alt="art" src="images/Art_P69.jpg"/></p>
<p class="noindent">which implies that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P70.jpg"/></p>
<p class="noindent">which is to say that the <em>i</em>th Fibonacci number <em>F<sub>i</sub></em> is equal to <img alt="art" src="images/Art_P71.jpg"/> rounded to the nearest integer. Thus, Fibonacci numbers grow exponentially.</p>
<a id="p70"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>3.3-1</em></strong></p>
<p class="noindent">Show that if <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) are monotonically increasing functions, then so are the functions <em>f</em> (<em>n</em>) + <em>g</em>(<em>n</em>) and <em>f</em> (<em>g</em>(<em>n</em>)), and if <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) are in addition nonnegative, then <em>f</em> (<em>n</em>) · <em>g</em>(<em>n</em>) is monotonically increasing.</p>
<p class="level3"><strong><em>3.3-2</em></strong></p>
<p class="noindent">Prove that <span class="font1">⌊</span><em>αn</em><span class="font1">⌋</span> + <span class="font1">⌈</span>(1 – <em>α</em>)<em>n</em><span class="font1">⌉</span> = <em>n</em> for any integer <em>n</em> and real number <em>α</em> in the range 0 ≤ <em>α</em> ≤ 1.</p>
<p class="level3"><strong><em>3.3-3</em></strong></p>
<p class="noindent">Use equation (3.14) or other means to show that (<em>n</em> + <em>o</em>(<em>n</em>))<em><sup>k</sup></em> = Θ(<em>n<sup>k</sup></em>) for any real constant <em>k</em>. Conclude that <span class="font1">⌈</span><em>n</em><span class="font1">⌉</span><em><sup>k</sup></em> = Θ(<em>n<sup>k</sup></em>) and <span class="font1">⌊</span><em>n</em><span class="font1">⌋</span><em><sup>k</sup></em> = Θ(<em>n<sup>k</sup></em>).</p>
<p class="level3"><strong><em>3.3-4</em></strong></p>
<p class="noindent">Prove the following:</p>
<p class="nl"><strong><em>a.</em></strong> Equation (3.21).</p>
<p class="nl"><strong><em>b.</em></strong> Equations (3.26)–(3.28).</p>
<p class="nl"><strong><em>c.</em></strong> lg(Θ(<em>n</em>)) = Θ(lg <em>n</em>).</p>
<p class="level3"><span class="font1">★</span> <strong><em>3.3-5</em></strong></p>
<p class="noindent">Is the function <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span>! polynomially bounded? Is the function <span class="font1">⌈</span>lg lg <em>n</em><span class="font1">⌉</span>! polynomially bounded?</p>
<p class="level3"><span class="font1">★</span> <strong><em>3.3-6</em></strong></p>
<p class="noindent">Which is asymptotically larger: lg(lg<sup>*</sup> <em>n</em>) or lg<sup>*</sup>(lg <em>n</em>)?</p>
<p class="level3"><strong><em>3.3-7</em></strong></p>
<p class="noindent">Show that the golden ratio <em><span class="symbolfont">ϕ</span></em> and its conjugate <img alt="art" src="images/phic.jpg"/> both satisfy the equation <em>x</em><sup>2</sup> = <em>x</em> + 1.</p>
<p class="level3"><strong><em>3.3-8</em></strong></p>
<p class="noindent">Prove by induction that the <em>i</em>th Fibonacci number satisfies the equation</p>
<p class="eql"><img alt="art" src="images/Art_P72.jpg"/></p>
<p class="noindent">where <em><span class="symbolfont">ϕ</span></em> is the golden ratio and <img alt="art" src="images/phic.jpg"/> is its conjugate.</p>
<p class="level3"><strong><em>3.3-9</em></strong></p>
<p class="noindent">Show that <em>k</em> lg <em>k</em> = Θ(<em>n</em>) implies <em>k</em> = Θ(<em>n</em>/lg <em>n</em>).</p>
<a id="p71"/>
</section>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-14"><strong>Problems</strong></p>
<section title="3-1 Asymptotic behavior of polynomials">
<p class="level2"><strong><em>3-1     Asymptotic behavior of polynomials</em></strong></p>
<p class="noindent">Let</p>
<p class="eql"><img alt="art" src="images/Art_P73.jpg"/></p>
<p class="noindent">where <em>a<sub>d</sub></em> &gt; 0, be a degree-<em>d</em> polynomial in <em>n</em>, and let <em>k</em> be a constant. Use the definitions of the asymptotic notations to prove the following properties.</p>
<p class="nl"><strong><em>a.</em></strong> If <em>k</em> ≥ <em>d</em>, then <em>p</em>(<em>n</em>) = <em>O</em>(<em>n<sup>k</sup></em>).</p>
<p class="nl"><strong><em>b.</em></strong> If <em>k</em> ≤ <em>d</em>, then <em>p</em>(<em>n</em>) = Ω(<em>n<sup>k</sup></em>).</p>
<p class="nl"><strong><em>c.</em></strong> If <em>k</em> = <em>d</em>, then <em>p</em>(<em>n</em>) = Θ(<em>n<sup>k</sup></em>).</p>
<p class="nl"><strong><em>d.</em></strong> If <em>k</em> &gt; <em>d</em>, then <em>p</em>(<em>n</em>) = <em>o</em>(<em>n<sup>k</sup></em>).</p>
<p class="nl"><strong><em>e.</em></strong> If <em>k</em> &lt; <em>d</em>, then <em>p</em>(<em>n</em>) = <em>ω</em>(<em>n<sup>k</sup></em>).</p>
</section>
<section title="3-2 Relative asymptotic growths">
<p class="level2"><strong><em>3-2     Relative asymptotic growths</em></strong></p>
<p class="noindent">Indicate, for each pair of expressions (<em>A</em>, <em>B</em>) in the table below whether <em>A</em> is <em>O</em>, <em>o</em>, Ω, <em>ω</em>, or Θ of <em>B</em>. Assume that <em>k</em> ≥ 1, <em><span class="font1">ϵ</span></em> &gt; 0, and <em>c</em> &gt; 1 are constants. Write your answer in the form of the table with “yes” or “no” written in each box.</p>
<p class="fig-img-l"><img alt="art" src="images/Art_P74.jpg"/></p>
<section title="3-3 Ordering by asymptotic growth rates">
<p class="level2"><strong><em>3-3     Ordering by asymptotic growth rates</em></strong></p>
<p class="nl"><strong><em>a.</em></strong> Rank the following functions by order of growth. That is, find an arrangement <em>g</em><sub>1</sub>, <em>g</em><sub>2</sub>, … , <em>g</em><sub>30</sub> of the functions satisfying <em>g</em><sub>1</sub> = Ω(<em>g</em><sub>2</sub>), <em>g</em><sub>2</sub> = Ω(<em>g</em><sub>3</sub>), … , <em>g</em><sub>29</sub> = Ω(<em>g</em><sub>30</sub>). Partition your list into equivalence classes such that functions <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) belong to the same class if and only if <em>f</em> (<em>n</em>) = Θ(<em>g</em>(<em>n</em>)).</p>
<a id="p72"/>
<table class="table2c">
<tr>
<td class="td2"><p class="center">lg(lg<sup>*</sup> <em>n</em>)</p></td>
<td class="td2"><p class="center">2<sup>lg* <em>n</em></sup></p></td>
<td class="td2"><p class="center"><img alt="art" src="images/Art_P75.jpg"/></p></td>
<td class="td2"><p class="center"><em>n</em><sup>2</sup></p></td>
<td class="td2"><p class="center"><em>n</em>!</p></td>
<td class="td2"><p class="center">(lg <em>n</em>)!</p></td>
</tr>
<tr>
<td class="td2"><p class="center">(3/2)<em><sup>n</sup></em></p></td>
<td class="td2"><p class="center"><em>n</em><sup>3</sup></p></td>
<td class="td2"><p class="center">lg<sup>2</sup> <em>n</em></p></td>
<td class="td2"><p class="center">lg(<em>n</em>!)</p></td>
<td class="td2"><p class="center"><img alt="art" src="images/Art_P76.jpg"/></p></td>
<td class="td2"><p class="center"><em>n</em><sup>1/lg <em>n</em></sup></p></td>
</tr>
<tr>
<td class="td2"><p class="center">ln ln <em>n</em></p></td>
<td class="td2"><p class="center">lg<sup>*</sup> <em>n</em></p></td>
<td class="td2"><p class="center"><em>n</em> · 2<em><sup>n</sup></em></p></td>
<td class="td2"><p class="center"><em>n</em><sup>lg lg <em>n</em></sup></p></td>
<td class="td2"><p class="center">ln <em>n</em></p></td>
<td class="td2"><p class="center">1</p></td>
</tr>
<tr>
<td class="td2"><p class="center">2<sup>lg <em>n</em></sup></p></td>
<td class="td2"><p class="center">(lg <em>n</em>)<sup>lg <em>n</em></sup></p></td>
<td class="td2"><p class="center"><em>e<sup>n</sup></em></p></td>
<td class="td2"><p class="center">4<sup>lg <em>n</em></sup></p></td>
<td class="td2"><p class="center">(<em>n</em> + 1)!</p></td>
<td class="td2"><p class="center"><img alt="art" src="images/Art_P77.jpg"/></p></td>
</tr>
<tr>
<td class="td2"><p class="center">lg<sup>*</sup>(lg <em>n</em>)</p></td>
<td class="td2"><p class="center"><img alt="art" src="images/Art_P78.jpg"/></p></td>
<td class="td2"><p class="center"><em>n</em></p></td>
<td class="td2"><p class="center">2<em><sup>n</sup></em></p></td>
<td class="td2"><p class="center"><em>n</em> lg <em>n</em></p></td>
<td class="td2"><p class="center"><img alt="art" src="images/Art_P79.jpg"/></p></td>
</tr>
</table>
<p class="nl"><strong><em>b.</em></strong> Give an example of a single nonnegative function <em>f</em> (<em>n</em>) such that for all functions <em>g<sub>i</sub></em>(<em>n</em>) in part (a), <em>f</em> (<em>n</em>) is neither <em>O</em>(<em>g<sub>i</sub></em>(<em>n</em>)) nor Ω(<em>g<sub>i</sub></em>(<em>n</em>)).</p>
</section>
<section title="3-4 Asymptotic notation properties">
<p class="level2"><strong><em>3-4     Asymptotic notation properties</em></strong></p>
<p class="noindent">Let <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) be asymptotically positive functions. Prove or disprove each of the following conjectures.</p>
<p class="nl-top"><strong><em>a.</em></strong> <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) implies <em>g</em>(<em>n</em>) = <em>O</em>(<em>f</em> (<em>n</em>)).</p>
<p class="nl"><strong><em>b.</em></strong> <em>f</em> (<em>n</em>) + <em>g</em>(<em>n</em>) = Θ(min {<em>f</em> (<em>n</em>), <em>g</em>(<em>n</em>)}).</p>
<p class="nl"><strong><em>c.</em></strong> <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) implies lg <em>f</em> (<em>n</em>) = <em>O</em>(lg <em>g</em>(<em>n</em>)), where lg <em>g</em>(<em>n</em>) ≥ 1 and <em>f</em> (<em>n</em>) ≥ 1 for all sufficiently large <em>n</em>.</p>
<p class="nl"><strong><em>d.</em></strong> <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) implies 2<sup><em>f</em>(<em>n</em>)</sup> = <em>O</em> (2<sup><em>g</em>(<em>n</em>)</sup>).</p>
<p class="nl"><strong><em>e.</em></strong> <em>f</em> (<em>n</em>) = <em>O</em> ((<em>f</em> (<em>n</em>))<sup>2</sup>).</p>
<p class="nl"><strong><em>f.</em></strong> <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) implies <em>g</em>(<em>n</em>) = Ω(<em>f</em> (<em>n</em>)).</p>
<p class="nl"><strong><em>g.</em></strong> <em>f</em> (<em>n</em>) = Θ(<em>f</em> (<em>n</em>/2)).</p>
<p class="nl"><strong><em>h.</em></strong> <em>f</em> (<em>n</em>) + <em>o</em>(<em>f</em> (<em>n</em>)) = Θ(<em>f</em> (<em>n</em>)).</p>
</section>
<section title="3-5 Manipulating asymptotic notation">
<p class="level2"><strong><em>3-5     Manipulating asymptotic notation</em></strong></p>
<p class="noindent">Let <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) be asymptotically positive functions. Prove the following identities:</p>
<p class="nl-top"><strong><em>a.</em></strong> Θ(Θ(<em>f</em> (<em>n</em>))) = Θ(<em>f</em> (<em>n</em>)).</p>
<p class="nl"><strong><em>b.</em></strong> Θ(<em>f</em> (<em>n</em>)) + <em>O</em>(<em>f</em> (<em>n</em>)) = Θ(<em>f</em> (<em>n</em>)).</p>
<p class="nl"><strong><em>c.</em></strong> Θ(<em>f</em> (<em>n</em>)) + Θ(<em>g</em>(<em>n</em>)) = Θ(<em>f</em> (<em>n</em>) + <em>g</em>(<em>n</em>)).</p>
<p class="nl"><strong><em>d.</em></strong> Θ(<em>f</em> (<em>n</em>)) · Θ(<em>g</em>(<em>n</em>)) = Θ(<em>f</em> (<em>n</em>) · <em>g</em>(<em>n</em>)).</p>
<a id="p73"/>
<p class="nl"><strong><em>e.</em></strong> Argue that for any real constants <em>a</em><sub>1</sub>, <em>b</em><sub>1</sub> &gt; 0 and integer constants <em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, the following asymptotic bound holds:</p>
<p class="eql"><img alt="art" src="images/Art_P80.jpg"/></p>
<p class="nl-1"><span class="font1">★</span> <strong><em>f.</em></strong> Prove that for <em>S</em> ⊆ <span class="struck">Z</span>, we have</p>
<p class="nl-para1"><img alt="art" src="images/Art_P81.jpg"/></p>
<p class="nl-para1">assuming that both sums converge.</p>
<p class="nl-1"><span class="font1">★</span> <strong><em>g.</em></strong> Show that for <em>S</em> ⊆ <span class="struck">Z</span>, the following asymptotic bound does not necessarily hold, even assuming that both products converge, by giving a counterexample:</p>
<p class="nl-para1"><img alt="art" src="images/Art_P82.jpg"/></p>
</section>
<section title="3-6 Variations on O and Ω">
<p class="level2"><strong><em>3-6     Variations on O and Ω</em></strong></p>
<p class="noindent">Some authors define Ω-notation in a slightly different way than this textbook does. We’ll use the nomenclature <img alt="art" src="images/Art_P83.jpg"/> (read “omega infinity”) for this alternative definition. We say that <img alt="art" src="images/Art_P84.jpg"/> if there exists a positive constant <em>c</em> such that <em>f</em> (<em>n</em>) ≥ <em>cg</em>(<em>n</em>) ≥ 0 for infinitely many integers <em>n</em>.</p>
<p class="nl"><strong><em>a.</em></strong> Show that for any two asymptotically nonnegative functions <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>), we have <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) or <img alt="art" src="images/Art_P85.jpg"/> (or both).</p>
<p class="nl"><strong><em>b.</em></strong> Show that there exist two asymptotically nonnegative functions <em>f</em> (<em>n</em>) and <em>g</em>(<em>n</em>) for which neither <em>f</em> (<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)) nor <em>f</em> (<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) holds.</p>
<p class="nl"><strong><em>c.</em></strong> Describe the potential advantages and disadvantages of using <img alt="art" src="images/Art_P86.jpg"/>-notation instead of Ω-notation to characterize the running times of programs.</p>
<p class="noindent1-top">Some authors also define <em>O</em> in a slightly different manner. We’ll use <em>O</em><sup>′</sup> for the alternative definition: <em>f</em> (<em>n</em>) = <em>O</em><sup>′</sup>(<em>g</em>(<em>n</em>)) if and only if |<em>f</em> (<em>n</em>)| = <em>O</em>(<em>g</em>(<em>n</em>)).</p>
<p class="nl"><strong><em>d.</em></strong> What happens to each direction of the “if and only if” in Theorem 3.1 on page 56 if we substitute <em>O</em><sup>′</sup> for <em>O</em> but still use Ω?</p>
<p class="noindent1-top">Some authors define <img alt="art" src="images/Otilde.jpg"/> (read “soft-oh”) to mean <em>O</em> with logarithmic factors ignored:</p>
<a id="p74"/>
<table class="table2">
<tr>
<td class="td1"><img alt="art" src="images/Art_P87.jpg"/></td>
<td class="td1"> : </td>
<td class="td1">there exist positive constants <em>c</em>, <em>k</em>, and <em>n</em><sub>0</sub> such that 0 ≤ <em>f</em> (<em>n</em>) ≤ <em>cg</em>(<em>n</em>) lg<em><sup>k</sup></em>(<em>n</em>) for all <em>n</em> ≥ <em>n</em><sub>0</sub>}.</td>
</tr>
</table>
<p class="nl"><strong><em>e.</em></strong> Define <img alt="art" src="images/Art_P88.jpg"/> and <img alt="art" src="images/Art_P89.jpg"/> in a similar manner. Prove the corresponding analog to Theorem 3.1.</p>
</section>
<section title="3-7 Iterated functions">
<p class="level2"><strong><em>3-7     Iterated functions</em></strong></p>
<p class="noindent">We can apply the iteration operator <sup>*</sup> used in the lg<sup>*</sup> function to any monotonically increasing function <em>f</em> (<em>n</em>) over the reals. For a given constant <em>c</em> ∈ <span class="struck">R</span>, we define the iterated function <img alt="art" src="images/Art_P90.jpg"/> by</p>
<p class="eql"><img alt="art" src="images/Art_P91.jpg"/></p>
<p class="noindent">which need not be well defined in all cases. In other words, the quantity <img alt="art" src="images/Art_P92.jpg"/> is the minimum number of iterated applications of the function <em>f</em> required to reduce its argument down to <em>c</em> or less.</p>
<p>For each of the functions <em>f</em> (<em>n</em>) and constants <em>c</em> in the table below, give as tight a bound as possible on <img alt="art" src="images/Art_P93.jpg"/>. If there is no <em>i</em> such that <em>f</em><sup>(<em>i</em>)</sup>(<em>n</em>) ≤ <em>c</em>, write “undefined” as your answer.</p>
<table class="table3">
<tr>
<td class="td2m"/>
<td class="td1b"><p class="center"><em>f</em> (<em>n</em>)</p></td>
<td class="td1br"><p class="center"><em>c</em></p></td>
<td class="td1br"><p class="center"><img alt="art" src="images/Art_P94.jpg"/></p></td>
</tr>
<tr>
<td class="td2m"><strong><em>a.</em></strong></td>
<td class="td1b"><p class="center"><em>n</em> – 1</p></td>
<td class="td1br"><p class="center">0</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>b.</em></strong></td>
<td class="td1b"><p class="center">lg <em>n</em></p></td>
<td class="td1br"><p class="center">1</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>c.</em></strong></td>
<td class="td1b"><p class="center"><em>n</em>/2</p></td>
<td class="td1br"><p class="center">1</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>d.</em></strong></td>
<td class="td1b"><p class="center"><em>n</em>/2</p></td>
<td class="td1br"><p class="center">2</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>e.</em></strong></td>
<td class="td1b"><p class="center"><img alt="art" src="images/Art_P95.jpg"/></p></td>
<td class="td1br"><p class="center">2</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>f.</em></strong></td>
<td class="td1b"><p class="center"><img alt="art" src="images/Art_P96.jpg"/></p></td>
<td class="td1br"><p class="center">1</p></td>
<td class="td1br"/>
</tr>
<tr>
<td class="td2m"><strong><em>g.</em></strong></td>
<td class="td1b"><p class="center"><em>n</em><sup>1/3</sup></p></td>
<td class="td1br"><p class="center">2</p></td>
<td class="td1br"/>
</tr>
</table>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-15"><strong>Chapter notes</strong></p>
<p class="noindent">Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>] traces the origin of the <em>O</em>-notation to a number-theory text by P. Bachmann in 1892. The <em>o</em>-notation was invented by E. Landau in 1909 for his discussion of the distribution of prime numbers. The Ω and Θ notations were advocated by Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_265">265</a>] to correct the popular, but technically sloppy, practice in the literature of using <em>O</em>-notation for both upper and lower bounds. As noted earlier in this chapter, many people continue to use the <em>O</em>-notation where the Θ-notation is more technically precise. The soft-oh notation <img alt="art" src="images/Otilde.jpg"/> in Problem 3-6 was introduced <a id="p75"/>by Babai, Luks, and Seress [<a epub:type="noteref" href="bibliography001.xhtml#endnote_31">31</a>], although it was originally written as <em>O</em>~. Some authors now define <img alt="art" src="images/Art_P97.jpg"/> as ignoring factors that are logarithmic in <em>g</em>(<em>n</em>), rather than in <em>n</em>. With this definition, we can say that <img alt="art" src="images/Art_P98.jpg"/>, but with the definition in Problem 3-6, this statement is not true. Further discussion of the history and development of asymptotic notations appears in works by Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_265">265</a>] and Brassard and Bratley [<a epub:type="noteref" href="bibliography001.xhtml#endnote_70">70</a>].</p>
<p>Not all authors define the asymptotic notations in the same way, although the various definitions agree in most common situations. Some of the alternative definitions encompass functions that are not asymptotically nonnegative, as long as their absolute values are appropriately bounded.</p>
<p>Equation (3.29) is due to Robbins [<a epub:type="noteref" href="bibliography001.xhtml#endnote_381">381</a>]. Other properties of elementary mathematical functions can be found in any good mathematical reference, such as Abramowitz and Stegun [<a epub:type="noteref" href="bibliography001.xhtml#endnote_1">1</a>] or Zwillinger [<a epub:type="noteref" href="bibliography001.xhtml#endnote_468">468</a>], or in a calculus book, such as Apostol [<a epub:type="noteref" href="bibliography001.xhtml#endnote_19">19</a>] or Thomas et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_433">433</a>]. Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>] and Graham, Knuth, and Patashnik [<a epub:type="noteref" href="bibliography001.xhtml#endnote_199">199</a>] contain a wealth of material on discrete mathematics as used in computer science.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> Within set notation, a colon means “such that.”</p>
</section>
</section>
</div>
</body>
</html>