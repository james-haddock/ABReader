<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p819"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="28 Matrix Operations">
<p class="chapter-title"><a href="toc.xhtml#chap-28"><strong><span class="blue1">28        Matrix Operations</span></strong></a></p>
<p class="noindent">Because operations on matrices lie at the heart of scientific computing, efficient algorithms for working with matrices have many practical applications. This chapter focuses on how to multiply matrices and solve sets of simultaneous linear equations. <a href="appendix004.xhtml">Appendix D</a> reviews the basics of matrices.</p>
<p><a href="chapter028.xhtml#Sec_28.1">Section 28.1</a> shows how to solve a set of linear equations using LUP decompositions. Then, <a href="chapter028.xhtml#Sec_28.2">Section 28.2</a> explores the close relationship between multiplying and inverting matrices. Finally, <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a> discusses the important class of symmetric positive-definite matrices and shows how to use them to find a least-squares solution to an overdetermined set of linear equations.</p>
<p>One important issue that arises in practice is <strong><em><span class="blue1">numerical stability</span></em></strong>. Because actual computers have limits to how precisely they can represent floating-point numbers, round-off errors in numerical computations may become amplified over the course of a computation, leading to incorrect results. Such computations are called <strong><em><span class="blue1">numerically unstable</span></em></strong>. Although we’ll briefly consider numerical stability on occasion, we won’t focus on it in this chapter. We refer you to the excellent book by Higham [<a epub:type="noteref" href="bibliography001.xhtml#endnote_216">216</a>] for a thorough discussion of stability issues.</p>
<p class="line1"/>
<section title="28.1 Solving systems of linear equations">
<a id="Sec_28.1"/>
<p class="level1" id="h1-162"><a href="toc.xhtml#Rh1-162"><strong>28.1    Solving systems of linear equations</strong></a></p>
<p class="noindent">Numerous applications need to solve sets of simultaneous linear equations. A linear system can be cast as a matrix equation in which each matrix or vector element belongs to a field, typically the real numbers <span class="double"><span class="font1">ℝ</span></span>. This section discusses how to solve a system of linear equations using a method called LUP decomposition.</p>
<p>The process starts with a set of linear equations in <em>n</em> unknowns <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>:</p>
<p class="eqr"><img alt="art" src="images/Art_P897.jpg"/></p>
<a id="p820"/>
<p class="noindent">A <strong><em><span class="blue1">solution</span></em></strong> to the equations (28.1) is a set of values for <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em> that satisfy all of the equations simultaneously. In this section, we treat only the case in which there are exactly <em>n</em> equations in <em>n</em> unknowns.</p>
<p>Next, rewrite equations (28.1) as the matrix-vector equation</p>
<p class="eql"><img alt="art" src="images/Art_P898.jpg"/></p>
<p class="noindent">or, equivalently, letting <em>A</em> = (<em>a<sub>ij</sub></em>), <em>x</em> = (<em>x<sub>i</sub></em>), and <em>b</em> = (<em>b<sub>i</sub></em>), as</p>
<p class="eqr"><img alt="art" src="images/Art_P899.jpg"/></p>
<p>If <em>A</em> is nonsingular, it possesses an inverse <em>A</em><sup>−1</sup>, and</p>
<p class="eqr"><img alt="art" src="images/Art_P900.jpg"/></p>
<p class="noindent">is the solution vector. We can prove that <em>x</em> is the unique solution to equation (28.2) as follows. If there are two solutions, <em>x</em> and <em>x</em>′, then <em>Ax</em> = <em>Ax</em>′ = <em>b</em> and, letting <em>I</em> denote an identity matrix,</p>
<table class="table2b">
<tr>
<td class="td2"><em>x</em></td>
<td class="td2">=</td>
<td class="td2"><em>Ix</em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">(<em>A</em><sup>−1</sup><em>A</em>)<em>x</em></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>A</em><sup>−1</sup>(<em>Ax</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>A</em><sup>−1</sup>(<em>Ax</em>′)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">(<em>A</em><sup>−1</sup><em>A</em>)<em>x</em>′</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>Ix</em>′</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>x</em>′.</td>
</tr>
</table>
<p>This section focuses on the case in which <em>A</em> is nonsingular or, equivalently (by Theorem D.1 on page 1220), the rank of <em>A</em> equals the number <em>n</em> of unknowns. There are other possibilities, however, which merit a brief discussion. If the number of equations is less than the number <em>n</em> of unknowns—or, more generally, if the rank of <em>A</em> is less than <em>n</em>—then the system is <strong><em><span class="blue1">underdetermined</span></em></strong>. An underdetermined system typically has infinitely many solutions, although it may have no <a id="p821"/>solutions at all if the equations are inconsistent. If the number of equations exceeds the number <em>n</em> of unknowns, the system is <strong><em><span class="blue1">overdetermined</span></em></strong>, and there may not exist any solutions. <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a> addresses the important problem of finding good approximate solutions to overdetermined systems of linear equations.</p>
<p>Let’s return to the problem of solving the system <em>Ax</em> = <em>b</em> of <em>n</em> equations in <em>n</em> unknowns. One option is to compute <em>A</em><sup>−1</sup> and then, using equation (28.3), multiply <em>b</em> by <em>A</em><sup>−1</sup>, yielding <em>x</em> = <em>A</em><sup>−1</sup><em>b</em>. This approach suffers in practice from numerical instability. Fortunately, another approach—LUP decomposition—is numerically stable and has the further advantage of being faster in practice.</p>
<p class="level4"><strong>Overview of LUP decomposition</strong></p>
<p class="noindent">The idea behind LUP decomposition is to find three <em>n</em> × <em>n</em> matrices <em>L</em>, <em>U</em>, and <em>P</em> such that</p>
<p class="eqr"><img alt="art" src="images/Art_P901.jpg"/></p>
<p class="noindent">where</p>
<ul class="ulnoindent" epub:type="list">
<li><em>L</em> is a unit lower-triangular matrix,</li>
<li class="litop"><em>U</em> is an upper-triangular matrix, and</li>
<li class="litop"><em>P</em> is a permutation matrix.</li></ul>
<p class="noindent">We call matrices <em>L</em>, <em>U</em>, and <em>P</em> satisfying equation (28.4) an <strong><em><span class="blue1">LUP decomposition</span></em></strong> of the matrix <em>A</em>. We’ll show that every nonsingular matrix <em>A</em> possesses such a decomposition.</p>
<p>Computing an LUP decomposition for the matrix <em>A</em> has the advantage that linear systems can be efficiently solved when they are triangular, as is the case for both matrices <em>L</em> and <em>U</em>. If you have an LUP decomposition for <em>A</em>, you can solve equation (28.2), <em>Ax</em> = <em>b</em>, by solving only triangular linear systems, as follows. Multiply both sides of <em>Ax</em> = <em>b</em> by <em>P</em>, yielding the equivalent equation <em>PAx</em> = <em>Pb</em>. By Exercise D.1-4 on page 1219, multiplying both sides by a permutation matrix amounts to permuting the equations (28.1). By the decomposition (28.4), substituting <em>LU</em> for <em>PA</em> gives</p>
<p class="eql"><em>LUx</em> = <em>Pb</em>.</p>
<p class="noindent">You can now solve this equation by solving two triangular linear systems. Define <em>y</em> = <em>Ux</em>, where <em>x</em> is the desired solution vector. First, solve the lower-triangular system</p>
<p class="eqr"><img alt="art" src="images/Art_P902.jpg"/></p>
<p class="noindent">for the unknown vector <em>y</em> by a method called “forward substitution.” Having solved for <em>y</em>, solve the upper-triangular system</p>
<a id="p822"/>
<p class="eqr"><img alt="art" src="images/Art_P903.jpg"/></p>
<p class="noindent">for the unknown <em>x</em> by a method called “back substitution.” Why does this process solve <em>Ax</em> = <em>b</em>? Because the permutation matrix <em>P</em> is invertible (see Exercise D.2-3 on page 1223), multiplying both sides of equation (28.4) by <em>P</em> <sup>−1</sup> gives <em>P</em><sup>−1</sup><em>PA</em> = <em>P</em><sup>−1</sup><em>LU</em>, so that</p>
<p class="eqr"><img alt="art" src="images/Art_P905.jpg"/></p>
<p class="noindent">Hence, the vector <em>x</em> that satisfies <em>Ux</em> = <em>y</em> is the solution to <em>Ax</em> = <em>b</em>:</p>
<table class="table2b">
<tr>
<td class="td2"><em>Ax</em></td>
<td class="td2">=</td>
<td class="td2"><em>P</em><sup>−1</sup><em>LUx</em></td>
<td class="td2">(by equation (28.7))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>P</em><sup>−1</sup><em>Ly</em></td>
<td class="td2">(by equation (28.6))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>P</em><sup>−1</sup><em>Pb</em></td>
<td class="td2">(by equation (28.5))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>b</em>.</td>
</tr>
</table>
<p>The next step is to show how forward and back substitution work and then attack the problem of computing the LUP decomposition itself.</p>
<p class="level4"><strong>Forward and back substitution</strong></p>
<p class="noindent"><strong><em><span class="blue1">Forward substitution</span></em></strong> can solve the lower-triangular system (28.5) in Θ(<em>n</em><sup>2</sup>) time, given <em>L</em>, <em>P</em>, and <em>b</em>. An array <em>π</em>[1 : <em>n</em>] provides a more compact format to represent the permutation <em>P</em> than an <em>n</em> × <em>n</em> matrix that is mostly 0s. For <em>i</em> = 1, 2, … , <em>n</em>, the entry <em>π</em>[<em>i</em>] indicates that <em>P</em><sub><em>i,π</em>[<em>i</em>]</sub> = 1 and <em>P<sub>ij</sub></em> = 0 for <em>j</em> ≠ <em>π</em>[<em>i</em>]. Thus, <em>PA</em> has <em>a</em><sub><em>π</em>[<em>i</em>],<em>j</em></sub> in row <em>i</em> and column <em>j</em>, and <em>Pb</em> has <em>b</em><sub><em>π</em>[<em>i</em>]</sub> as its <em>i</em>th element. Since <em>L</em> is unit lower-triangular, the matrix equation <em>Ly</em> = <em>Pb</em> is equivalent to the <em>n</em> equations</p>
<table class="table2b">
<tr>
<td class="td2">   <em>y</em><sub>1</sub></td>
<td class="td2">=</td>
<td class="td2"><em>b</em><sub><em>π</em>[1]</sub>,</td>
</tr>
<tr>
<td class="td2"><em>l</em><sub>21</sub><em>y</em><sub>1</sub> + <em>y</em><sub>2</sub></td>
<td class="td2">=</td>
<td class="td2"><em>b</em><sub><em>π</em>[2]</sub>,</td>
</tr>
<tr>
<td class="td2"><em>l</em><sub>31</sub><em>y</em><sub>1</sub> + <em>l</em><sub>32</sub><em>y</em><sub>2</sub> + <em>y</em><sub>3</sub></td>
<td class="td2">=</td>
<td class="td2"><em>b</em><sub><em>π</em>[3]</sub>,</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><span class="font1">⋮</span></td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"><em>l</em><sub><em>n</em>1</sub><em>y</em><sub>1</sub> + <em>l</em><sub><em>n</em>2</sub><em>y</em><sub>2</sub> + <em>l</em><sub><em>n</em>3</sub><em>y</em><sub>3</sub> + <span class="font1">⋯</span> + <em>y<sub>n</sub></em></td>
<td class="td2">=</td>
<td class="td2"><em>b</em><sub><em>π</em>[<em>n</em>]</sub>.</td>
</tr>
</table>
<p class="noindent">The first equation gives <em>y</em><sub>1</sub> = <em>b</em><sub><em>π</em>[1]</sub> directly. Knowing the value of <em>y</em><sub>1</sub>, you can substitute it into the second equation, yielding</p>
<p class="eql"><em>y</em><sub>2</sub> = <em>b</em><sub><em>π</em>[2]</sub> − <em>l</em><sub>21</sub><em>y</em><sub>1</sub>.</p>
<p class="noindent">Next, you can substitute both <em>y</em><sub>1</sub> and <em>y</em><sub>2</sub> into the third equation, obtaining</p>
<p class="eql"><em>y</em><sub>3</sub> = <em>b</em><sub><em>π</em>[3]</sub> − (<em>l</em><sub>31</sub><em>y</em><sub>1</sub> + <em>l</em><sub>32</sub><em>y</em><sub>2</sub>).</p>
<p class="noindent">In general, you substitute <em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, … , <em>y</em><sub><em>i</em>−1</sub> “forward” into the <em>i</em>th equation to solve for <em>y<sub>i</sub></em>:</p>
<p class="eql"><img alt="art" src="images/Art_P906.jpg"/></p>
<a id="p823"/>
<p>Once you’ve solved for <em>y</em>, you can solve for <em>x</em> in equation (28.6) using <strong><em><span class="blue1">back substitution</span></em></strong>, which is similar to forward substitution. This time, you solve the <em>n</em>th equation first and work backward to the first equation. Like forward substitution, this process runs in Θ(<em>n</em><sup>2</sup>) time. Since <em>U</em> is upper-triangular, the matrix equation <em>Ux</em> = <em>y</em> is equivalent to the <em>n</em> equations</p>
<table class="table2b">
<tr>
<td class="td2"><p class="right"><em>u</em><sub>11</sub><em>x</em><sub>1</sub> + <em>u</em><sub>12</sub><em>x</em><sub>2</sub> + <span class="font1">⋯</span> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>1,<em>n</em>−2</sub><em>x</em><sub><em>n</em>−2</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>1,<em>n</em>−1</sub><em>x</em><sub><em>n</em>−1</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>1<em>n</em></sub><em>x</em><sub><em>n</em></sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y</em><sub>1</sub>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>u</em><sub>22</sub><em>x</em><sub>2</sub> + <span class="font1">⋯</span> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>2,<em>n</em>−2</sub><em>x</em><sub><em>n</em>−2</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>2,<em>n</em>−1</sub><em>x</em><sub><em>n</em>−1</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub>2<em>n</em></sub><em>x</em><sub><em>n</em></sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y</em><sub>2</sub>,</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"/>
<td class="td2"/>
<td class="td2"/>
<td class="td2"><p class="center"><span class="font1">⋮</span></p></td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="right"><em>u</em><sub><em>n</em>−2,<em>n</em>−2</sub><em>x</em><sub><em>n</em>−2</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub><em>n</em>−2,<em>n</em>−1</sub><em>x</em><sub><em>n</em>−1</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub><em>n</em>−2,<em>n</em></sub><em>x</em><sub><em>n</em></sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y</em><sub><em>n</em>−2</sub>,</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"/>
<td class="td2"><p class="right"><em>u</em><sub><em>n</em>−1,<em>n</em>−1</sub><em>x</em><sub><em>n</em>−1</sub> +</p></td>
<td class="td2"><p class="right"><em>u</em><sub><em>n</em>−1,<em>n</em></sub><em>x<sub>n</sub></em></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y</em><sub><em>n</em>−1</sub>,</p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"/>
<td class="td2"/>
<td class="td2"><p class="right"><em>u</em><sub><em>n,n</em></sub><em>x<sub>n</sub></em></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y<sub>n</sub></em>.</p></td>
</tr>
</table>
<p class="noindent">Thus, you can solve for <em>x<sub>n</sub></em>, <em>x</em><sub><em>n</em>−1</sub>, … , <em>x</em><sub>1</sub> successively as follows:</p>
<table class="table2b">
<tr>
<td class="td2"><p class="noindent"><em>x<sub>n</sub></em></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>y<sub>n</sub></em>/<em>u</em><sub><em>n,n</em></sub>,</p></td>
</tr>
<tr>
<td class="td2"><p class="noindent"><em>x</em><sub><em>n</em>−1</sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">(<em>y</em><sub><em>n</em>−1</sub> − <em>u</em><sub><em>n</em>−1,<em>n</em></sub><em>x<sub>n</sub></em>)/<em>u</em><sub><em>n</em>−1,<em>n</em>−1</sub>,</p></td>
</tr>
<tr>
<td class="td2"><p class="noindent"><em>x</em><sub><em>n</em>−2</sub></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent">(<em>y</em><sub><em>n</em>−2</sub> − (<em>u</em><sub><em>n</em>−2,<em>n</em>−1</sub><em>x</em><sub><em>n</em>−1</sub> + <em>u</em><sub><em>n</em>−2,<em>n</em></sub><em>x<sub>n</sub></em>))/<em>u</em><sub><em>n</em>−2,<em>n</em>−2,</sub></p></td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center"><span class="font1">⋮</span></p></td>
<td class="td2"/>
</tr>
</table>
<p class="noindent">or, in general,</p>
<p class="eql"><img alt="art" src="images/Art_P907.jpg"/></p>
<p>Given <em>P</em>, <em>L</em>, <em>U</em>, and <em>b</em>, the procedure LUP-S<small>OLVE</small> on the next page solves for <em>x</em> by combining forward and back substitution. The permutation matrix <em>P</em> is represented by the array <em>π</em>. The procedure first solves for <em>y</em> using forward substitution in lines 2–3, and then it solves for <em>x</em> using backward substitution in lines 4–5. Since the summation within each of the <strong>for</strong> loops includes an implicit loop, the running time is Θ(<em>n</em><sup>2</sup>).</p>
<p>As an example of these methods, consider the system of linear equations defined by <em>Ax</em> = <em>b</em>, where</p>
<p class="eql"><img alt="art" src="images/Art_P908.jpg"/></p>
<a id="p824"/>
<div class="pull-quote1">
<p class="box-heading">LUP-S<small>OLVE</small>(<em>L</em>, <em>U</em>, <em>π</em>, <em>b</em>, <em>n</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent">let <em>x</em> and <em>y</em> be new vectors of length <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P909.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = <em>n</em> <strong>downto</strong> 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P910.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>x</em></p></td>
</tr>
</table>
</div>
<p class="noindent">and we want to solve for the unknown <em>x</em>. The LUP decomposition is</p>
<p class="eql"><img alt="art" src="images/Art_P911.jpg"/></p>
<p class="noindent">(You might want to verify that <em>PA</em> = <em>LU</em>.) Using forward substitution, solve <em>Ly</em> = <em>Pb</em> for <em>y</em>:</p>
<p class="eql"><img alt="art" src="images/Art_P912.jpg"/></p>
<p class="noindent">obtaining</p>
<p class="eql"><img alt="art" src="images/Art_P913.jpg"/></p>
<p class="noindent">by computing first <em>y</em><sub>1</sub>, then <em>y</em><sub>2</sub>, and finally <em>y</em><sub>3</sub>. Then, using back substitution, solve <em>Ux</em> = <em>y</em> for <em>x</em>:</p>
<p class="eql"><img alt="art" src="images/Art_P914.jpg"/></p>
<p class="noindent">thereby obtaining the desired answer</p>
<p class="eql"><img alt="art" src="images/Art_P915.jpg"/></p>
<p class="noindent">by computing first <em>x</em><sub>3</sub>, then <em>x</em><sub>2</sub>, and finally <em>x</em><sub>1</sub>.</p>
<p class="level4"><strong>Computing an LU decomposition</strong></p>
<p class="noindent">Given an LUP decomposition for a nonsingular matrix <em>A</em>, you can use forward and back substitution to solve the system <em>Ax</em> = <em>b</em> of linear equations. Now let’s see <a id="p825"/>how to efficiently compute an LUP decomposition for <em>A</em>. We start with the simpler case in which <em>A</em> is an <em>n</em> × <em>n</em> nonsingular matrix and <em>P</em> is absent (or, equivalently, <em>P</em> = <em>I<sub>n</sub></em>, the <em>n</em> × <em>n</em> identity matrix), so that <em>A</em> = <em>LU</em>. We call the two matrices <em>L</em> and <em>U</em> an <strong><em><span class="blue1">LU decomposition</span></em></strong> of <em>A</em>.</p>
<p>To create an LU decomposition, we’ll use a process known as <strong><em><span class="blue1">Gaussian elimination</span></em></strong>. Start by subtracting multiples of the first equation from the other equations in order to remove the first variable from those equations. Then subtract multiples of the second equation from the third and subsequent equations so that now the first and second variables are removed from them. Continue this process until the system that remains has an upper-triangular form—this is the matrix <em>U</em>. The matrix <em>L</em> comprises the row multipliers that cause variables to be eliminated.</p>
<p>To implement this strategy, let’s start with a recursive formulation. The input is an <em>n</em> × <em>n</em> nonsingular matrix <em>A</em>. If <em>n</em> = 1, then nothing needs to be done: just choose <em>L</em> = <em>I</em><sub>1</sub> and <em>U</em> = <em>A</em>. For <em>n</em> &gt; 1, break <em>A</em> into four parts:</p>
<p class="eqr"><img alt="art" src="images/Art_P916.jpg"/></p>
<p class="noindent">where <em>v</em> = (<em>a</em><sub>21</sub>, <em>a</em><sub>31</sub>, … , <em>a</em><sub><em>n</em>1</sub>) is a column (<em>n</em>−1)-vector, <em>w</em><sup>T</sup> = (<em>a</em><sub>12</sub>, <em>a</em><sub>13</sub>, … , <em>a</em><sub>1<em>n</em></sub>)<sup>T</sup> is a row (<em>n</em> − 1)-vector, and <em>A</em>′ is an (<em>n</em> − 1) × (<em>n</em> − 1) matrix. Then, using matrix algebra (verify the equations by simply multiplying through), factor <em>A</em> as</p>
<p class="eqr"><img alt="art" src="images/Art_P917.jpg"/></p>
<p class="noindent">The 0s in the first and second matrices of equation (28.9) are row and column (<em>n</em> − 1)-vectors, respectively. The term <em>vw</em><sup>T</sup>/<em>a</em><sub>11</sub> is an (<em>n</em> − 1) × (<em>n</em> − 1) matrix formed by taking the outer product of <em>v</em> and <em>w</em> and dividing each element of the result by <em>a</em><sub>11</sub>. Thus it conforms in size to the matrix <em>A</em>′ from which it is subtracted. The resulting (<em>n</em> − 1) × (<em>n</em> − 1) matrix</p>
<p class="eqr"><img alt="art" src="images/Art_P918.jpg"/></p>
<p class="noindent">is called the <strong><em><span class="blue1">Schur complement</span></em></strong> of <em>A</em> with respect to <em>a</em><sub>11</sub>.</p>
<p>We claim that if <em>A</em> is nonsingular, then the Schur complement is nonsingular, too. Why? Suppose that the Schur complement, which is (<em>n</em> − 1) × (<em>n</em> − 1), is singular. Then by Theorem D.1, it has row rank strictly less than <em>n</em> − 1. Because the bottom <em>n</em> − 1 entries in the first column of the matrix</p>
<a id="p826"/>
<p class="eql"><img alt="art" src="images/Art_P919.jpg"/></p>
<p class="noindent">are all 0, the bottom <em>n</em> − 1 rows of this matrix must have row rank strictly less than <em>n</em> − 1. The row rank of the entire matrix, therefore, is strictly less than <em>n</em>. Applying Exercise D.2-8 on page 1223 to equation (28.9), <em>A</em> has rank strictly less than <em>n</em>, and from Theorem D.1, we derive the contradiction that <em>A</em> is singular.</p>
<p>Because the Schur complement is nonsingular, it, too, has an LU decomposition, which we can find recursively. Let’s say that</p>
<p class="eql"><em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub>11</sub> = <em>L</em>′<em>U</em>′,</p>
<p class="noindent">where <em>L</em>′ is unit lower-triangular and <em>U</em>′ is upper-triangular. The LU decomposition of <em>A</em> is then <em>A</em> = <em>LU</em>, with</p>
<p class="eql"><img alt="art" src="images/Art_P920.jpg"/></p>
<p class="noindent">as shown by</p>
<p class="eql"><img alt="art" src="images/Art_P921.jpg"/></p>
<p class="noindent">Because <em>L</em>′ is unit lower-triangular, so is <em>L</em>, and because <em>U</em>′ is upper-triangular, so is <em>U</em>.</p>
<p>Of course, if <em>a</em><sub>11</sub> = 0, this method doesn’t work, because it divides by 0. It also doesn’t work if the upper leftmost entry of the Schur complement <em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub>11</sub> is 0, since the next step of the recursion will divide by it. The denominators in each step of LU decomposition are called <strong><em><span class="blue1">pivots</span></em></strong>, and they occupy the diagonal elements of the matrix <em>U</em>. The permutation matrix <em>P</em> included in LUP decomposition provides a way to avoid dividing by 0, as we’ll see below. Using permutations to avoid division by 0 (or by small numbers, which can contribute to numerical instability), is called <strong><em><span class="blue1">pivoting</span></em></strong>.</p>
<p>An important class of matrices for which LU decomposition always works correctly is the class of symmetric positive-definite matrices. Such matrices require no pivoting to avoid dividing by 0 in the recursive strategy outlined above. We will prove this result, as well as several others, in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a>.</p>
<a id="p827"/>
<p>The pseudocode in the procedure LU-D<small>ECOMPOSITION</small> follows the recursive strategy, except that an iteration loop replaces the recursion. (This transformation is a standard optimization for a “tail-recursive” procedure—one whose last operation is a recursive call to itself. See Problem 7-5 on page 202.) The procedure initializes the matrix <em>U</em> with 0s below the diagonal and matrix <em>L</em> with 1s on its diagonal and 0s above the diagonal. Each iteration works on a square submatrix, using its upper leftmost element as the pivot to compute the <em>v</em> and <em>w</em> vectors and the Schur complement, which becomes the square submatrix worked on by the next iteration.</p>
<div class="pull-quote1">
<p class="box-heading">LU-D<small>ECOMPOSITION</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  1</span></p></td>
<td class="td1" colspan="2">
<p class="noindent">let <em>L</em> and <em>U</em> be new <em>n</em> × <em>n</em> matrices</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  2</span></p></td>
<td class="td1" colspan="2">
<p class="noindent">initialize <em>U</em> with 0s below the diagonal</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  3</span></p></td>
<td class="td1" colspan="2">
<p class="noindent">initialize <em>L</em> with 1s on the diagonal and 0s above the diagonal</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  4</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>for</strong> <em>k</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  5</span></p></td>
<td class="td1" colspan="2"><p class="p2"><em>u<sub>kk</sub></em> = <em>a<sub>kk</sub></em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  6</span></p></td>
<td class="td1" colspan="2"><p class="p2"><strong>for</strong> <em>i</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  7</span></p></td>
<td class="td1"><p class="p3"><em>l<sub>ik</sub></em> = <em>a<sub>ik</sub></em>/<em>a<sub>kk</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>a<sub>ik</sub></em> holds <em>v<sub>i</sub></em></span></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  8</span></p></td>
<td class="td1"><p class="p3"><em>u<sub>ki</sub></em> = <em>a<sub>ki</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>a<sub>ki</sub></em> holds <em>w<sub>i</sub></em></span></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">  9</span></p></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> compute the Schur complement …</span></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">10</span></p></td>
<td class="td1" colspan="2"><p class="p3"><strong>for</strong> <em>j</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">11</span></p></td>
<td class="td1"><p class="p4"><em>a<sub>ij</sub></em> = <em>a<sub>ij</sub></em> − <em>l<sub>ik</sub>u<sub>kj</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> … and store it back into <em>A</em></span></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">12</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>return</strong> <em>L</em> and <em>U</em></p></td>
</tr>
</table>
</div>
<p>Each recursive step in the description above takes place in one iteration of the outer <strong>for</strong> loop of lines 4–11. Within this loop, line 5 determines the pivot to be <em>u<sub>kk</sub></em> = <em>a<sub>kk</sub></em>. The <strong>for</strong> loop in lines 6–8 (which does not execute when <em>k</em> = <em>n</em>) uses the <em>v</em> and <em>w</em> vectors to update <em>L</em> and <em>U</em>. Line 7 determines the below-diagonal elements of <em>L</em>, storing <em>v<sub>i</sub></em>/<em>a<sub>kk</sub></em> in <em>l<sub>ik</sub></em>, and line 8 computes the above-diagonal elements of <em>U</em>, storing <em>w<sub>i</sub></em> in <em>u<sub>ki</sub></em>. Finally, lines 9–11 compute the elements of the Schur complement and store them back into the matrix <em>A</em>. (There is no need to divide by <em>a<sub>kk</sub></em> in line 11 because that already happened when line 7 computed <em>l<sub>ik</sub></em>.) Because line 11 is triply nested, LU-D<small>ECOMPOSITION</small> runs in Θ(<em>n</em><sup>3</sup>) time.</p>
<p><a href="chapter028.xhtml#Fig_28-1">Figure 28.1</a> illustrates the operation of LU-D<small>ECOMPOSITION</small>. It shows a standard optimization of the procedure that stores the significant elements of <em>L</em> and <em>U</em> in place in the matrix <em>A</em>. Each element <em>a<sub>ij</sub></em> corresponds to either <em>l<sub>ij</sub></em> (if <em>i</em> &gt; <em>j</em>) or <em>u<sub>ij</sub></em> (if <em>i</em> ≤ <em>j</em>), so that the matrix <em>A</em> holds both <em>L</em> and <em>U</em> when the procedure terminates. To obtain the pseudocode for this optimization from the pseudocode for the LU-D<small>ECOMPOSITION</small> procedure, just replace each reference to <em>l</em> or <em>u</em> by <em>a</em>. You can verify that this transformation preserves correctness.</p>
<a id="p828"/>
<div class="divimage">
<p class="fig-imga" id="Fig_28-1"><img alt="art" src="images/Art_P922.jpg"/></p>
<p class="caption"><strong>Figure 28.1</strong> The operation of LU-D<small>ECOMPOSITION</small>. <strong>(a)</strong> The matrix <em>A</em>. <strong>(b)</strong> The result of the first iteration of the outer <strong>for</strong> loop of lines 4–11. The element <em>a</em><sub>11</sub> = 2 highlighted in blue is the pivot, the tan column is <em>v</em>/<em>a</em><sub>11</sub>, and the tan row is <em>w</em><sup>T</sup>. The elements of <em>U</em> computed thus far are above the horizontal line, and the elements of <em>L</em> are to the left of the vertical line. The Schur complement matrix <em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub>11</sub> occupies the lower right. <strong>(c)</strong> The result of the next iteration of the outer <strong>for</strong> loop, on the Schur complement matrix from part (b). The element <em>a</em><sub>22</sub> = 4 highlighted in blue is the pivot, and the tan column and row are <em>v</em>/<em>a</em><sub>22</sub> and <em>w</em><sup>T</sup> (in the partitioning of the Schur complement), respectively. Lines divide the matrix into the elements of <em>U</em> computed so far (above), the elements of <em>L</em> computed so far (left), and the new Schur complement (lower right). <strong>(d)</strong> After the next iteration, the matrix <em>A</em> is factored. The element 3 in the new Schur complement becomes part of <em>U</em> when the recursion terminates.) <strong>(e)</strong> The factorization <em>A</em> = <em>LU</em>.</p>
</div>
<p class="level4"><strong>Computing an LUP decomposition</strong></p>
<p class="noindent">If the diagonal of the matrix given to LU-D<small>ECOMPOSITION</small> contains any 0s, then the procedure will attempt to divide by 0, which would cause disaster. Even if the diagonal contains no 0s, but does have numbers with small absolute values, dividing by such numbers can cause numerical instabilities. Therefore, LUP decomposition pivots on entries with the largest absolute values that it can find.</p>
<p>In LUP decomposition, the input is an <em>n</em> × <em>n</em> nonsingular matrix <em>A</em>, with a goal of finding a permutation matrix <em>P</em>, a unit lower-triangular matrix <em>L</em>, and an upper-triangular matrix <em>U</em> such that <em>PA</em> = <em>LU</em>. Before partitioning the matrix <em>A</em>, as LU decomposition does, LUP decomposition moves a nonzero element, say <em>a</em><sub><em>k</em>1</sub>, from somewhere in the first column to the (1, 1) position of the matrix. For the greatest numerical stability, LUP decomposition chooses the element in the first column with the greatest absolute value as <em>a</em><sub><em>k</em>1</sub>. (The first column cannot contain only 0s, for then <em>A</em> would be singular, because its determinant would be 0, by Theorems D.4 and D.5 on page 1221.) In order to preserve the set of equations, LUP decomposition exchanges row 1 with row <em>k</em>, which is equivalent to multiplying <em>A</em> by a <a id="p829"/>permutation matrix <em>Q</em> on the left (Exercise D.1-4 on page 1219). Thus, the analog to equation (28.8) expresses <em>QA</em> as</p>
<p class="eql"><img alt="art" src="images/Art_P923.jpg"/></p>
<p class="noindent">where <em>v</em> = (<em>a</em><sub>21</sub>, <em>a</em><sub>31</sub>, … , <em>a</em><sub><em>n</em>1</sub>), except that <em>a</em><sub>11</sub> replaces <em>a</em><sub><em>k</em>1</sub>; <em>w</em><sup>T</sup> = (<em>a</em><sub><em>k</em>2</sub>, <em>a</em><sub><em>k</em>3</sub>, … , <em>a<sub>kn</sub></em>)<sup>T</sup>; and <em>A</em>′ is an (<em>n</em> − 1) × (<em>n</em> − 1) matrix. Since <em>a</em><sub><em>k</em>1</sub> ≠ 0, the analog to equation (28.9) guarantees no division by 0:</p>
<p class="eql"><img alt="art" src="images/Art_P924.jpg"/></p>
<p>Just as in LU decomposition, if <em>A</em> is nonsingular, then the Schur complement <em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub><em>k</em>1</sub> is nonsingular, too. Therefore, you can recursively find an LUP decomposition for it, with unit lower-triangular matrix <em>L</em>′, upper-triangular matrix <em>U</em>′, and permutation matrix <em>P</em>′, such that</p>
<p class="eql"><em>P</em>′(<em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub><em>k</em>1</sub>) = <em>L</em>′<em>U</em>′.</p>
<p class="noindent">Define</p>
<p class="eql"><img alt="art" src="images/Art_P925.jpg"/></p>
<p class="noindent">which is a permutation matrix, since it is the product of two permutation matrices (Exercise D.1-4 on page 1219). This definition of <em>P</em> gives</p>
<p class="eql"><img alt="art" src="images/Art_P926.jpg"/></p>
<a id="p830"/>
<p class="noindent">which yields the LUP decomposition. Because <em>L</em>′ is unit lower-triangular, so is <em>L</em>, and because <em>U</em>′ is upper-triangular, so is <em>U</em>.</p>
<p>Notice that in this derivation, unlike the one for LU decomposition, both the column vector <em>v</em>/<em>a</em><sub><em>k</em>1</sub> and the Schur complement <em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub><em>k</em>1</sub> are multiplied by the permutation matrix <em>P</em>′. The procedure LUP-<small>DECOMPOSITION</small> gives the pseudocode for LUP decomposition.</p>
<div class="pull-quote1">
<p class="box-heading">LUP-D<small>ECOMPOSITION</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  1</span></p></td>
<td class="td1" colspan="2">
<p class="noindent">let <em>π</em>[1 : <em>n</em>] be a new array</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  2</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  3</span></p></td>
<td class="td1"><p class="p2"><em>π</em>[<em>i</em>] = <em>i</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> initialize <em>π</em> to the identity permutation</span></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  4</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>for</strong> <em>k</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  5</span></p></td>
<td class="td1" colspan="2"><p class="p2"><em>p</em> = 0</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  6</span></p></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>i</em> = <em>k</em> <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> find largest absolute value in column <em>k</em></span></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  7</span></p></td>
<td class="td1" colspan="2">
<p class="p3"><strong>if</strong> |<em>a<sub>ik</sub></em>| &gt; <em>p</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  8</span></p></td>
<td class="td1" colspan="2">
<p class="p4"><em>p</em> = |<em>a<sub>ik</sub></em>|</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">  9</span></p></td>
<td class="td1"><p class="p4"><em>k</em>′ = <em>i</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> row number of the largest found so far</span></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">10</span></p></td>
<td class="td1" colspan="2"><p class="p2"><strong>if</strong> <em>p</em> == 0</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">11</span></p></td>
<td class="td1" colspan="2">
<p class="p3"><strong>error</strong> “singular matrix”</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">12</span></p></td>
<td class="td1" colspan="2"><p class="p2">exchange <em>π</em>[<em>k</em>] with <em>π</em>[<em>k</em>′]</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">13</span></p></td>
<td class="td1" colspan="2"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">14</span></p></td>
<td class="td1"><p class="p3">exchange <em>a<sub>ki</sub></em> with <em>a<sub>k′i</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> exchange rows <em>k</em> and <em>k</em>′</span></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">15</span></p></td>
<td class="td1" colspan="2"><p class="p2"><strong>for</strong> <em>i</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">16</span></p></td>
<td class="td1" colspan="2">
<p class="p3"><em>a<sub>ik</sub></em> = <em>a<sub>ik</sub></em>/<em>a<sub>kk</sub></em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">17</span></p></td>
<td class="td1" colspan="2">
<p class="p3"><strong>for</strong> <em>j</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">18</span></p></td>
<td class="td1"><p class="p4"><em>a<sub>ij</sub></em> = <em>a<sub>ij</sub></em> − <em>a<sub>ik</sub>a<sub>kj</sub></em></p></td>
<td class="td1"><span class="red"><strong>//</strong> compute <em>L</em> and <em>U</em> in place in <em>A</em></span></td>
</tr>
</table>
</div>
<p>Like LU-D<small>ECOMPOSITION</small>, the LUP-D<small>ECOMPOSITION</small> procedure replaces the recursion with an iteration loop. As an improvement over a direct implementation of the recursion, the procedure dynamically maintains the permutation matrix <em>P</em> as an array <em>π</em>, where <em>π</em>[<em>i</em>] = <em>j</em> means that the <em>i</em>th row of <em>P</em> contains a 1 in column <em>j</em>. The LUP-D<small>ECOMPOSITION</small> procedure also implements the improvement mentioned earlier, computing <em>L</em> and <em>U</em> in place in the matrix <em>A</em>. Thus, when the procedure terminates,</p>
<p class="eql"><img alt="art" src="images/Art_P927.jpg"/></p>
<p><a href="chapter028.xhtml#Fig_28-2">Figure 28.2</a> illustrates how LUP-D<small>ECOMPOSITION</small> factors a matrix. Lines 2–3 initialize the array <em>π</em> to represent the identity permutation. The outer <strong>for</strong> loop of lines 4–18 implements the recursion, finding an LUP decomposition of <a id="p831"/>the (<em>n</em> − <em>k</em> + 1) × (<em>n</em> − <em>k</em> + 1) submatrix whose upper left is in row <em>k</em> and column <em>k</em>. Each time through the outer loop, lines 5–9 determine the element <em>a</em><sub><em>k′k</em></sub> with the largest absolute value of those in the current first column (column <em>k</em>) of the (<em>n</em> − <em>k</em> + 1) × (<em>n</em> − <em>k</em> + 1) submatrix that the procedure is currently working on. If all elements in the current first column are 0, lines 10–11 report that the matrix is singular. To pivot, line 12 exchanges <em>π</em>[<em>k</em>′] with <em>π</em>[<em>k</em>], and lines 13–14 exchange the <em>k</em>th and <em>k</em>′th rows of <em>A</em>, thereby making the pivot element <em>a<sub>kk</sub></em>. (The entire rows are swapped because in the derivation of the method above, not only is <em>A</em>′ − <em>vw</em><sup>T</sup>/<em>a</em><sub><em>k</em>1</sub> multiplied by <em>P</em>′, but so is <em>v</em>/<em>a</em><sub><em>k</em>1</sub>.) Finally, the Schur complement is computed by lines 15–18 in much the same way as it is computed by lines 6–11 of LU-D<small>ECOMPOSITION</small>, except that here the operation is written to work in place.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_28-2"><img alt="art" src="images/Art_P928.jpg"/></p>
<p class="caption"><strong>Figure 28.2</strong> The operation of LUP-D<small>ECOMPOSITION</small>. <strong>(a)</strong> The input matrix <em>A</em> with the identity permutation of the rows in yellow on the left. The first step of the algorithm determines that the element 5 highlighted in blue in the third row is the pivot for the first column. <strong>(b)</strong> Rows 1 and 3 are swapped and the permutation is updated. The tan column and row represent <em>v</em> and <em>w</em><sup>T</sup>. <strong>(c)</strong> The vector <em>v</em> is replaced by <em>v</em>/5, and the lower right of the matrix is updated with the Schur complement. Lines divide the matrix into three regions: elements of <em>U</em> (above), elements of <em>L</em> (left), and elements of the Schur complement (lower right). <strong>(d)–(f)</strong> The second step. <strong>(g)–(i)</strong> The third step. No further changes occur on the fourth (final) step. <strong>(j)</strong> The LUP decomposition <em>PA</em> = <em>LU</em>.</p>
</div>
<a id="p832"/>
<p>Because of its triply nested loop structure, LUP-D<small>ECOMPOSITION</small> has a running time of Θ(<em>n</em><sup>3</sup>), which is the same as that of LU-D<small>ECOMPOSITION</small>. Thus, pivoting costs at most a constant factor in time.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>28.1-1</em></strong></p>
<p class="noindent">Solve the equation</p>
<p class="eql"><img alt="art" src="images/Art_P929.jpg"/></p>
<p class="noindent">by using forward substitution.</p>
<p class="level3"><strong><em>28.1-2</em></strong></p>
<p class="noindent">Find an LU decomposition of the matrix</p>
<p class="eql"><img alt="art" src="images/Art_P930.jpg"/></p>
<p class="level3"><strong><em>28.1-3</em></strong></p>
<p class="noindent">Solve the equation</p>
<p class="eql"><img alt="art" src="images/Art_P931.jpg"/></p>
<p class="noindent">by using an LUP decomposition.</p>
<p class="level3"><strong><em>28.1-4</em></strong></p>
<p class="noindent">Describe the LUP decomposition of a diagonal matrix.</p>
<a id="p833"/>
<p class="level3"><strong><em>28.1-5</em></strong></p>
<p class="noindent">Describe the LUP decomposition of a permutation matrix, and prove that it is unique.</p>
<p class="level3"><strong><em>28.1-6</em></strong></p>
<p class="noindent">Show that for all <em>n</em> ≥ 1, there exists a singular <em>n</em> × <em>n</em> matrix that has an LU decomposition.</p>
<p class="level3"><strong><em>28.1-7</em></strong></p>
<p class="noindent">In LU-D<small>ECOMPOSITION</small>, is it necessary to perform the outermost <strong>for</strong> loop iteration when <em>k</em> = <em>n</em>? How about in LUP-D<small>ECOMPOSITION</small>?</p>
</section>
<p class="line1"/>
<section title="28.2 Inverting matrices">
<a id="Sec_28.2"/>
<p class="level1" id="h1-163"><a href="toc.xhtml#Rh1-163"><strong>28.2    Inverting matrices</strong></a></p>
<p class="noindent">Although you can use equation (28.3) to solve a system of linear equations by computing a matrix inverse, in practice you are better off using more numerically stable techniques, such as LUP decomposition. Sometimes, however, you really do need to compute a matrix inverse. This section shows how to use LUP decomposition to compute a matrix inverse. It also proves that matrix multiplication and computing the inverse of a matrix are equivalently hard problems, in that (subject to technical conditions) an algorithm for one can solve the other in the same asymptotic running time. Thus, you can use Strassen’s algorithm (see <a href="chapter004.xhtml#Sec_4.2">Section 4.2</a>) for matrix multiplication to invert a matrix. Indeed, Strassen’s original paper was motivated by the idea that a set of a linear equations could be solved more quickly than by the usual method.</p>
<p class="level4"><strong>Computing a matrix inverse from an LUP decomposition</strong></p>
<p class="noindent">Suppose that you have an LUP decomposition of a matrix <em>A</em> in the form of three matrices <em>L</em>, <em>U</em>, and <em>P</em> such that <em>PA</em> = <em>LU</em>. Using LUP-S<small>OLVE</small>, you can solve an equation of the form <em>Ax</em> = <em>b</em> in Θ(<em>n</em><sup>2</sup>) time. Since the LUP decomposition depends on <em>A</em> but not <em>b</em>, you can run LUP-S<small>OLVE</small> on a second set of equations of the form <em>Ax</em> = <em>b</em>′ in Θ(<em>n</em><sup>2</sup>) additional time. In general, once you have the LUP decomposition of <em>A</em>, you can solve, in Θ(<em>kn</em><sup>2</sup>) time, <em>k</em> versions of the equation <em>Ax</em> = <em>b</em> that differ only in the vector <em>b</em>.</p>
<p>Let’s think of the equation</p>
<p class="eqr"><img alt="art" src="images/Art_P932.jpg"/></p>
<p class="noindent">which defines the matrix <em>X</em>, the inverse of <em>A</em>, as a set of <em>n</em> distinct equations of the form <em>Ax</em> = <em>b</em>. To be precise, let <em>X<sub>i</sub></em> denote the <em>i</em>th column of <em>X</em>, and recall that the <a id="p834"/>unit vector <em>e<sub>i</sub></em> is the <em>i</em>th column of <em>I<sub>n</sub></em>. You can then solve equation (28.11) for <em>X</em> by using the LUP decomposition for <em>A</em> to solve each equation</p>
<p class="eql"><em>AX<sub>i</sub></em> = <em>e<sub>i</sub></em></p>
<p class="noindent">separately for <em>X<sub>i</sub></em>. Once you have the LUP decomposition, you can compute each of the <em>n</em> columns <em>X<sub>i</sub></em> in Θ(<em>n</em><sup>2</sup>) time, and so you can compute <em>X</em> from the LUP decomposition of <em>A</em> in Θ(<em>n</em><sup>3</sup>) time. Since you find the LUP decomposition of <em>A</em> in Θ(<em>n</em><sup>3</sup>) time, you can compute the inverse <em>A</em><sup>−1</sup> of a matrix <em>A</em> in Θ(<em>n</em><sup>3</sup>) time.</p>
<p class="level4"><strong>Matrix multiplication and matrix inversion</strong></p>
<p class="noindent">Now let’s see how the theoretical speedups obtained for matrix multiplication translate to speedups for matrix inversion. In fact, we’ll prove something stronger: matrix inversion is equivalent to matrix multiplication, in the following sense. If <em>M</em>(<em>n</em>) denotes the time to multiply two <em>n</em> × <em>n</em> matrices, then a nonsingular <em>n</em> × <em>n</em> matrix can be inverted in <em>O</em>(<em>M</em>(<em>n</em>)) time. Moreover, if <em>I</em>(<em>n</em>) denotes the time to invert a nonsingular <em>n</em> × <em>n</em> matrix, then two <em>n</em> × <em>n</em> matrices can be multiplied in <em>O</em>(<em>I</em>(<em>n</em>)) time. We prove these results as two separate theorems.</p>
<p class="theo"><strong><em>Theorem 28.1 (Multiplication is no harder than inversion)</em></strong></p>
<p class="noindent">If an <em>n</em> × <em>n</em> matrix can be inverted in <em>I</em>(<em>n</em>) time, where <em>I</em>(<em>n</em>) = Ω(<em>n</em><sup>2</sup>) and <em>I</em>(<em>n</em>) satisfies the regularity condition <em>I</em>(3<em>n</em>) = <em>O</em>(<em>I</em>(<em>n</em>)), then two <em>n</em> × <em>n</em> matrices can be multiplied in <em>O</em>(<em>I</em>(<em>n</em>)) time.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let <em>A</em> and <em>B</em> be <em>n</em> × <em>n</em> matrices. To compute their product <em>C</em> = <em>AB</em>, define the 3<em>n</em> × 3<em>n</em> matrix <em>D</em> by</p>
<p class="eql"><img alt="art" src="images/Art_P933.jpg"/></p>
<p class="noindent">The inverse of <em>D</em> is</p>
<p class="eql"><img alt="art" src="images/Art_P934.jpg"/></p>
<p class="noindent">and thus to compute the product <em>AB</em>, just take the upper right <em>n</em> × <em>n</em> submatrix of <em>D</em><sup>−1</sup>.</p>
<p>Constructing matrix <em>D</em> takes Θ(<em>n</em><sup>2</sup>) time, which is <em>O</em>(<em>I</em>(<em>n</em>)) from the assumption that <em>I</em>(<em>n</em>) = Ω(<em>n</em><sup>2</sup>), and inverting <em>D</em> takes <em>O</em>(<em>I</em>(3<em>n</em>)) = <em>O</em>(<em>I</em>(<em>n</em>)) time, by the regularity condition on <em>I</em>(<em>n</em>). We thus have <em>M</em>(<em>n</em>) = <em>O</em>(<em>I</em>(<em>n</em>)).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Note that <em>I</em>(<em>n</em>) satisfies the regularity condition whenever <em>I</em>(<em>n</em>) = Θ(<em>n<sup>c</sup></em> lg<sup><em>d</em></sup><em>n</em>) for any constants <em>c</em> &gt; 0 and <em>d</em> ≥ 0.</p>
<a id="p835"/>
<p>The proof that matrix inversion is no harder than matrix multiplication relies on some properties of symmetric positive-definite matrices proved in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a>.</p>
<p class="theo"><strong><em>Theorem 28.2 (Inversion is no harder than multiplication)</em></strong></p>
<p class="noindent">Suppose that two <em>n</em> × <em>n</em> real matrices can be multiplied in <em>M</em>(<em>n</em>) time, where <em>M</em>(<em>n</em>) = Ω(<em>n</em><sup>2</sup>) and <em>M</em>(<em>n</em>) satisfies the following two regularity conditions:</p>
<ol class="olnoindent" epub:type="list">
<li><em>M</em>(<em>n</em> + <em>k</em>) = <em>O</em>(<em>M</em>(<em>n</em>)) for any <em>k</em> in the range 0 ≤ <em>k</em> &lt; <em>n</em>, and</li>
<li class="litop"><em>M</em>(<em>n</em>/2) ≤ <em>cM</em>(<em>n</em>) for some constant <em>c</em> &lt; 1/2.</li></ol>
<p class="noindent">Then the inverse of any real nonsingular <em>n</em>×<em>n</em> matrix can be computed in <em>O</em>(<em>M</em>(<em>n</em>)) time.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let <em>A</em> be an <em>n</em> × <em>n</em> matrix with real-valued entries that is nonsingular. Assume that <em>n</em> is an exact power of 2 (i.e., <em>n</em> = 2<em><sup>l</sup></em> for some integer <em>l</em>); we’ll see at the end of the proof what to do if <em>n</em> is not an exact power of 2.</p>
<p>For the moment, assume that the <em>n</em> × <em>n</em> matrix <em>A</em> is symmetric and positive-definite. Partition each of <em>A</em> and its inverse <em>A</em><sup>−1</sup> into four <em>n</em>/2 × <em>n</em>/2 submatrices:</p>
<p class="eqr"><img alt="art" src="images/Art_P935.jpg"/></p>
<p class="noindent">Then, if we let</p>
<p class="eqr"><img alt="art" src="images/Art_P936.jpg"/></p>
<p class="noindent">be the Schur complement of <em>A</em> with respect to <em>B</em> (we’ll see more about this form of Schur complement in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a>), we have</p>
<p class="eqr"><img alt="art" src="images/Art_P937.jpg"/></p>
<p class="noindent">since <em>AA</em><sup>−1</sup> = <em>I<sub>n</sub></em>, as you can verify by performing the matrix multiplication. Because <em>A</em> is symmetric and positive-definite, Lemmas 28.4 and 28.5 in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a> imply that <em>B</em> and <em>S</em> are both symmetric and positive-definite. By Lemma 28.3 in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a>, therefore, the inverses <em>B</em><sup>−1</sup> and <em>S</em><sup>−1</sup> exist, and by Exercise D.2-6 on page 1223, <em>B</em><sup>−1</sup> and <em>S</em><sup>−1</sup> are symmetric, so that (<em>B</em><sup>−1</sup>)<sup>T</sup> = <em>B</em><sup>−1</sup> and (<em>S</em><sup>−1</sup>)<sup>T</sup> = <em>S</em><sup>−1</sup>. Therefore, to compute the submatrices</p>
<table class="table2b">
<tr>
<td class="td2"><em>R</em></td>
<td class="td2">=</td>
<td class="td2"><em>B</em><sup>−1</sup> + <em>B</em><sup>−1</sup><em>C</em><sup>T</sup><em>S</em><sup>−1</sup><em>CB</em><sup>−1</sup>,</td>
</tr>
<tr>
<td class="td2"><em>T</em></td>
<td class="td2">=</td>
<td class="td2">−<em>B</em><sup>−1</sup><em>C</em><sup>T</sup><em>S</em><sup>−1</sup>,</td>
</tr>
<tr>
<td class="td2"><em>U</em></td>
<td class="td2">=</td>
<td class="td2">−<em>S</em><sup>−1</sup><em>CB</em><sup>−1</sup>, and</td>
</tr>
<tr>
<td class="td2"><em>V</em></td>
<td class="td2">=</td>
<td class="td2"><em>S</em><sup>−1</sup></td>
</tr>
</table>
<p class="noindent">of <em>A</em><sup>−1</sup>, do the following, where all matrices mentioned are <em>n</em>/2 × <em>n</em>/2:</p>
<a id="p836"/>
<ol class="olnoindent" epub:type="list">
<li>Form the submatrices <em>B</em>, <em>C</em>, <em>C</em><sup>T</sup>, and <em>D</em> of <em>A</em>.</li>
<li class="litop">Recursively compute the inverse <em>B</em><sup>−1</sup> of <em>B</em>.</li>
<li class="litop">Compute the matrix product <em>W</em> = <em>CB</em><sup>−1</sup>, and then compute its transpose <em>W</em><sup>T</sup>, which equals <em>B</em><sup>−1</sup><em>C</em><sup>T</sup> (by Exercise D.1-2 on page 1219 and (<em>B</em><sup>−1</sup>)<sup>T</sup> = <em>B</em><sup>−1</sup>).</li>
<li class="litop">Compute the matrix product <em>X</em> = <em>WC</em><sup>T</sup>, which equals <em>CB</em><sup>−1</sup><em>C</em><sup>T</sup>, and then compute the matrix <em>S</em> = <em>D</em> − <em>X</em> = <em>D</em> − <em>CB</em><sup>−1</sup><em>C</em><sup>T</sup>.</li>
<li class="litop">Recursively compute the inverse <em>S</em><sup>−1</sup> of <em>S</em>.</li>
<li class="litop">Compute the matrix product <em>Y</em> = <em>S</em><sup>−1</sup><em>W</em>, which equals <em>S</em><sup>−1</sup><em>CB</em><sup>−1</sup>, and then compute its transpose <em>Y</em><sup>T</sup>, which equals <em>B</em><sup>−1</sup><em>C</em><sup>T</sup><em>S</em><sup>−1</sup> (by Exercise D.1-2, (<em>B</em><sup>−1</sup>)<sup>T</sup> = <em>B</em><sup>−1</sup>, and (<em>S</em><sup>−1</sup>)<sup>T</sup> = <em>S</em><sup>−1</sup>).</li>
<li class="litop">Compute the matrix product <em>Z</em> = <em>W</em><sup>T</sup><em>Y</em>, which equals <em>B</em><sup>−1</sup><em>C</em><sup>T</sup><em>S</em><sup>−1</sup><em>CB</em><sup>−1</sup>.</li>
<li class="litop">Set <em>R</em> = <em>B</em><sup>−1</sup> + <em>Z</em>.</li>
<li class="litop">Set <em>T</em> = −<em>Y</em><sup>T</sup>.</li>
<li class="litop">Set <em>U</em> = −<em>Y</em>.</li>
<li class="litop">Set <em>V</em> = <em>S</em><sup>−1</sup>.</li></ol>
<p>Thus, to invert an <em>n</em>×<em>n</em> symmetric positive-definite matrix, invert two <em>n</em>/2×<em>n</em>/2 matrices in steps 2 and 5; perform four multiplications of <em>n</em>/2 × <em>n</em>/2 matrices in steps 3, 4, 6, and 7; plus incur an additional cost of <em>O</em>(<em>n</em><sup>2</sup>) for extracting submatrices from <em>A</em>, inserting submatrices into <em>A</em><sup>−1</sup>, and performing a constant number of additions, subtractions, and transposes on <em>n</em>/2 × <em>n</em>/2 matrices. The running time is given by the recurrence</p>
<p class="eqr"><img alt="art" src="images/Art_P938.jpg"/></p>
<p class="noindent">The second line follows from the assumption that <em>M</em>(<em>n</em>) = Ω(<em>n</em><sup>2</sup>) and from the second regularity condition in the statement of the theorem, which implies that 4<em>M</em>(<em>n</em>/2) &lt; 2<em>M</em>(<em>n</em>). Because <em>M</em>(<em>n</em>) = Ω(<em>n</em><sup>2</sup>), case 3 of the master theorem (Theorem 4.1) applies to the recurrence (28.15), giving the <em>O</em>(<em>M</em>(<em>n</em>)) result.</p>
<p>It remains to prove how to obtain the same asymptotic running time for matrix multiplication as for matrix inversion when <em>A</em> is invertible but not symmetric and positive-definite. The basic idea is that for any nonsingular matrix <em>A</em>, the matrix <em>A</em><sup>T</sup><em>A</em> is symmetric (by Exercise D.1-2) and positive-definite (by Theorem D.6 on page 1222). The trick, then, is to reduce the problem of inverting <em>A</em> to the problem of inverting <em>A</em><sup>T</sup><em>A</em>.</p>
<p>The reduction is based on the observation that when <em>A</em> is an <em>n</em> × <em>n</em> nonsingular matrix, we have</p>
<a id="p837"/>
<p class="eql"><em>A</em><sup>−1</sup> = (<em>A</em><sup>T</sup><em>A</em>)<sup>−1</sup><em>A</em><sup>T</sup>,</p>
<p class="noindent">since ((<em>A</em><sup>T</sup><em>A</em>)<sup>−1</sup><em>A</em><sup>T</sup>)<em>A</em> = (<em>A</em><sup>T</sup><em>A</em>)<sup>−1</sup>(<em>A</em><sup>T</sup><em>A</em>) = <em>I<sub>n</sub></em> and a matrix inverse is unique. Therefore, to compute <em>A</em><sup>−1</sup>, first multiply <em>A</em><sup>T</sup> by <em>A</em> to obtain <em>A</em><sup>T</sup><em>A</em>, then invert the symmetric positive-definite matrix <em>A</em><sup>T</sup><em>A</em> using the above divide-and-conquer algorithm, and finally multiply the result by <em>A</em><sup>T</sup>. Each of these three steps takes <em>O</em>(<em>M</em>(<em>n</em>)) time, and thus any nonsingular matrix with real entries can be inverted in <em>O</em>(<em>M</em>(<em>n</em>)) time.</p>
<p>The above proof assumed that <em>A</em> is an <em>n</em> × <em>n</em> matrix, where <em>n</em> is an exact power of 2. If <em>n</em> is not an exact power of 2, then let <em>k</em> &lt; <em>n</em> be such that <em>n</em> + <em>k</em> is an exact power of 2, and define the (<em>n</em> + <em>k</em>) × (<em>n</em> + <em>k</em>) matrix <em>A</em>′ as</p>
<p class="eql"><img alt="art" src="images/Art_P939.jpg"/></p>
<p class="noindent">Then the inverse of <em>A</em>′ is</p>
<p class="eql"><img alt="art" src="images/Art_P940.jpg"/></p>
<p class="noindent">Apply the method of the proof to <em>A</em>′ to compute the inverse of <em>A</em>′, and take the first <em>n</em> rows and <em>n</em> columns of the result as the desired answer <em>A</em><sup>−1</sup>. The first regularity condition on <em>M</em>(<em>n</em>) ensures that enlarging the matrix in this way increases the running time by at most a constant factor.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The proof of Theorem 28.2 suggests how to solve the equation <em>Ax</em> = <em>b</em> by using LU decomposition without pivoting, so long as <em>A</em> is nonsingular. Let <em>y</em> = <em>A</em><sup>T</sup><em>b</em>. Multiply both sides of the equation <em>Ax</em> = <em>b</em> by <em>A</em><sup>T</sup>, yielding (<em>A</em><sup>T</sup><em>A</em>)<em>x</em> = <em>A</em><sup>T</sup><em>b</em> = <em>y</em>. This transformation doesn’t affect the solution <em>x</em>, since <em>A</em><sup>T</sup> is invertible. Because <em>A</em><sup>T</sup><em>A</em> is symmetric positive-definite, it can be factored by computing an LU decomposition. Then, use forward and back substitution to solve for <em>x</em> in the equation (<em>A</em><sup>T</sup><em>A</em>)<em>x</em> = <em>y</em>. Although this method is theoretically correct, in practice the procedure LUP-D<small>ECOMPOSITION</small> works much better. LUP decomposition requires fewer arithmetic operations by a constant factor, and it has somewhat better numerical properties.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>28.2-1</em></strong></p>
<p class="noindent">Let <em>M</em>(<em>n</em>) be the time to multiply two <em>n</em> × <em>n</em> matrices, and let <em>S</em>(<em>n</em>) denote the time required to square an <em>n</em> × <em>n</em> matrix. Show that multiplying and squaring matrices have essentially the same difficulty: an <em>M</em>(<em>n</em>)-time matrix-multiplication algorithm implies an <em>O</em>(<em>M</em>(<em>n</em>))-time squaring algorithm, and an <em>S</em>(<em>n</em>)-time squaring algorithm implies an <em>O</em>(<em>S</em>(<em>n</em>))-time matrix-multiplication algorithm.</p>
<a id="p838"/>
<p class="level3"><strong><em>28.2-2</em></strong></p>
<p class="noindent">Let <em>M</em>(<em>n</em>) be the time to multiply two <em>n</em> × <em>n</em> matrices. Show that an <em>M</em>(<em>n</em>)-time matrix-multiplication algorithm implies an <em>O</em>(<em>M</em>(<em>n</em>))-time LUP-decomposition algorithm. (The LUP decomposition your method produces need not be the same as the result produced by the LUP-D<small>ECOMPOSITION</small> procedure.)</p>
<p class="level3"><strong><em>28.2-3</em></strong></p>
<p class="noindent">Let <em>M</em>(<em>n</em>) be the time to multiply two <em>n</em> × <em>n</em> boolean matrices, and let <em>T</em>(<em>n</em>) be the time to find the transitive closure of an <em>n</em> × <em>n</em> boolean matrix. (See <a href="chapter023.xhtml#Sec_23.2">Section 23.2</a>.) Show that an <em>M</em>(<em>n</em>)-time boolean matrix-multiplication algorithm implies an <em>O</em>(<em>M</em>(<em>n</em>) lg <em>n</em>)-time transitive-closure algorithm, and a <em>T</em>(<em>n</em>)-time transitive-closure algorithm implies an <em>O</em>(<em>T</em> (<em>n</em>))-time boolean matrix-multiplication algorithm.</p>
<p class="level3"><strong><em>28.2-4</em></strong></p>
<p class="noindent">Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements are drawn from the field of integers modulo 2? Explain.</p>
<p class="level3"><span class="font1">★</span> <strong><em>28.2-5</em></strong></p>
<p class="noindent">Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of complex numbers, and prove that your generalization works correctly. (<em>Hint:</em> Instead of the transpose of <em>A</em>, use the <strong><em><span class="blue1">conjugate transpose</span></em></strong> <em>A</em>*, which you obtain from the transpose of <em>A</em> by replacing every entry with its complex conjugate. Instead of symmetric matrices, consider <strong><em><span class="blue1">Hermitian</span></em></strong> matrices, which are matrices <em>A</em> such that <em>A</em> = <em>A</em>*.)</p>
</section>
<p class="line1"/>
<section title="28.3 Symmetric positive-definite matrices and least-squares approximation">
<a id="Sec_28.3"/>
<p class="level1" id="h1-164"><a href="toc.xhtml#Rh1-164"><strong>28.3    Symmetric positive-definite matrices and least-squares approximation</strong></a></p>
<p class="noindent">Symmetric positive-definite matrices have many interesting and desirable properties. An <em>n</em> × <em>n</em> matrix <em>A</em> is <strong><em><span class="blue1">symmetric positive-definite</span></em></strong> if <em>A</em> = <em>A</em><sup>T</sup>(<em>A</em> is symmetric) and <em>x</em><sup>T</sup><em>Ax</em> &gt; 0 for all <em>n</em>-vectors <em>x</em> ≠ 0 (<em>A</em> is positive-definite). Symmetric positive-definite matrices are nonsingular, and an LU decomposition on them will not divide by 0. This section proves these and several other important properties of symmetric positive-definite matrices. We’ll also see an interesting application to curve fitting by a least-squares approximation.</p>
<p>The first property we prove is perhaps the most basic.</p>
<p class="lem"><strong><em>Lemma 28.3</em></strong></p>
<p class="noindent">Any positive-definite matrix is nonsingular.</p>
<a id="p839"/>
<p class="prof"><strong><em>Proof</em></strong>   Suppose that a matrix <em>A</em> is singular. Then by Corollary D.3 on page 1221, there exists a nonzero vector <em>x</em> such that <em>Ax</em> = 0. Hence, <em>x</em><sup>T</sup><em>Ax</em> = 0, and <em>A</em> cannot be positive-definite.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The proof that an LU decomposition on a symmetric positive-definite matrix <em>A</em> won’t divide by 0 is more involved. We begin by proving properties about certain submatrices of <em>A</em>. Define the <em>k</em>th <strong><em><span class="blue1">leading submatrix</span></em></strong> of <em>A</em> to be the matrix <em>A<sub>k</sub></em> consisting of the intersection of the first <em>k</em> rows and first <em>k</em> columns of <em>A</em>.</p>
<p class="lem"><strong><em>Lemma 28.4</em></strong></p>
<p class="noindent">If <em>A</em> is a symmetric positive-definite matrix, then every leading submatrix of <em>A</em> is symmetric and positive-definite.</p>
<p class="prof"><strong><em>Proof</em></strong>   Since <em>A</em> is symmetric, each leading submatrix <em>A<sub>k</sub></em> is also symmetric. We’ll prove that <em>A<sub>k</sub></em> is positive-definite by contradiction. If <em>A<sub>k</sub></em> is not positive-definite, then there exists a <em>k</em>-vector <em>x<sub>k</sub></em> ≠ 0 such that <img alt="art" src="images/Art_P941.jpg"/>. Let <em>A</em> be <em>n</em> × <em>n</em>, and</p>
<p class="eqr"><img alt="art" src="images/Art_P942.jpg"/></p>
<p class="noindent">for submatrices <em>B</em> (which is (<em>n</em>−<em>k</em>)×<em>k</em>) and <em>C</em> (which is (<em>n</em>−<em>k</em>)×(<em>n</em>−<em>k</em>)). Define the <em>n</em>-vector <img alt="art" src="images/Art_P943.jpg"/>, where <em>n</em> − <em>k</em> 0s follow <em>x<sub>k</sub></em>. Then we have</p>
<p class="eql"><img alt="art" src="images/Art_P944.jpg"/></p>
<p class="noindent">which contradicts <em>A</em> being positive-definite.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We now turn to some essential properties of the Schur complement. Let <em>A</em> be a symmetric positive-definite matrix, and let <em>A<sub>k</sub></em> be a leading <em>k</em> × <em>k</em> submatrix of <em>A</em>. Partition <em>A</em> once again according to equation (28.16). Equation (28.10) generalizes to define the <strong><em><span class="blue1">Schur complement</span></em></strong> <em>S</em> of <em>A</em> with respect to <em>A<sub>k</sub></em> as</p>
<p class="eqr"><img alt="art" src="images/Art_P945.jpg"/></p>
<p class="noindent">(By Lemma 28.4, <em>A<sub>k</sub></em> is symmetric and positive-definite, and therefore, <img alt="art" src="images/Art_P946.jpg"/> exists by Lemma 28.3, and <em>S</em> is well defined.) The earlier definition (28.10) of the Schur complement is consistent with equation (28.17) by letting <em>k</em> = 1.</p>
<p>The next lemma shows that the Schur-complement matrices of symmetric positive-definite matrices are themselves symmetric and positive-definite. We used this <a id="p840"/>result in Theorem 28.2, and its corollary will help prove that LU decomposition works for symmetric positive-definite matrices.</p>
<p class="lem"><strong><em>Lemma 28.5 (Schur complement lemma)</em></strong></p>
<p class="noindent">If <em>A</em> is a symmetric positive-definite matrix and <em>A<sub>k</sub></em> is a leading <em>k</em> × <em>k</em> submatrix of <em>A</em>, then the Schur complement <em>S</em> of <em>A</em> with respect to <em>A<sub>k</sub></em> is symmetric and positive-definite.</p>
<p class="prof"><strong><em>Proof</em></strong>   Because <em>A</em> is symmetric, so is the submatrix <em>C</em>. By Exercise D.2-6 on page 1223, the product <img alt="art" src="images/BA.jpg"/> is symmetric. Since <em>C</em> and <img alt="art" src="images/BA.jpg"/> are symmetric, then by Exercise D.1-1 on page 1219, so is <em>S</em>.</p>
<p>It remains to show that <em>S</em> is positive-definite. Consider the partition of <em>A</em> given in equation (28.16). For any nonzero vector <em>x</em>, we have <em>x</em><sup>T</sup><em>Ax</em> &gt; 0 by the assumption that <em>A</em> is positive-definite. Let the subvectors <em>y</em> and <em>z</em> consist of the first <em>k</em> and last <em>n</em> − <em>k</em> elements in <em>x</em>, respectively, and thus they are compatible with <em>A<sub>k</sub></em> and <em>C</em>, respectively. Because <img alt="art" src="images/Art_P947.jpg"/> exists, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P948.jpg"/></p>
<p class="noindent">This last equation, which you can verify by multiplying through, amounts to “completing the square” of the quadratic form. (See Exercise 28.3-2.)</p>
<p>Since <em>x</em><sup>T</sup><em>Ax</em> &gt; 0 holds for any nonzero <em>x</em>, pick any nonzero <em>z</em> and then choose <img alt="art" src="images/Art_P949.jpg"/>, which causes the first term in equation (28.18) to vanish, leaving</p>
<p class="eql"><img alt="art" src="images/Art_P950.jpg"/></p>
<p class="noindent">as the value of the expression. For any <em>z</em> ≠ 0, we therefore have <em>z</em><sup>T</sup><em>Sz</em> = <em>x</em><sup>T</sup><em>Ax</em> &gt; 0, and thus <em>S</em> is positive-definite.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="cor"><strong><em>Corollary 28.6</em></strong></p>
<p class="noindent">LU decomposition of a symmetric positive-definite matrix never causes a division by 0.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let <em>A</em> be an <em>n</em> × <em>n</em> symmetric positive-definite matrix. In fact, we’ll prove a stronger result than the statement of the corollary: every pivot is strictly positive. The first pivot is <em>a</em><sub>11</sub>. Let <em>e</em><sub>1</sub> be the length-<em>n</em> unit vector ( 1 0 0 <span class="font1">⋯</span> 0 )<sup>T</sup>, so that <img alt="art" src="images/Art_P951.jpg"/>, which is positive because <em>e</em><sub>1</sub> is nonzero and <em>A</em> is positive <a id="p841"/>definite. Since the first step of LU decomposition produces the Schur complement of <em>A</em> with respect to <em>A</em><sub>1</sub> = (<em>a</em><sub>11</sub>), Lemma 28.5 implies by induction that all pivots are positive.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Least-squares approximation</strong></p>
<p class="noindent">One important application of symmetric positive-definite matrices arises in fitting curves to given sets of data points. You are given a set of <em>m</em> data points</p>
<p class="eql">(<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>), (<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>), … , (<em>x<sub>m</sub></em>, <em>y<sub>m</sub></em>),</p>
<p class="noindent">where you know that the <em>y<sub>i</sub></em> are subject to measurement errors. You wish to determine a function <em>F</em>(<em>x</em>) such that the approximation errors</p>
<p class="eqr"><img alt="art" src="images/Art_P952.jpg"/></p>
<p class="noindent">are small for <em>i</em> = 1, 2, … , <em>m</em>. The form of the function <em>F</em> depends on the problem at hand. Let’s assume that it has the form of a linearly weighted sum</p>
<p class="eql"><img alt="art" src="images/Art_P953.jpg"/></p>
<p class="noindent">where the number <em>n</em> of summands and the specific <strong><em><span class="blue1">basis functions</span></em></strong> <em>f<sub>j</sub></em> are chosen based on knowledge of the problem at hand. A common choice is <em>f<sub>j</sub></em>(<em>x</em>) = <em>x</em><sup><em>j</em>−1</sup>, which means that</p>
<p class="eql"><em>F</em>(<em>x</em>) = <em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>x</em> + <em>c</em><sub>3</sub><em>x</em><sup>2</sup> + <span class="font1">⋯</span> + <em>c</em><sub><em>n</em></sub><em>x</em><sup><em>n</em>−1</sup></p>
<p class="noindent">is a polynomial of degree <em>n</em> − 1 in <em>x</em>. Thus, if you are given <em>m</em> data points (<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>), (<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>), … , (<em>x<sub>m</sub></em>, <em>y<sub>m</sub></em>), you need to calculate <em>n</em> coefficients <em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, … , <em>c<sub>n</sub></em> that minimize the approximation errors <em>η</em><sub>1</sub>, <em>η</em><sub>2</sub>, … , <em>η<sub>m</sub></em>.</p>
<p>By choosing <em>n</em> = <em>m</em>, you can calculate each <em>y<sub>i</sub></em> <em>exactly</em> in equation (28.19). Such a high-degree polynomial <em>F</em> “fits the noise” as well as the data, however, and generally gives poor results when used to predict <em>y</em> for previously unseen values of <em>x</em>. It is usually better to choose <em>n</em> significantly smaller than <em>m</em> and hope that by choosing the coefficients <em>c<sub>j</sub></em> well, you can obtain a function <em>F</em> that finds the significant patterns in the data points without paying undue attention to the noise. Some theoretical principles exist for choosing <em>n</em>, but they are beyond the scope of this text. In any case, once you choose a value of <em>n</em> that is less than <em>m</em>, you end up with an overdetermined set of equations whose solution you wish to approximate. Let’s see how to do so.</p>
<a id="p842"/>
<p class="noindent">Let</p>
<p class="eql"><img alt="art" src="images/Art_P954.jpg"/></p>
<p class="noindent">denote the matrix of values of the basis functions at the given points, that is, <em>a<sub>ij</sub></em> = <em>f<sub>j</sub></em>(<em>x<sub>i</sub></em>). Let <em>c</em> = (<em>c<sub>k</sub></em>) denote the desired <em>n</em>-vector of coefficients. Then,</p>
<p class="eql"><img alt="art" src="images/Art_P955.jpg"/></p>
<p class="noindent">is the <em>m</em>-vector of “predicted values” for <em>y</em>. Thus,</p>
<p class="eql"><em>η</em> = <em>Ac</em> − <em>y</em></p>
<p class="noindent">is the <em>m</em>-vector of <strong><em><span class="blue1">approximation errors</span></em></strong>.</p>
<p>To minimize approximation errors, let’s minimize the norm of the error vector <em>η</em>, which gives a <strong><em><span class="blue1">least-squares solution</span></em></strong>, since</p>
<p class="eql"><img alt="art" src="images/Art_P956.jpg"/></p>
<p class="noindent">Because</p>
<p class="eql"><img alt="art" src="images/Art_P957.jpg"/></p>
<p class="noindent">to minimize <span class="font1">∥</span><em>η</em><span class="font1">∥</span>, differentiate <span class="font1">∥</span><em>η</em><span class="font1">∥</span><sup>2</sup> with respect to each <em>c<sub>k</sub></em> and then set the result to 0:</p>
<p class="eqr"><img alt="art" src="images/Art_P958.jpg"/></p>
<p class="noindent">The <em>n</em> equations (28.20) for <em>k</em> = 1, 2, … , <em>n</em> are equivalent to the single matrix equation</p>
<a id="p843"/>
<p class="eql">(<em>Ac</em> − <em>y</em>)<sup>T</sup> <em>A</em> = 0</p>
<p class="noindent">or, equivalently (using Exercise D.1-2 on page 1219), to</p>
<p class="eql"><em>A</em><sup>T</sup>(<em>Ac</em> − <em>y</em>) = 0,</p>
<p class="noindent">which implies</p>
<p class="eqr"><img alt="art" src="images/Art_P959.jpg"/></p>
<p class="noindent">In statistics, equation (28.21) is called the <strong><em><span class="blue1">normal equation</span></em></strong>. The matrix <em>A</em><sup>T</sup><em>A</em> is symmetric by Exercise D.1-2, and if <em>A</em> has full column rank, then by Theorem D.6 on page 1222, <em>A</em><sup>T</sup><em>A</em> is positive-definite as well. Hence, (<em>A</em><sup>T</sup><em>A</em>)<sup>−1</sup> exists, and the solution to equation (28.21) is</p>
<p class="eqr"><img alt="art" src="images/Art_P960.jpg"/></p>
<p class="noindent">where the matrix <em>A</em><sup>+</sup> = ((<em>A</em><sup>T</sup><em>A</em>)<sup>−1</sup><em>A</em><sup>T</sup>) is the <strong><em><span class="blue1">pseudoinverse</span></em></strong> of the matrix <em>A</em>. The pseudoinverse naturally generalizes the notion of a matrix inverse to the case in which <em>A</em> is not square. (Compare equation (28.22) as the approximate solution to <em>Ac</em> = <em>y</em> with the solution <em>A</em><sup>−1</sup><em>b</em> as the exact solution to <em>Ax</em> = <em>b</em>.)</p>
<p>As an example of producing a least-squares fit, suppose that you have five data points</p>
<table class="table2b">
<tr>
<td class="td2">(<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>)</td>
<td class="td2">=</td>
<td class="td2">(−1, 2),</td>
</tr>
<tr>
<td class="td2">(<em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>)</td>
<td class="td2">=</td>
<td class="td2">(1, 1),</td>
</tr>
<tr>
<td class="td2">(<em>x</em><sub>3</sub>, <em>y</em><sub>3</sub>)</td>
<td class="td2">=</td>
<td class="td2">(2, 1),</td>
</tr>
<tr>
<td class="td2">(<em>x</em><sub>4</sub>, <em>y</em><sub>4</sub>)</td>
<td class="td2">=</td>
<td class="td2">(3, 0),</td>
</tr>
<tr>
<td class="td2">(<em>x</em><sub>5</sub>, <em>y</em><sub>5</sub>)</td>
<td class="td2">=</td>
<td class="td2">(5, 3),</td>
</tr>
</table>
<p class="noindent">shown as orange dots in <a href="chapter028.xhtml#Fig_28-3">Figure 28.3</a>, and you want to fit these points with a quadratic polynomial</p>
<p class="eql"><em>F</em>(<em>x</em>) = <em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>x</em> + <em>c</em><sub>3</sub><em>x</em><sup>2</sup>.</p>
<p class="noindent">Start with the matrix of basis-function values</p>
<p class="eql"><img alt="art" src="images/Art_P961.jpg"/></p>
<p class="noindent">whose pseudoinverse is</p>
<a id="p844"/>
<div class="divimage">
<p class="fig-imga" id="Fig_28-3"><img alt="art" src="images/Art_P962.jpg"/></p>
<p class="caption"><strong>Figure 28.3</strong> The least-squares fit of a quadratic polynomial to the set of five data points {(−1, 2), (1, 1), (2, 1), (3, 0), (5, 3)}. The orange dots are the data points, and the blue dots are their estimated values predicted by the polynomial <em>F</em>(<em>x</em>) = 1.2 − 0.757<em>x</em> + 0.214<em>x</em><sup>2</sup>, the quadratic polynomial that minimizes the sum of the squared errors, plotted in blue. Each orange line shows the error for one data point.</p>
</div>
<p class="eql"><img alt="art" src="images/Art_P963.jpg"/></p>
<p class="noindent">Multiplying <em>y</em> by <em>A</em><sup>+</sup> gives the coefficient vector</p>
<p class="eql"><img alt="art" src="images/Art_P964.jpg"/></p>
<p class="noindent">which corresponds to the quadratic polynomial</p>
<p class="eql"><em>F</em>(<em>x</em>) = 1.200 − 0.757<em>x</em> + 0.214<em>x</em><sup>2</sup></p>
<p class="noindent">as the closest-fitting quadratic to the given data, in a least-squares sense.</p>
<p>As a practical matter, you would typically solve the normal equation (28.21) by multiplying <em>y</em> by <em>A</em><sup>T</sup> and then finding an LU decomposition of <em>A</em><sup>T</sup><em>A</em>. If <em>A</em> has full rank, the matrix <em>A</em><sup>T</sup><em>A</em> is guaranteed to be nonsingular, because it is symmetric and positive-definite. (See Exercise D.1-2 and Theorem D.6.)</p>
<a id="p845"/>
<div class="divimage">
<p class="fig-imga" id="Fig_28-4"><img alt="art" src="images/Art_P965.jpg"/></p>
<p class="caption"><strong>Figure 28.4</strong> A least-squares fit of a curve of the form</p>
<p class="eqls"><em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>x</em> + <em>c</em><sub>3</sub><em>x</em><sup>2</sup> + <em>c</em><sub>4</sub> sin(2<em>πx</em>) + <em>c</em><sub>5</sub> cos(2<em>πx</em>)</p>
<p class="noindent">for the carbon-dioxide concentrations measured in Mauna Loa, Hawaii from 1990<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> to 2019, where <em>x</em> is the number of years elapsed since 1990. This curve is the famous “Keeling curve,” illustrating curve-fitting to nonpolynomial formulas. The sine and cosine terms allow modeling of seasonal variations in CO<sub>2</sub> concentrations. The red curve shows the measured CO<sub>2</sub> concentrations. The best fit, shown in black, has the form</p>
<p class="eqls">352.83 + 1.39<em>x</em> + 0.02<em>x</em><sup>2</sup> + 2.83 sin(2<em>πx</em>) − 0.94 cos(2<em>πx</em>).</p>
</div>
<p>We close this section with an example in <a href="chapter028.xhtml#Fig_28-4">Figure 28.4</a>, illustrating that a curve can also fit a nonpolynomial function. The curve confirms one aspect of climate change: that carbon dioxide (CO<sub>2</sub>) concentrations have steadily increased over a period of 29 years. Linear and quadratic terms model the annual increase, and sine and cosine terms model seasonal variations.</p>
<a id="p846"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>28.3-1</em></strong></p>
<p class="noindent">Prove that every diagonal element of a symmetric positive-definite matrix is positive.</p>
<p class="level3"><strong><em>28.3-2</em></strong></p>
<p class="noindent">Let <img alt="art" src="images/Art_P966.jpg"/> be a 2 × 2 symmetric positive-definite matrix. Prove that its determinant <em>ac</em> − <em>b</em><sup>2</sup> is positive by “completing the square” in a manner similar to that used in the proof of Lemma 28.5.</p>
<p class="level3"><strong><em>28.3-3</em></strong></p>
<p class="noindent">Prove that the maximum element in a symmetric positive-definite matrix lies on the diagonal.</p>
<p class="level3"><strong><em>28.3-4</em></strong></p>
<p class="noindent">Prove that the determinant of each leading submatrix of a symmetric positive-definite matrix is positive.</p>
<p class="level3"><strong><em>28.3-5</em></strong></p>
<p class="noindent">Let <em>A<sub>k</sub></em> denote the <em>k</em>th leading submatrix of a symmetric positive-definite matrix <em>A</em>. Prove that det(<em>A<sub>k</sub></em>)/det(<em>A</em><sub><em>k</em>−1</sub>) is the <em>k</em>th pivot during LU decomposition, where, by convention, det(<em>A</em><sub>0</sub>) = 1.</p>
<p class="level3"><strong><em>28.3-6</em></strong></p>
<p class="noindent">Find the function of the form</p>
<p class="eql"><em>F</em>(<em>x</em>) = <em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>x</em> lg <em>x</em> + <em>c</em><sub>3</sub><em>e<sup>x</sup></em></p>
<p class="noindent">that is the best least-squares fit to the data points</p>
<p class="eql">(1, 1), (2, 1), (3, 3), (4, 8).</p>
<p class="level3"><strong><em>28.3-7</em></strong></p>
<p class="noindent">Show that the pseudoinverse <em>A</em><sup>+</sup> satisfies the following four equations:</p>
<table class="table2b">
<tr>
<td class="td2"><p class="right"><em>AA</em><sup>+</sup><em>A</em></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>A</em>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right"><em>A</em><sup>+</sup><em>AA</em><sup>+</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>A</em><sup>+</sup>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">(<em>AA</em><sup>+</sup>)<sup>T</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>AA</em><sup>+</sup>,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">(<em>A</em><sup>+</sup><em>A</em>)<sup>T</sup></p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="noindent"><em>A</em><sup>+</sup><em>A</em>.</p></td>
</tr>
</table>
<a id="p847"/>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-165"><strong>Problems</strong></p>
<section title="28-1 Tridiagonal systems of linear equations">
<p class="level2"><strong><em>28-1     Tridiagonal systems of linear equations</em></strong></p>
<p class="noindent">Consider the tridiagonal matrix</p>
<p class="eql"><img alt="art" src="images/Art_P967.jpg"/></p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Find an LU decomposition of <em>A</em>.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Solve the equation <em>Ax</em> = ( 1 1 1 1 1 )<sup>T</sup> by using forward and back substitution.</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Find the inverse of <em>A</em>.</p>
<p class="nl-1list-d"><strong><em>d.</em></strong> Show how to solve the equation <em>Ax</em> = <em>b</em> for any <em>n</em> × <em>n</em> symmetric positive-definite, tridiagonal matrix <em>A</em> and any <em>n</em>-vector <em>b</em> in <em>O</em>(<em>n</em>) time by performing an LU decomposition. Argue that any method based on forming <em>A</em><sup>−1</sup> is asymptotically more expensive in the worst case.</p>
<p class="nl-1list-d"><strong><em>e.</em></strong> Show how to solve the equation <em>Ax</em> = <em>b</em> for any <em>n</em> × <em>n</em> nonsingular, tridiagonal matrix <em>A</em> and any <em>n</em>-vector <em>b</em> in <em>O</em>(<em>n</em>) time by performing an LUP decomposition.</p>
</section>
<section title="28-2 Splines">
<p class="level2"><strong><em>28-2     Splines</em></strong></p>
<p class="noindent">A practical method for interpolating a set of points with a curve is to use <strong><em><span class="blue1">cubic splines</span></em></strong>. You are given a set {(<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>) : <em>i</em> = 0, 1, … , <em>n</em>} of <em>n</em> + 1 point-value pairs, where <em>x</em><sub>0</sub> &lt; <em>x</em><sub>1</sub> &lt; <span class="font1">⋯</span> &lt; <em>x<sub>n</sub></em>. Your goal is to fit a piecewise-cubic curve (spline) <em>f</em>(<em>x</em>) to the points. That is, the curve <em>f</em>(<em>x</em>) is made up of <em>n</em> cubic polynomials <em>f<sub>i</sub></em>(<em>x</em>) = <em>a<sub>i</sub></em> + <em>b<sub>i</sub>x</em> + <em>c</em><sub><em>i</em></sub><em>x</em><sup>2</sup> + <em>d</em><sub><em>i</em></sub><em>x</em><sup>3</sup> for <em>i</em> = 0, 1, … , <em>n</em> − 1, where if <em>x</em> falls in the range <em>x<sub>i</sub></em> ≤ <em>x</em> ≤ <em>x</em><sub><em>i</em>+1</sub>, then the value of the curve is given by <em>f</em>(<em>x</em>) = <em>f<sub>i</sub></em>(<em>x</em> − <em>x<sub>i</sub></em>). The points <em>x<sub>i</sub></em> at which the cubic polynomials are “pasted” together are called <strong><em><span class="blue1">knots</span></em></strong>. For simplicity, assume that <em>x<sub>i</sub></em> = <em>i</em> for <em>i</em> = 0, 1, … , <em>n</em>.</p>
<p>To ensure continuity of <em>f</em>(<em>x</em>), require that</p>
<table class="table2b">
<tr>
<td class="td2"><em>f</em>(<em>x<sub>i</sub></em>)</td>
<td class="td2">=</td>
<td class="td2"><em>f<sub>i</sub></em>(0)</td>
<td class="td2">=</td>
<td class="td2"><em>y<sub>i</sub></em>,</td>
</tr>
<tr>
<td class="td2"><em>f</em>(<em>x</em><sub><em>i</em>+1</sub>)</td>
<td class="td2">=</td>
<td class="td2"><em>f<sub>i</sub></em>(1)</td>
<td class="td2">=</td>
<td class="td2"><em>y</em><sub><em>i</em>+1</sub></td>
</tr>
</table>
<p class="noindent">for <em>i</em> = 0, 1, … , <em>n</em> − 1. To ensure that <em>f</em>(<em>x</em>) is sufficiently smooth, also require the first derivative to be continuous at each knot:</p>
<a id="p848"/>
<p class="eql"><img alt="art" src="images/Art_P968.jpg"/></p>
<p class="noindent">for <em>i</em> = 0, 1, … , <em>n</em> − 2.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Suppose that for <em>i</em> = 0, 1, … , <em>n</em>, in addition to the point-value pairs {(<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>)}, you are also given the first derivative <em>D<sub>i</sub></em> = <em>f</em>′(<em>x<sub>i</sub></em>) at each knot. Express each coefficient <em>a<sub>i</sub></em>, <em>b<sub>i</sub></em>, <em>c<sub>i</sub></em>, and <em>d<sub>i</sub></em> in terms of the values <em>y<sub>i</sub></em>, <em>y</em><sub><em>i</em>+1</sub>, <em>D<sub>i</sub></em>, and <em>D</em><sub><em>i</em>+1</sub>. (Remember that <em>x<sub>i</sub></em> = <em>i</em>.) How quickly can you compute the 4<em>n</em> coefficients from the point-value pairs and first derivatives?</p>
<p class="noindent1-top">The question remains of how to choose the first derivatives of <em>f</em>(<em>x</em>) at the knots. One method is to require the second derivatives to be continuous at the knots:</p>
<p class="eql"><img alt="art" src="images/Art_P969.jpg"/></p>
<p class="noindent">for <em>i</em> = 0, 1, … , <em>n</em>−2. At the first and last knots, assume that <img alt="art" src="images/Art_P970.jpg"/> and <img alt="art" src="images/Art_P971.jpg"/>. These assumptions make <em>f</em>(<em>x</em>) a <strong><em><span class="blue1">natural</span></em></strong> cubic spline.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Use the continuity constraints on the second derivative to show that for <em>i</em> = 1, 2, … , <em>n</em> − 1,</p>
<p class="eqr"><img alt="art" src="images/Art_P972.jpg"/></p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Show that</p>
<p class="eqr"><img alt="art" src="images/Art_P973.jpg"/></p>
<p class="nl-1list-d"><strong><em>d.</em></strong> Rewrite equations (28.23)–(28.25) as a matrix equation involving the vector <em>D</em> = (<em>D</em><sub>0</sub> <em>D</em><sub>1</sub> <em>D</em><sub>2</sub> <span class="font1">⋯</span> <em>D<sub>n</sub></em>)<sup>T</sup> of unknowns. What attributes does the matrix in your equation have?</p>
<p class="nl-1list-d"><strong><em>e.</em></strong> Argue that a natural cubic spline can interpolate a set of <em>n</em> + 1 point-value pairs in <em>O</em>(<em>n</em>) time (see Problem 28-1).</p>
<p class="nl-1list-d"><strong><em>f.</em></strong> Show how to determine a natural cubic spline that interpolates a set of <em>n</em> + 1 points (<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>) satisfying <em>x</em><sub>0</sub> &lt; <em>x</em><sub>1</sub> &lt; <span class="font1">⋯</span> &lt; <em>x<sub>n</sub></em>, even when <em>x<sub>i</sub></em> is not necessarily equal to <em>i</em>. What matrix equation must your method solve, and how quickly does your algorithm run?</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-166"><strong>Chapter notes</strong></p>
<p class="noindent">Many excellent texts describe numerical and scientific computation in much greater detail than we have room for here. The following are especially readable: George <a id="p849"/>and Liu [<a epub:type="noteref" href="bibliography001.xhtml#endnote_180">180</a>], Golub and Van Loan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_192">192</a>], Press, Teukolsky, Vetterling, and Flannery [<a epub:type="noteref" href="bibliography001.xhtml#endnote_365">365</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_366">366</a>], and Strang [<a epub:type="noteref" href="bibliography001.xhtml#endnote_422">422</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_423">423</a>].</p>
<p>Golub and Van Loan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_192">192</a>] discuss numerical stability. They show why det(<em>A</em>) is not necessarily a good indicator of the stability of a matrix <em>A</em>, proposing instead to use <span class="font1">∥</span><em>A</em><span class="font1">∥</span><sub>∞</sub> <span class="font1">∥</span><em>A</em><sup>−1</sup><span class="font1">∥</span><sub>∞</sub>, where <img alt="art" src="images/Art_P974.jpg"/>. They also address the question of how to compute this value without actually computing <em>A</em><sup>−1</sup>.</p>
<p>Gaussian elimination, upon which the LU and LUP decompositions are based, was the first systematic method for solving linear systems of equations. It was also one of the earliest numerical algorithms. Although it was known earlier, its discovery is commonly attributed to C. F. Gauss (1777–1855). In his famous paper [<a epub:type="noteref" href="bibliography001.xhtml#endnote_424">424</a>], Strassen showed that an <em>n</em>×<em>n</em> matrix can be inverted in <em>O</em>(<em>n</em><sup>lg 7</sup>) time. Winograd [<a epub:type="noteref" href="bibliography001.xhtml#endnote_460">460</a>] originally proved that matrix multiplication is no harder than matrix inversion, and the converse is due to Aho, Hopcroft, and Ullman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_5">5</a>].</p>
<p>Another important matrix decomposition is the <strong><em><span class="blue1">singular value decomposition</span></em></strong>, or <strong><em><span class="blue1">SVD</span></em></strong>. The SVD factors an <em>m</em> × <em>n</em> matrix <em>A</em> into <img alt="art" src="images/Art_P975.jpg"/>, where Σ is an <em>m</em>×<em>n</em> matrix with nonzero values only on the diagonal, <em>Q</em><sub>1</sub> is <em>m</em>×<em>m</em> with mutually orthonormal columns, and <em>Q</em><sub>2</sub> is <em>n</em> × <em>n</em>, also with mutually orthonormal columns. Two vectors are <strong><em><span class="blue1">orthonormal</span></em></strong> if their inner product is 0 and each vector has a norm of 1. The books by Strang [<a epub:type="noteref" href="bibliography001.xhtml#endnote_422">422</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_423">423</a>] and Golub and Van Loan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_192">192</a>] contain good treatments of the SVD.</p>
<p>Strang [<a epub:type="noteref" href="bibliography001.xhtml#endnote_423">423</a>] has an excellent presentation of symmetric positive-definite matrices and of linear algebra in general.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> The year in which <em>Introduction to Algorithms</em> was first published.</p>
</section>
</section>
</div>
</body>
</html>