<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p126"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="5 Probabilistic Analysis and Randomized Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-5"><strong><span class="blue1">5          Probabilistic Analysis and Randomized Algorithms</span></strong></a></p>
<p class="noindent">This chapter introduces probabilistic analysis and randomized algorithms. If you are unfamiliar with the basics of probability theory, you should read <a href="appendix003.xhtml#Sec_C.1">Sections C.1</a>–<a href="appendix003.xhtml#Sec_C.4">C.4</a> of <a href="appendix003.xhtml">Appendix C</a>, which review this material. We’ll revisit probabilistic analysis and randomized algorithms several times throughout this book.</p>
<p class="line1"/>
<section title="5.1 The hiring problem">
<a id="Sec_5.1"/>
<p class="level1" id="h1-25"><a href="toc.xhtml#Rh1-25"><strong>5.1      The hiring problem</strong></a></p>
<p class="noindent">Suppose that you need to hire a new office assistant. Your previous attempts at hiring have been unsuccessful, and you decide to use an employment agency. The employment agency sends you one candidate each day. You interview that person and then decide either to hire that person or not. You must pay the employment agency a small fee to interview an applicant. To actually hire an applicant is more costly, however, since you must fire your current office assistant and also pay a substantial hiring fee to the employment agency. You are committed to having, at all times, the best possible person for the job. Therefore, you decide that, after interviewing each applicant, if that applicant is better qualified than the current office assistant, you will fire the current office assistant and hire the new applicant. You are willing to pay the resulting price of this strategy, but you wish to estimate what that price will be.</p>
<p>The procedure H<small>IRE</small>-A<small>SSISTANT</small> on the facing page expresses this strategy for hiring in pseudocode. The candidates for the office assistant job are numbered 1 through <em>n</em> and interviewed in that order. The procedure assumes that after interviewing candidate <em>i</em>, you can determine whether candidate <em>i</em> is the best candidate you have seen so far. It starts by creating a dummy candidate, numbered 0, who is less qualified than each of the other candidates.</p>
<p>The cost model for this problem differs from the model described in <a href="chapter002.xhtml">Chapter 2</a>. We focus not on the running time of H<small>IRE</small>-A<small>SSISTANT</small>, but instead on the fees paid for interviewing and hiring. On the surface, analyzing the cost of this algorithm <a id="p127"/>may seem very different from analyzing the running time of, say, merge sort. The analytical techniques used, however, are identical whether we are analyzing cost or running time. In either case, we are counting the number of times certain basic operations are executed.</p>
<div class="pull-quote1">
<p class="box-heading">H<small>IRE</small>-A<small>SSISTANT</small>(<em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><em>best</em> = 0</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> candidate 0 is a least-qualified dummy candidate</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1" colspan="2"><p class="p2">interview candidate <em>i</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1" colspan="2"><p class="p2"><strong>if</strong> candidate <em>i</em> is better than candidate <em>best</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1" colspan="2">
<p class="p3"><em>best</em> = <em>i</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1" colspan="2">
<p class="p3">hire candidate <em>i</em></p></td>
</tr>
</table>
</div>
<p>Interviewing has a low cost, say <em>c<sub>i</sub></em>, whereas hiring is expensive, costing <em>c<sub>h</sub></em>. Letting <em>m</em> be the number of people hired, the total cost associated with this algorithm is <em>O</em>(<em>c<sub>i</sub>n</em> + <em>c<sub>h</sub>m</em>). No matter how many people you hire, you always interview <em>n</em> candidates and thus always incur the cost <em>c<sub>i</sub>n</em> associated with interviewing. We therefore concentrate on analyzing <em>c<sub>h</sub>m</em>, the hiring cost. This quantity depends on the order in which you interview candidates.</p>
<p>This scenario serves as a model for a common computational paradigm. Algorithms often need to find the maximum or minimum value in a sequence by examining each element of the sequence and maintaining a current “winner.” The hiring problem models how often a procedure updates its notion of which element is currently winning.</p>
<p class="level4"><strong>Worst-case analysis</strong></p>
<p class="noindent">In the worst case, you actually hire every candidate that you interview. This situation occurs if the candidates come in strictly increasing order of quality, in which case you hire <em>n</em> times, for a total hiring cost of <em>O</em>(<em>c<sub>h</sub>n</em>).</p>
<p>Of course, the candidates do not always come in increasing order of quality. In fact, you have no idea about the order in which they arrive, nor do you have any control over this order. Therefore, it is natural to ask what we expect to happen in a typical or average case.</p>
<p class="level4"><strong>Probabilistic analysis</strong></p>
<p class="noindent"><strong><em><span class="blue1">Probabilistic analysis</span></em></strong> is the use of probability in the analysis of problems. Most commonly, we use probabilistic analysis to analyze the running time of an algorithm. Sometimes we use it to analyze other quantities, such as the hiring cost in <a id="p128"/>procedure H<small>IRE</small>-A<small>SSISTANT</small>. In order to perform a probabilistic analysis, we must use knowledge of, or make assumptions about, the distribution of the inputs. Then we analyze our algorithm, computing an average-case running time, where we take the average, or expected value, over the distribution of the possible inputs. When reporting such a running time, we refer to it as the <strong><em><span class="blue1">average-case running time</span></em></strong>.</p>
<p>You must be careful in deciding on the distribution of inputs. For some problems, you may reasonably assume something about the set of all possible inputs, and then you can use probabilistic analysis as a technique for designing an efficient algorithm and as a means for gaining insight into a problem. For other problems, you cannot characterize a reasonable input distribution, and in these cases you cannot use probabilistic analysis.</p>
<p>For the hiring problem, we can assume that the applicants come in a random order. What does that mean for this problem? We assume that you can compare any two candidates and decide which one is better qualified, which is to say that there is a total order on the candidates. (See <a href="appendix002.xhtml#Sec_B.2">Section B.2</a> for the definition of a total order.) Thus, you can rank each candidate with a unique number from 1 through <em>n</em>, using <em>rank</em>(<em>i</em>) to denote the rank of applicant <em>i</em>, and adopt the convention that a higher rank corresponds to a better qualified applicant. The ordered list <span class="font1">〈</span><em>rank</em>(1), <em>rank</em>(2), … , <em>rank</em>(<em>n</em>)<span class="font1">〉</span> is a permutation of the list <span class="font1">〈</span>1, 2, … , <em>n</em><span class="font1">〉</span>. Saying that the applicants come in a random order is equivalent to saying that this list of ranks is equally likely to be any one of the <em>n</em>! permutations of the numbers 1 through <em>n</em>. Alternatively, we say that the ranks form a <strong><em><span class="blue1">uniform random permutation</span></em></strong>, that is, each of the possible <em>n</em>! permutations appears with equal probability.</p>
<p><a href="chapter005.xhtml#Sec_5.2">Section 5.2</a> contains a probabilistic analysis of the hiring problem.</p>
<p class="level4"><strong>Randomized algorithms</strong></p>
<p class="noindent">In order to use probabilistic analysis, you need to know something about the distribution of the inputs. In many cases, you know little about the input distribution. Even if you do know something about the distribution, you might not be able to model this knowledge computationally. Yet, probability and randomness often serve as tools for algorithm design and analysis, by making part of the algorithm behave randomly.</p>
<p>In the hiring problem, it may seem as if the candidates are being presented to you in a random order, but you have no way of knowing whether they really are. Thus, in order to develop a randomized algorithm for the hiring problem, you need greater control over the order in which you’ll interview the candidates. We will, therefore, change the model slightly. The employment agency sends you a list of the <em>n</em> candidates in advance. On each day, you choose, randomly, which candidate to interview. Although you know nothing about the candidates (besides their names), we have made a significant change. Instead of accepting the order given <a id="p129"/>to you by the employment agency and hoping that it’s random, you have instead gained control of the process and enforced a random order.</p>
<p>More generally, we call an algorithm <strong><em><span class="blue1">randomized</span></em></strong> if its behavior is determined not only by its input but also by values produced by a <strong><em><span class="blue1">random-number generator</span></em></strong>. We assume that we have at our disposal a random-number generator R<small>ANDOM</small>. A call to R<small>ANDOM</small>(<em>a</em>, <em>b</em>) returns an integer between <em>a</em> and <em>b</em>, inclusive, with each such integer being equally likely. For example, R<small>ANDOM</small>(0, 1) produces 0 with probability 1/2, and it produces 1 with probability 1/2. A call to R<small>ANDOM</small>(3, 7) returns any one of 3, 4, 5, 6, or 7, each with probability 1/5. Each integer returned by R<small>ANDOM</small> is independent of the integers returned on previous calls. You may imagine R<small>ANDOM</small> as rolling a (<em>b</em> – <em>a</em> + 1)-sided die to obtain its output. (In practice, most programming environments offer a <strong><em><span class="blue1">pseudorandom-number generator</span></em></strong>: a deterministic algorithm returning numbers that “look” statistically random.)</p>
<p>When analyzing the running time of a randomized algorithm, we take the expectation of the running time over the distribution of values returned by the random number generator. We distinguish these algorithms from those in which the input is random by referring to the running time of a randomized algorithm as an <strong><em><span class="blue1">expected running time</span></em></strong>. In general, we discuss the average-case running time when the probability distribution is over the inputs to the algorithm, and we discuss the expected running time when the algorithm itself makes random choices.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>5.1-1</em></strong></p>
<p class="noindent">Show that the assumption that you are always able to determine which candidate is best, in line 4 of procedure H<small>IRE</small>-A<small>SSISTANT</small>, implies that you know a total order on the ranks of the candidates.</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.1-2</em></strong></p>
<p class="noindent">Describe an implementation of the procedure R<small>ANDOM</small>(<em>a</em>, <em>b</em>) that makes calls only to R<small>ANDOM</small>(0, 1). What is the expected running time of your procedure, as a function of <em>a</em> and <em>b</em>?</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.1-3</em></strong></p>
<p class="noindent">You wish to implement a program that outputs 0 with probability 1/2 and 1 with probability 1/2. At your disposal is a procedure B<small>IASED</small>-R<small>ANDOM</small> that outputs either 0 or 1, but it outputs 1 with some probability <em>p</em> and 0 with probability 1 – <em>p</em>, where 0 &lt; <em>p</em> &lt; 1. You do not know what <em>p</em> is. Give an algorithm that uses B<small>IASED</small>-R<small>ANDOM</small> as a subroutine, and returns an unbiased answer, returning 0 with probability 1/2 and 1 with probability 1/2. What is the expected running time of your algorithm as a function of <em>p</em>?</p>
<a id="p130"/>
</section>
<p class="line1"/>
<section title="5.2 Indicator random variables">
<a id="Sec_5.2"/>
<p class="level1" id="h1-26"><a href="toc.xhtml#Rh1-26"><strong>5.2      Indicator random variables</strong></a></p>
<p class="noindent">In order to analyze many algorithms, including the hiring problem, we use indicator random variables. Indicator random variables provide a convenient method for converting between probabilities and expectations. Given a sample space <em>S</em> and an event <em>A</em>, the <strong><em><span class="blue1">indicator random variable</span></em></strong> I {<em>A</em>} associated with event <em>A</em> is defined as</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P245.jpg"/></p>
<p>As a simple example, let us determine the expected number of heads obtained when flipping a fair coin. The sample space for a single coin flip is <em>S</em> = {<em>H</em>, <em>T</em>}, with Pr {<em>H</em>} = Pr {<em>T</em>} = 1/2. We can then define an indicator random variable <em>X<sub>H</sub></em>, associated with the coin coming up heads, which is the event <em>H</em>. This variable counts the number of heads obtained in this flip, and it is 1 if the coin comes up heads and 0 otherwise. We write</p>
<p class="eql"><img alt="art" src="images/Art_P246.jpg"/></p>
<p class="noindent">The expected number of heads obtained in one flip of the coin is simply the expected value of our indicator variable <em>X<sub>H</sub></em>:</p>
<table class="table2b">
<tr>
<td class="td2">E [<em>X<sub>H</sub></em>]</td>
<td class="td2m">=</td>
<td class="td2">E [I {<em>H</em>}]</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1 · Pr {<em>H</em>} + 0 · Pr {<em>T</em>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1 · (1/2) + 0 · (1/2)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1/2.</td>
</tr>
</table>
<p class="noindent">Thus the expected number of heads obtained by one flip of a fair coin is 1/2. As the following lemma shows, the expected value of an indicator random variable associated with an event <em>A</em> is equal to the probability that <em>A</em> occurs.</p>
<p class="lemma"><strong><em>Lemma 5.1</em></strong></p>
<p class="noindent">Given a sample space <em>S</em> and an event <em>A</em> in the sample space <em>S</em>, let <em>X<sub>A</sub></em> = I {<em>A</em>}. Then E [<em>X<sub>A</sub></em>] = Pr {<em>A</em>}.</p>
<p class="proof"><strong><em>Proof</em></strong>   By the definition of an indicator random variable from equation (5.1) and the definition of expected value, we have</p>
<table class="table2b">
<tr>
<td class="td2">E [<em>X<sub>A</sub></em>]</td>
<td class="td2m">=</td>
<td class="td2">E [I {<em>A</em>}]</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1 · Pr {<em>A</em>} + 0 · Pr {<em><span class="overline">A</span></em>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">Pr {<em>A</em>},</td>
</tr>
</table>
<a id="p131"/>
<p class="noindent">where <em><span class="overline">A</span></em> denotes <em>S</em> – <em>A</em>, the complement of <em>A</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="spb1">Although indicator random variables may seem cumbersome for an application such as counting the expected number of heads on a flip of a single coin, they are useful for analyzing situations that perform repeated random trials. In <a href="appendix003.xhtml">Appendix C</a>, for example, indicator random variables provide a simple way to determine the expected number of heads in <em>n</em> coin flips. One option is to consider separately the probability of obtaining 0 heads, 1 head, 2 heads, etc. to arrive at the result of equation (C.41) on page 1199. Alternatively, we can employ the simpler method proposed in equation (C.42), which uses indicator random variables implicitly. Making this argument more explicit, let <em>X<sub>i</sub></em> be the indicator random variable associated with the event in which the <em>i</em>th flip comes up heads: <em>X<sub>i</sub></em> = I {the <em>i</em>th flip results in the event <em>H</em>}. Let <em>X</em> be the random variable denoting the total number of heads in the <em>n</em> coin flips, so that</p>
<p class="eql"><img alt="art" src="images/Art_P247.jpg"/></p>
<p class="noindent">In order to compute the expected number of heads, take the expectation of both sides of the above equation to obtain</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P248.jpg"/></p>
<p class="noindent">By Lemma 5.1, the expectation of each of the random variables is E [<em>X<sub>i</sub></em>] = 1/2 for <em>i</em> = 1, 2, … , <em>n</em>. Then we can compute the sum of the expectations: <img alt="art" src="images/Art_P249.jpg"/>. But equation (5.2) calls for the expectation of the sum, not the sum of the expectations. How can we resolve this conundrum? Linearity of expectation, equation (C.24) on page 1192, to the rescue: <em>the expectation of the sum always equals the sum of the expectations</em>. Linearity of expectation applies even when there is dependence among the random variables. Combining indicator random variables with linearity of expectation gives us a powerful technique to compute expected values when multiple events occur. We now can compute the expected number of heads:</p>
<p class="eql"><img alt="art" src="images/Art_P250.jpg"/></p>
<a id="p132"/>
<p class="noindent">Thus, compared with the method used in equation (C.41), indicator random variables greatly simplify the calculation. We use indicator random variables throughout this book.</p>
<p class="level4"><strong>Analysis of the hiring problem using indicator random variables</strong></p>
<p class="noindent">Returning to the hiring problem, we now wish to compute the expected number of times that you hire a new office assistant. In order to use a probabilistic analysis, let’s assume that the candidates arrive in a random order, as discussed in <a href="chapter005.xhtml#Sec_5.1">Section 5.1</a>. (We’ll see in <a href="chapter005.xhtml#Sec_5.3">Section 5.3</a> how to remove this assumption.) Let <em>X</em> be the random variable whose value equals the number of times you hire a new office assistant. We could then apply the definition of expected value from equation (C.23) on page 1192 to obtain</p>
<p class="eql"><img alt="art" src="images/Art_P251.jpg"/></p>
<p class="noindent">but this calculation would be cumbersome. Instead, let’s simplify the calculation by using indicator random variables.</p>
<p>To use indicator random variables, instead of computing E [<em>X</em>] by defining just one variable denoting the number of times you hire a new office assistant, think of the process of hiring as repeated random trials and define <em>n</em> variables indicating whether each particular candidate is hired. In particular, let <em>X<sub>i</sub></em> be the indicator random variable associated with the event in which the <em>i</em>th candidate is hired. Thus,</p>
<p class="eql"><img alt="art" src="images/Art_P252.jpg"/></p>
<p class="noindent">and</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P253.jpg"/></p>
<p class="noindent">Lemma 5.1 gives</p>
<p class="eql">E [<em>X<sub>i</sub></em>] = Pr {candidate <em>i</em> is hired},</p>
<p class="noindent">and we must therefore compute the probability that lines 5–6 of H<small>IRE</small>-A<small>SSISTANT</small> are executed.</p>
<p>Candidate <em>i</em> is hired, in line 6, exactly when candidate <em>i</em> is better than each of candidates 1 through <em>i</em> – 1. Because we have assumed that the candidates arrive in a random order, the first <em>i</em> candidates have appeared in a random order. Any one of these first <em>i</em> candidates is equally likely to be the best qualified so far. Candidate <em>i</em> has a probability of 1/<em>i</em> of being better qualified than candidates 1 through <em>i</em> – 1 and thus a probability of 1/<em>i</em> of being hired. By Lemma 5.1, we conclude that</p>
<a id="p133"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P254.jpg"/></p>
<p class="noindent">Now we can compute E [<em>X</em>]:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P255.jpg"/></p>
<p class="noindent">Even though you interview <em>n</em> people, you actually hire only approximately ln <em>n</em> of them, on average. We summarize this result in the following lemma.</p>
<p class="lemma"><strong><em>Lemma 5.2</em></strong></p>
<p class="noindent">Assuming that the candidates are presented in a random order, algorithm H<small>IRE</small>-A<small>SSISTANT</small> has an average-case total hiring cost of <em>O</em>(<em>c<sub>h</sub></em> ln <em>n</em>).</p>
<p class="proof"><strong><em>Proof</em></strong>   The bound follows immediately from our definition of the hiring cost and equation (5.6), which shows that the expected number of hires is approximately ln <em>n</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The average-case hiring cost is a significant improvement over the worst-case hiring cost of <em>O</em>(<em>c<sub>h</sub>n</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>5.2-1</em></strong></p>
<p class="noindent">In H<small>IRE</small>-A<small>SSISTANT</small>, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability that you hire exactly <em>n</em> times?</p>
<p class="level3"><strong><em>5.2-2</em></strong></p>
<p class="noindent">In H<small>IRE</small>-A<small>SSISTANT</small>, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice?</p>
<p class="level3"><strong><em>5.2-3</em></strong></p>
<p class="noindent">Use indicator random variables to compute the expected value of the sum of <em>n</em> dice.</p>
<a id="p134"/>
<p class="level3"><strong><em>5.2-4</em></strong></p>
<p class="noindent">This exercise asks you to (partly) verify that linearity of expectation holds even if the random variables are not independent. Consider two 6-sided dice that are rolled independently. What is the expected value of the sum? Now consider the case where the first die is rolled normally and then the second die is set equal to the value shown on the first die. What is the expected value of the sum? Now consider the case where the first die is rolled normally and the second die is set equal to 7 minus the value of the first die. What is the expected value of the sum?</p>
<p class="level3"><strong><em>5.2-5</em></strong></p>
<p class="noindent">Use indicator random variables to solve the following problem, which is known as the <strong><em><span class="blue1">hat-check problem</span></em></strong>. Each of <em>n</em> customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their own hat?</p>
<p class="level3"><strong><em>5.2-6</em></strong></p>
<p class="noindent">Let <em>A</em>[1 : <em>n</em>] be an array of <em>n</em> distinct numbers. If <em>i</em> &lt; <em>j</em> and <em>A</em>[<em>i</em>] &gt; <em>A</em>[<em>j</em>], then the pair (<em>i</em>, <em>j</em>) is called an <strong><em><span class="blue1">inversion</span></em></strong> of <em>A</em>. (See Problem 2-4 on page 47 for more on inversions.) Suppose that the elements of <em>A</em> form a uniform random permutation of <span class="font1">〈</span>1, 2, … , <em>n</em><span class="font1">〉</span>. Use indicator random variables to compute the expected number of inversions.</p>
</section>
<p class="line1"/>
<section title="5.3 Randomized algorithms">
<a id="Sec_5.3"/>
<p class="level1" id="h1-27"><a href="toc.xhtml#Rh1-27"><strong>5.3      Randomized algorithms</strong></a></p>
<p class="noindent">In the previous section, we showed how knowing a distribution on the inputs can help us to analyze the average-case behavior of an algorithm. What if you do not know the distribution? Then you cannot perform an average-case analysis. As mentioned in <a href="chapter005.xhtml#Sec_5.1">Section 5.1</a>, however, you might be able to use a randomized algorithm.</p>
<p>For a problem such as the hiring problem, in which it is helpful to assume that all permutations of the input are equally likely, a probabilistic analysis can guide us when developing a randomized algorithm. Instead of <em>assuming</em> a distribution of inputs, we <em>impose</em> a distribution. In particular, before running the algorithm, let’s randomly permute the candidates in order to enforce the property that every permutation is equally likely. Although we have modified the algorithm, we still expect to hire a new office assistant approximately ln <em>n</em> times. But now we expect this to be the case for <em>any</em> input, rather than for inputs drawn from a particular distribution.</p>
<p>Let us further explore the distinction between probabilistic analysis and randomized algorithms. In <a href="chapter005.xhtml#Sec_5.2">Section 5.2</a>, we claimed that, assuming that the candidates <a id="p135"/>arrive in a random order, the expected number of times you hire a new office assistant is about ln <em>n</em>. This algorithm is deterministic: for any particular input, the number of times a new office assistant is hired is always the same. Furthermore, the number of times you hire a new office assistant differs for different inputs, and it depends on the ranks of the various candidates. Since this number depends only on the ranks of the candidates, to represent a particular input, we can just list, in order, the ranks <span class="font1">〈</span><em>rank</em>(1), <em>rank</em>(2), … , <em>rank</em>(<em>n</em>)<span class="font1">〉</span> of the candidates. Given the rank list <em>A</em><sub>1</sub> = <span class="font1">〈</span>1, 2, 3, 4, 5, 6, 7, 8, 9, 10<span class="font1">〉</span>, a new office assistant is always hired 10 times, since each successive candidate is better than the previous one, and lines 5–6 of H<small>IRE</small>-A<small>SSISTANT</small> are executed in each iteration. Given the list of ranks <em>A</em><sub>2</sub> = <span class="font1">〈</span>10, 9, 8, 7, 6, 5, 4, 3, 2, 1<span class="font1">〉</span>, a new office assistant is hired only once, in the first iteration. Given a list of ranks <em>A</em><sub>3</sub> = <span class="font1">〈</span>5, 2, 1, 8, 4, 7, 10, 9, 3, 6<span class="font1">〉</span>, a new office assistant is hired three times, upon interviewing the candidates with ranks 5, 8, and 10. Recalling that the cost of our algorithm depends on how many times you hire a new office assistant, we see that there are expensive inputs such as <em>A</em><sub>1</sub>, inexpensive inputs such as <em>A</em><sub>2</sub>, and moderately expensive inputs such as <em>A</em><sub>3</sub>.</p>
<p>Consider, on the other hand, the randomized algorithm that first permutes the list of candidates and then determines the best candidate. In this case, we randomize in the algorithm, not in the input distribution. Given a particular input, say <em>A</em><sub>3</sub> above, we cannot say how many times the maximum is updated, because this quantity differs with each run of the algorithm. The first time you run the algorithm on <em>A</em><sub>3</sub>, it might produce the permutation <em>A</em><sub>1</sub> and perform 10 updates. But the second time you run the algorithm, it might produce the permutation <em>A</em><sub>2</sub> and perform only one update. The third time you run the algorithm, it might perform some other number of updates. Each time you run the algorithm, its execution depends on the random choices made and is likely to differ from the previous execution of the algorithm. For this algorithm and many other randomized algorithms, <em>no particular input elicits its worst-case behavior</em>. Even your worst enemy cannot produce a bad input array, since the random permutation makes the input order irrelevant. The randomized algorithm performs badly only if the random-number generator produces an “unlucky” permutation.</p>
<p>For the hiring problem, the only change needed in the code is to randomly permute the array, as done in the R<small>ANDOMIZED</small>-H<small>IRE</small>-A<small>SSISTANT</small> procedure. This simple change creates a randomized algorithm whose performance matches that obtained by assuming that the candidates were presented in a random order.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ANDOMIZED</small>-H<small>IRE</small>-A<small>SSISTANT</small>(<em>n</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent">randomly permute the list of candidates</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent">H<small>IRE</small>-A<small>SSISTANT</small>(<em>n</em>)</p></td>
</tr>
</table>
</div>
<a id="p136"/>
<p class="lemma"><strong><em>Lemma 5.3</em></strong></p>
<p class="noindent">The expected hiring cost of the procedure R<small>ANDOMIZED</small>-H<small>IRE</small>-A<small>SSISTANT</small> is <em>O</em>(<em>c<sub>h</sub></em> ln <em>n</em>).</p>
<p class="proof"><strong><em>Proof</em></strong>   Permuting the input array achieves a situation identical to that of the probabilistic analysis of H<small>IRE</small>-A<small>SSISTANT</small> in Secetion 5.2.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">By carefully comparing Lemmas 5.2 and 5.3, you can see the difference between probabilistic analysis and randomized algorithms. Lemma 5.2 makes an assumption about the input. Lemma 5.3 makes no such assumption, although randomizing the input takes some additional time. To remain consistent with our terminology, we couched Lemma 5.2 in terms of the average-case hiring cost and Lemma 5.3 in terms of the expected hiring cost. In the remainder of this section, we discuss some issues involved in randomly permuting inputs.</p>
<p class="level4"><strong>Randomly permuting arrays</strong></p>
<p class="noindent">Many randomized algorithms randomize the input by permuting a given input array. We’ll see elsewhere in this book other ways to randomize an algorithm, but now, let’s see how we can randomly permute an array of <em>n</em> elements. The goal is to produce a <strong><em><span class="blue1">uniform random permutation</span></em></strong>, that is, a permutation that is as likely as any other permutation. Since there are <em>n</em>! possible permutations, we want the probability that any particular permutation is produced to be 1/<em>n</em>!.</p>
<p>You might think that to prove that a permutation is a uniform random permutation, it suffices to show that, for each element <em>A</em>[<em>i</em>], the probability that the element winds up in position <em>j</em> is 1/<em>n</em>. Exercise 5.3-4 shows that this weaker condition is, in fact, insufficient.</p>
<p>Our method to generate a random permutation permutes the array <strong><em><span class="blue1">in place</span></em></strong>: at most a constant number of elements of the input array are ever stored outside the array. The procedure R<small>ANDOMLY</small>-P<small>ERMUTE</small> permutes an array <em>A</em>[1 : <em>n</em>] in place in Θ(<em>n</em>) time. In its <em>i</em>th iteration, it chooses the element <em>A</em>[<em>i</em>] randomly from among elements <em>A</em>[<em>i</em>] through <em>A</em>[<em>n</em>]. After the <em>i</em>th iteration, <em>A</em>[<em>i</em>] is never altered.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ANDOMLY</small>-P<small>ERMUTE</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2">swap <em>A</em>[<em>i</em>] with <em>A</em>[R<small>ANDOM</small>(<em>i</em>, <em>n</em>)]</p></td>
</tr>
</table>
</div>
<p>We use a loop invariant to show that procedure R<small>ANDOMLY</small>-P<small>ERMUTE</small> produces a uniform random permutation. A <strong><em><span class="blue1">k-permutation</span></em></strong> on a set of <em>n</em> elements is a sequence <a id="p137"/>containing <em>k</em> of the <em>n</em> elements, with no repetitions. (See page 1180 in <a href="appendix003.xhtml">Appendix C</a>.) There are <em>n</em>!/(<em>n</em> – <em>k</em>)! such possible <em>k</em>-permutations.</p>
<p class="lemma"><strong><em>Lemma 5.4</em></strong></p>
<p class="noindent">Procedure R<small>ANDOMLY</small>-P<small>ERMUTE</small> computes a uniform random permutation.</p>
<p class="proof"><strong><em>Proof</em></strong>   We use the following loop invariant:</p>
<div class="pull-quote">
<p class="pq-noindent">Just prior to the <em>i</em>th iteration of the <strong>for</strong> loop of lines 1–2, for each possible (<em>i</em> – 1)-permutation of the <em>n</em> elements, the subarray <em>A</em>[1 : <em>i</em> – 1] contains this (<em>i</em> – 1)-permutation with probability (<em>n</em> – <em>i</em> + 1)!/<em>n</em>!.</p>
</div>
<p class="noindent">We need to show that this invariant is true prior to the first loop iteration, that each iteration of the loop maintains the invariant, that the loop terminates, and that the invariant provides a useful property to show correctness when the loop terminates.</p>
<p class="para-hang1"><strong>Initialization:</strong> Consider the situation just before the first loop iteration, so that <em>i</em> = 1. The loop invariant says that for each possible 0-permutation, the subarray <em>A</em>[1 : 0] contains this 0-permutation with probability (<em>n</em> – <em>i</em> + 1)!/<em>n</em>! = <em>n</em>!/<em>n</em>! = 1. The subarray <em>A</em>[1 : 0] is an empty subarray, and a 0-permutation has no elements. Thus, <em>A</em>[1 : 0] contains any 0-permutation with probability 1, and the loop invariant holds prior to the first iteration.</p>
<p class="para-hang1"><strong>Maintenance:</strong> By the loop invariant, we assume that just before the <em>i</em>th iteration, each possible (<em>i</em> – 1)-permutation appears in the subarray <em>A</em>[1 : <em>i</em> – 1] with probability (<em>n</em> – <em>i</em> + 1)!/<em>n</em>!. We shall show that after the <em>i</em>th iteration, each possible <em>i</em>-permutation appears in the subarray <em>A</em>[1 : <em>i</em>] with probability (<em>n</em> – <em>i</em>)!/<em>n</em>!. Incrementing <em>i</em> for the next iteration then maintains the loop invariant.</p>
<p class="php">Let us examine the <em>i</em>th iteration. Consider a particular <em>i</em>-permutation, and denote the elements in it by <span class="font1">〈</span><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>i</sub></em><span class="font1">〉</span>. This permutation consists of an (<em>i</em> – 1)-permutation <span class="font1">〈</span><em>x</em><sub>1</sub>, … , <em>x</em><sub><em>i</em>–1</sub><span class="font1">〉</span> followed by the value <em>x</em><sub><em>i</em></sub> that the algorithm places in <em>A</em>[<em>i</em>]. Let <em>E</em><sub>1</sub> denote the event in which the first <em>i</em> – 1 iterations have created the particular (<em>i</em> – 1)-permutation <span class="font1">〈</span><em>x</em><sub>1</sub>, … , <em>x</em><sub><em>i</em>–1</sub><span class="font1">〉</span> in <em>A</em>[1 : <em>i</em> – 1]. By the loop invariant, Pr {<em>E</em><sub>1</sub>} = (<em>n</em> – <em>i</em> + 1)!/<em>n</em>!. Let <em>E</em><sub>2</sub> be the event that the <em>i</em>th iteration puts <em>x</em><sub><em>i</em></sub> in position <em>A</em>[<em>i</em>]. The <em>i</em>-permutation <span class="font1">〈</span><em>x</em><sub>1</sub>, … , <em>x<sub>i</sub></em><span class="font1">〉</span> appears in <em>A</em>[1 : <em>i</em>] precisely when both <em>E</em><sub>1</sub> and <em>E</em><sub>2</sub> occur, and so we wish to compute Pr {<em>E</em><sub>2</sub> ∩ <em>E</em><sub>1</sub>}. Using equation (C.16) on page 1187, we have</p>
<p class="phpt">Pr {<em>E</em><sub>2</sub> ∩ <em>E</em><sub>1</sub>} = Pr {<em>E</em><sub>2</sub> | <em>E</em><sub>1</sub>} Pr {<em>E</em><sub>1</sub>}.</p>
<p class="php">The probability Pr {<em>E</em><sub>2</sub> | <em>E</em><sub>1</sub>} equals 1/(<em>n</em> – <em>i</em> + 1) because in line 2 the algorithm chooses <em>x<sub>i</sub></em> randomly from the <em>n</em> – <em>i</em> + 1 values in positions <em>A</em>[<em>i</em> : <em>n</em>]. Thus, we have</p>
<a id="p138"/>
<p class="phpt"><img alt="art" src="images/Art_P256.jpg"/></p>
<p class="para-hang1"><strong>Termination:</strong> The loop terminates, since it is a <strong>for</strong> loop iterating <em>n</em> times. At termination, <em>i</em> = <em>n</em> + 1, and we have that the subarray <em>A</em>[1 : <em>n</em>] is a given <em>n</em>-permutation with probability (<em>n</em> – (<em>n</em> + 1) + 1)!/<em>n</em>! = 0!/<em>n</em>! = 1/<em>n</em>!.</p>
<p class="php">Thus, R<small>ANDOMLY</small>-P<small>ERMUTE</small> produces a uniform random permutation.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">A randomized algorithm is often the simplest and most efficient way to solve a problem.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>5.3-1</em></strong></p>
<p class="noindent">Professor Marceau objects to the loop invariant used in the proof of Lemma 5.4. He questions whether it holds prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no 0-permutations. Therefore, the probability that an empty subarray contains a 0-permutation should be 0, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure R<small>ANDOMLY</small>-P<small>ERMUTE</small> so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.4 for your procedure.</p>
<p class="level3"><strong><em>5.3-2</em></strong></p>
<p class="noindent">Professor Kelp decides to write a procedure that produces at random any permutation except the <strong><em><span class="blue1">identity permutation</span></em></strong>, in which every element ends up where it started. He proposes the procedure P<small>ERMUTE</small>-W<small>ITHOUT</small>-I<small>DENTITY</small>. Does this procedure do what Professor Kelp intends?</p>
<div class="pull-quote1">
<p class="box-heading">P<small>ERMUTE</small>-W<small>ITHOUT</small>-I<small>DENTITY</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> – 1</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2">swap <em>A</em>[<em>i</em>] with <em>A</em>[R<small>ANDOM</small>(<em>i</em> + 1, <em>n</em>)]</p></td>
</tr>
</table>
</div>
<p class="level3"><strong><em>5.3-3</em></strong></p>
<p class="noindent">Consider the P<small>ERMUTE</small>-W<small>ITH</small>-A<small>LL</small> procedure on the facing page, which instead of swapping element <em>A</em>[<em>i</em>] with a random element from the subarray <em>A</em>[<em>i</em> : <em>n</em>], swaps it with a random element from anywhere in the array. Does P<small>ERMUTE</small>-W<small>ITH</small>-A<small>LL</small> produce a uniform random permutation? Why or why not?</p>
<a id="p139"/>
<div class="pull-quote1">
<p class="box-heading">P<small>ERMUTE</small>-W<small>ITH</small>-A<small>LL</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2">swap <em>A</em>[<em>i</em>] with <em>A</em>[R<small>ANDOM</small>(1, <em>n</em>)]</p></td>
</tr>
</table>
</div>
<p class="level3"><strong><em>5.3-4</em></strong></p>
<p class="noindent">Professor Knievel suggests the procedure P<small>ERMUTE</small>-B<small>Y</small>-C<small>YCLE</small> to generate a uniform random permutation. Show that each element <em>A</em>[<em>i</em>] has a 1/<em>n</em> probability of winding up in any particular position in <em>B</em>. Then show that Professor Knievel is mistaken by showing that the resulting permutation is not uniformly random.</p>
<div class="pull-quote1">
<p class="box-heading">P<small>ERMUTE</small>-B<small>Y</small>-C<small>YCLE</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent">let <em>B</em>[1 : <em>n</em>] be a new array</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><em>offset</em> = R<small>ANDOM</small>(1, <em>n</em>)</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><em>dest</em> = <em>i</em> + <em>offset</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>dest</em> &gt; <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p3"><em>dest</em> = <em>dest</em> – <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p2"><em>B</em>[<em>dest</em>] = <em>A</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>B</em></p></td>
</tr>
</table>
</div>
<p class="level3"><strong><em>5.3-5</em></strong></p>
<p class="noindent">Professor Gallup wants to create a <strong><em><span class="blue1">random sample</span></em></strong> of the set {1, 2, 3, … , <em>n</em>}, that is, an <em>m</em>-element subset <em>S</em>, where 0 ≤ <em>m</em> ≤ <em>n</em>, such that each <em>m</em>-subset is equally likely to be created. One way is to set <em>A</em>[<em>i</em>] = <em>i</em>, for <em>i</em> = 1, 2, 3, … , <em>n</em>, call R<small>ANDOMLY</small>-P<small>ERMUTE</small>(<em>A</em>), and then take just the first <em>m</em> array elements. This method makes <em>n</em> calls to the R<small>ANDOM</small> procedure. In Professor Gallup’s application, <em>n</em> is much larger than <em>m</em>, and so the professor wants to create a random sample with fewer calls to R<small>ANDOM</small>.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>ANDOM</small>-S<small>AMPLE</small>(<em>m</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><em>S</em> = ∅</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>k</em> = <em>n</em> – <em>m</em> + 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> iterates <em>m</em> times</span></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><em>i</em> = R<small>ANDOM</small>(1, <em>k</em>)</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>i</em> ∈ <em>S</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p3"><em>S</em> = <em>S</em> <span class="font1">⋃</span> {<em>k</em>}</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p2"><strong>else</strong> <em>S</em> = <em>S</em> <span class="font1">⋃</span> {<em>i</em>}</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>S</em></p></td>
<td class="td1"/>
</tr>
</table>
</div>
<a id="p140"/>
<p>Show that the procedure R<small>ANDOM</small>-S<small>AMPLE</small> on the previous page returns a random <em>m</em>-subset <em>S</em> of {1, 2, 3, … , <em>n</em>}, in which each <em>m</em>-subset is equally likely, while making only <em>m</em> calls to R<small>ANDOM</small>.</p>
</section>
<p class="line1"/>
<section title="⋆ 5.4 Probabilistic analysis and further uses of indicator random variables">
<a id="Sec_5.4"/>
<p class="level1-1" id="h1-28"><a href="toc.xhtml#Rh1-28"><span class="font1">★</span> <strong>5.4      Probabilistic analysis and further uses of indicator random variables</strong></a></p>
<p class="noindent">This advanced section further illustrates probabilistic analysis by way of four examples. The first determines the probability that in a room of <em>k</em> people, two of them share the same birthday. The second example examines what happens when randomly tossing balls into bins. The third investigates “streaks” of consecutive heads when flipping coins. The final example analyzes a variant of the hiring problem in which you have to make decisions without actually interviewing all the candidates.</p>
<section title="5.4.1 The birthday paradox">
<p class="level2" id="Sec_5.4.1"><strong>5.4.1    The birthday paradox</strong></p>
<p class="noindent">Our first example is the <strong><em><span class="blue1">birthday paradox</span></em></strong>. How many people must there be in a room before there is a 50% chance that two of them were born on the same day of the year? The answer is surprisingly few. The paradox is that it is in fact far fewer than the number of days in a year, or even half the number of days in a year, as we shall see.</p>
<p>To answer this question, we index the people in the room with the integers 1, 2, … , <em>k</em>, where <em>k</em> is the number of people in the room. We ignore the issue of leap years and assume that all years have <em>n</em> = 365 days. For <em>i</em> = 1, 2, … , <em>k</em>, let <em>b<sub>i</sub></em> be the day of the year on which person <em>i</em>’s birthday falls, where 1 ≤ <em>b<sub>i</sub></em> ≤ <em>n</em>. We also assume that birthdays are uniformly distributed across the <em>n</em> days of the year, so that Pr {<em>b<sub>i</sub></em> = <em>r</em>} = 1/<em>n</em> for <em>i</em> = 1, 2, … , <em>k</em> and <em>r</em> = 1, 2, … , <em>n</em>.</p>
<p>The probability that two given people, say <em>i</em> and <em>j</em>, have matching birthdays depends on whether the random selection of birthdays is independent. We assume from now on that birthdays are independent, so that the probability that <em>i</em>’s birthday and <em>j</em>’s birthday both fall on day <em>r</em> is</p>
<p class="eql"><img alt="art" src="images/Art_P257.jpg"/></p>
<p class="noindent">Thus, the probability that they both fall on the same day is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P258.jpg"/></p>
<a id="p141"/>
<p class="noindent">More intuitively, once <em>b<sub>i</sub></em> is chosen, the probability that <em>b<sub>j</sub></em> is chosen to be the same day is 1/<em>n</em>. As long as the birthdays are independent, the probability that <em>i</em> and <em>j</em> have the same birthday is the same as the probability that the birthday of one of them falls on a given day.</p>
<p>We can analyze the probability of at least 2 out of <em>k</em> people having matching birthdays by looking at the complementary event. The probability that at least two of the birthdays match is 1 minus the probability that all the birthdays are different. The event <em>B<sub>k</sub></em> that <em>k</em> people have distinct birthdays is</p>
<p class="eql"><img alt="art" src="images/Art_P259.jpg"/></p>
<p class="noindent">where <em>A<sub>i</sub></em> is the event that person <em>i</em>’s birthday is different from person <em>j</em>’s for all <em>j</em> &lt; <em>i</em>. Since we can write <em>B<sub>k</sub></em> = <em>A<sub>k</sub></em> ∩ <em>B</em><sub><em>k</em>–1</sub>, we obtain from equation (C.18) on page 1189 the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P260.jpg"/></p>
<p class="noindent">where we take Pr {<em>B</em><sub>1</sub>} = Pr {<em>A</em><sub>1</sub>} = 1 as an initial condition. In other words, the probability that <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b<sub>k</sub></em> are distinct birthdays equals the probability that <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b</em><sub><em>k</em>–1</sub> are distinct birthdays multiplied by the probability that <em>b<sub>k</sub></em> ≠ <em>b<sub>i</sub></em> for <em>i</em> = 1, 2, … , <em>k</em> – 1, given that <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b</em><sub><em>k</em>–1</sub> are distinct.</p>
<p>If <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, … , <em>b</em><sub><em>k</em>–1</sub> are distinct, the conditional probability that <em>b<sub>k</sub></em> ≠ <em>b<sub>i</sub></em> for <em>i</em> = 1, 2, … , <em>k</em> – 1 is Pr {<em>A<sub>k</sub></em> | <em>B</em><sub><em>k</em>–1</sub>} = (<em>n</em> – <em>k</em> + 1)/<em>n</em>, since out of the <em>n</em> days, <em>n</em> – (<em>k</em> – 1) days are not taken. We iteratively apply the recurrence (5.8) to obtain</p>
<p class="eql"><img alt="art" src="images/Art_P261.jpg"/></p>
<p class="noindent">Inequality (3.14) on page 66, 1 + <em>x</em> ≤ <em>e<sup>x</sup></em>, gives us</p>
<a id="p142"/>
<p class="eql"><img alt="art" src="images/Art_P262.jpg"/></p>
<p class="noindent">when –<em>k</em>(<em>k</em> – 1)/2<em>n</em> ≤ ln(1/2). The probability that all <em>k</em> birthdays are distinct is at most 1/2 when <em>k</em>(<em>k</em> – 1) ≥ 2<em>n</em> ln 2 or, solving the quadratic equation, when <img alt="art" src="images/Art_P263.jpg"/>. For <em>n</em> = 365, we must have <em>k</em> ≥ 23. Thus, if at least 23 people are in a room, the probability is at least 1/2 that at least two people have the same birthday. Since a year on Mars is 669 Martian days long, it takes 31 Martians to get the same effect.</p>
<p class="level4"><strong>An analysis using indicator random variables</strong></p>
<p class="noindent">Indicator random variables afford a simpler but approximate analysis of the birthday paradox. For each pair (<em>i</em>, <em>j</em>) of the <em>k</em> people in the room, define the indicator random variable <em>X<sub>ij</sub></em>, for 1 ≤ <em>i</em> &lt; <em>j</em> ≤ <em>k</em>, by</p>
<p class="eql"><img alt="art" src="images/Art_P264.jpg"/></p>
<p class="noindent">By equation (5.7), the probability that two people have matching birthdays is 1/<em>n</em>, and thus by Lemma 5.1 on page 130, we have</p>
<table class="table2b">
<tr>
<td class="td2">E [<em>X<sub>ij</sub></em>]</td>
<td class="td2m">=</td>
<td class="td2">Pr {person <em>i</em> and person <em>j</em> have the same birthday}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1/<em>n</em>.</td>
</tr>
</table>
<p class="noindent">Letting <em>X</em> be the random variable that counts the number of pairs of individuals having the same birthday, we have</p>
<p class="eql"><img alt="art" src="images/Art_P265.jpg"/></p>
<p class="noindent">Taking expectations of both sides and applying linearity of expectation, we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P266.jpg"/></p>
<a id="p143"/>
<p class="noindent">When <em>k</em>(<em>k</em> – 1) ≥ 2<em>n</em>, therefore, the expected number of pairs of people with the same birthday is at least 1. Thus, if we have at least <img alt="art" src="images/Art_P267.jpg"/> individuals in a room, we can expect at least two to have the same birthday. For <em>n</em> = 365, if <em>k</em> = 28, the expected number of pairs with the same birthday is (28 · 27)/(2 · 365) ≈ 1.0356. Thus, with at least 28 people, we expect to find at least one matching pair of birthdays. On Mars, with 669 days per year, we need at least 38 Martians.</p>
<p>The first analysis, which used only probabilities, determined the number of people required for the probability to exceed 1/2 that a matching pair of birthdays exists, and the second analysis, which used indicator random variables, determined the number such that the expected number of matching birthdays is 1. Although the exact numbers of people differ for the two situations, they are the same asymptotically: <img alt="art" src="images/Art_P268.jpg"/>.</p>
</section>
<section title="5.4.2 Balls and bins">
<p class="level2" id="Sec_5.4.2"><strong>5.4.2    Balls and bins</strong></p>
<p class="noindent">Consider a process in which you randomly toss identical balls into <em>b</em> bins, numbered 1, 2, … , <em>b</em>. The tosses are independent, and on each toss the ball is equally likely to end up in any bin. The probability that a tossed ball lands in any given bin is 1/<em>b</em>. If we view the ball-tossing process as a sequence of Bernoulli trials (see <a href="appendix003.xhtml#Sec_C.4">Appendix C.4</a>), where success means that the ball falls in the given bin, then each trial has a probability 1/<em>b</em> of success. This model is particularly useful for analyzing hashing (see <a href="chapter011.xhtml">Chapter 11</a>), and we can answer a variety of interesting questions about the ball-tossing process. (Problem C-2 asks additional questions about balls and bins.)</p>
<ul class="ulnoindent" epub:type="list">
<li><em>How many balls fall in a given bin?</em> The number of balls that fall in a given bin follows the binomial distribution <em>b</em>(<em>k</em>;<em>n</em>, 1/<em>b</em>). If you toss <em>n</em> balls, equation (C.41) on page 1199 tells us that the expected number of balls that fall in the given bin is <em>n</em>/<em>b</em>.</li>
<li class="litop"><em>How many balls must you toss, on the average, until a given bin contains a ball?</em> The number of tosses until the given bin receives a ball follows the geometric distribution with probability 1/<em>b</em> and, by equation (C.36) on page 1197, the expected number of tosses until success is 1/(1/<em>b</em>) = <em>b</em>.</li>
<li class="litop"><em>How many balls must you toss until every bin contains at least one ball?</em> Let us call a toss in which a ball falls into an empty bin a “hit.” We want to know the expected number <em>n</em> of tosses required to get <em>b</em> hits.
<a id="p144"/>
<p class="noindent1-top">Using the hits, we can partition the <em>n</em> tosses into stages. The <em>i</em>th stage consists of the tosses after the (<em>i</em> – 1)st hit up to and including the <em>i</em>th hit. The first stage consists of the first toss, since you are guaranteed to have a hit when all bins are empty. For each toss during the <em>i</em>th stage, <em>i</em> – 1 bins contain balls and <em>b</em> – <em>i</em> + 1 bins are empty. Thus, for each toss in the <em>i</em>th stage, the probability of obtaining a hit is (<em>b</em> – <em>i</em> + 1)/<em>b</em>.</p>
<p class="noindent1-top">Let <em>n<sub>i</sub></em> denote the number of tosses in the <em>i</em>th stage. The number of tosses required to get <em>b</em> hits is <img alt="art" src="images/Art_P269.jpg"/>. Each random variable <em>n<sub>i</sub></em> has a geometric distribution with probability of success (<em>b</em> – <em>i</em> + 1)/<em>b</em> and thus, by equation (C.36), we have</p>
<p class="eql"><img alt="art" src="images/Art_P270.jpg"/></p>
<p class="noindent">By linearity of expectation, we have</p>
<p class="eql"><img alt="art" src="images/Art_P271.jpg"/></p>
<p class="noindent">It therefore takes approximately <em>b</em> ln <em>b</em> tosses before we can expect that every bin has a ball. This problem is also known as the <strong><em><span class="blue1">coupon collector’s problem</span></em></strong>, which says that if you are trying to collect each of <em>b</em> different coupons, then you should expect to acquire approximately <em>b</em> ln <em>b</em> randomly obtained coupons in order to succeed.</p>
</li></ul>
</section>
<section title="5.4.3 Streaks">
<p class="level2" id="Sec_5.4.3"><strong>5.4.3    Streaks</strong></p>
<p class="noindent">Suppose that you flip a fair coin <em>n</em> times. What is the longest streak of consecutive heads that you expect to see? We’ll prove upper and lower bounds separately to show that the answer is Θ(lg <em>n</em>).</p>
<a id="p145"/>
<p>We first prove that the expected length of the longest streak of heads is <em>O</em>(lg <em>n</em>). The probability that each coin flip is a head is 1/2. Let <em>A<sub>ik</sub></em> be the event that a streak of heads of length at least <em>k</em> begins with the <em>i</em>th coin flip or, more precisely, the event that the <em>k</em> consecutive coin flips <em>i</em>, <em>i</em> + 1, … , <em>i</em> + <em>k</em> – 1 yield only heads, where 1 ≤ <em>k</em> ≤ <em>n</em> and 1 ≤ <em>i</em> ≤ <em>n</em> – <em>k</em> + 1. Since coin flips are mutually independent, for any given event <em>A<sub>ik</sub></em>, the probability that all <em>k</em> flips are heads is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P272.jpg"/></p>
<p class="noindent">and thus the probability that a streak of heads of length at least 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> begins in position <em>i</em> is quite small. There are at most <em>n</em> – 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> + 1 positions where such a streak can begin. The probability that a streak of heads of length at least 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> begins anywhere is therefore</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P273.jpg"/></p>
<p>We can use inequality (5.10) to bound the length of the longest streak. For <em>j</em> = 0, 1, 2, … , <em>n</em>, let <em>L<sub>j</sub></em> be the event that the longest streak of heads has length exactly <em>j</em>, and let <em>L</em> be the length of the longest streak. By the definition of expected value, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P274.jpg"/></p>
<a id="p146"/>
<p class="noindent">We could try to evaluate this sum using upper bounds on each Pr {<em>L<sub>j</sub></em>} similar to those computed in inequality (5.10). Unfortunately, this method yields weak bounds. We can use some intuition gained by the above analysis to obtain a good bound, however. For no individual term in the summation in equation (5.11) are both the factors <em>j</em> and Pr {<em>L<sub>j</sub></em>} large. Why? When <em>j</em> ≥ 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span>, then Pr {<em>L<sub>j</sub></em>} is very small, and when <em>j</em> &lt; 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span>, then <em>j</em> is fairly small. More precisely, since the events <em>L<sub>j</sub></em> for <em>j</em> = 0, 1, … , <em>n</em> are disjoint, the probability that a streak of heads of length at least 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> begins anywhere is <img alt="art" src="images/Art_P275.jpg"/>. Inequality (5.10) tells us that the probability that a streak of heads of length at least 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> begins anywhere is less than 1/<em>n</em>, which means that <img alt="art" src="images/Art_P276.jpg"/>. Also, noting that <img alt="art" src="images/Art_P277.jpg"/>, we have that <img alt="art" src="images/Art_P278.jpg"/>. Thus, we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P279.jpg"/></p>
<p>The probability that a streak of heads exceeds <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> flips diminishes quickly with <em>r</em>. Let’s get a rough bound on the probability that a streak of at least <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> heads occurs, for <em>r</em> ≥ 1. The probability that a streak of at least <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> heads starts in position <em>i</em> is</p>
<p class="eql"><img alt="art" src="images/Art_P280.jpg"/></p>
<p class="noindent">A streak of at least <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> heads cannot start in the last <em>n</em> – <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> + 1 flips, but let’s overestimate the probability of such a streak by allowing it to start anywhere within the <em>n</em> coin flips. Then the probability that a streak of at least <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> heads <a id="p147"/>occurs is at most</p>
<p class="eql"><img alt="art" src="images/Art_P281.jpg"/></p>
<p class="noindent">Equivalently, the probability is at least 1 – 1/<em>n</em><sup><em>r</em>–1</sup> that the longest streak has length less than <em>r</em> <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span>.</p>
<p>As an example, during <em>n</em> = 1000 coin flips, the probability of encountering a streak of at least 2 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> = 20 heads is at most 1/<em>n</em> = 1/1000. The chance of a streak of at least 3 <span class="font1">⌈</span>lg <em>n</em><span class="font1">⌉</span> = 30 heads is at most 1/<em>n</em><sup>2</sup> = 1/1,000,000.</p>
<p>Let’s now prove a complementary lower bound: the expected length of the longest streak of heads in <em>n</em> coin flips is Ω(lg <em>n</em>). To prove this bound, we look for streaks of length <em>s</em> by partitioning the <em>n</em> flips into approximately <em>n</em>/<em>s</em> groups of <em>s</em> flips each. If we choose <em>s</em> = <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span>, we’ll see that it is likely that at least one of these groups comes up all heads, which means that it’s likely that the longest streak has length at least <em>s</em> = Ω(lg <em>n</em>). We’ll then show that the longest streak has expected length Ω(lg <em>n</em>).</p>
<p>Let’s partition the <em>n</em> coin flips into at least <span class="font1">⌊</span><em>n</em>/ <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span><span class="font1">⌋</span> groups of <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span> consecutive flips and bound the probability that no group comes up all heads. By equation (5.9), the probability that the group starting in position <em>i</em> comes up all heads is</p>
<p class="eql"><img alt="art" src="images/Art_P282.jpg"/></p>
<p class="noindent">The probability that a streak of heads of length at least <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span> does not begin in position <em>i</em> is therefore at most <img alt="art" src="images/Art_P283.jpg"/>. Since the <span class="font1">⌊</span><em>n</em>/ <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span><span class="font1">⌋</span> groups are formed from mutually exclusive, independent coin flips, the probability that every one of these groups <em>fails</em> to be a streak of length <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span> is at most</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P284.jpg"/></p>
<a id="p148"/>
<p class="noindent">For this argument, we used inequality (3.14), 1 + <em>x</em> ≤ <em>e<sup>x</sup></em>, on page 66 and the fact, which you may verify, that <img alt="art" src="images/Art_P285.jpg"/> for sufficiently large <em>n</em>.</p>
<p>We want to bound the probability that the longest streak equals or exceeds <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span>. To do so, let <em>L</em> be the event that the longest streak of heads equals or exceeds <em>s</em> = <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span>. Let <em><span class="overline">L</span></em> be the complementary event, that the longest streak of heads is strictly less than <em>s</em>, so that Pr {<em>L</em>} + Pr {<em><span class="overline">L</span></em>} = 1. Let <em>F</em> be the event that every group of <em>s</em> flips fails to be a streak of <em>s</em> heads. By inequality (5.12), we have Pr {<em>F</em>} = <em>O</em>(1/<em>n</em>). If the longest streak of heads is less than <em>s</em>, then certainly every group of <em>s</em> flips fails to be a streak of <em>s</em> heads, which means that event <em><span class="overline">L</span></em> implies event <em>F</em>. Of course, event <em>F</em> could occur even if event <em><span class="overline">L</span></em> does not (for example, if a streak of <em>s</em> or more heads crosses over the boundary between two groups), and so we have Pr {<em><span class="overline">L</span></em>} ≤ Pr {<em>F</em>} = <em>O</em>(1/<em>n</em>). Since Pr {<em>L</em>} + Pr {<em><span class="overline">L</span></em>} = 1, we have that</p>
<table class="table2b">
<tr>
<td class="td2">Pr {<em>L</em>}</td>
<td class="td2m">=</td>
<td class="td2">1 – Pr {<em><span class="overline">L</span></em>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≥</td>
<td class="td2">1 – Pr {<em>F</em>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2m">=</td>
<td class="td2">1 – <em>O</em>(1/<em>n</em>).</td>
</tr>
</table>
<p class="noindent">That is, the probability that the longest streak equals or exceeds <span class="font1">⌊</span>(lg <em>n</em>)/2<span class="font1">⌋</span> is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P286.jpg"/></p>
<p>We can now calculate a lower bound on the expected length of the longest streak, beginning with equation (5.11) and proceeding in a manner similar to our analysis of the upper bound:</p>
<p class="eql"><img alt="art" src="images/Art_P287.jpg"/></p>
<a id="p149"/>
<p>As with the birthday paradox, we can obtain a simpler, but approximate, analysis using indicator random variables. Instead of determining the expected length of the longest streak, we’ll find the expected number of streaks with at least a given length. Let <em>X<sub>ik</sub></em> = I {<em>A<sub>ik</sub></em>} be the indicator random variable associated with a streak of heads of length at least <em>k</em> beginning with the <em>i</em>th coin flip. To count the total number of such streaks, define</p>
<p class="eql"><img alt="art" src="images/Art_P288.jpg"/></p>
<p class="noindent">Taking expectations and using linearity of expectation, we have</p>
<p class="eql"><img alt="art" src="images/Art_P289.jpg"/></p>
<p>By plugging in various values for <em>k</em>, we can calculate the expected number of streaks of length at least <em>k</em>. If this expected number is large (much greater than 1), then we expect many streaks of length <em>k</em> to occur, and the probability that one occurs is high. If this expected number is small (much less than 1), then we expect to see few streaks of length <em>k</em>, and the probability that one occurs is low. If <em>k</em> = <em>c</em> lg <em>n</em>, for some positive constant <em>c</em>, we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P290.jpg"/></p>
<p class="noindent">If <em>c</em> is large, the expected number of streaks of length <em>c</em> lg <em>n</em> is small, and we conclude that they are unlikely to occur. On the other hand, if <em>c</em> = 1/2, then we <a id="p150"/>obtain E [<em>X</em><sub>(1/2) lg <em>n</em></sub>] = Θ(1/<em>n</em><sup>1/2–1</sup>) = Θ(<em>n</em><sup>1/2</sup>), and we expect there to be numerous streaks of length (1/2) lg <em>n</em>. Therefore, one streak of such a length is likely to occur. We can conclude that the expected length of the longest streak is Θ(lg <em>n</em>).</p>
</section>
<section title="5.4.4 The online hiring problem">
<p class="level2" id="Sec_5.4.4"><strong>5.4.4    The online hiring problem</strong></p>
<p class="noindent">As a final example, let’s consider a variant of the hiring problem. Suppose now that you do not wish to interview all the candidates in order to find the best one. You also want to avoid hiring and firing as you find better and better applicants. Instead, you are willing to settle for a candidate who is close to the best, in exchange for hiring exactly once. You must obey one company requirement: after each interview you must either immediately offer the position to the applicant or immediately reject the applicant. What is the trade-off between minimizing the amount of interviewing and maximizing the quality of the candidate hired?</p>
<p>We can model this problem in the following way. After meeting an applicant, you are able to give each one a score. Let <em>score</em>(<em>i</em>) denote the score you give to the <em>i</em>th applicant, and assume that no two applicants receive the same score. After you have seen <em>j</em> applicants, you know which of the <em>j</em> has the highest score, but you do not know whether any of the remaining <em>n</em> – <em>j</em> applicants will receive a higher score. You decide to adopt the strategy of selecting a positive integer <em>k</em> &lt; <em>n</em>, interviewing and then rejecting the first <em>k</em> applicants, and hiring the first applicant thereafter who has a higher score than all preceding applicants. If it turns out that the best-qualified applicant was among the first <em>k</em> interviewed, then you hire the <em>n</em>th applicant—the last one interviewed. We formalize this strategy in the procedure O<small>NLINE</small>-M<small>AXIMUM</small>(<em>k</em>, <em>n</em>), which returns the index of the candidate you wish to hire.</p>
<div class="pull-quote1">
<p class="box-heading">O<small>NLINE</small>-M<small>AXIMUM</small>(<em>k</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><em>best</em>-<em>score</em> = –∞</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>k</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>score</em>(<em>i</em>) &gt; <em>best</em>-<em>score</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p3"><em>best-score</em> = <em>score</em>(<em>i</em>)</p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = <em>k</em> + 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>score</em>(<em>i</em>) &gt; <em>best-score</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p3"><strong>return</strong> <em>i</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>n</em></p></td>
</tr>
</table>
</div>
<p>If we determine, for each possible value of <em>k</em>, the probability that you hire the most qualified applicant, then you can choose the best possible <em>k</em> and implement the strategy with that value. For the moment, assume that <em>k</em> is fixed. Let <a id="p151"/><em>M</em>(<em>j</em>) = max {<em>score</em>(<em>i</em>) : 1 ≤ <em>i</em> ≤ <em>j</em>} denote the maximum score among applicants 1 through <em>j</em>. Let <em>S</em> be the event that you succeed in choosing the best-qualified applicant, and let <em>S<sub>i</sub></em> be the event that you succeed when the best-qualified applicant is the <em>i</em>th one interviewed. Since the various <em>S<sub>i</sub></em> are disjoint, we have that <img alt="art" src="images/Art_P291.jpg"/>. Noting that you never succeed when the best-qualified applicant is one of the first <em>k</em>, we have that Pr {<em>S<sub>i</sub></em>} = 0 for <em>i</em> = 1, 2, … , <em>k</em>. Thus, we obtain</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P292.jpg"/></p>
<p>We now compute Pr {<em>S<sub>i</sub></em>}. In order to succeed when the best-qualified applicant is the <em>i</em>th one, two things must happen. First, the best-qualified applicant must be in position <em>i</em>, an event which we denote by <em>B<sub>i</sub></em>. Second, the algorithm must not select any of the applicants in positions <em>k</em> + 1 through <em>i</em> – 1, which happens only if, for each <em>j</em> such that <em>k</em> + 1 ≤ <em>j</em> ≤ <em>i</em> – 1, line 6 finds that <em>score</em>(<em>j</em>) &lt; <em>best-score</em>. (Because scores are unique, we can ignore the possibility of <em>score</em>(<em>j</em>) = <em>best-score</em>.) In other words, all of the values <em>score</em>(<em>k</em> + 1) through <em>score</em>(<em>i</em> – 1) must be less than <em>M</em>(<em>k</em>). If any are greater than <em>M</em>(<em>k</em>), the algorithm instead returns the index of the first one that is greater. We use <em>O<sub>i</sub></em> to denote the event that none of the applicants in position <em>k</em> + 1 through <em>i</em> – 1 are chosen. Fortunately, the two events <em>B<sub>i</sub></em> and <em>O<sub>i</sub></em> are independent. The event <em>O<sub>i</sub></em> depends only on the relative ordering of the values in positions 1 through <em>i</em> – 1, whereas <em>B<sub>i</sub></em> depends only on whether the value in position <em>i</em> is greater than the values in all other positions. The ordering of the values in positions 1 through <em>i</em> – 1 does not affect whether the value in position <em>i</em> is greater than all of them, and the value in position <em>i</em> does not affect the ordering of the values in positions 1 through <em>i</em> – 1. Thus, we can apply equation (C.17) on page 1188 to obtain</p>
<p class="eql">Pr {<em>S<sub>i</sub></em>} = Pr {<em>B<sub>i</sub></em> ∩ <em>O<sub>i</sub></em>} = Pr {<em>B<sub>i</sub></em>} Pr {<em>O<sub>i</sub></em>}.</p>
<p class="noindent">We have Pr {<em>B<sub>i</sub></em>} = 1/<em>n</em> since the maximum is equally likely to be in any one of the <em>n</em> positions. For event <em>O<sub>i</sub></em> to occur, the maximum value in positions 1 through <em>i</em> –1, which is equally likely to be in any of these <em>i</em> – 1 positions, must be in one of the first <em>k</em> positions. Consequently, Pr {<em>O<sub>i</sub></em>} = <em>k</em>/(<em>i</em> – 1) and Pr {<em>S<sub>i</sub></em>} = <em>k</em>/(<em>n</em>(<em>i</em> – 1)). Using equation (5.14), we have</p>
<p class="eql"><img alt="art" src="images/Art_P293.jpg"/></p>
<a id="p152"/>
<p class="noindent">We approximate by integrals to bound this summation from above and below. By the inequalities (A.19) on page 1150, we have</p>
<p class="eql"><img alt="art" src="images/Art_P294.jpg"/></p>
<p class="noindent">Evaluating these definite integrals gives us the bounds</p>
<p class="eql"><img alt="art" src="images/Art_P295.jpg"/></p>
<p class="noindent">which provide a rather tight bound for Pr {<em>S</em>}. Because you wish to maximize your probability of success, let us focus on choosing the value of <em>k</em> that maximizes the lower bound on Pr {<em>S</em>}. (Besides, the lower-bound expression is easier to maximize than the upper-bound expression.) Differentiating the expression (<em>k</em>/<em>n</em>)(ln <em>n</em> – ln <em>k</em>) with respect to <em>k</em>, we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P296.jpg"/></p>
<p class="noindent">Setting this derivative equal to 0, we see that you maximize the lower bound on the probability when ln <em>k</em> = ln <em>n</em> – 1 = ln(<em>n</em>/<em>e</em>) or, equivalently, when <em>k</em> = <em>n</em>/<em>e</em>. Thus, if you implement our strategy with <em>k</em> = <em>n</em>/<em>e</em>, you succeed in hiring the best-qualified applicant with probability at least 1/<em>e</em>.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>5.4-1</em></strong></p>
<p class="noindent">How many people must there be in a room before the probability that someone has the same birthday as you do is at least 1/2? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than 1/2?</p>
<p class="level3"><strong><em>5.4-2</em></strong></p>
<p class="noindent">How many people must there be in a room before the probability that two people have the same birthday is at least 0.99? For that many people, what is the expected number of pairs of people who have the same birthday?</p>
<a id="p153"/>
<p class="level3"><strong><em>5.4-3</em></strong></p>
<p class="noindent">You toss balls into <em>b</em> bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses?</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.4-4</em></strong></p>
<p class="noindent">For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer.</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.4-5</em></strong></p>
<p class="noindent">How many people should be invited to a party in order to make it likely that there are <em>three</em> people with the same birthday?</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.4-6</em></strong></p>
<p class="noindent">What is the probability that a <em>k</em>-string (defined on page 1179) over a set of size <em>n</em> forms a <em>k</em>-permutation? How does this question relate to the birthday paradox?</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.4-7</em></strong></p>
<p class="noindent">You toss <em>n</em> balls into <em>n</em> bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball?</p>
<p class="level3"><span class="font1">★</span> <strong><em>5.4-8</em></strong></p>
<p class="noindent">Sharpen the lower bound on streak length by showing that in <em>n</em> flips of a fair coin, the probability is at least 1 – 1/<em>n</em> that a streak of length lg <em>n</em> – 2 lg lg <em>n</em> consecutive heads occurs.</p>
</section>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-29"><strong>Problems</strong></p>
<p class="level2"><strong><em>5-1     Probabilistic counting</em></strong></p>
<p class="noindent">With a <em>b</em>-bit counter, we can ordinarily only count up to 2<sup><em>b</em></sup> – 1. With R. Morris’s <strong><em><span class="blue1">probabilistic counting</span></em></strong>, we can count up to a much larger value at the expense of some loss of precision.</p>
<p>We let a counter value of <em>i</em> represent a count of <em>n<sub>i</sub></em> for <em>i</em> = 0, 1, … , 2<sup><em>b</em></sup> – 1, where the <em>n<sub>i</sub></em> form an increasing sequence of nonnegative values. We assume that the initial value of the counter is 0, representing a count of <em>n</em><sub>0</sub> = 0. The I<small>NCREMENT</small> operation works on a counter containing the value <em>i</em> in a probabilistic manner. If <em>i</em> = 2<sup><em>b</em></sup> – 1, then the operation reports an overflow error. Otherwise, the I<small>NCREMENT</small> operation increases the counter by 1 with probability 1/(<em>n</em><sub><em>i</em> + 1</sub> – <em>n<sub>i</sub></em>), and it leaves the counter unchanged with probability 1 – 1/(<em>n</em><sub><em>i</em> + 1</sub> – <em>n<sub>i</sub></em>).</p>
<a id="p154"/>
<p>If we select <em>n<sub>i</sub></em> = <em>i</em> for all <em>i</em> ≥ 0, then the counter is an ordinary one. More interesting situations arise if we select, say, <em>n<sub>i</sub></em> = 2<sup><em>i</em> – 1</sup> for <em>i</em> &gt; 0 or <em>n<sub>i</sub></em> = <em>F<sub>i</sub></em> (the <em>i</em>th Fibonacci number—see equation (3.31) on page 69).</p>
<p>For this problem, assume that <img alt="art" src="images/Art_P297.jpg"/> is large enough that the probability of an overflow error is negligible.</p>
<p class="nl"><strong><em>a.</em></strong> Show that the expected value represented by the counter after <em>n</em> I<small>NCREMENT</small> operations have been performed is exactly <em>n</em>.</p>
<p class="nl"><strong><em>b.</em></strong> The analysis of the variance of the count represented by the counter depends on the sequence of the <em>n<sub>i</sub></em>. Let us consider a simple case: <em>n<sub>i</sub></em> = 100<em>i</em> for all <em>i</em> ≥ 0. Estimate the variance in the value represented by the register after <em>n</em> I<small>NCREMENT</small> operations have been performed.</p>
<p class="level2"><strong><em>5-2     Searching an unsorted array</em></strong></p>
<p class="noindent">This problem examines three algorithms for searching for a value <em>x</em> in an unsorted array <em>A</em> consisting of <em>n</em> elements.</p>
<p>Consider the following randomized strategy: pick a random index <em>i</em> into <em>A</em>. If <em>A</em>[<em>i</em>] = <em>x</em>, then terminate; otherwise, continue the search by picking a new random index into <em>A</em>. Continue picking random indices into <em>A</em> until you find an index <em>j</em> such that <em>A</em>[<em>j</em>] = <em>x</em> or until every element of <em>A</em> has been checked. This strategy may examine a given element more than once, because it picks from the whole set of indices each time.</p>
<p class="nl-top"><strong><em>a.</em></strong> Write pseudocode for a procedure R<small>ANDOM</small>-S<small>EARCH</small> to implement the strategy above. Be sure that your algorithm terminates when all indices into <em>A</em> have been picked.</p>
<p class="nl"><strong><em>b.</em></strong> Suppose that there is exactly one index <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the expected number of indices into <em>A</em> that must be picked before <em>x</em> is found and R<small>ANDOM</small>-S<small>EARCH</small> terminates?</p>
<p class="nl"><strong><em>c.</em></strong> Generalizing your solution to part (b), suppose that there are <em>k</em> ≥ 1 indices <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the expected number of indices into <em>A</em> that must be picked before <em>x</em> is found and R<small>ANDOM</small>-S<small>EARCH</small> terminates? Your answer should be a function of <em>n</em> and <em>k</em>.</p>
<p class="nl"><strong><em>d.</em></strong> Suppose that there are no indices <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the expected number of indices into <em>A</em> that must be picked before all elements of <em>A</em> have been checked and R<small>ANDOM</small>-S<small>EARCH</small> terminates?</p>
<p class="noindent1-top">Now consider a deterministic linear search algorithm. The algorithm, which we call D<small>ETERMINISTIC</small>-S<small>EARCH</small>, searches <em>A</em> for <em>x</em> in order, considering <em>A</em>[1], <em>A</em>[2], <a id="p155"/><em>A</em>[3], … , <em>A</em>[<em>n</em>] until either it finds <em>A</em>[<em>i</em>] = <em>x</em> or it reaches the end of the array. Assume that all possible permutations of the input array are equally likely.</p>
<p class="nl-top"><strong><em>e.</em></strong> Suppose that there is exactly one index <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the average-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>? What is the worst-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>?</p>
<p class="nl"><strong><em>f.</em></strong> Generalizing your solution to part (e), suppose that there are <em>k</em> ≥ 1 indices <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the average-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>? What is the worst-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>? Your answer should be a function of <em>n</em> and <em>k</em>.</p>
<p class="nl"><strong><em>g.</em></strong> Suppose that there are no indices <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>. What is the average-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>? What is the worst-case running time of D<small>ETERMINISTIC</small>-S<small>EARCH</small>?</p>
<p class="noindent1-top">Finally, consider a randomized algorithm S<small>CRAMBLE</small>-S<small>EARCH</small> that first randomly permutes the input array and then runs the deterministic linear search given above on the resulting permuted array.</p>
<p class="nl-top"><strong><em>h.</em></strong> Letting <em>k</em> be the number of indices <em>i</em> such that <em>A</em>[<em>i</em>] = <em>x</em>, give the worst-case and expected running times of S<small>CRAMBLE</small>-S<small>EARCH</small> for the cases in which <em>k</em> = 0 and <em>k</em> = 1. Generalize your solution to handle the case in which <em>k</em> ≥ 1.</p>
<p class="nl"><strong><em>i.</em></strong> Which of the three searching algorithms would you use? Explain your answer.</p>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-30"><strong>Chapter notes</strong></p>
<p class="noindent">Bollobás [<a epub:type="noteref" href="bibliography001.xhtml#endnote_65">65</a>], Hofri [<a epub:type="noteref" href="bibliography001.xhtml#endnote_223">223</a>], and Spencer [<a epub:type="noteref" href="bibliography001.xhtml#endnote_420">420</a>] contain a wealth of advanced probabilistic techniques. The advantages of randomized algorithms are discussed and surveyed by Karp [<a epub:type="noteref" href="bibliography001.xhtml#endnote_249">249</a>] and Rabin [<a epub:type="noteref" href="bibliography001.xhtml#endnote_372">372</a>]. The textbook by Motwani and Raghavan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_336">336</a>] gives an extensive treatment of randomized algorithms.</p>
<p>The R<small>ANDOMLY</small>-P<small>ERMUTE</small> procedure is by Durstenfeld [<a epub:type="noteref" href="bibliography001.xhtml#endnote_128">128</a>], based on an earlier procedure by Fisher and Yates [<a epub:type="noteref" href="bibliography001.xhtml#endnote_143">143</a>, p. 34].</p>
<p>Several variants of the hiring problem have been widely studied. These problems are more commonly referred to as “secretary problems.” Examples of work in this area are the paper by Ajtai, Meggido, and Waarts [<a epub:type="noteref" href="bibliography001.xhtml#endnote_11">11</a>] and another by Kleinberg [<a epub:type="noteref" href="bibliography001.xhtml#endnote_258">258</a>], which ties the secretary problem to online ad auctions.</p>
</section>
</section>
</div>
</body>
</html>