<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p748"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="26 Parallel Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-26"><strong><span class="blue1">26        Parallel Algorithms</span></strong></a></p>
<p class="noindent">The vast majority of algorithms in this book are <strong><em><span class="blue">serial algorithms</span></em></strong> suitable for running on a uniprocessor computer that executes only one instruction at a time. This chapter extends our algorithmic model to encompass <strong><em><span class="blue">parallel algorithms</span></em></strong>, where multiple instructions can execute simultaneously. Specifically, we’ll explore the elegant model of task-parallel algorithms, which are amenable to algorithmic design and analysis. Our study focuses on fork-join parallel algorithms, the most basic and best understood kind of task-parallel algorithm. Fork-join parallel algorithms can be expressed cleanly using simple linguistic extensions to ordinary serial code. Moreover, they can be implemented efficiently in practice.</p>
<p>Parallel computers—computers with multiple processing units—are ubiquitous. Handheld, laptop, desktop, and cloud machines are all <strong><em><span class="blue">multicore computers</span></em></strong>, or simply, <strong><em><span class="blue">multicores</span></em></strong>, containing multiple processing “cores.” Each processing core is a full-fledged processor that can directly access any location in a common <strong><em><span class="blue">shared memory</span></em></strong>. Multicores can be aggregated into larger systems, such as clusters, by using a network to interconnect them. These multicore clusters usually have a <strong><em><span class="blue">distributed memory</span></em></strong>, where one multicore’s memory cannot be accessed directly by a processor in another multicore. Instead, the processor must explicitly send a message over the cluster network to a processor in the remote multicore to request any data it requires. The most powerful clusters are supercomputers, comprising many thousands of multicores. But since shared-memory programming tends to be conceptually easier than distributed-memory programming, and multicore machines are widely available, this chapter focuses on parallel algorithms for multicores.</p>
<p>One approach to programming multicores is <strong><em><span class="blue">thread parallelism</span></em></strong>. This processor-centric parallel-programming model employs a software abstraction of “virtual processors,” or <strong><em><span class="blue">threads</span></em></strong> that share a common memory. Each thread maintains its own program counter and can execute code independently of the other threads. The operating system loads a thread onto a processing core for execution and switches it out when another thread needs to run.</p>
<a id="p749"/>
<p>Unfortunately, programming a shared-memory parallel computer using threads tends to be difficult and error-prone. One reason is that it can be complicated to dynamically partition the work among the threads so that each thread receives approximately the same load. For any but the simplest of applications, the programmer must use complex communication protocols to implement a scheduler that load-balances the work.</p>
<p class="level4"><strong>Task-parallel programming</strong></p>
<p class="noindent">The difficulty of thread programming has led to the creation of <strong><em><span class="blue">task-parallel platforms</span></em></strong>, which provide a layer of software on top of threads to coordinate, schedule, and manage the processors of a multicore. Some task-parallel platforms are built as runtime libraries, but others provide full-fledged parallel languages with compiler and runtime support.</p>
<p><strong><em><span class="blue">Task-parallel programming</span></em></strong> allows parallelism to be specified in a “processor-oblivious” fashion, where the programmer identifies what computational tasks may run in parallel but does not indicate which thread or processor performs the task. Thus, the programmer is freed from worrying about communication protocols, load balancing, and other vagaries of thread programming. The task-parallel platform contains a scheduler, which automatically load-balances the tasks across the processors, thereby greatly simplifying the programmer’s chore. <strong><em><span class="blue">Task-parallel algorithms</span></em></strong> provide a natural extension to ordinary serial algorithms, allowing performance to be reasoned about mathematically using “work/span analysis.”</p>
<p class="level4"><strong>Fork-join parallelism</strong></p>
<p class="noindent">Although the functionality of task-parallel environments is still evolving and increasing, almost all support <strong><em><span class="blue">fork-join parallelism</span></em></strong>, which is typically embodied in two linguistic features: <strong><em><span class="blue">spawning</span></em></strong> and <strong><em><span class="blue">parallel loops</span></em></strong>. Spawning allows a subroutine to be “forked”: executed like a subroutine call, except that the caller can continue to execute while the spawned subroutine computes its result. A parallel loop is like an ordinary <strong>for</strong> loop, except that multiple iterations of the loop can execute at the same time.</p>
<p><strong><em><span class="blue">Fork-join</span></em></strong> parallel algorithms employ spawning and parallel loops to describe parallelism. A key aspect of this parallel model, inherited from the task-parallel model but different from the thread model, is that the programmer does not specify which tasks in a computation <em>must</em> run in parallel, only which tasks <em>may</em> run in parallel. The underlying runtime system uses threads to load-balance the tasks across the processors. This chapter investigates parallel algorithms described in the fork-join model, as well as how the underlying runtime system can schedule task-parallel computations (which include fork-join computations) efficiently.</p>
<a id="p750"/>
<p>Fork-join parallelism offers several important advantages:</p>
<ul class="ulnoindent" epub:type="list">
<li>The fork-join programming model is a simple extension of the familiar serial programming model used in most of this book. To describe a fork-join parallel algorithm, the pseudocode in this book needs just three added keywords: <strong>parallel</strong>, <strong>spawn</strong>, and <strong>sync</strong>. Deleting these parallel keywords from the parallel pseudocode results in ordinary serial pseudocode for the same problem, which we call the “serial projection” of the parallel algorithm.</li>
<li class="litop">The underlying task-parallel model provides a theoretically clean way to quantify parallelism based on the notions of “work” and “span.”</li>
<li class="litop">Spawning allows many divide-and-conquer algorithms to be parallelized naturally. Moreover, just as serial divide-and-conquer algorithms lend themselves to analysis using recurrences, so do parallel algorithms in the fork-join model.</li>
<li class="litop">The fork-join programming model is faithful to how multicore programming has been evolving in practice. A growing number of multicore environments support one variant or another of fork-join parallel programming, including Cilk [<a epub:type="noteref" href="bibliography001.xhtml#endnote_290">290</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_291">291</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_383">383</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_396">396</a>], Habanero-Java [<a epub:type="noteref" href="bibliography001.xhtml#endnote_466">466</a>], the Java Fork-Join Framework [<a epub:type="noteref" href="bibliography001.xhtml#endnote_279">279</a>], OpenMP [<a epub:type="noteref" href="bibliography001.xhtml#endnote_81">81</a>], Task Parallel Library [<a epub:type="noteref" href="bibliography001.xhtml#endnote_289">289</a>], Threading Building Blocks [<a epub:type="noteref" href="bibliography001.xhtml#endnote_376">376</a>], and X10 [<a epub:type="noteref" href="bibliography001.xhtml#endnote_82">82</a>].</li></ul>
<p><a href="chapter026.xhtml#Sec_26.1">Section 26.1</a> introduces parallel pseudocode, shows how the execution of a task-parallel computation can be modeled as a directed acyclic graph, and presents the metrics of work, span, and parallelism, which you can use to analyze parallel algorithms. <a href="chapter026.xhtml#Sec_26.2">Section 26.2</a> investigates how to multiply matrices in parallel, and <a href="chapter026.xhtml#Sec_26.3">Section 26.3</a> tackles the tougher problem of designing an efficient parallel merge sort.</p>
<p class="line1"/>
<section title="26.1 The basics of fork-join parallelism">
<a id="Sec_26.1"/>
<p class="level1" id="h1-152"><a href="toc.xhtml#Rh1-152"><strong>26.1    The basics of fork-join parallelism</strong></a></p>
<p class="noindent">Our exploration of parallel programming begins with the problem of computing Fibonacci numbers recursively in parallel. We’ll look at a straightforward serial Fibonacci calculation, which, although inefficient, serves as a good illustration of how to express parallelism in pseudocode.</p>
<p>Recall that the Fibonacci numbers are defined by equation (3.31) on page 69:</p>
<p class="eql"><img alt="art" src="images/Art_P824.jpg"/></p>
<p class="noindent">To calculate the <em>n</em>th Fibonacci number recursively, you could use the ordinary serial algorithm in the procedure F<small>IB</small> on the facing page. You would not really want to <a id="p751"/>compute large Fibonacci numbers this way, because this computation does needless repeated work, but parallelizing it can be instructive.</p>
<div class="pull-quote1">
<p class="box-heading">F<small>IB</small> (<em>n</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>n</em> ≤ 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>return</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>else</strong> <em>x</em> = F<small>IB</small> (<em>n</em> − 1)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>y</em> = F<small>IB</small> (<em>n</em> − 2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>return</strong> <em>x</em> + <em>y</em></p></td>
</tr>
</table>
</div>
<p>To analyze this algorithm, let <em>T</em> (<em>n</em>) denote the running time of F<small>IB</small> (<em>n</em>). Since F<small>IB</small> (<em>n</em>) contains two recursive calls plus a constant amount of extra work, we obtain the recurrence</p>
<p class="eql"><em>T</em> (<em>n</em>) = <em>T</em> (<em>n</em> − 1) + <em>T</em> (<em>n</em> − 2) + Θ(1).</p>
<p class="noindent">This recurrence has solution <em>T</em> (<em>n</em>) = Θ(<em>F</em><sub><em>n</em></sub>), which we can establish by using the substitution method (see <a href="chapter004.xhtml#Sec_4.3">Section 4.3</a>). To show that <em>T</em> (<em>n</em>) = <em>O</em>(<em>F</em><sub><em>n</em></sub>), we’ll adopt the inductive hypothesis that <em>T</em> (<em>n</em>) ≤ <em>aF</em><sub><em>n</em></sub> − <em>b</em>, where <em>a</em> &gt; 1 and <em>b</em> &gt; 0 are constants. Substituting, we obtain</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em> (<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">(<em>aF</em><sub><em>n</em>−<em>1</em></sub> − <em>b</em>) + (<em>aF</em><sub><em>n</em>−<em>2</em></sub> − <em>b</em>) + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>a</em>(<em>F</em><sub><em>n</em>−1</sub> + <em>F</em><sub><em>n</em>−2</sub>) − 2<em>b</em> + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>aF</em><sub><em>n</em></sub> − <em>b</em>,</td>
</tr>
</table>
<p class="noindent">if we choose <em>b</em> large enough to dominate the upper-bound constant in the Θ(1) term. We can then choose <em>a</em> large enough to upper-bound the Θ(1) base case for small <em>n</em>. To show that <em>T</em> (<em>n</em>) = Ω(<em>F<sub>n</sub></em>), we use the inductive hypothesis <em>T</em> (<em>n</em>) ≥ <em>aF</em><sub><em>n</em></sub> − <em>b</em>. Substituting and following reasoning similar to the asymptotic upper-bound argument, we establish this hypothesis by choosing <em>b</em> smaller than the lower-bound constant in the Θ(1) term and <em>a</em> small enough to lower-bound the Θ(1) base case for small <em>n</em>. Theorem 3.1 on page 56 then establishes that <em>T</em> (<em>n</em>) = Θ(<em>F</em><sub><em>n</em></sub>), as desired. Since <em>F</em><sub><em>n</em></sub> = Θ(<em><span class="symbolfont">ϕ</span></em><sup><em>n</em></sup>), where <img alt="art" src="images/Art_P825.jpg"/> is the golden ratio, by equation (3.34) on page 69, it follows that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P826.jpg"/></p>
<p class="noindent">Thus this procedure is a particularly slow way to compute Fibonacci numbers, since it runs in exponential time. (See Problem 31-3 on page 954 for faster ways.)</p>
<p>Let’s see why the algorithm is inefficient. <a href="chapter026.xhtml#Fig_26-1">Figure 26.1</a> shows the tree of recursive procedure instances created when computing <em>F</em><sub>6</sub> with the F<small>IB</small> procedure. The call to F<small>IB</small>(6) recursively calls F<small>IB</small>(5) and then F<small>IB</small>(4). But, the call to F<small>IB</small>(5) also <a id="p752"/>results in a call to F<small>IB</small>(4). Both instances of F<small>IB</small>(4) return the same result (<em>F</em><sub>4</sub> = 3). Since the F<small>IB</small> procedure does not memoize (recall the definition of “memoize” from page 368), the second call to F<small>IB</small>(4) replicates the work that the first call performs, which is wasteful.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_26-1"><img alt="art" class="width100" src="images/Art_P827.jpg"/></p>
<p class="caption"><strong>Figure 26.1</strong> The invocation tree for F<small>IB</small>(6). Each node in the tree represents a procedure instance whose children are the procedure instances it calls during its execution. Since each instance of F<small>IB</small> with the same argument does the same work to produce the same result, the inefficiency of this algorithm for computing the Fibonacci numbers can be seen by the vast number of repeated calls to compute the same thing. The portion of the tree shaded blue appears in task-parallel form in <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a>.</p>
</div>
<p>Although the F<small>IB</small> procedure is a poor way to compute Fibonacci numbers, it can help us warm up to parallelism concepts. Perhaps the most basic concept is to understand is that if two parallel tasks operate on entirely different data, then—absent other interference—they each produce the same outcomes when executed at the same time as when they run serially one after the other. Within F<small>IB</small> (<em>n</em>), for example, the two recursive calls in line 3 to F<small>IB</small> (<em>n</em> − 1) and in line 4 to F<small>IB</small> (<em>n</em> − 2) can safely execute in parallel because the computation performed by one in no way affects the other.</p>
<p class="level4"><strong>Parallel keywords</strong></p>
<p class="noindent">The P-F<small>IB</small> procedure on the next page computes Fibonacci numbers, but using the <strong><em><span class="blue">parallel keywords</span></em> spawn</strong> and <strong>sync</strong> to indicate parallelism in the pseudocode.</p>
<p>If the keywords <strong>spawn</strong> and <strong>sync</strong> are deleted from P-F<small>IB</small>, the resulting pseudocode text is identical to F<small>IB</small> (other than renaming the procedure in the header <a id="p753"/> and in the two recursive calls). We define the <strong><em><span class="blue">serial projection</span></em></strong><sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> of a parallel algorithm to be the serial algorithm that results from ignoring the parallel directives, which in this case can be done by omitting the keywords <strong>spawn</strong> and <strong>sync</strong>. For <strong>parallel for</strong> loops, which we’ll see later on, we omit the keyword <strong>parallel</strong>. Indeed, our parallel pseudocode possesses the elegant property that its serial projection is always ordinary serial pseudocode to solve the same problem.</p>
<div class="pull-quote1">
<p class="box-heading">P-F<small>IB</small> (<em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>if</strong> <em>n</em> ≤ 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>else</strong> <em>x</em> = <strong>spawn</strong> P-F<small>IB</small> (<em>n</em> − 1)</p></td>
<td class="td1"><span class="red"><strong>//</strong> don’t wait for subroutine to return</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>y</em> = P-F<small>IB</small> (<em>n</em> − 2)</p></td>
<td class="td1"><span class="red"><strong>//</strong> in parallel with spawned subroutine</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>sync</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> wait for spawned subroutine to finish</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong> <em>x</em> + <em>y</em></p></td>
</tr>
</table>
</div>
<p class="level4"><strong>Semantics of parallel keywords</strong></p>
<p class="noindent"><strong><em><span class="blue">Spawning</span></em></strong> occurs when the keyword <strong>spawn</strong> precedes a procedure call, as in line 3 of P-F<small>IB</small>. The semantics of a spawn differs from an ordinary procedure call in that the procedure instance that executes the spawn—the <strong><em><span class="blue">parent</span></em></strong>—may continue to execute in parallel with the spawned subroutine—its <strong><em><span class="blue">child</span></em></strong>—instead of waiting for the child to finish, as would happen in a serial execution. In this case, while the spawned child is computing P-F<small>IB</small> (<em>n</em> − 1), the parent may go on to compute P-F<small>IB</small> (<em>n</em>−2) in line 4 in parallel with the spawned child. Since the P-F<small>IB</small> procedure is recursive, these two subroutine calls themselves create nested parallelism, as do their children, thereby creating a potentially vast tree of subcomputations, all executing in parallel.</p>
<p>The keyword <strong>spawn</strong> does not say, however, that a procedure <em>must</em> execute in parallel with its spawned children, only that it <em>may</em>. The parallel keywords express the <strong><em><span class="blue">logical parallelism</span></em></strong> of the computation, indicating which parts of the computation may proceed in parallel. At runtime, it is up to a <strong><em><span class="blue">scheduler</span></em></strong> to determine which subcomputations actually run in parallel by assigning them to available processors <a id="p754"/>as the computation unfolds. We’ll discuss the theory behind task-parallel schedulers shortly (on page 759).</p>
<p>A procedure cannot safely use the values returned by its spawned children until after it executes a <strong>sync</strong> statement, as in line 5. The keyword <strong>sync</strong> indicates that the procedure must wait as necessary for all its spawned children to finish before proceeding to the statement after the <strong>sync</strong>—the “join” of a fork-join parallel computation. The P-F<small>IB</small> procedure requires a <strong>sync</strong> before the <strong>return</strong> statement in line 6 to avoid the anomaly that would occur if <em>x</em> and <em>y</em> were summed before P-F<small>IB</small> (<em>n</em> − 1) had finished and its return value had been assigned to <em>x</em>. In addition to explicit join synchronization provided by the <strong>sync</strong> statement, it is convenient to assume that every procedure executes a <strong>sync</strong> implicitly before it returns, thus ensuring that all children finish before their parent finishes.</p>
<p class="level4"><strong>A graph model for parallel execution</strong></p>
<p class="noindent">It helps to view the execution of a parallel computation—the dynamic stream of runtime instructions executed by processors under the direction of a parallel program—as a directed acyclic graph <em>G</em> = (<em>V</em>, <em>E</em>), called a <strong><em><span class="blue">(parallel) trace</span></em></strong>.<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> Conceptually, the vertices in <em>V</em> are executed instructions, and the edges in <em>E</em> represent dependencies between instructions, where (<em>u</em>, <em>v</em>) ∈ <em>E</em> means that the parallel program required instruction <em>u</em> to execute before instruction <em>v</em>.</p>
<p>It’s sometimes inconvenient, especially if we want to focus on the parallel structure of a computation, for a vertex of a trace to represent only one executed instruction. Consequently, if a chain of instructions contains no parallel or procedural control (no <strong>spawn</strong>, <strong>sync</strong>, procedure call, or <strong>return</strong>—via either an explicit <strong>return</strong> statement or the return that happens implicitly upon reaching the end of a procedure), we group the entire chain into a single <strong><em><span class="blue">strand</span></em></strong>. As an example, <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a> shows the trace that results from computing P-F<small>IB</small>(4) in the portion of <a href="chapter026.xhtml#Fig_26-1">Figure 26.1</a> shaded blue. Strands do not include instructions that involve parallel or procedural control. These control dependencies must be represented as edges in the trace.</p>
<p>When a parent procedure calls a child, the trace contains an edge (<em>u</em>, <em>v</em>) from the strand <em>u</em> in the parent that executes the call to the first strand <em>v</em> of the spawned child, as illustrated in <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a> by the edge from the orange strand in P-F<small>IB</small>(4) to the blue strand in P-F<small>IB</small>(2). When the last strand <em>v</em>′ in the child returns, the trace contains an edge (<em>v</em>′, <em>u</em>′) to the strand <em>u</em>′, where <em>u</em>′ is the successor strand of <em>u</em> in the parent, as with the edge from the white strand in P-F<small>IB</small>(2) to the white strand in P-F<small>IB</small>(4).</p>
<a id="p755"/>
<div class="divimage">
<p class="fig-imga" id="Fig_26-2"><img alt="art" src="images/Art_P828.jpg"/></p>
<p class="caption"><strong>Figure 26.2</strong> The trace of P-F<small>IB</small>(4) corresponding to the shaded portion of <a href="chapter026.xhtml#Fig_26-1">Figure 26.1</a>. Each circle represents one strand, with blue circles representing any instructions executed in the part of the procedure (instance) up to the spawn of P-F<small>IB</small> (<em>n</em> − 1) in line 3; orange circles representing the instructions executed in the part of the procedure that calls P-F<small>IB</small> (<em>n</em> − 2) in line 4 up to the <strong>sync</strong> in line 5, where it suspends until the spawn of P-F<small>IB</small> (<em>n</em> − 1) returns; and white circles representing the instructions executed in the part of the procedure after the <strong>sync</strong>, where it sums <em>x</em> and <em>y</em>, up to the point where it returns the result. Strands belonging to the same procedure are grouped into a rounded rectangle, blue for spawned procedures and tan for called procedures. Assuming that each strand takes unit time, the work is 17 time units, since there are 17 strands, and the span is 8 time units, since the critical path—shown with blue edges—contains 8 strands.</p>
</div>
<p>When the parent spawns a child, however, the trace is a little different. The edge (<em>u</em>, <em>v</em>) goes from parent to child as with a call, such as the edge from the blue strand in P-F<small>IB</small>(4) to the blue strand in P-F<small>IB</small>(3), but the trace contains another edge (<em>u</em>, <em>u</em>′) as well, indicating that <em>u</em>’s successor strand <em>u</em>′ can continue to execute while <em>v</em> is executing. The edge from the blue strand in P-F<small>IB</small>(4) to the orange strand in P-F<small>IB</small>(4) illustrates one such edge. As with a call, there is an edge from the last strand <em>v</em>′ in the child, but with a spawn, it no longer goes to <em>u</em>’s successor. Instead, the edge is (<em>v</em>′, <em>x</em>), where <em>x</em> is the strand immediately following the <strong>sync</strong> in the parent that ensures that the child has finished, as with the edge from the white strand in P-F<small>IB</small>(3) to the white strand in P-F<small>IB</small>(4).</p>
<p>You can figure out what parallel control created a particular trace. If a strand has two successors, one of them must have been spawned, and if a strand has multiple predecessors, the predecessors joined because of a <strong>sync</strong> statement. Thus, in the general case, the set <em>V</em> forms the set of strands, and the set <em>E</em> of directed edges represents dependencies between strands induced by parallel and procedural <a id="p756"/>control. If <em>G</em> contains a directed path from strand <em>u</em> to strand <em>v</em>, we say that the two strands are <strong><em><span class="blue">(logically) in series</span></em></strong>. If there is no path in <em>G</em> either from <em>u</em> to <em>v</em> or from <em>v</em> to <em>u</em>, the strands are <strong><em><span class="blue">(logically) in parallel</span></em></strong>.</p>
<p>A fork-join parallel trace can be pictured as a dag of strands embedded in an <strong><em><span class="blue">invocation tree</span></em></strong> of procedure instances. For example, <a href="chapter026.xhtml#Fig_26-1">Figure 26.1</a> shows the invocation tree for F<small>IB</small>(6), which also serves as the invocation tree for P-F<small>IB</small>(6), the edges between procedure instances now representing either calls or spawns. <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a> zooms in on the subtree that is shaded blue, showing the strands that constitute each procedure instance in P-F<small>IB</small>(4). All directed edges connecting strands run either within a procedure or along undirected edges of the invocation tree in <a href="chapter026.xhtml#Fig_26-1">Figure 26.1</a>. (More general task-parallel traces that are not fork-join traces may contain some directed edges that do not run along the undirected tree edges.)</p>
<p>Our analyses generally assume that parallel algorithms execute on an <strong><em><span class="blue">ideal parallel computer</span></em></strong>, which consists of a set of processors and a <strong><em><span class="blue">sequentially consistent</span></em></strong> shared memory. To understand sequential consistency, you first need to know that memory is accessed by <strong><em><span class="blue">load instructions</span></em></strong>, which copy data from a location in the memory to a register within a processor, and by <strong><em><span class="blue">store instructions</span></em></strong>, which copy data from a processor register to a location in the memory. A single line of pseudocode can entail several such instructions. For example, the line <em>x</em> = <em>y</em> + <em>z</em> could result in load instructions to fetch each of <em>y</em> and <em>z</em> from memory into a processor, an instruction to add them together inside the processor, and a store instruction to place the result <em>x</em> back into memory. In a parallel computer, several processors might need to load or store at the same time. Sequential consistency means that even if multiple processors attempt to access the memory simultaneously, the shared memory behaves as if exactly one instruction from one of the processors is executed at a time, even though the actual transfer of data may happen at the same time. It is as if the instructions were executed one at a time sequentially according to some global linear order among all the processors that preserves the individual orders in which each processor executes its own instructions.</p>
<p>For task-parallel computations, which are scheduled onto processors automatically by a runtime system, the sequentially consistent shared memory behaves as if a parallel computation’s executed instructions were executed one by one in the order of a topological sort (see <a href="chapter020.xhtml#Sec_20.4">Section 20.4</a>) of its trace. That is, you can reason about the execution by imagining that the individual instructions (not generally the strands, which may aggregate many instructions) are interleaved in some linear order that preserves the partial order of the trace. Depending on scheduling, the linear order could vary from one run of the program to the next, but the behavior of any execution is always as if the instructions executed serially in a linear order consistent with the dependencies within the trace.</p>
<p>In addition to making assumptions about semantics, the ideal parallel-computer model makes some performance assumptions. Specifically, it assumes that each <a id="p757"/>processor in the machine has equal computing power, and it ignores the cost of scheduling. Although this last assumption may sound optimistic, it turns out that for algorithms with sufficient “parallelism” (a term we’ll define precisely a little later), the overhead of scheduling is generally minimal in practice.</p>
<p class="level4"><strong>Performance measures</strong></p>
<p class="noindent">We can gauge the theoretical efficiency of a task-parallel algorithm using <strong><em><span class="blue">work/span analysis</span></em></strong>, which is based on two metrics: “work” and “span.” The <strong><em><span class="blue">work</span></em></strong> of a task-parallel computation is the total time to execute the entire computation on one processor. In other words, the work is the sum of the times taken by each of the strands. If each strand takes unit time, the work is just the number of vertices in the trace. The <strong><em><span class="blue">span</span></em></strong> is the fastest possible time to execute the computation on an unlimited number of processors, which corresponds to the sum of the times taken by the strands along a longest path in the trace, where “longest” means that each strand is weighted by its execution time. Such a longest path is called the <strong><em><span class="blue">critical path</span></em></strong> of the trace, and thus the span is the weight of the longest (weighted) path in the trace. (<a href="chapter022.xhtml#Sec_22.2">Section 22.2</a>, pages 617–619 shows how to find a critical path in a dag <em>G</em> = (<em>V</em>, <em>E</em>) in Θ(<em>V</em> + <em>E</em>) time.) For a trace in which each strand takes unit time, the span equals the number of strands on the critical path. For example, the trace of <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a> has 17 vertices in all and 8 vertices on its critical path, so that if each strand takes unit time, its work is 17 time units and its span is 8 time units.</p>
<p>The actual running time of a task-parallel computation depends not only on its work and its span, but also on how many processors are available and how the scheduler allocates strands to processors. To denote the running time of a task-parallel computation on <em>P</em> processors, we subscript by <em>P</em>. For example, we might denote the running time of an algorithm on <em>P</em> processors by <em>T</em><sub><em>P</em></sub>. The work is the running time on a single processor, or <em>T</em><sub>1</sub>. The span is the running time if we could run each strand on its own processor—in other words, if we had an unlimited number of processors—and so we denote the span by <em>T</em><sub>∞</sub>.</p>
<p>The work and span provide lower bounds on the running time <em>T</em><sub><em>P</em></sub> of a task-parallel computation on <em>P</em> processors:</p>
<ul class="ulnoindent" epub:type="list">
<li>In one step, an ideal parallel computer with <em>P</em> processors can do at most <em>P</em> units of work, and thus in <em>T</em><sub><em>P</em></sub> time, it can perform at most <em>P T<sub>P</sub></em> work. Since the total work to do is <em>T</em><sub>1</sub>, we have <em>P T<sub>P</sub></em> ≥ <em>T</em><sub>1</sub>. Dividing by <em>P</em> yields the <strong><em><span class="blue">work law</span></em></strong>:
<p class="eqr"><img alt="art" src="images/Art_P829.jpg"/></p></li>
<li class="litop">A <em>P</em>-processor ideal parallel computer cannot run any faster than a machine with an unlimited number of processors. Looked at another way, a machine <a id="p758"/>with an unlimited number of processors can emulate a <em>P</em>-processor machine by using just <em>P</em> of its processors. Thus, the <strong><em><span class="blue">span law</span></em></strong> follows:
<p class="eqr"><img alt="art" src="images/Art_P830.jpg"/></p></li></ul>
<p>We define the <strong><em><span class="blue">speedup</span></em></strong> of a computation on <em>P</em> processors by the ratio <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub>, which says how many times faster the computation runs on <em>P</em> processors than on one processor. By the work law, we have <em>T</em><sub><em>P</em></sub> ≥ <em>T</em><sub>1</sub>/<em>P</em>, which implies that <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> ≤ <em>P</em>. Thus, the speedup on a <em>P</em>-processor ideal parallel computer can be at most <em>P</em>. When the speedup is linear in the number of processors, that is, when <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> = Θ(<em>P</em>), the computation exhibits <strong><em><span class="blue">linear speedup</span></em></strong>. <strong><em><span class="blue">Perfect linear speedup</span></em></strong> occurs when <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> = <em>P</em>.</p>
<p>The ratio <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub> of the work to the span gives the <strong><em><span class="blue">parallelism</span></em></strong> of the parallel computation. We can view the parallelism from three perspectives. As a ratio, the parallelism denotes the average amount of work that can be performed in parallel for each step along the critical path. As an upper bound, the parallelism gives the maximum possible speedup that can be achieved on any number of processors. Perhaps most important, the parallelism provides a limit on the possibility of attaining perfect linear speedup. Specifically, once the number of processors exceeds the parallelism, the computation cannot possibly achieve perfect linear speedup. To see this last point, suppose that <em>P</em> &gt; <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub>, in which case the span law implies that the speedup satisfies <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> ≤ <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub> &lt; <em>P</em>. Moreover, if the number <em>P</em> of processors in the ideal parallel computer greatly exceeds the parallelism—that is, if <em>P</em> <span class="font1">≫</span> <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub>—then <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> <span class="font1">≪</span> <em>P</em>, so that the speedup is much less than the number of processors. In other words, if the number of processors exceeds the parallelism, adding even more processors makes the speedup less perfect.</p>
<p>As an example, consider the computation P-F<small>IB</small>(4) in <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a>, and assume that each strand takes unit time. Since the work is <em>T</em><sub>1</sub> = 17 and the span is <em>T</em><sub>∞</sub> = 8, the parallelism is <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub> = 17/8 = 2.125. Consequently, achieving much more than double the performance is impossible, no matter how many processors execute the computation. For larger input sizes, however, we’ll see that P-F<small>IB</small> (<em>n</em>) exhibits substantial parallelism.</p>
<p>We define the <strong><em><span class="blue">(parallel) slackness</span></em></strong> of a task-parallel computation executed on an ideal parallel computer with <em>P</em> processors to be the ratio (<em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub>)/<em>P</em> = <em>T</em><sub>1</sub>/(<em>P T</em><sub>∞</sub>), which is the factor by which the parallelism of the computation exceeds the number of processors in the machine. Restating the bounds on speedup, if the slackness is less than 1, perfect linear speedup is impossible, because <em>T</em><sub>1</sub>/(<em>P T</em><sub>∞</sub>) &lt; 1 and the span law imply that <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> ≤ <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub> &lt; <em>P</em>. Indeed, as the slackness decreases from 1 and approaches 0, the speedup of the computation diverges further and further from perfect linear speedup. If the slackness is less than 1, additional parallelism in an algorithm can have a great impact on its <a id="p759"/>execution efficiency. If the slackness is greater than 1, however, the work per processor is the limiting constraint. We’ll see that as the slackness increases from 1, a good scheduler can achieve closer and closer to perfect linear speedup. But once the slackness is much greater than 1, the advantage of additional parallelism shows diminishing returns.</p>
<p class="level4"><strong>Scheduling</strong></p>
<p class="noindent">Good performance depends on more than just minimizing the work and span. The strands must also be scheduled efficiently onto the processors of the parallel machine. Our fork-join parallel-programming model provides no way for a programmer to specify which strands to execute on which processors. Instead, we rely on the runtime system’s scheduler to map the dynamically unfolding computation to individual processors. In practice, the scheduler maps the strands to static threads, and the operating system schedules the threads on the processors themselves. But this extra level of indirection is unnecessary for our understanding of scheduling. We can just imagine that the scheduler maps strands to processors directly.</p>
<p>A task-parallel scheduler must schedule the computation without knowing in advance when procedures will be spawned or when they will finish—that is, it must operate <strong><em><span class="blue">online</span></em></strong>. Moreover, a good scheduler operates in a distributed fashion, where the threads implementing the scheduler cooperate to load-balance the computation. Provably good online, distributed schedulers exist, but analyzing them is complicated. Instead, to keep our analysis simple, we’ll consider an online <strong><em><span class="blue">centralized</span></em></strong> scheduler that knows the global state of the computation at any moment.</p>
<p>In particular, we’ll analyze <strong><em><span class="blue">greedy schedulers</span></em></strong>, which assign as many strands to processors as possible in each time step, never leaving a processor idle if there is work that can be done. We’ll classify each step of a greedy scheduler as follows:</p>
<ul class="ulnoindent" epub:type="list">
<li><strong><em><span class="blue">Complete step</span></em></strong>: At least <em>P</em> strands are <strong><em><span class="blue">ready</span></em></strong> to execute, meaning that all strands on which they depend have finished execution. A greedy scheduler assigns any <em>P</em> of the ready strands to the processors, completely utilizing all the processor resources.</li>
<li class="litop"><strong><em><span class="blue">Incomplete step</span></em></strong>: Fewer than <em>P</em> strands are ready to execute. A greedy scheduler assigns each ready strand to its own processor, leaving some processors idle for the step, but executing all the ready strands.</li></ul>
<p>The work law tells us that the fastest running time <em>T</em><sub><em>P</em></sub> that we can hope for on <em>P</em> processors must be at least <em>T</em><sub>1</sub>/<em>P</em>. The span law tells us that the fastest possible running time must be at least <em>T</em><sub>∞</sub>. The following theorem shows that greedy scheduling is provably good in that it achieves the sum of these two lower bounds as an upper bound.</p>
<a id="p760"/>
<p class="theo"><strong><em>Theorem 26.1</em></strong></p>
<p class="noindent">On an ideal parallel computer with <em>P</em> processors, a greedy scheduler executes a task-parallel computation with work <em>T</em><sub>1</sub> and span <em>T</em><sub>∞</sub> in time</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P831.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   Without loss of generality, assume that each strand takes unit time. (If necessary, replace each longer strand by a chain of unit-time strands.) We’ll consider complete and incomplete steps separately.</p>
<p>In each complete step, the <em>P</em> processors together perform a total of <em>P</em> work. Thus, if the number of complete steps is <em>k</em>, the total work executing all the complete steps is <em>kP</em>. Since the greedy scheduler doesn’t execute any strand more than once and only <em>T</em><sub>1</sub> work needs to be performed, it follows that <em>kP</em> ≤ <em>T</em><sub>1</sub>, from which we can conclude that the number <em>k</em> of complete steps is at most <em>T</em><sub>1</sub>/<em>P</em>.</p>
<p>Now, let’s consider an incomplete step. Let <em>G</em> be the trace for the entire computation, let <em>G</em>′ be the subtrace of <em>G</em> that has yet to be executed at the start of the incomplete step, and let <em>G</em>″ be the subtrace remaining to be executed after the incomplete step. Consider the set <em>R</em> of strands that are ready at the beginning of the incomplete step, where |<em>R</em>| &lt; <em>P</em>. By definition, if a strand is ready, all its predecessors in trace <em>G</em> have executed. Thus the predecessors of strands in <em>R</em> do not belong to <em>G</em>′. A longest path in <em>G</em>′ must necessarily start at a strand in <em>R</em>, since every other strand in <em>G</em>′ has a predecessor and thus could not start a longest path. Because the greedy scheduler executes all ready strands during the incomplete step, the strands of <em>G</em>″ are exactly those in <em>G</em>′ minus the strands in <em>R</em>. Consequently, the length of a longest path in <em>G</em>″ must be 1 less than the length of a longest path in <em>G</em>′. In other words, every incomplete step decreases the span of the trace remaining to be executed by 1. Hence, the number of incomplete steps can be at most <em>T</em><sub>∞</sub>.</p>
<p>Since each step is either complete or incomplete, the theorem follows.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The following corollary shows that a greedy scheduler always performs well.</p>
<p class="coro"><strong><em>Corollary 26.2</em></strong></p>
<p class="noindent">The running time <em>T</em><sub><em>P</em></sub> of any task-parallel computation scheduled by a greedy scheduler on a <em>P</em>-processor ideal parallel computer is within a factor of 2 of optimal.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let <em>T</em>*<sub><em>P</em></sub> be the running time produced by an optimal scheduler on a machine with <em>P</em> processors, and let <em>T</em><sub>1</sub> and <em>T</em><sub>∞</sub> be the work and span of the computation, respectively. Since the work and span laws—inequalities (26.2) and (26.3)—give <img alt="art" src="images/Art_P831a.jpg"/>, Theorem 26.1 implies that</p>
<p class="eql"><img alt="art" src="images/Art_P831b.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<a id="p761"/>
<p>The next corollary shows that, in fact, a greedy scheduler achieves near-perfect linear speedup on any task-parallel computation as the slackness grows.</p>
<p class="cor"><strong><em>Corollary 26.3</em></strong></p>
<p class="noindent">Let <em>T</em><sub><em>P</em></sub> be the running time of a task-parallel computation produced by a greedy scheduler on an ideal parallel computer with <em>P</em> processors, and let <em>T</em><sub>1</sub> and <em>T</em><sub>∞</sub>be the work and span of the computation, respectively. Then, if <em>P</em> <span class="font1">≪</span> <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub>, or equivalently, the parallel slackness is much greater than 1, we have <em>T</em><sub><em>P</em></sub> ≈ <em>T</em><sub>1</sub>/<em>P</em>, a speedup of approximately <em>P</em>.</p>
<p class="prof"><strong><em>Proof</em></strong>   If we suppose that <em>P</em> <span class="font1">≪</span> <em>T</em><sub>1</sub>/<em>T</em><sub>∞</sub>, then it follows that <em>T</em><sub>∞</sub> <span class="font1">≪</span> <em>T</em><sub>1</sub>/<em>P</em>, and hence Theorem 26.1 gives <em>T</em><sub><em>P</em></sub> ≤ <em>T</em><sub>1</sub>/<em>P</em> + <em>T</em><sub>∞</sub> ≈ <em>T</em><sub>1</sub>/<em>P</em>. Since the work law (26.2) dictates that <em>T</em><sub><em>P</em></sub> ≥ <em>T</em><sub>1</sub>/<em>P</em>, we conclude that <em>T</em><sub><em>P</em></sub> ≈ <em>T</em><sub>1</sub>/<em>P</em>, which is a speedup of <em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub> ≈ <em>P</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The <span class="font1">≪</span> symbol denotes “much less,” but how much is “much less”? As a rule of thumb, a slackness of at least 10—that is, 10 times more parallelism than processors—generally suffices to achieve good speedup. Then, the span term in the greedy bound, inequality (26.4), is less than 10% of the work-per-processor term, which is good enough for most engineering situations. For example, if a computation runs on only 10 or 100 processors, it doesn’t make sense to value parallelism of, say 1,000,000, over parallelism of 10,000, even with the factor of 100 difference. As Problem 26-2 shows, sometimes reducing extreme parallelism yields algorithms that are better with respect to other concerns and which still scale up well on reasonable numbers of processors.</p>
<p class="level4"><strong>Analyzing parallel algorithms</strong></p>
<p class="noindent">We now have all the tools we need to analyze parallel algorithms using work/span analysis, allowing us to bound an algorithm’s running time on any number of processors. Analyzing the work is relatively straightforward, since it amounts to nothing more than analyzing the running time of an ordinary serial algorithm, namely, the serial projection of the parallel algorithm. You should already be familiar with analyzing work, since that is what most of this textbook is about! Analyzing the span is the new thing that parallelism engenders, but it’s generally no harder once you get the hang of it. Let’s investigate the basic ideas using the P-F<small>IB</small> program.</p>
<p>Analyzing the work <em>T</em><sub>1</sub>(<em>n</em>) of P-F<small>IB</small> (<em>n</em>) poses no hurdles, because we’ve already done it. The serial projection of P-F<small>IB</small> is effectively the original F<small>IB</small> procedure, and hence, we have <em>T</em><sub>1</sub>(<em>n</em>) = <em>T</em> (<em>n</em>) = Θ(<em><span class="symbolfont">ϕ</span></em><sup><em>n</em></sup>) from equation (26.1).</p>
<p><a href="chapter026.xhtml#Fig_26-3">Figure 26.3</a> illustrates how to analyze the span. If two traces are joined in series, their spans add to form the span of their composition, whereas if they are joined <a id="p762"/>in parallel, the span of their composition is the maximum of the spans of the two traces. As it turns out, the trace of any fork-join parallel computation can be built up from single strands by series-parallel composition.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_26-3"><img alt="art" class="width100" src="images/Art_P832.jpg"/></p>
<p class="caption"><strong>Figure 26.3</strong> Series-parallel composition of parallel traces. <strong>(a)</strong> When two traces are joined in series, the work of the composition is the sum of their work, and the span of the composition is the sum of their spans. <strong>(b)</strong> When two traces are joined in parallel, the work of the composition remains the sum of their work, but the span of the composition is only the maximum of their spans.</p>
</div>
<p>Armed with an understanding of series-parallel composition, we can analyze the span of P-F<small>IB</small> (<em>n</em>). The spawned call to P-F<small>IB</small> (<em>n</em> − 1) in line 3 runs in parallel with the call to P-F<small>IB</small> (<em>n</em> − 2) in line 4. Hence, we can express the span of P-F<small>IB</small> (<em>n</em>) as the recurrence</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em><sub>∞</sub>(<em>n</em>)</td>
<td class="td2">=</td>
<td class="td2">max {<em>T</em><sub>∞</sub>(<em>n</em> − 1), <em>T</em><sub>∞</sub>(<em>n</em> − 2)} + Θ(1)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>T</em><sub>∞</sub>(<em>n</em> − 1) + Θ(1),</td>
</tr>
</table>
<p class="noindent">which has solution <em>T</em><sub>∞</sub>(<em>n</em>) = Θ(<em>n</em>). (The second equality above follows from the first because P-F<small>IB</small> (<em>n</em> − 1) uses P-F<small>IB</small> (<em>n</em> − 2) in its computation, so that the span of P-F<small>IB</small> (<em>n</em> − 1) must be at least as large as the span of P-F<small>IB</small> (<em>n</em> − 2).)</p>
<p>The parallelism of P-F<small>IB</small> (<em>n</em>) is <em>T</em><sub>1</sub>(<em>n</em>)/<em>T</em><sub>∞</sub>(<em>n</em>) = Θ(<em><span class="symbolfont">ϕ</span></em><sup><em>n</em></sup>/<em>n</em>), which grows dramatically as <em>n</em> gets large. Thus, Corollary 26.3 tells us that on even the largest parallel computers, a modest value for <em>n</em> suffices to achieve near perfect linear speedup for P-F<small>IB</small> (<em>n</em>), because this procedure exhibits considerable parallel slackness.</p>
<p class="level4"><strong>Parallel loops</strong></p>
<p class="noindent">Many algorithms contain loops for which all the iterations can operate in parallel. Although the <strong>spawn</strong> and <strong>sync</strong> keywords can be used to parallelize such loops, it is more convenient to specify directly that the iterations of such loops can run in parallel. Our pseudocode provides this functionality via the <strong>parallel</strong> keyword, which precedes the <strong>for</strong> keyword in a <strong>for</strong> loop statement.</p>
<a id="p763"/>
<p>As an example, consider the problem of multiplying a square <em>n</em> × <em>n</em> matrix <em>A</em> = (<em>a<sub>ij</sub></em>) by an <em>n</em>-vector <em>x</em> = (<em>x</em><sub><em>j</em></sub>). The resulting <em>n</em>-vector <em>y</em> = (<em>y</em><sub><em>i</em></sub>) is given by the equation</p>
<p class="eql"><img alt="art" src="images/Art_P833.jpg"/></p>
<p class="noindent">for <em>i</em> = 1, 2, … , <em>n</em>. The P-M<small>AT</small>-V<small>EC</small> procedure performs matrix-vector multiplication (actually, <em>y</em> = <em>y</em> + <em>Ax</em>) by computing all the entries of <em>y</em> in parallel. The <strong>parallel for</strong> keywords in line 1 of P-M<small>AT</small>-V<small>EC</small> indicate that the <em>n</em> iterations of the loop body, which includes a serial <strong>for</strong> loop, may be run in parallel. The initialization <em>y</em> = 0, if desired, should be performed before calling the procedure (and can be done with a <strong>parallel for</strong> loop).</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>AT</small>-V<small>EC</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>)</p>
<table class="table2">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> parallel loop</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> serial loop</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="p3"><em>y</em><sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub> + <em>a</em><sub><em>ij</em></sub> <em>x</em><sub><em>j</em></sub></p></td>
</tr>
</table>
</div>
<p>Compilers for fork-join parallel programs can implement <strong>parallel for</strong> loops in terms of <strong>spawn</strong> and <strong>sync</strong> by using recursive spawning. For example, for the <strong>parallel for</strong> loop in lines 1–3, a compiler can generate the auxiliary subroutine P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> and call P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>, 1, <em>n</em>) in the place where the loop would be in the compiled code. As <a href="chapter026.xhtml#Fig_26-4">Figure 26.4</a> illustrates, this procedure recursively spawns the first half of the iterations of the loop to execute in parallel (line 5) with the second half of the iterations (line 6) and then executes a <strong>sync</strong> (line 7), thereby creating a binary tree of parallel execution. Each leaf represents a base case, which is the serial <strong>for</strong> loop of lines 2–3.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>, <em>i</em>, <em>i′</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>i</em> == <em>i</em>′</p></td>
<td class="td1"><span class="red"><strong>//</strong> just one iteration to do?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> mimic P-M<small>AT</small>-V<small>EC</small> serial loop</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="p3"><em>y</em><sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub> + <em>a</em><sub><em>ij</em></sub> <em>x</em><sub><em>j</em></sub></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="noindent"><strong>else</strong> <em>mid</em> = <span class="font1">⌊</span>(<em>i</em> + <em>i</em>′)/2<span class="font1">⌋</span></p></td>
<td class="td1"><span class="red"><strong>//</strong> parallel divide-and-conquer</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>spawn</strong> P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>, <em>i</em>, <em>mid</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="p2">P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>, <em>mid</em> + 1, <em>i′</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>sync</strong></p></td>
</tr>
</table>
</div>
<p>To calculate the work <em>T</em><sub>1</sub>(<em>n</em>) of P-M<small>AT</small>-V<small>EC</small> on an <em>n</em>×<em>n</em> matrix, simply compute the running time of its serial projection, which comes from replacing the <strong>parallel</strong> <a id="p764"/><strong>for</strong> loop in line 1 with an ordinary <strong>for</strong> loop. The running time of the resulting serial pseudocode is Θ(<em>n</em><sup>2</sup>), which means that <em>T</em><sub>1</sub>(<em>n</em>) = Θ(<em>n</em><sup>2</sup>). This analysis seems to ignore the overhead for recursive spawning in implementing the parallel loops, however. Indeed, the overhead of recursive spawning does increase the work of a parallel loop compared with that of its serial projection, but not asymptotically. To see why, observe that since the tree of recursive procedure instances is a full binary tree, the number of internal nodes is one less than the number of leaves (see Exercise B.5-3 on page 1175). Each internal node performs constant work to divide the iteration range, and each leaf corresponds to a base case, which takes at least constant time (Θ(<em>n</em>) time in this case). Thus, by amortizing the overhead of recursive spawning over the work of the iterations in the leaves, we see that the overall work increases by at most a constant factor.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_26-4"><img alt="art" class="width100" src="images/Art_P834.jpg"/></p>
<p class="caption"><strong>Figure 26.4</strong> A trace for the computation of P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>x</em>, <em>y</em>, 8, 1, 8). The two numbers within each rounded rectangle give the values of the last two parameters (<em>i</em> and <em>i</em>′ in the procedure header) in the invocation (spawn, in blue, or call, in tan) of the procedure. The blue circles represent strands corresponding to the part of the procedure up to the spawn of P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> in line 5. The orange circles represent strands corresponding to the part of the procedure that calls P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small> in line 6 up to the <strong>sync</strong> in line 7, where it suspends until the spawned subroutine in line 5 returns. The white circles represent strands corresponding to the (negligible) part of the procedure after the <strong>sync</strong> up to the point where it returns.</p>
</div>
<p>To reduce the overhead of recursive spawning, task-parallel platforms sometimes <strong><em><span class="blue">coarsen</span></em></strong> the leaves of the recursion by executing several iterations in a single leaf, either automatically or under programmer control. This optimization comes at the expense of reducing the parallelism. If the computation has sufficient parallel slackness, however, near-perfect linear speedup won’t be sacrificed.</p>
<a id="p765"/>
<p>Although recursive spawning doesn’t affect the work of a parallel loop asymptotically, we must take it into account when analyzing the span. Consider a parallel loop with <em>n</em> iterations in which the <em>i</em>th iteration has span <em>iter</em><sub>∞</sub>(<em>i</em>). Since the depth of recursion is logarithmic in the number of iterations, the parallel loop’s span is</p>
<p class="eql"><em>T</em><sub>∞</sub>(<em>n</em>) = Θ(lg <em>n</em>) + max {<em>iter</em><sub>∞</sub>(<em>i</em>) : 1 ≤ <em>i</em> ≤ <em>n</em>}.</p>
<p>For example, let’s compute the span of the doubly nested loops in lines 1–3 of P-M<small>AT</small>-V<small>EC</small>. The span for the <strong>parallel for</strong> loop control is Θ(lg <em>n</em>). For each iteration of the outer parallel loop, the inner serial <strong>for</strong> loop contains <em>n</em> iterations of line 3. Since each iteration takes constant time, the total span for the inner serial <strong>for</strong> loop is Θ(<em>n</em>), no matter which iteration of the outer <strong>parallel for</strong> loop it’s in. Thus, taking the maximum over all iterations of the outer loop and adding in the Θ(lg <em>n</em>) for loop control yields an overall span of <em>T</em><sub>∞</sub><em>n</em> = Θ(<em>n</em>) + Θ(lg <em>n</em>) = Θ(<em>n</em>) for the procedure. Since the work is Θ(<em>n</em><sup>2</sup>), the parallelism is Θ(<em>n</em><sup>2</sup>)/Θ(<em>n</em>) = Θ(<em>n</em>). (Exercise 26.1-7 asks you to provide an implementation with even more parallelism.)</p>
<p class="level4"><strong>Race conditions</strong></p>
<p class="noindent">A parallel algorithm is <strong><em><span class="blue">deterministic</span></em></strong> if it always does the same thing on the same input, no matter how the instructions are scheduled on the multicore computer. It is <strong><em><span class="blue">nondeterministic</span></em></strong> if its behavior might vary from run to run when the input is the same. A parallel algorithm that is intended to be deterministic may nevertheless act nondeterministically, however, if it contains a difficult-to-diagnose bug called a “determinacy race.”</p>
<p>Famous race bugs include the Therac-25 radiation therapy machine, which killed three people and injured several others, and the Northeast Blackout of 2003, which left over 50 million people in the United States without power. These pernicious bugs are notoriously hard to find. You can run tests in the lab for days without a failure, only to discover that your software sporadically crashes in the field, sometimes with dire consequences.</p>
<p>A <strong><em><span class="blue">determinacy race</span></em></strong> occurs when two logically parallel instructions access the same memory location and at least one of the instructions modifies the value stored in the location. The toy procedure R<small>ACE</small>-E<small>XAMPLE</small> on the following page illustrates a determinacy race. After initializing <em>x</em> to 0 in line 1, R<small>ACE</small>-E<small>XAMPLE</small> creates two parallel strands, each of which increments <em>x</em> in line 3. Although it might seem that a call of R<small>ACE</small>-E<small>XAMPLE</small> should always print the value 2 (its serial projection certainly does), it could instead print the value 1. Let’s see how this anomaly might occur.</p>
<p>When a processor increments <em>x</em>, the operation is not indivisible, but is composed of a sequence of instructions:</p>
<a id="p766"/>
<div class="divimage">
<p class="fig-imga" id="Fig_26-5"><img alt="art" src="images/Art_P835.jpg"/></p>
<p class="caption"><strong>Figure 26.5</strong> Illustration of the determinacy race in R<small>ACE</small>-E<small>XAMPLE</small>. <strong>(a)</strong> A trace showing the dependencies among individual instructions. The processor registers are <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub>. Instructions unrelated to the race, such as the implementation of loop control, are omitted. <strong>(b)</strong> An execution sequence that elicits the bug, showing the values of <em>x</em> in memory and registers <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub> for each step in the execution sequence.</p>
</div>
<div class="pull-quote1">
<p class="box-heading">R<small>ACE</small>-E<small>XAMPLE</small> ( )</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>x</em> = 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> 2</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>x</em> = <em>x</em> + 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> determinacy race</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="noindent">print <em>x</em></p></td>
</tr>
</table>
</div>
<ul class="ulnoindent" epub:type="list">
<li>Load <em>x</em> from memory into one of the processor’s registers.</li>
<li class="litop">Increment the value in the register.</li>
<li class="litop">Store the value in the register back into <em>x</em> in memory.</li></ul>
<p class="noindent"><a href="chapter026.xhtml#Fig_26-5">Figure 26.5(a)</a> illustrates a trace representing the execution of R<small>ACE</small>-E<small>XAMPLE</small>, with the strands broken down to individual instructions. Recall that since an ideal parallel computer supports sequential consistency, you can view the parallel execution of a parallel algorithm as an interleaving of instructions that respects the dependencies in the trace. Part (b) of the figure shows the values in an execution of the computation that elicits the anomaly. The value <em>x</em> is kept in memory, and <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub> are processor registers. In step 1, one of the processors sets <em>x</em> to 0. In steps 2 and 3, processor 1 loads <em>x</em> from memory into its register <em>r</em><sub>1</sub> and increments it, producing the value 1 in <em>r</em><sub>1</sub>. At that point, processor 2 comes into the picture, executing instructions 4–6. Processor 2 loads <em>x</em> from memory into register <em>r</em><sub>2</sub>; increments it, producing the value 1 in <em>r</em><sub>2</sub>; and then stores this value into <em>x</em>, setting <em>x</em> to 1. Now, processor 1 resumes with step 7, storing the value 1 in <em>r</em><sub>1</sub> into <em>x</em>, which <a id="p767"/>leaves the value of <em>x</em> unchanged. Therefore, step 8 prints the value 1, rather than the value 2 that the serial projection would print.</p>
<p>Let’s recap what happened. By sequential consistency, the effect of the parallel execution is as if the executed instructions of the two processors are interleaved. If processor 1 executes all its instructions before processor 2, a trivial interleaving, the value 2 is printed. Conversely, if processor 2 executes all its instructions before processor 1, the value 2 is still printed. When the instructions of the two processors interleave nontrivially, however, it is possible, as in this example execution, that one of the updates to <em>x</em> is lost, resulting in the value 1 being printed.</p>
<p>Of course, many executions do not elicit the bug. That’s the problem with determinacy races. Generally, most instruction orderings produce correct results, such as any where the instructions on the left branch execute before the instructions on the right branch, or vice versa. But some orderings generate improper results when the instructions interleave. Consequently, races can be extremely hard to test for. Your program may fail, but you may be unable to reliably reproduce the failure in subsequent tests, confounding your attempts to locate the bug in your code and fix it. Task-parallel programming environments often provide race-detection productivity tools to help you isolate race bugs.</p>
<p>Many parallel programs in the real world are intentionally nondeterministic. They contain determinacy races, but they mitigate the dangers of nondeterminism through the use of mutual-exclusion locks and other methods of synchronization. For our purposes, however, we’ll insist on an absence of determinacy races in the algorithms we develop. Nondeterministic programs are indeed interesting, but nondeterministic programming is a more advanced topic and unnecessary for a wide swath of interesting parallel algorithms.</p>
<p>To ensure that algorithms are deterministic, any two strands that operate in parallel should be <strong><em><span class="blue">mutually noninterfering</span></em></strong>: they only read, and do not modify, any memory locations accessed by both of them. Consequently, in a <strong>parallel for</strong> construct, such as the outer loop of P-M<small>AT</small>-V<small>EC</small>, we want all the iterations of the body, including any code an iteration executes in subroutines, to be mutually noninterfering. And between a <strong>spawn</strong> and its corresponding <strong>sync</strong>, we want the code executed by the spawned child and the code executed by the parent to be mutually noninterfering, once again including invoked subroutines.</p>
<p>As an example of how easy it is to write code with unintentional races, the P-M<small>AT</small>-V<small>EC</small>-W<small>RONG</small> procedure on the next page is a faulty parallel implementation of matrix-vector multiplication that achieves a span of Θ(lg <em>n</em>) by parallelizing the inner <strong>for</strong> loop. This procedure is incorrect, unfortunately, due to determinacy races when updating <em>y</em><sub><em>i</em></sub> in line 3, which executes in parallel for all <em>n</em> values of <em>j</em>.</p>
<p>Index variables of <strong>parallel for</strong> loops, such as <em>i</em> in line 1 and <em>j</em> in line 2, do not cause races between iterations. Conceptually, each iteration of the loop creates an independent variable to hold the index of that iteration during that iteration’s <a id="p768"/>execution of the loop body. Even if two parallel iterations both access the same index variable, they really are accessing different variable instances—hence different memory locations—and no race occurs.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>AT</small>-V<small>EC</small>-W<small>RONG</small> (<em>A</em>, <em>x</em>, <em>y</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>parallel for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p3"><em>y</em><sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub> + <em>a</em><sub><em>ij</em></sub><em>x</em><sub><em>j</em></sub></p></td>
<td class="td1"><span class="red"><strong>//</strong> determinacy race</span></td>
</tr>
</table>
</div>
<p>A parallel algorithm with races can sometimes be deterministic. As an example, two parallel threads might store the same value into a shared variable, and it wouldn’t matter which stored the value first. For simplicity, however, we generally prefer code without determinacy races, even if the races are benign. And good parallel programmers frown on code with determinacy races that cause nondeterministic behavior, if deterministic code that performs comparably is an option.</p>
<p>But nondeterministic code does have its place. For example, you can’t implement a parallel hash table, a highly practical data structure, without writing code containing determinacy races. Much research has centered around how to extend the fork-join model to incorporate limited “structured” nondeterminism while avoiding the full measure of complications that arise when nondeterminism is completely unrestricted.</p>
<p class="level4"><strong>A chess lesson</strong></p>
<p class="noindent">To illustrate the power of work/span analysis, this section closes with a true story that occurred during the development of one of the first world-class parallel chess-playing programs [<a epub:type="noteref" href="bibliography001.xhtml#endnote_106">106</a>] many years ago. The timings below have been simplified for exposition.</p>
<p>The chess program was developed and tested on a 32-processor computer, but it was designed to run on a supercomputer with 512 processors. Since the supercomputer availability was limited and expensive, the developers ran benchmarks on the small computer and extrapolated performance to the large computer.</p>
<p>At one point, the developers incorporated an optimization into the program that reduced its running time on an important benchmark on the small machine from <em>T</em><sub>32</sub> = 65 seconds to <img alt="art" src="images/Art_P835a.jpg"/> seconds. Yet, the developers used the work and span performance measures to conclude that the optimized version, which was faster on 32 processors, would actually be slower than the original version on the 512 processors of the large machine. As a result, they abandoned the “optimization.”</p>
<p>Here is their work/span analysis. The original version of the program had work <em>T</em><sub>1</sub> = 2048 seconds and span <em>T</em><sub>∞</sub>= 1 second. Let’s treat inequality (26.4) on <a id="p769"/>page 760 as the equation <em>T</em><sub><em>P</em></sub> = <em>T</em><sub>1</sub>/<em>P</em> + <em>T</em><sub>∞</sub>, which we can use as an approximation to the running time on <em>P</em> processors. Then indeed we have <em>T</em><sub>32</sub> = 2048/32 + 1 = 65. With the optimization, the work becomes <em>T</em>′<sub>1</sub> = 1024 seconds, and the span becomes <em>T</em>′<sub>∞</sub> = 8 seconds. Our approximation gives <em>T</em>′<sub>32</sub> = 1024/32 + 8 = 40.</p>
<p>The relative speeds of the two versions switch when we estimate their running times on 512 processors, however. The first version has a running time of <em>T</em><sub>512</sub> = 2048/512+1 = 5 seconds, and the second version runs in <img alt="art" src="images/Art_P835b.jpg"/> seconds. The optimization that speeds up the program on 32 processors makes the program run for twice as long on 512 processors! The optimized version’s span of 8, which is not the dominant term in the running time on 32 processors, becomes the dominant term on 512 processors, nullifying the advantage from using more processors. The optimization does not scale up.</p>
<p>The moral of the story is that work/span analysis, and measurements of work and span, can be superior to measured running times alone in extrapolating an algorithm’s scalability.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>26.1-1</em></strong></p>
<p class="noindent">What does a trace for the execution of a serial algorithm look like?</p>
<p class="level3"><strong><em>26.1-2</em></strong></p>
<p class="noindent">Suppose that line 4 of P-F<small>IB</small> spawns P-F<small>IB</small> (<em>n</em> − 2), rather than calling it as is done in the pseudocode. How would the trace of P-F<small>IB</small>(4) in <a href="chapter026.xhtml#Fig_26-2">Figure 26.2</a> change? What is the impact on the asymptotic work, span, and parallelism?</p>
<p class="level3"><strong><em>26.1-3</em></strong></p>
<p class="noindent">Draw the trace that results from executing P-F<small>IB</small>(5). Assuming that each strand in the computation takes unit time, what are the work, span, and parallelism of the computation? Show how to schedule the trace on 3 processors using greedy scheduling by labeling each strand with the time step in which it is executed.</p>
<p class="level3"><strong><em>26.1-4</em></strong></p>
<p class="noindent">Prove that a greedy scheduler achieves the following time bound, which is slightly stronger than the bound proved in Theorem 26.1:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P836.jpg"/></p>
<p class="level3"><strong><em>26.1-5</em></strong></p>
<p class="noindent">Construct a trace for which one execution by a greedy scheduler can take nearly twice the time of another execution by a greedy scheduler on the same number of processors. Describe how the two executions would proceed.</p>
<a id="p770"/>
<p class="level3"><strong><em>26.1-6</em></strong></p>
<p class="noindent">Professor Karan measures her deterministic task-parallel algorithm on 4, 10, and 64 processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded <em>T</em><sub>4</sub> = 80 seconds, <em>T</em><sub>10</sub> = 42 seconds, and <em>T</em><sub>64</sub> = 10 seconds. Argue that the professor is either lying or incompetent. (<em>Hint:</em> Use the work law (26.2), the span law (26.3), and inequality (26.5) from Exercise 26.1-4.)</p>
<p class="level3"><strong><em>26.1-7</em></strong></p>
<p class="noindent">Give a parallel algorithm to multiply an <em>n</em> × <em>n</em> matrix by an <em>n</em>-vector that achieves Θ(<em>n</em><sup>2</sup>/lg <em>n</em>) parallelism while maintaining Θ(<em>n</em><sup>2</sup>) work.</p>
<p class="level3"><strong><em>26.1-8</em></strong></p>
<p class="noindent">Analyze the work, span, and parallelism of the procedure P-T<small>RANSPOSE</small>, which transposes an <em>n</em> × <em>n</em> matrix <em>A</em> in place.</p>
<div class="pull-quote1">
<p class="box-heading">P-T<small>RANSPOSE</small> (<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>j</em> = 2 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>j</em> − 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p3">exchange <em>a</em><sub><em>ij</em></sub> with <em>a</em><sub><em>ji</em></sub></p></td>
</tr>
</table>
</div>
<p class="level3"><strong><em>26.1-9</em></strong></p>
<p class="noindent">Suppose that instead of a <strong>parallel for</strong> loop in line 2, the P-T<small>RANSPOSE</small> procedure in Exercise 26.1-8 had an ordinary <strong>for</strong> loop. Analyze the work, span, and parallelism of the resulting algorithm.</p>
<p class="level3"><strong><em>26.1-10</em></strong></p>
<p class="noindent">For what number of processors do the two versions of the chess program run equally fast, assuming that <em>T</em><sub><em>P</em></sub> = <em>T</em><sub>1</sub>/<em>P</em> + <em>T</em><sub>∞</sub>?</p>
</section>
<p class="line1"/>
<section title="26.2 Parallel matrix multiplication">
<a id="Sec_26.2"/>
<p class="level1" id="h1-153"><a href="toc.xhtml#Rh1-153"><strong>26.2    Parallel matrix multiplication</strong></a></p>
<p class="noindent">In this section, we’ll explore how to parallelize the three matrix-multiplication algorithms from <a href="chapter004.xhtml#Sec_4.1">Sections 4.1</a> and <a href="chapter004.xhtml#Sec_4.2">4.2</a>. We’ll see that each algorithm can be parallelized in a straightforward fashion using either parallel loops or recursive spawning. We’ll analyze them using work/span analysis, and we’ll see that each parallel algorithm attains the same performance on one processor as its corresponding serial algorithm, while scaling up to large numbers of processors.</p>
<a id="p771"/>
<p class="level4"><strong>A parallel algorithm for matrix multiplication using parallel loops</strong></p>
<p class="noindent">The first algorithm we’ll study is P-M<small>ATRIX</small>-M<small>ULTIPLY</small>, which simply parallelizes the two outer loops in the procedure M<small>ATRIX</small>-M<small>ULTIPLY</small> on page 81.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>ATRIX</small>-M<small>ULTIPLY</small> (<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> compute entries in each of <em>n</em> rows</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><strong>parallel for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> compute <em>n</em> entries in row <em>i</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p3"><strong>for</strong> <em>k</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p4"><em>c</em><sub><em>ij</em></sub> = <em>c</em><sub><em>ij</em></sub> + <em>a</em><sub><em>ik</em></sub> · <em>b</em><sub><em>kj</em></sub></p></td>
<td class="td1"><span class="red"><strong>//</strong> add in another term of equation (4.1)</span></td>
</tr>
</table>
</div>
<p>Let’s analyze P-M<small>ATRIX</small>-M<small>ULTIPLY</small>. Since the serial projection of the algorithm is just M<small>ATRIX</small>-M<small>ULTIPLY</small>, the work is the same as the running time of M<small>ATRIX</small>-M<small>ULTIPLY</small>: <em>T</em><sub>1</sub>(<em>n</em>) = Θ(<em>n</em><sup>3</sup>). The span is <em>T</em><sub>∞</sub>(<em>n</em>) = Θ(<em>n</em>), because it follows a path down the tree of recursion for the <strong>parallel for</strong> loop starting in line 1, then down the tree of recursion for the <strong>parallel for</strong> loop starting in line 2, and then executes all <em>n</em> iterations of the ordinary <strong>for</strong> loop starting in line 3, resulting in a total span of Θ(lg <em>n</em>) + Θ(lg <em>n</em>) + Θ(<em>n</em>) = Θ(<em>n</em>). Thus the parallelism is Θ(<em>n</em><sup>3</sup>)/Θ(<em>n</em>) = Θ(<em>n</em><sup>2</sup>). (Exercise 26.2-3 asks you to parallelize the inner loop to obtain a parallelism of Θ(<em>n</em><sup>3</sup>/lg <em>n</em>), which you cannot do straightforwardly using <strong>parallel for</strong>, because you would create races.)</p>
<p class="level4"><strong>A parallel divide-and-conquer algorithm for matrix multiplication</strong></p>
<p class="noindent"><a href="chapter004.xhtml#Sec_4.1">Section 4.1</a> shows how to multiply <em>n</em> × <em>n</em> matrices serially in Θ(<em>n</em><sup>3</sup>) time using a divide-and-conquer strategy. Let’s see how to parallelize that algorithm using recursive spawning instead of calls.</p>
<p>The serial M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure on page 83 takes as input three <em>n</em> × <em>n</em> matrices <em>A</em>, <em>B</em>, and <em>C</em> and performs the matrix calculation <em>C</em> = <em>C</em> + <em>A</em> · <em>B</em> by recursively performing eight multiplications of <em>n</em>/2 × <em>n</em>/2 submatrices of <em>A</em> and <em>B</em>. The P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure on the following page implements the same divide-and-conquer strategy, but it uses spawning to perform the eight multiplications in parallel. To avoid determinacy races in updating the elements of <em>C</em>, it creates a temporary matrix <em>D</em> to store four of the submatrix products. At the end, it adds <em>C</em> and <em>D</em> together to produce the final result. (Problem 26-2 asks you to eliminate the temporary matrix <em>D</em> at the expense of some parallelism.)</p>
<p>Lines 2–3 of P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> handle the base case of multiplying 1 × 1 matrices. The remainder of the procedure deals with the recursive case. Line 4 allocates a temporary matrix <em>D</em>, and lines 5–7 zero it. Line 8 partitions each of the four matrices <em>A</em>, <em>B</em>, <em>C</em>, and <em>D</em> into <em>n</em>/2 × <em>n</em>/2 submatrices. (As <a id="p772"/>with M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> on page 83, we’re glossing over the subtle issue of how to use index calculations to represent submatrix sections of a matrix.) The spawned recursive call in line 9 sets <em>C</em><sub>11</sub> = <em>C</em><sub>11</sub> + <em>A</em><sub>11</sub> · <em>B</em><sub>11</sub>, so that <em>C</em><sub>11</sub> accumulates the first of the two terms in equation (4.5) on page 82. Similarly, lines 10–12 cause each of <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, and <em>C</em><sub>22</sub> in parallel to accumulate the first of the two terms in equations (4.6)–(4.8), respectively. Line 13 sets the submatrix <em>D</em><sub>11</sub> to the submatrix product <em>A</em><sub>12</sub> · <em>B</em><sub>21</sub>, so that <em>D</em><sub>11</sub> equals the second of the two terms in equation (4.5). Lines 14–16 set each of <em>D</em><sub>12</sub>, <em>D</em><sub>21</sub>, and <em>D</em><sub>22</sub> in parallel to the second of the two terms in equations (4.6)–(4.8), respectively. The <strong>sync</strong> statement in line 17 ensures that all the spawned submatrix products in lines 9–16 have been computed, after which the doubly nested <strong>parallel for</strong> loops in lines 18–20 add the elements of <em>D</em> to the corresponding elements of <em>C</em>.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w1a"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>n</em> == 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> just one element in each matrix?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1" colspan="2"><p class="p2"><em>c</em><sub>11</sub> = <em>c</em><sub>11</sub> + <em>a</em><sub>11</sub> · <em>b</em><sub>11</sub></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="noindent">let <em>D</em> be a new <em>n</em> × <em>n</em> matrix</p></td>
<td class="td1"><span class="red"><strong>//</strong> temporary matrix</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> set <em>D</em> = 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>parallel for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2"><p class="p3"><em>d</em><sub><em>ij</em></sub> = 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1" colspan="2"><p class="noindent">partition <em>A</em>, <em>B</em>, <em>C</em>, and <em>D</em> into <em>n</em>/2 × <em>n</em>/2 submatrices <em>A</em><sub>11</sub>, <em>A</em><sub>12</sub>, <em>A</em><sub>21</sub>, <em>A</em><sub>22</sub>; <em>B</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>B</em><sub>21</sub>, <em>B</em><sub>22</sub>; <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, <em>C</em><sub>22</sub>; and <em>D</em><sub>11</sub>, <em>D</em><sub>12</sub>, <em>D</em><sub>21</sub>, <em>D</em><sub>22</sub>; respectively</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>11</sub>, <em>B</em><sub>11</sub>, <em>C</em><sub>11</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>11</sub>, <em>B</em><sub>12</sub>, <em>C</em><sub>12</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>21</sub>, <em>B</em><sub>11</sub>, <em>C</em><sub>21</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>21</sub>, <em>B</em><sub>12</sub>, <em>C</em><sub>22</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>12</sub>, <em>B</em><sub>21</sub>, <em>D</em><sub>11</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>12</sub>, <em>B</em><sub>22</sub>, <em>D</em><sub>12</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>22</sub>, <em>B</em><sub>21</sub>, <em>D</em><sub>21</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">16</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> (<em>A</em><sub>22</sub>, <em>B</em><sub>22</sub>, <em>D</em><sub>22</sub>, <em>n</em>/2)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">17</span></td>
<td class="td1"><p class="noindent"><strong>sync</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> wait for spawned submatrix products</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">18</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> update <em>C</em> = <em>C</em> + D</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">19</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>parallel for</strong> <em>j</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">20</span></td>
<td class="td1" colspan="2"><p class="p3"><em>c</em><sub><em>ij</em></sub> = <em>c</em><sub><em>ij</em></sub> + <em>d</em><sub><em>ij</em></sub></p></td>
</tr>
</table>
</div>
<p>Let’s analyze the P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure. We start by analyzing the work <em>M</em><sub>1</sub>(<em>n</em>), echoing the serial running-time analysis of its progenitor M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>. The recursive case allocates and zeros the <a id="p773"/>temporary matrix <em>D</em> in Θ(<em>n</em><sup>2</sup>) time, partitions in Θ(1) time, performs eight recursive multiplications of <em>n</em>/2 × <em>n</em>/2 matrices, and finishes up with the Θ(<em>n</em><sup>2</sup>) work from adding two <em>n</em>×<em>n</em> matrices. Thus the work outside the spawned recursive calls is Θ(<em>n</em><sup>2</sup>), and the recurrence for the work <em>M</em><sub>1</sub>(<em>n</em>) becomes</p>
<table class="table2b">
<tr>
<td class="td2"><em>M</em><sub>1</sub>(<em>n</em>)</td>
<td class="td2">=</td>
<td class="td2">8<em>M</em><sub>1</sub>(<em>n</em>/2) + Θ(<em>n</em><sup>2</sup>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">Θ(<em>n</em><sup>3</sup>)</td>
</tr>
</table>
<p class="noindent">by case 1 of the master theorem (Theorem 4.1). Not surprisingly, the work of this parallel algorithm is asymptotically the same as the running time of the procedure M<small>ATRIX</small>-M<small>ULTIPLY</small> on page 81, with its triply nested loops.</p>
<p>Let’s determine the span <em>M</em><sub>∞</sub>(<em>n</em>) of P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>. Because the eight parallel recursive spawns all execute on matrices of the same size, the maximum span for any recursive spawn is just the span of a single one of them, or <em>M</em><sub>∞</sub>(<em>n</em>/2). The span for the doubly nested <strong>parallel for</strong> loops in lines 5–7 is Θ(lg <em>n</em>) because each loop control adds Θ(lg <em>n</em>) to the constant span of line 7. Similarly, the doubly nested <strong>parallel for</strong> loops in lines 18–20 add another Θ(lg <em>n</em>). Matrix partitioning by index calculation has Θ(1) span, which is dominated by the Θ(lg <em>n</em>) span of the nested loops. We obtain the recurrence</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P837.jpg"/></p>
<p class="noindent">Since this recurrence falls under case 2 of the master theorem with <em>k</em> = 1, the solution is <em>M</em><sub>∞</sub>(<em>n</em>) = Θ(lg<sup>2</sup> <em>n</em>).</p>
<p>The parallelism of P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> is <em>M</em><sub>1</sub>(<em>n</em>)/<em>M</em><sub>∞</sub>(<em>n</em>) = Θ(<em>n</em><sup>3</sup>/lg<sup>2</sup><em>n</em>), which is huge. (Problem 26-2 asks you to simplify this parallel algorithm at the expense of just a little less parallelism.)</p>
<p class="level4"><strong>Parallelizing Strassen’s method</strong></p>
<p class="noindent">To parallelize Strassen’s algorithm, we can follow the same general outline as on pages 86–87, but use spawning. You may find it helpful to compare each step below with the corresponding step there. We’ll analyze costs as we go along to develop recurrences <em>T</em><sub>1</sub>(<em>n</em>) and <em>T</em><sub>∞</sub>(<em>n</em>) for the overall work and span, respectively.</p>
<ol class="olnoindent" epub:type="list">
<li>If <em>n</em> = 1, the matrices each contain a single element. Perform a single scalar multiplication and a single scalar addition, and return. Otherwise, partition the input matrices <em>A</em> and <em>B</em> and output matrix <em>C</em> into <em>n</em>/2 × <em>n</em>/2 submatrices, as in equation (4.2) on page 82. This step takes Θ(1) work and Θ(1) span by index calculation.</li>
<li class="litop">Create <em>n</em>/2 × <em>n</em>/2 matrices <em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, … , <em>S</em><sub>10</sub>, each of which is the sum or difference of two submatrices from step 1. Create and zero the entries of seven <em>n</em>/2×<em>n</em>/2 matrices <em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P</em><sub>7</sub> to hold seven <em>n</em>/2×<em>n</em>/2 matrix products. All <a id="p774"/>17 matrices can be created, and the <em>P</em><sub><em>i</em></sub> initialized, with doubly nested <strong>parallel for</strong> loops using Θ(<em>n</em><sup>2</sup>) work and Θ(lg <em>n</em>) span.</li>
<li class="litop">Using the submatrices from step 1 and the matrices <em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, … , <em>S</em><sub>10</sub> created in step 2, recursively spawn computations of each of the seven <em>n</em>/2 × <em>n</em>/2 matrix products <em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P</em><sub>7</sub>, taking 7<em>T</em><sub>1</sub>(<em>n</em>/2) work and <em>T</em><sub>∞</sub>(<em>n</em>/2) span.</li>
<li class="litop">Update the four submatrices <em>C</em><sub>11</sub>, <em>C</em><sub>12</sub>, <em>C</em><sub>21</sub>, <em>C</em><sub>22</sub> of the result matrix <em>C</em> by adding or subtracting various <em>P</em><sub><em>i</em></sub> matrices. Using doubly nested <strong>parallel for</strong> loops, computing all four submatrices takes Θ(<em>n</em><sup>2</sup>) work and Θ(lg <em>n</em>) span.</li></ol>
<p>Let’s analyze this algorithm. Since the serial projection is the same as the original serial algorithm, the work is just the running time of the serial projection, namely, Θ(<em>n</em><sup>lg 7</sup>). As we did with P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, we can devise a recurrence for the span. In this case, seven recursive calls execute in parallel, but since they all operate on matrices of the same size, we obtain the same recurrence (26.6) as we did for P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, with solution Θ(lg<sup>2</sup> <em>n</em>). Thus the parallel version of Strassen’s method has parallelism Θ(<em>n</em><sup>lg 7</sup>/lg<sup>2</sup> <em>n</em>), which is large. Although the parallelism is slightly less than that of P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, that’s just because the work is also less.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>26.2-1</em></strong></p>
<p class="noindent">Draw the trace for computing P-M<small>ATRIX</small>-M<small>ULTIPLY</small> on 2 × 2 matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Assuming that each strand executes in unit time, analyze the work, span, and parallelism of this computation.</p>
<p class="level3"><strong><em>26.2-2</em></strong></p>
<p class="noindent">Repeat Exercise 26.2-1 for P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>.</p>
<p class="level3"><strong><em>26.2-3</em></strong></p>
<p class="noindent">Give pseudocode for a parallel algorithm that multiplies two <em>n</em> × <em>n</em> matrices with work Θ(<em>n</em><sup>3</sup>) but span only Θ(lg <em>n</em>). Analyze your algorithm.</p>
<p class="level3"><strong><em>26.2-4</em></strong></p>
<p class="noindent">Give pseudocode for an efficient parallel algorithm that multiplies a <em>p</em> × <em>q</em> matrix by a <em>q</em> × <em>r</em> matrix. Your algorithm should be highly parallel even if any of <em>p</em>, <em>q</em>, and <em>r</em> equal 1. Analyze your algorithm.</p>
<a id="p775"/>
<p class="level3"><strong><em>26.2-5</em></strong></p>
<p class="noindent">Give pseudocode for an efficient parallel version of the Floyd-Warshall algorithm (see <a href="chapter023.xhtml#Sec_23.2">Section 23.2</a>), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm.</p>
</section>
<p class="line1"/>
<section title="26.3 Parallel merge sort">
<a id="Sec_26.3"/>
<p class="level1" id="h1-154"><a href="toc.xhtml#Rh1-154"><strong>26.3    Parallel merge sort</strong></a></p>
<p class="noindent">We first saw serial merge sort in <a href="chapter002.xhtml#Sec_2.3.1">Section 2.3.1</a>, and in <a href="chapter002.xhtml#Sec_2.3.2">Section 2.3.2</a> we analyzed its running time and showed it to be Θ(<em>n</em> lg <em>n</em>). Because merge sort already uses the divide-and-conquer method, it seems like a terrific candidate for implementing using fork-join parallelism.</p>
<p>The procedure P-M<small>ERGE</small>-S<small>ORT</small> modifies merge sort to spawn the first recursive call. Like its serial counterpart M<small>ERGE</small>-S<small>ORT</small> on page 39, the P-M<small>ERGE</small>-S<small>ORT</small> procedure sorts the subarray <em>A</em>[<em>p</em> : <em>r</em>]. After the <strong>sync</strong> statement in line 8 ensures that the two recursive spawns in lines 5 and 7 have finished, P-M<small>ERGE</small>-S<small>ORT</small> calls the P-M<small>ERGE</small> procedure, a parallel merging algorithm, which is on page 779, but you don’t need to bother looking at it right now.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>ERGE</small>-S<small>ORT</small> (<em>A</em>, <em>p</em>, <em>r</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>p</em> ≥ <em>r</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> zero or one element?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="noindent"><em>q</em> = <span class="font1">⌊</span>(<em>p</em> + <em>r</em>)/2<span class="font1">⌋</span></p></td>
<td class="td1"><span class="red"><strong>//</strong> midpoint of <em>A</em>[<em>p</em> : <em>r</em>]</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1" colspan="2"><p class="noindent"><span class="red"><strong>//</strong> Recursively sort <em>A</em>[<em>p</em> : <em>q</em>] in parallel.</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ERGE</small>-S<small>ORT</small> (<em>A</em>, <em>p</em>, <em>q</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1" colspan="2"><p class="noindent"><span class="red"><strong>//</strong> Recursively sort <em>A</em>[<em>q</em> + 1 : <em>r</em>] in parallel.</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ERGE</small>-S<small>ORT</small> (<em>A</em>, <em>q</em> + 1, <em>r</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="noindent"><strong>sync</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> wait for spawns</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2"><p class="noindent"><span class="red"><strong>//</strong> Merge <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] into <em>A</em>[<em>p</em> : <em>r</em>].</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1" colspan="2"><p class="noindent">P-M<small>ERGE</small> (<em>A</em>, <em>p</em>, <em>q</em>, <em>r</em>)</p></td>
</tr>
</table>
</div>
<p>First, let’s use work/span analysis to get some intuition for why we need a parallel merge procedure. After all, it may seem as though there should be plenty of parallelism just by parallelizing M<small>ERGE</small>-S<small>ORT</small> without worrying about parallelizing the merge. But what would happen if the call to P-M<small>ERGE</small> in line 10 of P-M<small>ERGE</small>-S<small>ORT</small> were replaced by a call to the serial M<small>ERGE</small> procedure on page 36? Let’s call the pseudocode so modified P-NAIVE-M<small>ERGE</small>-S<small>ORT</small>.</p>
<p>Let <em>T</em><sub>1</sub>(<em>n</em>) be the (worst-case) work of P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small> on an <em>n</em>-element subarray, where <em>n</em> = <em>r</em> −<em>p</em> + 1 is the number of elements in <em>A</em>[<em>p</em> : <em>r</em>], and let <em>T</em><sub>∞</sub>(<em>n</em>) <a id="p776"/>be the span. Because M<small>ERGE</small> is serial with running time Θ(<em>n</em>), both its work and span are Θ(<em>n</em>). Since the serial projection of P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small> is exactly M<small>ERGE</small>-S<small>ORT</small>, its work is <em>T</em><sub>1</sub>(<em>n</em>) = Θ(<em>n</em> lg <em>n</em>). The two recursive calls in lines 5 and 7 run in parallel, and so its span is given by the recurrence</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em><sub>∞</sub>(<em>n</em>)</td>
<td class="td2">=</td>
<td class="td2"><em>T</em><sub>∞</sub>(<em>n</em>/2) + Θ(<em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">Θ(<em>n</em>),</td>
</tr>
</table>
<p class="noindent">by case 1 of the master theorem. Thus the parallelism of P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small> is <em>T</em><sub>1</sub>(<em>n</em>)/<em>T</em><sub>∞</sub>(<em>n</em>) = Θ(lg <em>n</em>), which is an unimpressive amount of parallelism. To sort a million elements, for example, since lg 10<sup>6</sup> ≈ 20, it might achieve linear speedup on a few processors, but it would not scale up to dozens of processors.</p>
<p>The parallelism bottleneck in P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small> is plainly the M<small>ERGE</small> procedure. If we asymptotically reduce the span of merging, the master theorem dictates that the span of parallel merge sort will also get smaller. When you look at the pseudocode for M<small>ERGE</small>, it may seem that merging is inherently serial, but it’s not. We can fashion a parallel merging algorithm. The goal is to reduce the span of parallel merging asymptotically, but if we want an efficient parallel algorithm, we must ensure that the Θ(<em>n</em>) bound on work doesn’t increase.</p>
<p><a href="chapter026.xhtml#Fig_26-6">Figure 26.6</a> depicts the divide-and-conquer strategy that we’ll use in P-M<small>ERGE</small>. The heart of the algorithm is a recursive auxiliary procedure P-M<small>ERGE</small>-A<small>UX</small> that merges two sorted subarrays of an array <em>A</em> into a subarray of another array <em>B</em> in parallel. Specifically, P-M<small>ERGE</small>-A<small>UX</small> merges <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] and <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] into subarray <em>B</em>[<em>p</em><sub>3</sub> : <em>r</em><sub>3</sub>], where <em>r</em><sub>3</sub> = <em>p</em><sub>3</sub> + (<em>r</em><sub>1</sub> − <em>p</em><sub>1</sub> + 1) + (<em>r</em><sub>2</sub> − <em>p</em><sub>2</sub> + 1) − 1 = <em>p</em><sub>3</sub> + (<em>r</em><sub>1</sub> − <em>p</em><sub>1</sub>) + (<em>r</em><sub>2</sub> − <em>p</em><sub>2</sub>) + 1.</p>
<p>The key idea of the recursive merging algorithm in P-M<small>ERGE</small>-A<small>UX</small> is to split each of the two sorted subarrays of <em>A</em> around a pivot <em>x</em>, such that all the elements in the lower part of each subarray are at most <em>x</em> and all the elements in the upper part of each subarray are at least <em>x</em>. The procedure can then recurse in parallel on two subtasks: merging the two lower parts, and merging the two upper parts. The trick is to find a pivot <em>x</em> so that the recursion is not too lopsided. We don’t want a situation such as that in Q<small>UICKSORT</small> on page 183, where bad partitioning elements lead to a dramatic loss of asymptotic efficiency. We could opt to partition around a random element, as R<small>ANDOMIZED</small>-Q<small>UICKSORT</small> on page 192 does, but because the input subarrays are sorted, P-M<small>ERGE</small>-A<small>UX</small> can quickly determine a pivot that always works well.</p>
<p>Specifically, the recursive merging algorithm picks the pivot <em>x</em> as the middle element of the larger of the two input subarrays, which we can assume without loss of generality is <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>], since otherwise, the two subarrays can just switch roles. That is, <em>x</em> = <em>A</em>[<em>q</em><sub>1</sub>], where <em>q</em><sub>1</sub> = <span class="font1">⌊</span>(<em>p</em><sub>1</sub> + <em>r</em><sub>1</sub>)/2<span class="font1">⌋</span>. Because <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] is sorted, <em>x</em> is a median of the subarray elements: every element in <em>A</em>[<em>p</em><sub>1</sub> : <em>q</em><sub>1</sub> − 1] is no more than <em>x</em>, and every element in <em>A</em>[<em>q</em><sub>1</sub> + 1 : <em>r</em><sub>1</sub>] is no less than <em>x</em>. Then the <a id="p777"/>algorithm finds the “split point” <em>q</em><sub>2</sub> in the smaller subarray <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] such that all the elements in <em>A</em>[<em>p</em><sub>2</sub> : <em>q</em><sub>2</sub>−1] (if any) are at most <em>x</em> and all the elements in <em>A</em>[<em>q</em><sub>2</sub> : <em>r</em><sub>2</sub>] (if any) are at least <em>x</em>. Intuitively, the subarray <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] would still be sorted if <em>x</em> were inserted between <em>A</em>[<em>q</em><sub>2</sub>−1] and <em>A</em>[<em>q</em><sub>2</sub>] (although the algorithm doesn’t do that). Since <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] is sorted, a minor variant of binary search (see Exercise 2.3-6) with <em>x</em> as the search key can find the split point <em>q</em><sub>2</sub> in Θ(lg <em>n</em>) time in the worst case. As we’ll see when we get to the analysis, even if <em>x</em> splits <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] badly—<em>x</em> is either smaller than all the subarray elements or larger—we’ll still have at least 1/4 of the elements in each of the two recursive merges. Thus the larger of the recursive merges operates on at most 3/4 elements, and the recursion is guaranteed to bottom out after Θ(lg <em>n</em>) recursive calls.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_26-6"><img alt="art" src="images/Art_P838.jpg"/></p>
<p class="caption"><strong>Figure 26.6</strong> The idea behind P-M<small>ERGE</small>-A<small>UX</small>, which merges two sorted subarrays <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] and <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] into the subarray <em>B</em>[<em>p</em><sub>3</sub> : <em>r</em><sub>3</sub>] in parallel. Letting <em>x</em> = <em>A</em>[<em>q</em><sub>1</sub>] (shown in yellow) be a median of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] and <em>q</em><sub>2</sub> be a place in <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] such that <em>x</em> would fall between <em>A</em>[<em>q</em><sub>2</sub> − 1] and <em>A</em>[<em>q</em><sub>2</sub>], every element in the subarrays <em>A</em>[<em>p</em><sub>1</sub> : <em>q</em><sub>1</sub> − 1] and <em>A</em>[<em>p</em><sub>2</sub> : <em>q</em><sub>2</sub> − 1] (shown in orange) is at most <em>x</em>, and every element in the subarrays <em>A</em>[<em>q</em><sub>1</sub> + 1 : <em>r</em><sub>1</sub>] and <em>A</em>[<em>q</em><sub>2</sub> + 1 : <em>r</em><sub>2</sub>] (shown in blue) is at least <em>x</em>. To merge, compute the index <em>q</em><sub>3</sub> where <em>x</em> belongs in <em>B</em>[<em>p</em><sub>3</sub> : <em>r</em><sub>3</sub>], copy <em>x</em> into <em>B</em>[<em>q</em><sub>3</sub>], and then recursively merge <em>A</em>[<em>p</em><sub>1</sub> : <em>q</em><sub>1</sub> − 1] with <em>A</em>[<em>p</em><sub>2</sub> : <em>q</em><sub>2</sub> − 1] into <em>B</em>[<em>p</em><sub>3</sub> : <em>q</em><sub>3</sub> − 1] and <em>A</em>[<em>q</em><sub>1</sub> + 1 : <em>r</em><sub>1</sub>] with <em>A</em>[<em>q</em><sub>2</sub> : <em>r</em><sub>2</sub>] into <em>B</em>[<em>q</em><sub>3</sub> + 1 : <em>r</em><sub>3</sub>].</p>
</div>
<p>Now let’s put these ideas into pseudocode. We start with the serial procedure F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> (<em>A</em>, <em>p</em>, <em>r</em>, <em>x</em>) on the next page, which takes as input a sorted subarray <em>A</em>[<em>p</em> : <em>r</em>] and a key <em>x</em>. The procedure returns a split point of <em>A</em>[<em>p</em> : <em>r</em>]: an index <em>q</em> in the range <em>p</em> ≤ <em>q</em> ≤ <em>r</em> + 1 such that all the elements in <em>A</em>[<em>p</em> : <em>q</em> − 1] (if any) are at most <em>x</em> and all the elements in <em>A</em>[<em>q</em> : <em>r</em>] (if any) are at least <em>x</em>.</p>
<p>The F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> procedure uses binary search to find the split point. Lines 1 and 2 establish the range of indices for the search. Each time through the <strong>while</strong> loop, line 5 compares the middle element of the range with the search key <em>x</em>, and lines 6 and 7 narrow the search range to either the lower half or the upper half of the subarray, depending on the result of the test. In the end, after the range has been narrowed to a single index, line 8 returns that index as the split point.</p>
<a id="p778"/>
<div class="pull-quote1">
<p class="box-heading">F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> (<em>A</em>, <em>p</em>, <em>r</em>, <em>x</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>low</em> = <em>p</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> low end of search range</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><em>high</em> = <em>r</em> + 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> high end of search range</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>while</strong> <em>low</em> &lt; <em>high</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> more than one element?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>mid</em> = <span class="font1">⌊</span>(<em>low</em> + <em>high</em>)/2<span class="font1">⌋</span></p></td>
<td class="td1"><span class="red"><strong>//</strong> midpoint of range</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>x</em> ≤ <em>A</em>[<em>mid</em>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> is answer <em>q</em> ≤ <em>mid</em>?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p3"><em>high</em> = <em>mid</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> narrow search to <em>A</em>[<em>low</em> : <em>mid</em>]</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p2"><strong>else</strong> <em>low</em> = <em>mid</em> + 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> narrow search to <em>A</em>[<em>mid</em> + 1 : <em>high</em>]</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>return</strong> <em>low</em></p></td>
</tr>
</table>
</div>
<p>Because F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> contains no parallelism, its span is just its serial running time, which is also its work. On a subarray <em>A</em>[<em>p</em> : <em>r</em>] of size <em>n</em> = <em>r</em> − <em>p</em> + 1, each iteration of the <strong>while</strong> loop halves the search range, which means that the loop terminates after Θ(lg <em>n</em>) iterations. Since each iteration takes constant time, the algorithm runs in Θ(lg <em>n</em>) (worst-case) time. Thus the procedure has work and span Θ(lg <em>n</em>).</p>
<p>Let’s now look at the pseudocode for the parallel merging procedure P-M<small>ERGE</small> on the next page. Most of the pseudocode is devoted to the recursive procedure P-M<small>ERGE</small>-A<small>UX</small>. The procedure P-M<small>ERGE</small> itself is just a “wrapper” that sets up for P-M<small>ERGE</small>-A<small>UX</small>. It allocates a new array <em>B</em>[<em>p</em> : <em>r</em>] to hold the output of P-M<small>ERGE</small>-A<small>UX</small> in line 1. It then calls P-M<small>ERGE</small>-A<small>UX</small> in line 2, passing the indices of the two subarrays to be merged and providing <em>B</em> as the output destination of the merged result, starting at index <em>p</em>. After P-M<small>ERGE</small>-A<small>UX</small> returns, lines 3–4 perform a parallel copy of the output <em>B</em>[<em>p</em> : <em>r</em>] into the subarray <em>A</em>[<em>p</em> : <em>r</em>], which is where P-M<small>ERGE</small>-S<small>ORT</small> expects it.</p>
<p>The P-M<small>ERGE</small>-A<small>UX</small> procedure is the interesting part of the algorithm. Let’s start by understanding the parameters of this recursive parallel procedure. The input array <em>A</em> and the four indices <em>p</em><sub>1</sub>, <em>r</em><sub>1</sub>, <em>p</em><sub>2</sub>, <em>r</em><sub>2</sub> specify the subarrays <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] and <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] to be merged. The array <em>B</em> and the index <em>p</em><sub>3</sub> indicate that the merged result should be stored into <em>B</em>[<em>p</em><sub>3</sub> : <em>r</em><sub>3</sub>], where <em>r</em><sub>3</sub> = <em>p</em><sub>3</sub> + (<em>r</em><sub>1</sub> − <em>p</em><sub>1</sub>)+ (<em>r</em><sub>2</sub> − <em>p</em><sub>2</sub>)+ 1, as we saw earlier. The end index <em>r</em><sub>3</sub> of the output subarray is not needed by the pseudocode, but it helps conceptually to name the end index, as in the comment in line 13.</p>
<p>The procedure begins by checking the base case of the recursion and doing some bookkeeping to simplify the rest of the pseudocode. Lines 1 and 2 test whether the two subarrays are both empty, in which case the procedure returns. Line 3 checks whether the first subarray contains fewer elements than the second subarray. Since the number of elements in the first subarray is <em>r</em><sub>1</sub> − <em>p</em><sub>1</sub> + 1 and the number in the second subarray is <em>r</em><sub>2</sub> − <em>p</em><sub>2</sub> + 1, the test omits the two “+1’s.” If the first subarray <a id="p779"/>is the smaller of the two, lines 4 and 5 switch the roles of the subarrays so that <em>A</em>[<em>p</em><sub>1</sub>, <em>r</em><sub>1</sub>] refers to the larger subarray for the balance of the procedure.</p>
<div class="pull-quote1">
<p class="box-heading">P-M<small>ERGE</small> (<em>A</em>, <em>p</em>, <em>q</em>, <em>r</em>)</p>
<table class="table1">
<tr>
<td class="td1w1a"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent">let <em>B</em>[<em>p</em> : <em>r</em>] be a new array</p></td>
<td class="td1"><span class="red"><strong>//</strong> allocate scratch array</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="noindent">P-M<small>ERGE</small>-A<small>UX</small> (<em>A</em>, <em>p</em>, <em>q</em>, <em>q</em> + 1, <em>r</em>, <em>B</em>, <em>p</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> merge from <em>A</em> into <em>B</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = <em>p</em> <strong>to</strong> <em>r</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> copy <em>B</em> back to <em>A</em> in parallel</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1" colspan="2"><p class="p2"><em>A</em>[<em>i</em>] = <em>B</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1" colspan="3"><p class="box-headinga">P-M<small>ERGE</small>-A<small>UX</small> (<em>A</em>, <em>p</em><sub>1</sub>, <em>r</em><sub>1</sub>, <em>p</em><sub>2</sub>, <em>r</em><sub>2</sub>, <em>B</em>, <em>p</em><sub>3</sub>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>p</em><sub>1</sub> &gt; <em>r</em><sub>1</sub> and <em>p</em><sub>2</sub> &gt; <em>r</em><sub>2</sub></p></td>
<td class="td1"><span class="red"><strong>//</strong> are both subarrays empty?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>r</em><sub>1</sub> − <em>p</em><sub>1</sub> &lt; <em>r</em><sub>2</sub> − <em>p</em><sub>2</sub></p></td>
<td class="td1"><span class="red"><strong>//</strong> second subarray bigger?</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="p2">exchange <em>p</em><sub>1</sub> with <em>p</em><sub>2</sub></p></td>
<td class="td1"><span class="red"><strong>//</strong> swap subarray roles</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1" colspan="2"><p class="p2">exchange <em>r</em><sub>1</sub> with <em>r</em><sub>2</sub></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="noindent"><em>q</em><sub>1</sub> = <span class="font1">⌊</span>(<em>p</em><sub>1</sub> + <em>r</em><sub>1</sub>)/2<span class="font1">⌋</span></p></td>
<td class="td1"><span class="red"><strong>//</strong> midpoint of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>]</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1"><p class="noindent"><em>x</em> = <em>A</em>[<em>q</em><sub>1</sub>]</p></td>
<td class="td1"><span class="red"><strong>//</strong> median of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>] is pivot <em>x</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="noindent"><em>q</em><sub>2</sub> = F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> (<em>A</em>, <em>p</em><sub>2</sub>, <em>r</em><sub>2</sub>, <em>x</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> split <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] around <em>x</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1"><p class="noindent"><em>q</em><sub>3</sub> = <em>p</em><sub>3</sub> + (<em>q</em><sub>1</sub> − <em>p</em><sub>1</sub>) + (<em>q</em><sub>2</sub> − <em>p</em><sub>2</sub>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> where <em>x</em> belongs in <em>B</em> …</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><p class="noindent"><em>B</em>[<em>q</em><sub>3</sub>] = <em>x</em></p></td>
<td class="td1"><span class="red"><strong>//</strong> … put it there</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1" colspan="2"><p class="noindent"><span class="red"><strong>//</strong> Recursively merge <em>A</em>[<em>p</em><sub>1</sub> : <em>q</em><sub>1</sub> − 1] and <em>A</em>[<em>p</em><sub>2</sub> : <em>q</em><sub>2</sub> − 1] into <em>B</em>[<em>p</em><sub>3</sub> : <em>q</em><sub>3</sub> − 1].</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ERGE</small>-A<small>UX</small> (<em>A</em>, <em>p</em><sub>1</sub>, <em>q</em><sub>1</sub> − 1, <em>p</em><sub>2</sub>, <em>q</em><sub>2</sub> − 1, <em>B</em>, <em>p</em><sub>3</sub>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2"><p class="noindent"><span class="red"><strong>//</strong> Recursively merge <em>A</em>[<em>q</em><sub>1</sub> + 1 : <em>r</em><sub>1</sub>] and <em>A</em>[<em>q</em><sub>2</sub> : <em>r</em><sub>2</sub>] into <em>B</em>[<em>q</em><sub>3</sub> + 1 : <em>r</em><sub>3</sub>].</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>spawn</strong> P-M<small>ERGE</small>-A<small>UX</small> (<em>A</em>, <em>q</em><sub>1</sub> + 1, <em>r</em><sub>1</sub>, <em>q</em><sub>2</sub>, <em>r</em><sub>2</sub>, <em>B</em>, <em>q</em><sub>3</sub> + 1)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1"><p class="noindent"><strong>sync</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> wait for spawns</span></td>
</tr>
</table>
</div>
<p>We’re now at the crux of P-M<small>ERGE</small>-A<small>UX</small>: implementing the parallel divide-and-conquer strategy. As we continue our pseudocode walk, you may find it helpful to refer again to <a href="chapter026.xhtml#Fig_26-6">Figure 26.6</a>.</p>
<p>First the divide step. Line 6 computes the midpoint <em>q</em><sub>1</sub> of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>], which indexes a median <em>x</em> = <em>A</em>[<em>q</em><sub>1</sub>] of this subarray to be used as the pivot, and line 7 determines <em>x</em> itself. Next, line 8 uses the F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> procedure to find the index <em>q</em><sub>2</sub> in <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>] such that all elements in <em>A</em>[<em>p</em><sub>2</sub> : <em>q</em><sub>2</sub> − 1] are at most <em>x</em> and all the elements in <em>A</em>[<em>q</em><sub>2</sub> : <em>r</em><sub>2</sub>] are at least <em>x</em>. Line 9 computes the index <em>q</em><sub>3</sub> of the element that divides the output subarray <em>B</em>[<em>p</em><sub>3</sub> : <em>r</em><sub>3</sub>] into <em>B</em>[<em>p</em><sub>3</sub> : <em>q</em><sub>3</sub> − 1] and <em>B</em>[<em>q</em><sub>3</sub> + 1 : <em>r</em><sub>3</sub>], and then line 10 puts <em>x</em> directly into <em>B</em>[<em>q</em><sub>3</sub>], which is where it belongs in the output.</p>
<p>Next is the conquer step, which is where the parallel recursion occurs. Lines 12 and 14 each spawn P-M<small>ERGE</small>-A<small>UX</small> to recursively merge from <em>A</em> into <em>B</em>, the first to merge the smaller elements and the second to merge the larger elements. The <a id="p780"/><strong>sync</strong> statement in line 15 ensures that the subproblems finish before the procedure returns.</p>
<p>There is no combine step, as <em>B</em>[<em>p</em> : <em>r</em>] already contains the correct sorted output.</p>
<p class="level4"><strong>Work/span analysis of parallel merging</strong></p>
<p class="noindent">Let’s first analyze the worst-case span <em>T</em><sub>∞</sub>(<em>n</em>) of P-M<small>ERGE</small>-A<small>UX</small> on input subarrays that together contain a total of <em>n</em> elements. The call to F<small>IND</small>-S<small>PLIT</small>-P<small>OINT</small> in line 8 contributes Θ(lg <em>n</em>) to the span in the worst case, and the procedure performs at most a constant amount of additional serial work outside of the two recursive spawns in lines 12 and 14.</p>
<p>Because the two recursive spawns operate logically in parallel, only one of them contributes to the overall worst-case span. We claimed earlier that neither recursive invocation ever operates on more than 3<em>n</em>/4 elements. Let’s see why. Let <em>n</em><sub>1</sub> = <em>r</em><sub>1</sub> − <em>p</em><sub>1</sub> + 1 and <em>n</em><sub>2</sub> = <em>r</em><sub>2</sub> − <em>p</em><sub>2</sub> + 1, where <em>n</em> = <em>n</em><sub>1</sub> + <em>n</em><sub>2</sub>, be the sizes of the two subarrays when line 6 starts executing, that is, after we have established that <em>n</em><sub>2</sub> ≤ <em>n</em><sub>1</sub> by swapping the roles of the two subarrays, if necessary. Since the pivot <em>x</em> is a median of of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>], in the worst case, a recursive merge involves at most <em>n</em><sub>1</sub>/2 elements of <em>A</em>[<em>p</em><sub>1</sub> : <em>r</em><sub>1</sub>], but it might involve all <em>n</em><sub>2</sub> of the elements of <em>A</em>[<em>p</em><sub>2</sub> : <em>r</em><sub>2</sub>]. Thus we can bound the number of elements involved in a recursive invocation of P-M<small>ERGE</small>-A<small>UX</small> by</p>
<table class="table2b">
<tr>
<td class="td2"><em>n</em><sub>1</sub>/2 + <em>n</em><sub>2</sub></td>
<td class="td2">=</td>
<td class="td2">(2<em>n</em><sub>1</sub> + 4<em>n</em><sub>2</sub>)/4</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2">(3<em>n</em><sub>1</sub> + 3<em>n</em><sub>2</sub>)/4 (since <em>n</em><sub>2</sub> ≤ <em>n</em><sub>1</sub>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">3<em>n</em>/4,</td>
</tr>
</table>
<p class="noindent">proving the claim.</p>
<p>The worst-case span of P-M<small>ERGE</small>-A<small>UX</small> can therefore be described by the following recurrence:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P839.jpg"/></p>
<p class="noindent">Because this recurrence falls under case 2 of the master theorem with <em>k</em> = 1, its solution is <em>T</em><sub>∞</sub>(<em>n</em>) = Θ(lg <sup>2</sup> <em>n</em>).</p>
<p>Now let’s verify that the work <em>T</em><sub>1</sub>(<em>n</em>) of P-M<small>ERGE</small>-A<small>UX</small> on <em>n</em> elements is linear. A lower bound of Ω(<em>n</em>) is straightforward, since each of the <em>n</em> elements is copied from array <em>A</em> to array <em>B</em>. We’ll show that <em>T</em><sub>1</sub>(<em>n</em>) = <em>O</em>(<em>n</em>) by deriving a recurrence for the worst-case work. The binary search in line 8 costs Θ(lg <em>n</em>) in the worst case, which dominates the other work outside of the recursive spawns. For the recursive spawns, observe that although lines 12 and 14 might merge different numbers of elements, the two recursive spawns together merge at most <em>n</em> − 1 elements (since <em>x</em> = <em>A</em>[<em>q</em>] is not merged). Moreover, as we saw when analyzing the span, a recursive spawn operates on at most 3<em>n</em>/4 elements. We therefore obtain the recurrence</p>
<a id="p781"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P840.jpg"/></p>
<p class="noindent">where <em>α</em> lies in the range 1/4 ≤ <em>α</em> ≤ 3/4. The value of <em>α</em> can vary from one recursive invocation to another.</p>
<p>We’ll use the substitution method (see <a href="chapter004.xhtml#Sec_4.3">Section 4.3</a>) to prove that the above recurrence (26.8) has solution <em>T</em><sub>1</sub>(<em>n</em>) = <em>O</em>(<em>n</em>). (You could also use the Akra-Bazzi method from <a href="chapter004.xhtml#Sec_4.7">Section 4.7</a>.) Assume that <em>T</em><sub>1</sub>(<em>n</em>) ≤ <em>c</em><sub>1</sub><em>n</em> − <em>c</em><sub>2</sub> lg <em>n</em> for some positive constants <em>c</em><sub>1</sub> and <em>c</em><sub>2</sub>. Using the properties of logarithms on pages 66–67—in particular, to deduce that lg <em>α</em> + lg(1 − <em>α</em>) = −Θ(1)—substitution yields</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em><sub>1</sub>(<em>n</em>)</td>
<td class="td2">≤</td>
<td class="td2">(<em>c</em><sub>1</sub><em>αn</em> − <em>c</em><sub>2</sub> lg(<em>αn</em>)) + (<em>c</em><sub>1</sub>(1 − <em>α</em>)<em>n</em> − <em>c</em><sub>2</sub> lg((1 − <em>α</em>)<em>n</em>)) + Θ(lg <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>c</em><sub>1</sub>(<em>α</em> + (1 − <em>α</em>))<em>n</em> − <em>c</em><sub>2</sub>(lg(<em>αn</em>) + lg((1 − <em>α</em>)<em>n</em>)) + Θ(lg <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>c</em><sub>1</sub><em>n</em> − <em>c</em><sub>2</sub>(lg <em>α</em> + lg <em>n</em> + lg(1 − <em>α</em>) + lg <em>n</em>) + Θ(lg <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>c</em><sub>1</sub><em>n</em> − <em>c</em><sub>2</sub> lg <em>n</em> − <em>c</em><sub>2</sub>(lg <em>n</em> + lg <em>α</em> + lg(1 − <em>α</em>)) + Θ(lg <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>c</em><sub>1</sub><em>n</em> − <em>c</em><sub>2</sub> lg <em>n</em> − <em>c</em><sub>2</sub>(lg <em>n</em> − Θ(1)) + Θ(lg <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2"><em>c</em><sub>1</sub><em>n</em> − <em>c</em><sub>2</sub> lg <em>n</em>,</td>
</tr>
</table>
<p class="noindent">if we choose <em>c</em><sub>2</sub> large enough that the <em>c</em><sub>2</sub>(lg <em>n</em> − Θ(1)) term dominates the Θ(lg <em>n</em>) term for sufficiently large <em>n</em>. Furthermore, we can choose <em>c</em><sub>1</sub> large enough to satisfy the implied Θ(1) base cases of the recurrence, completing the induction. The lower and upper bounds of Ω(<em>n</em>) and <em>O</em>(<em>n</em>) give <em>T</em><sub>1</sub>(<em>n</em>) = Θ(<em>n</em>), asymptotically the same work as for serial merging.</p>
<p>The execution of the pseudocode in the P-M<small>ERGE</small> procedure itself does not add asymptotically to the work and span of P-M<small>ERGE</small>-A<small>UX</small>. The <strong>parallel for</strong> loop in lines 3–4 has Θ(lg <em>n</em>) span due to the loop control, and each iteration runs in constant time. Thus the Θ(lg<sup>2</sup><em>n</em>) span of P-M<small>ERGE</small>-A<small>UX</small> dominates, yielding Θ(lg<sup>2</sup><em>n</em>) span overall for P-M<small>ERGE</small>. The <strong>parallel for</strong> loop contains Θ(<em>n</em>) work, matching the asymptotic work of P-M<small>ERGE</small>-A<small>UX</small> and yielding Θ(<em>n</em>) work overall for P-M<small>ERGE</small>.</p>
<p class="level4"><strong>Analysis of parallel merge sort</strong></p>
<p class="noindent">The “heavy lifting” is done. Now that we have determined the work and span of P-M<small>ERGE</small>, we can analyze P-M<small>ERGE</small>-S<small>ORT</small>. Let <em>T</em><sub>1</sub>(<em>n</em>) and <em>T</em><sub>∞</sub>(<em>n</em>) be the work and span, respectively, of P-M<small>ERGE</small>-S<small>ORT</small> on an array of <em>n</em> elements. The call to P-M<small>ERGE</small> in line 10 of P-M<small>ERGE</small>-S<small>ORT</small> dominates the costs of lines 1–3, for both work and span. Thus we obtain the recurrence</p>
<p class="eql"><em>T</em><sub>1</sub>(<em>n</em>) = 2<em>T</em><sub>1</sub>(<em>n</em>/2) + Θ(<em>n</em>)</p>
<p class="noindent1-top">for the work of P-M<small>ERGE</small>-S<small>ORT</small>, and we obtain the recurrence</p>
<p class="eql"><em>T</em><sub>∞</sub>(<em>n</em>) = <em>T</em><sub>∞</sub>(<em>n</em>/2) + Θ(lg<sup>2</sup> <em>n</em>)</p>
<a id="p782"/>
<p class="noindent">for its span. The work recurrence has solution <em>T</em><sub>1</sub>(<em>n</em>) = Θ(<em>n</em> lg <em>n</em>) by case 2 of the master theorem with <em>k</em> = 0. The span recurrence has solution <em>T</em><sub>∞</sub> (<em>n</em>) = Θ(lg<sup>3</sup> <em>n</em>), also by case 2 of the master theorem, but with <em>k</em> = 2.</p>
<p>Parallel merging gives P-M<small>ERGE</small>-S<small>ORT</small> a parallelism advantage over P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small>. The parallelism of P-N<small>AIVE</small>-M<small>ERGE</small>-S<small>ORT</small>, which calls the serial M<small>ERGE</small> procedure, is only Θ(lg <em>n</em>). For P-M<small>ERGE</small>-S<small>ORT</small>, the parallelism is</p>
<table class="table2b">
<tr>
<td class="td2"><em>T</em><sub>1</sub>(<em>n</em>)/<em>T</em><sub>∞</sub>(<em>n</em>)</td>
<td class="td2">=</td>
<td class="td2">Θ(<em>n</em> lg <em>n</em>)/Θ(lg<sup>3</sup> <em>n</em>)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">Θ(<em>n</em>/lg<sup>2</sup> <em>n</em>),</td>
</tr>
</table>
<p class="noindent">which is much better, both in theory and in practice. A good implementation in practice would sacrifice some parallelism by coarsening the base case in order to reduce the constants hidden by the asymptotic notation. For example, you could switch to an efficient serial sort, perhaps quicksort, when the number of elements to be sorted is sufficiently small.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>26.3-1</em></strong></p>
<p class="noindent">Explain how to coarsen the base case of P-M<small>ERGE</small>.</p>
<p class="level3"><strong><em>26.3-2</em></strong></p>
<p class="noindent">Instead of finding a median element in the larger subarray, as P-M<small>ERGE</small> does, suppose that the merge procedure finds a median of all the elements in the two sorted subarrays using the result of Exercise 9.3-10. Give pseudocode for an efficient parallel merging procedure that uses this median-finding procedure. Analyze your algorithm.</p>
<p class="level3"><strong><em>26.3-3</em></strong></p>
<p class="noindent">Give an efficient parallel algorithm for partitioning an array around a pivot, as is done by the P<small>ARTITION</small> procedure on page 184. You need not partition the array in place. Make your algorithm as parallel as possible. Analyze your algorithm. (<em>Hint:</em> You might need an auxiliary array and might need to make more than one pass over the input elements.)</p>
<p class="level3"><strong><em>26.3-4</em></strong></p>
<p class="noindent">Give a parallel version of FFT on page 890. Make your implementation as parallel as possible. Analyze your algorithm.</p>
<p class="level3"><span class="font1">★</span> <strong><em>26.3-5</em></strong></p>
<p class="noindent">Show how to parallelize S<small>ELECT</small> from <a href="chapter009.xhtml#Sec_9.3">Section 9.3</a>. Make your implementation as parallel as possible. Analyze your algorithm.</p>
<a id="p783"/>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-155"><strong>Problems</strong></p>
<section title="26-1 Implementing parallel loops using recursive spawning">
<p class="level2"><strong><em>26-1     Implementing parallel loops using recursive spawning</em></strong></p>
<p class="noindent">Consider the parallel procedure S<small>UM</small>-A<small>RRAYS</small> for performing pairwise addition on <em>n</em>-element arrays <em>A</em>[1 : <em>n</em>] and <em>B</em>[1 : <em>n</em>], storing the sums in <em>C</em> [1 : <em>n</em>].</p>
<div class="pull-quote1">
<p class="box-heading">S<small>UM</small>-A<small>RRAYS</small> (<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1c">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><em>C</em> [<em>i</em>] = <em>A</em>[<em>i</em>] + <em>B</em>[<em>i</em>]</p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>a.</em></strong> Rewrite the parallel loop in S<small>UM</small>-A<small>RRAYS</small> using recursive spawning in the manner of P-M<small>AT</small>-V<small>EC</small>-R<small>ECURSIVE</small>. Analyze the parallelism.</p>
<p class="space-break">Consider another implementation of the parallel loop in S<small>UM</small>-A<small>RRAYS</small> given by the procedure S<small>UM</small>-A<small>RRAYS</small>′, where the value <em>grain</em>-<em>size</em> must be specified.</p>
<div class="pull-quote1">
<p class="box-heading">S<small>UM</small>-A<small>RRAYS</small>′(<em>A</em>, <em>B</em>, <em>C</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>grain</em>-<em>size</em> = ?</p></td>
<td class="td1"><span class="red"><strong>//</strong> to be determined</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>r</em> = <span class="font1">⌈</span><em>n</em>/<em>grain</em>-<em>size</em><span class="font1">⌉</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>for</strong> <em>k</em> = 0 <strong>to</strong> <em>r</em> − 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>spawn</strong> A<small>DD</small>-S<small>UBARRAY</small> (<em>A</em>, <em>B</em>, <em>C</em>, <em>k</em> · <em>grain</em>-<em>size</em> + 1,</p></td>
</tr>
<tr>
<td class="td1"/>
<td class="td1" colspan="2"><p class="p7">min {(<em>k</em> + 1) · <em>grain</em>-<em>size</em>, <em>n</em>})</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>sync</strong></p></td>
</tr>
<tr>
<td class="td1" colspan="3"><p class="box-headinga">A<small>DD</small>-S<small>UBARRAY</small> (<em>A</em>, <em>B</em>, <em>C</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>for</strong> <em>k</em> = <em>i</em> <strong>to</strong> <em>j</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="p2"><em>C</em> [<em>k</em>] = <em>A</em>[<em>k</em>] + <em>B</em>[<em>k</em>]</p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>b.</em></strong> Suppose that you set <em>grain</em>-<em>size</em> = 1. What is the resulting parallelism?</p>
<p class="nl"><strong><em>c.</em></strong> Give a formula for the span of S<small>UM</small>-A<small>RRAYS</small>′ in terms of <em>n</em> and <em>grain</em>-<em>size</em>. Derive the best value for <em>grain</em>-<em>size</em> to maximize parallelism.</p>
</section>
<section title="26-2 Avoiding a temporary matrix in recursive matrix multiplication">
<p class="level2"><strong><em>26-2     Avoiding a temporary matrix in recursive matrix multiplication</em></strong></p>
<p class="noindent">The P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> procedure on page 772 must allocate a temporary matrix <em>D</em> of size <em>n</em> × <em>n</em>, which can adversely affect the constants hidden by the Θ-notation. The procedure has high parallelism, however: Θ(<em>n</em><sup>3</sup>/log<sup>2</sup> <em>n</em>). <a id="p784"/>For example, ignoring the constants in the Θ-notation, the parallelism for multiplying 1000 × 1000 matrices comes to approximately 1000<sup>3</sup>/10<sup>2</sup> = 10<sup>7</sup>, since lg 1000 ≈ 10. Most parallel computers have far fewer than 10 million processors.</p>
<p class="nl"><strong><em>a.</em></strong> Parallelize M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small> without using temporary matrices so that it retains its Θ(<em>n</em><sup>3</sup>) work. (<em>Hint:</em> Spawn the recursive calls, but insert a <strong>sync</strong> in a judicious location to avoid races.)</p>
<p class="nl"><strong><em>b.</em></strong> Give and solve recurrences for the work and span of your implementation.</p>
<p class="nl"><strong><em>c.</em></strong> Analyze the parallelism of your implementation. Ignoring the constants in the Θ-notation, estimate the parallelism on 1000 × 1000 matrices. Compare with the parallelism of P-M<small>ATRIX</small>-M<small>ULTIPLY</small>-R<small>ECURSIVE</small>, and discuss whether the trade-off would be worthwhile.</p>
</section>
<section title="26-3 Parallel matrix algorithms">
<p class="level2"><strong><em>26-3     Parallel matrix algorithms</em></strong></p>
<p class="noindent">Before attempting this problem, it may be helpful to read <a href="chapter028.xhtml">Chapter 28</a>.</p>
<p class="nl"><strong><em>a.</em></strong> Parallelize the L<small>U</small>-D<small>ECOMPOSITION</small> procedure on page 827 by giving pseudocode for a parallel version of this algorithm. Make your implementation as parallel as possible, and analyze its work, span, and parallelism.</p>
<p class="nl"><strong><em>b.</em></strong> Do the same for L<small>UP</small>-D<small>ECOMPOSITION</small> on page 830.</p>
<p class="nl"><strong><em>c.</em></strong> Do the same for L<small>UP</small>-S<small>OLVE</small> on page 824.</p>
<p class="nl"><strong><em>d.</em></strong> Using equation (28.14) on page 835, write pseudocode for a parallel algorithm to invert a symmetric positive-definite matrix. Make your implementation as parallel as possible, and analyze its work, span, and parallelism.</p>
</section>
<section title="26-4 Parallel reductions and scan (prefix) computations">
<p class="level2"><strong><em>26-4     Parallel reductions and scan (prefix) computations</em></strong></p>
<p class="noindent">A <strong><span class="blue">⊗-<em>reduction</em></span></strong> of an array <em>x</em>[1 : <em>n</em>], where ⊗ is an associative operator, is the value <em>y</em> = <em>x</em>[1] ⊗ <em>x</em>[2] ⊗ <span class="font1">⋯</span> ⊗ <em>x</em>[<em>n</em>]. The R<small>EDUCE</small> procedure computes the ⊗-reduction of a subarray <em>x</em>[<em>i</em> : <em>j</em>] serially.</p>
<div class="pull-quote1">
<p class="box-heading">R<small>EDUCE</small> (<em>x</em>, <em>i</em>, <em>j</em>)</p>
<table class="table1n">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>y</em> = <em>x</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>k</em> = <em>i</em> + 1 <strong>to</strong> <em>j</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>y</em> = <em>y</em> ⊗ <em>x</em>[<em>k</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>y</em></p></td>
</tr>
</table>
</div>
<a id="p785"/>
<p class="nl"><strong><em>a.</em></strong> Design and analyze a parallel algorithm P-R<small>EDUCE</small> that uses recursive spawning to perform the same function with Θ(<em>n</em>) work and Θ(lg <em>n</em>) span.</p>
<p class="space-break">A related problem is that of computing a <strong><span class="blue">⊗</span><em><span class="blue">-scan</span></em></strong>, sometimes called a <strong><span class="blue">⊗</span><em><span class="blue">-prefix computation</span></em></strong>, on an array <em>x</em>[1 : <em>n</em>], where ⊗ is once again an associative operator. The ⊗-scan, implemented by the serial procedure S<small>CAN</small>, produces the array <em>y</em>[1 : <em>n</em>] given by</p>
<table class="table2b">
<tr>
<td class="td2"><em>y</em>[1]</td>
<td class="td2">=</td>
<td class="td2"><em>x</em>[1],</td>
</tr>
<tr>
<td class="td2"><em>y</em>[2]</td>
<td class="td2">=</td>
<td class="td2"><em>x</em>[1] ⊗ <em>x</em>[2],</td>
</tr>
<tr>
<td class="td2"><em>y</em>[3]</td>
<td class="td2">=</td>
<td class="td2"><em>x</em>[1] ⊗ <em>x</em>[2] ⊗ <em>x</em>[3],</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><span class="font1">⋮</span></td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"><em>y</em>[<em>n</em>]</td>
<td class="td2">=</td>
<td class="td2"><em>x</em>[1] ⊗ <em>x</em>[2] ⊗ <em>x</em>[3] ⊗ <span class="font1">⋯</span> ⊗ <em>x</em>[<em>n</em>],</td>
</tr>
</table>
<p class="noindent">that is, all prefixes of the array <em>x</em> “summed” using the ⊗ operator.</p>
<div class="pull-quote1">
<p class="box-heading">S<small>CAN</small> (<em>x</em>, <em>n</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent">let <em>y</em>[1 : <em>n</em>] be a new array</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><em>y</em>[1] = <em>x</em>[1]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>y</em>[<em>i</em>] = <em>y</em>[<em>i</em> − 1] ⊗ 1 ⊗ <em>x</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>y</em></p></td>
</tr>
</table>
</div>
<p>Parallelizing S<small>CAN</small> is not straightforward. For example, simply changing the <strong>for</strong> loop to a <strong>parallel for</strong> loop would create races, since each iteration of the loop body depends on the previous iteration. The procedures P-S<small>CAN</small>-1 and P-S<small>CAN</small>-1-A<small>UX</small> perform the ⊗-scan in parallel, albeit inefficiently.</p>
<div class="pull-quote1">
<p class="box-heading">P-S<small>CAN</small>-1(<em>x</em>, <em>n</em>)</p>
<table class="table1c1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent">let <em>y</em>[1] : <em>n</em> be a new array</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent">P-S<small>CAN</small>-1-A<small>UX</small> (<em>x</em>, <em>y</em>, 1, <em>n</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>y</em></p></td>
</tr>
<tr>
<td class="td1" colspan="2"><p class="box-headinga">P-S<small>CAN</small>-1-A<small>UX</small> (<em>x</em>, <em>y</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>parallel for</strong> <em>l</em> = <em>i</em> <strong>to</strong> <em>j</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><em>y</em>[<em>l</em>] = P-R<small>EDUCE</small> (<em>x</em>, 1, <em>l</em>)</p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>b.</em></strong> Analyze the work, span, and parallelism of P-S<small>CAN</small>-1.</p>
<a id="p786"/>
<p>The procedures P-S<small>CAN</small>-2 and P-S<small>CAN</small>-2-A<small>UX</small> use recursive spawning to perform a more efficient ⊗-scan.</p>
<div class="pull-quote1">
<p class="box-heading">P-S<small>CAN</small>-2(<em>x</em>, <em>n</em>)</p>
<table class="table1a1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent">let <em>y</em>[1] : <em>n</em> be a new array</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent">P-S<small>CAN</small>-2-A<small>UX</small> (<em>x</em>, <em>y</em>, 1, <em>n</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>y</em></p></td>
</tr>
<tr>
<td class="td1" colspan="2"><p class="box-headinga">P-S<small>CAN</small>-2-A<small>UX</small> (<em>x</em>, <em>y</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>i</em> == <em>j</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="p2"><em>y</em>[<em>i</em>] = <em>x</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>else</strong> <em>k</em> = <span class="font1">⌊</span>(<em>i</em> + <em>j</em>)/2<span class="font1">⌋</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>spawn</strong> P-S<small>CAN</small>-2-A<small>UX</small> (<em>x</em>, <em>y</em>, <em>i</em>, <em>k</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2">P-S<small>CAN</small>-2-A<small>UX</small> (<em>x</em>, <em>y</em>, <em>k</em> + 1, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2"><strong>sync</strong></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p2"><strong>parallel for</strong> <em>l</em> = <em>k</em> + 1 <strong>to</strong> <em>j</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><p class="p3"><em>y</em>[<em>l</em>] = <em>y</em>[<em>k</em>] ⊗ <em>y</em>[<em>l</em>]</p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>c.</em></strong> Argue that P-S<small>CAN</small>-2 is correct, and analyze its work, span, and parallelism.</p>
<p class="space-break">To improve on both P-S<small>CAN</small>-1 and P-S<small>CAN</small>-2, perform the ⊗-scan in two distinct passes over the data. The first pass gathers the terms for various contiguous subarrays of <em>x</em> into a temporary array <em>t</em>, and the second pass uses the terms in <em>t</em> to compute the final result <em>y</em>. The pseudocode in the procedures P-S<small>CAN</small>-3, P-S<small>CAN</small>-U<small>P</small>, and P-S<small>CAN</small>-D<small>OWN</small> on the facing page implements this strategy, but certain expressions have been omitted.</p>
<p class="nl"><strong><em>d.</em></strong> Fill in the three missing expressions in line 8 of P-S<small>CAN</small>-U<small>P</small> and lines 5 and 6 of P-S<small>CAN</small>-D<small>OWN</small>. Argue that with the expressions you supplied, P-S<small>CAN</small>-3 is correct. (<em>Hint:</em> Prove that the value <em>v</em> passed to P-S<small>CAN</small>-D<small>OWN</small> (<em>v</em>, <em>x</em>, <em>t</em>, <em>y</em>, <em>i</em>, <em>j</em>) satisfies <em>v</em> = <em>x</em>[1] ⊗ <em>x</em>[2] ⊗ <span class="font1">⋯</span> ⊗ <em>x</em>[<em>i</em> − 1].)</p>
<p class="nl"><strong><em>e.</em></strong> Analyze the work, span, and parallelism of P-S<small>CAN</small>-3.</p>
<p class="nl"><strong><em>f.</em></strong> Describe how to rewrite P-S<small>CAN</small>-3 so that it doesn’t require the use of the temporary array <em>t</em>.</p>
<p class="nl-1a"><strong><span class="font1">★</span> <em>g.</em></strong> Give an algorithm P-S<small>CAN</small>-4(<em>x</em>, <em>n</em>) for a scan that operates in place. It should place its output in <em>x</em> and require only constant auxiliary storage.</p>
<p class="nl"><strong><em>h.</em></strong> Describe an efficient parallel algorithm that uses a +-scan to determine whether a string of parentheses is well formed. For example, the string ( ( ) ( ) ) ( ) <a id="p787"/>is well formed, but the string ( ( ) ) ) ( ( ) is not. (<em>Hint:</em> Interpret ( as a 1 and ) as a −1, and then perform a +-scan.)</p>
<div class="pull-quote1">
<p class="box-heading">P-S<small>CAN</small>-3(<em>x</em>, <em>n</em>)</p>
<table class="table1a1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent">let <em>y</em>[1] : <em>n</em> and <em>t</em>[1 : <em>n</em>] be new arrays</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>y</em>[1] = <em>x</em>[1]</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>if</strong> <em>n</em> &gt; 1</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="p2">P-S<small>CAN</small>-U<small>P</small> (<em>x</em>, <em>t</em>, 2, <em>n</em>)</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="p2">P-S<small>CAN</small>-D<small>OWN</small> (<em>x</em>[1], <em>x</em>, <em>t</em>, <em>y</em>, 2, <em>n</em>)</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>return</strong> <em>y</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1" colspan="4"><p class="box-headinga">P-S<small>CAN</small>-U<small>P</small> (<em>x</em>, <em>t</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>if</strong> <em>i</em> == <em>j</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong> <em>x</em>[<em>i</em>]</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>else</strong></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="p2"><em>k</em> = <span class="font1">⌊</span>(<em>i</em> + <em>j</em>)/2<span class="font1">⌋</span></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="p2"><em>t</em>[<em>k</em>] = <strong>spawn</strong> P-S<small>CAN</small>-U<small>P</small> (<em>x</em>, <em>t</em>, <em>i</em>, <em>k</em>)</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="p2"><em>right</em> = P-S<small>CAN</small>-U<small>P</small> (<em>x</em>, <em>t</em>, <em>k</em> + 1, <em>j</em>)</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>sync</strong></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><p class="p2"><strong>return</strong> ____</p></td>
<td class="td1"><span class="red"><strong>//</strong> fill in the blank</span></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1" colspan="4"><p class="box-headinga">P-S<small>CAN</small>-D<small>OWN</small> (<em>v</em>, <em>x</em>, <em>t</em>, <em>y</em>, <em>i</em>, <em>j</em>)</p></td>
</tr>
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>if</strong> <em>i</em> == <em>j</em></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="p2"><em>y</em>[<em>i</em>] = <em>v</em> ⊗ <em>x</em>[<em>i</em>]</p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>else</strong></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="p2"><em>k</em> = <span class="font1">⌊</span>(<em>i</em> + <em>j</em>)/2<span class="font1">⌋</span></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>spawn</strong> P-S<small>CAN</small>-D<small>OWN</small> (____, <em>x</em>, <em>t</em>, <em>y</em>, <em>i</em>, <em>k</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> fill in the blank</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="p2">P-S<small>CAN</small>-D<small>OWN</small> (____, <em>x</em>, <em>t</em>, <em>y</em>, <em>k</em> + 1, <em>j</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> fill in the blank</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>sync</strong></p></td>
<td class="td1"/>
</tr>
</table>
</div>
</section>
<section title="26-5 Parallelizing a simple stencil calculation">
<p class="level2"><strong><em>26-5     Parallelizing a simple stencil calculation</em></strong></p>
<p class="noindent">Computational science is replete with algorithms that require the entries of an array to be filled in with values that depend on the values of certain already computed neighboring entries, along with other information that does not change over the course of the computation. The pattern of neighboring entries does not change during the computation and is called a <strong><em><span class="blue">stencil</span></em></strong>. For example, <a href="chapter014.xhtml#Sec_14.4">Section 14.4</a> presents a stencil algorithm to compute a longest common subsequence, where the value in entry <em>c</em>[<em>i</em>, <em>j</em>] depends only on the values in <em>c</em>[<em>i</em> − 1, <em>j</em>], <em>c</em>[<em>i</em>, <em>j</em> − 1], and <em>c</em>[<em>i</em> − 1, <em>j</em> − 1], <a id="p788"/>as well as the elements <em>x</em><sub><em>i</em></sub> and <em>y</em><sub><em>j</em></sub> within the two sequences given as inputs. The input sequences are fixed, but the algorithm fills in the two-dimensional array <em>c</em> so that it computes entry <em>c</em>[<em>i</em>, <em>j</em>] after computing all three entries <em>c</em>[<em>i</em> − 1, <em>j</em>], <em>c</em>[<em>i</em>, <em>j</em> − 1], and <em>c</em>[<em>i</em> − 1, <em>j</em> − 1].</p>
<p>This problem examines how to use recursive spawning to parallelize a simple stencil calculation on an <em>n</em> × <em>n</em> array <em>A</em> in which the value placed into entry <em>A</em>[<em>i</em>, <em>j</em>] depends only on values in <em>A</em>[<em>i</em>′, <em>j</em>′], where <em>i</em>′ ≤ <em>i</em> and <em>j</em>′ ≤ <em>j</em> (and of course, <em>i</em>′ ≠ <em>i</em> or <em>j</em>′ ≠ <em>j</em>). In other words, the value in an entry depends only on values in entries that are above it and/or to its left, along with static information outside of the array. Furthermore, we assume throughout this problem that once the entries upon which <em>A</em>[<em>i</em>, <em>j</em>] depends have been filled in, the entry <em>A</em>[<em>i</em>, <em>j</em>] can be computed in Θ(1) time (as in the L<small>CS</small>-L<small>ENGTH</small> procedure of <a href="chapter014.xhtml#Sec_14.4">Section 14.4</a>).</p>
<p>Partition the <em>n</em> × <em>n</em> array <em>A</em> into four <em>n</em>/2 × <em>n</em>/2 subarrays as follows:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P841.jpg"/></p>
<p class="noindent">You can immediately fill in subarray <em>A</em><sub>11</sub> recursively, since it does not depend on the entries in the other three subarrays. Once the computation of <em>A</em><sub>11</sub> finishes, you can fill in <em>A</em><sub>12</sub> and <em>A</em><sub>21</sub> recursively in parallel, because although they both depend on <em>A</em><sub>11</sub>, they do not depend on each other. Finally, you can fill in <em>A</em><sub>22</sub> recursively.</p>
<p class="nl"><strong><em>a.</em></strong> Give parallel pseudocode that performs this simple stencil calculation using a divide-and-conquer algorithm S<small>IMPLE</small>-S<small>TENCIL</small> based on the decomposition (26.9) and the discussion above. (Don’t worry about the details of the base case, which depends on the specific stencil.) Give and solve recurrences for the work and span of this algorithm in terms of <em>n</em>. What is the parallelism?</p>
<p class="nl"><strong><em>b.</em></strong> Modify your solution to part (a) to divide an <em>n</em> × <em>n</em> array into nine <em>n</em>/3 × <em>n</em>/3 subarrays, again recursing with as much parallelism as possible. Analyze this algorithm. How much more or less parallelism does this algorithm have compared with the algorithm from part (a)?</p>
<p class="nl"><strong><em>c.</em></strong> Generalize your solutions to parts (a) and (b) as follows. Choose an integer <em>b</em> ≥ 2. Divide an <em>n</em> × <em>n</em> array into <em>b</em><sup>2</sup> subarrays, each of size <em>n</em>/<em>b</em> × <em>n</em>/<em>b</em>, recursing with as much parallelism as possible. In terms of <em>n</em> and <em>b</em>, what are the work, span, and parallelism of your algorithm? Argue that, using this approach, the parallelism must be <em>o</em>(<em>n</em>) for any choice of <em>b</em> ≥ 2. (<em>Hint:</em> For this argument, show that the exponent of <em>n</em> in the parallelism is strictly less than 1 for any choice of <em>b</em> ≥ 2.)</p>
<p class="nl"><strong><em>d.</em></strong> Give pseudocode for a parallel algorithm for this simple stencil calculation that achieves Θ(<em>n</em>/lg <em>n</em>) parallelism. Argue using notions of work and span that <a id="p789"/>the problem has Θ(<em>n</em>) inherent parallelism. Unfortunately, simple fork-join parallelism does not let you achieve this maximal parallelism.</p>
</section>
<section title="26-6 Randomized parallel algorithms">
<p class="level2"><strong><em>26-6     Randomized parallel algorithms</em></strong></p>
<p class="noindent">Like serial algorithms, parallel algorithms can employ random-number generators. This problem explores how to adapt the measures of work, span, and parallelism to handle the expected behavior of randomized task-parallel algorithms. It also asks you to design and analyze a parallel algorithm for randomized quicksort.</p>
<p class="nl"><strong><em>a.</em></strong> Explain how to modify the work law (26.2), span law (26.3), and greedy scheduler bound (26.4) to work with expectations when <em>T</em><sub><em>P</em></sub>, <em>T</em><sub>1</sub>, and <em>T</em><sub>∞</sub>are all random variables.</p>
<p class="nl"><strong><em>b.</em></strong> Consider a randomized parallel algorithm for which 1% of the time, <em>T</em><sub>1</sub> = 10<sup>4</sup> and <em>T</em><sub>10,000</sub> = 1, but for the remaining 99% of the time, <em>T</em><sub>1</sub> = <em>T</em><sub>10,000</sub> = 10<sup>9</sup>. Argue that the <strong><em><span class="blue">speedup</span></em></strong> of a randomized parallel algorithm should be defined as <em>E</em>[<em>T</em><sub>1</sub>]/<em>E</em>[<em>T</em><sub><em>P</em></sub>], rather than <em>E</em>[<em>T</em><sub>1</sub>/<em>T</em><sub><em>P</em></sub>].</p>
<p class="nl"><strong><em>c.</em></strong> Argue that the <strong><em><span class="blue">parallelism</span></em></strong> of a randomized task-parallel algorithm should be defined as the ratio <em>E</em>[<em>T</em><sub>1</sub>]/<em>E</em>[<em>T</em><sub>∞</sub>].</p>
<p class="nl"><strong><em>d.</em></strong> Parallelize the R<small>ANDOMIZED</small>-Q<small>UICKSORT</small> algorithm on page 192 by using recursive spawning to produce P-R<small>ANDOMIZED</small>-Q<small>UICKSORT</small>. (Do not parallelize R<small>ANDOMIZED</small>-P<small>ARTITION</small>.)</p>
<p class="nl"><strong><em>e.</em></strong> Analyze your parallel algorithm for randomized quicksort. (<em>Hint:</em> Review the analysis of R<small>ANDOMIZED</small>-S<small>ELECT</small> on page 230.)</p>
<p class="nl"><strong><em>f.</em></strong> Parallelize R<small>ANDOMIZED</small>-S<small>ELECT</small> on page 230. Make your implementation as parallel as possible. Analyze your algorithm. (<em>Hint:</em> Use the partitioning algorithm from Exercise 26.3-3.)</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-156"><strong>Chapter notes</strong></p>
<p class="noindent">Parallel computers and algorithmic models for parallel programming have been around in various forms for years. Prior editions of this book included material on sorting networks and the PRAM (Parallel Random-Access Machine) model. The data-parallel model [<a epub:type="noteref" href="bibliography001.xhtml#endnote_58">58</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_217">217</a>] is another popular algorithmic programming model, which features operations on vectors and matrices as primitives. The notion of sequential consistency is due to Lamport [<a epub:type="noteref" href="bibliography001.xhtml#endnote_275">275</a>].</p>
<p>Graham [<a epub:type="noteref" href="bibliography001.xhtml#endnote_197">197</a>] and Brent [<a epub:type="noteref" href="bibliography001.xhtml#endnote_71">71</a>] showed that there exist schedulers achieving the bound of Theorem 26.1. Eager, Zahorjan, and Lazowska [<a epub:type="noteref" href="bibliography001.xhtml#endnote_129">129</a>] showed that <a id="p790"/>any greedy scheduler achieves this bound and proposed the methodology of using work and span (although not by those names) to analyze parallel algorithms. Blelloch [<a epub:type="noteref" href="bibliography001.xhtml#endnote_57">57</a>] developed an algorithmic programming model based on work and span (which he called “depth”) for data-parallel programming. Blumofe and Leiserson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_63">63</a>] gave a distributed scheduling algorithm for task-parallel computations based on randomized “work-stealing” and showed that it achieves the bound <em>E</em>[<em>T</em><sub><em>P</em></sub>] ≤ <em>T</em><sub>1</sub>/<em>P</em> + <em>O</em>(<em>T</em><sub>∞</sub>). Arora, Blumofe, and Plaxton [<a epub:type="noteref" href="bibliography001.xhtml#endnote_20">20</a>] and Blelloch, Gibbons, and Matias [<a epub:type="noteref" href="bibliography001.xhtml#endnote_61">61</a>] also provided provably good algorithms for scheduling task-parallel computations. The recent literature contains many algorithms and strategies for scheduling parallel programs.</p>
<p>The parallel pseudocode and programming model were influenced by Cilk [<a epub:type="noteref" href="bibliography001.xhtml#endnote_290">290</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_291">291</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_383">383</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_396">396</a>]. The open-source project OpenCilk (<a href="http://www.opencilk.org">www.opencilk.org</a>) provides Cilk programming as an extension to the C and C++ programming languages. All of the parallel algorithms in this chapter can be coded straightforwardly in Cilk.</p>
<p>Concerns about nondeterministic parallel programs were expressed by Lee [<a epub:type="noteref" href="bibliography001.xhtml#endnote_281">281</a>] and Bocchino, Adve, Adve, and Snir [<a epub:type="noteref" href="bibliography001.xhtml#endnote_64">64</a>]. The algorithms literature contains many algorithmic strategies (see, for example, [<a epub:type="noteref" href="bibliography001.xhtml#endnote_60">60</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_85">85</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_118">118</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_140">140</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_160">160</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_282">282</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_283">283</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_412">412</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_461">461</a>]) for detecting races and extending the fork-join model to avoid or safely embrace various kinds of nondeterminism. Blelloch, Fineman, Gibbons, and Shun [<a epub:type="noteref" href="bibliography001.xhtml#endnote_59">59</a>] showed that deterministic parallel algorithms can often be as fast as, or even faster than, their nondeterministic counterparts.</p>
<p>Several of the parallel algorithms in this chapter appeared in unpublished lecture notes by C. E. Leiserson and H. Prokop and were originally implemented in Cilk. The parallel merge-sorting algorithm was inspired by an algorithm due to Akl [<a epub:type="noteref" href="bibliography001.xhtml#endnote_12">12</a>].</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> In mathematics, a projection is an idempotent function, that is, a function <em>f</em> such that <em>f</em> <span class="font1">○</span> <em>f</em> = <em>f</em>. In this case, the function <em>f</em> maps the set <span class="script">P</span> of fork-join programs to the set <span class="script">P</span><sub><em>S</em></sub> ⊂ <span class="script">P</span> of serial programs, which are themselves fork-join programs with no parallelism. For a fork-join program <em>x</em> ∈ <span class="script">P</span>, since we have <em>f</em> (<em>f</em> (<em>x</em>)) = <em>f</em> (<em>x</em>), the serial projection, as we have defined it, is indeed a mathematical projection.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> Also called a <strong><em><span class="blue">computation dag</span></em></strong> in the literature.</p>
</section>
</section>
</div>
</body>
</html>