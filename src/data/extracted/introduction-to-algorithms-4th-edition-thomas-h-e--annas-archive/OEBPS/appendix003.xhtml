<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p1178"/>
<p class="line-c"/>
<section epub:type="backmatter" title="C Counting and Probability">
<p class="chapter-title"><a href="toc.xhtml#app-3"><strong><span class="blue1">C Counting and Probability</span></strong></a></p>
<p class="noindent">This appendix reviews elementary combinatorics and probability theory. If you have a good background in these areas, you may want to skim the beginning of this appendix lightly and concentrate on the later sections. Most of this book’s chapters do not require probability, but for some chapters it is essential.</p>
<p><a href="appendix003.xhtml#Sec_C.1">Section C.1</a> reviews elementary results in counting theory, including standard formulas for counting permutations and combinations. The axioms of probability and basic facts concerning probability distributions form <a href="appendix003.xhtml#Sec_C.2">Section C.2</a>. Random variables are introduced in <a href="appendix003.xhtml#Sec_C.3">Section C.3</a>, along with the properties of expectation and variance. <a href="appendix003.xhtml#Sec_C.4">Section C.4</a> investigates the geometric and binomial distributions that arise from studying Bernoulli trials. The study of the binomial distribution continues in <a href="appendix003.xhtml#Sec_C.5">Section C.5</a>, an advanced discussion of the “tails” of the distribution.</p>
<p class="line1"/>
<section title="C.1 Counting">
<a id="Sec_C.1"/>
<p class="level1" id="h1-225"><a href="toc.xhtml#Rh1-225"><strong>C.1 Counting</strong></a></p>
<p class="noindent">Counting theory tries to answer the question “How many?” without actually enumerating all the choices. For example, you might ask, “How many different <em>n</em>-bit numbers are there?” or “How many orderings of <em>n</em> distinct elements are there?” This section reviews the elements of counting theory. Since some of the material assumes a basic understanding of sets, you might wish to start by reviewing the material in <a href="appendix002.xhtml#Sec_B.1">Section B.1</a>.</p>
<p class="level4"><strong>Rules of sum and product</strong></p>
<p class="noindent">We can sometimes express a set of items that we wish to count as a union of disjoint sets or as a Cartesian product of sets.</p>
<p>The <span class="blue1"><strong><em>rule of sum</em></strong></span> says that the number of ways to choose one element from one of two <em>disjoint</em> sets is the sum of the cardinalities of the sets. That is, if <em>A</em> and <em>B</em> are two finite sets with no members in common, then |<em>A</em> ∪ <em>B</em>| = |<em>A</em>| + |<em>B</em>|, which <a id="p1179"/>follows from equation (B.3) on page 1156. For example, if each position on a car’s license plate is a letter or a digit, then the number of possibilities for each position is 26 + 10 = 36, since there are 26 choices if it is a letter and 10 choices if it is a digit.</p>
<p>The <strong><em><span class="blue1">rule of product</span></em></strong> says that the number of ways to choose an ordered pair is the number of ways to choose the first element times the number of ways to choose the second element. That is, if <em>A</em> and <em>B</em> are two finite sets, then |<em>A</em> × <em>B</em>| = |<em>A</em>|·|<em>B</em>|, which is simply equation (B.4) on page 1157. For example, if an ice-cream parlor offers 28 flavors of ice cream and four toppings, the number of possible sundaes with one scoop of ice cream and one topping is 28 · 4 = 112.</p>
<p class="level4"><strong>Strings</strong></p>
<p class="noindent">A <strong><em><span class="blue1">string</span></em></strong> over a finite set <em>S</em> is a sequence of elements of <em>S</em>. For example, there are eight binary strings of length 3:</p>
<p class="eql">000, 001, 010, 011, 100, 101, 110, 111.</p>
<p class="noindent">(Here we use the shorthand of omitting the angle brackets when denoting a sequence.) We sometimes call a string of length <em>k</em> a <strong><em><span class="blue1">k-string</span></em></strong>. A <strong><em><span class="blue1">substring</span></em></strong> <em>s</em>′ of a string <em>s</em> is an ordered sequence of consecutive elements of <em>s</em>. A <strong><em><span class="blue1">k-substring</span></em></strong> of a string is a substring of length <em>k</em>. For example, 010 is a 3-substring of 01101001 (the 3-substring that begins in position 4), but 111 is not a substring of 01101001.</p>
<p>We can view a <em>k</em>-string over a set <em>S</em> as an element of the Cartesian product <em>S<sup>k</sup></em> of <em>k</em>-tuples, which means that there are |<em>S</em>|<em><sup>k</sup></em> strings of length <em>k</em>. For example, the number of binary <em>k</em>-strings is 2<em><sup>k</sup></em>. Intuitively, to construct a <em>k</em>-string over an <em>n</em>-set, there are <em>n</em> ways to pick the first element; for each of these choices, there are <em>n</em> ways to pick the second element; and so forth <em>k</em> times. This construction leads to the <em>k</em>-fold product <img alt="art" src="images/Art_P1636.jpg"/> as the number of <em>k</em>-strings.</p>
<p class="level4"><strong>Permutations</strong></p>
<p class="noindent">A <strong><em><span class="blue1">permutation</span></em></strong> of a finite set <em>S</em> is an ordered sequence of all the elements of <em>S</em>, with each element appearing exactly once. For example, if <em>S</em> = {<em>a</em>, <em>b</em>, <em>c</em>}, then <em>S</em> has 6 permutations:</p>
<p class="eql"><em>abc</em>, <em>acb</em>, <em>bac</em>, <em>bca</em>, <em>cab</em>, <em>cba</em>.</p>
<p class="noindent">(Again, we use the shorthand of omitting the angle brackets when denoting a sequence.) There are <em>n</em>! permutations of a set of <em>n</em> elements, since there are <em>n</em> ways to choose the first element of the sequence, <em>n</em> − 1 ways for the second element, <em>n</em> − 2 ways for the third, and so on.</p>
<a id="p1180"/>
<p>A <strong><em><span class="blue1">k-permutation</span></em></strong> of <em>S</em> is an ordered sequence of <em>k</em> elements of <em>S</em>, with no element appearing more than once in the sequence. (Thus, an ordinary permutation is an <em>n</em>-permutation of an <em>n</em>-set.) Here are the 2-permutations of the set {<em>a</em>, <em>b</em>, <em>c</em>, <em>d</em>}:</p>
<p class="eql"><em>ab</em>, <em>ac</em>, <em>ad</em>, <em>ba</em>, <em>bc</em>, <em>bd</em>, <em>ca</em>, <em>cb</em>, <em>cd</em>, <em>da</em>, <em>db</em>, <em>dc</em>.</p>
<p class="noindent">The number of <em>k</em>-permutations of an <em>n</em>-set is</p>
<p class="eqr"><img alt="art" src="images/Art_P1637.jpg"/></p>
<p class="noindent">since there are <em>n</em> ways to choose the first element, <em>n</em> − 1 ways to choose the second element, and so on, until <em>k</em> elements are chosen, with the last element chosen from the remaining <em>n</em> − <em>k</em> + 1 elements. For the above example, with <em>n</em> = 4 and <em>k</em> = 2, the formula (C.1) evaluates to 4!/2! = 12, matching the number of 2-permutations listed.</p>
<p class="level4"><strong>Combinations</strong></p>
<p class="noindent">A <strong><em><span class="blue1">k-combination</span></em></strong> of an <em>n</em>-set <em>S</em> is simply a <em>k</em>-subset of <em>S</em>. For example, the 4-set {<em>a, b, c, d</em>} has six 2-combinations:</p>
<p class="eql"><em>ab</em>, <em>ac</em>, <em>ad</em>, <em>bc</em>, <em>bd</em>, <em>cd</em>.</p>
<p class="noindent">(Here we use the shorthand of omitting the braces around each subset.) To construct a <em>k</em>-combination of an <em>n</em>-set, choose <em>k</em> distinct (different) elements from the <em>n</em>-set. The order of selecting the elements does not matter.</p>
<p>We can express the number of <em>k</em>-combinations of an <em>n</em>-set in terms of the number of <em>k</em>-permutations of an <em>n</em>-set. Every <em>k</em>-combination has exactly <em>k</em>! permutations of its elements, each of which is a distinct <em>k</em>-permutation of the <em>n</em>-set. Thus the number of <em>k</em>-combinations of an <em>n</em>-set is the number of <em>k</em>-permutations divided by <em>k</em>!. From equation (C.1), this quantity is</p>
<p class="eqr"><img alt="art" src="images/Art_P1638.jpg"/></p>
<p class="noindent">For <em>k</em> = 0, this formula tells us that the number of ways to choose 0 elements from an <em>n</em>-set is 1 (not 0), since 0! = 1.</p>
<p class="level4"><strong>Binomial coefficients</strong></p>
<p class="noindent">The notation <img alt="art" src="images/Art_P1639.jpg"/> (read “<em>n</em> choose <em>k</em>”) denotes the number of <em>k</em>-combinations of an <em>n</em>-set. Equation (C.2) gives</p>
<p class="eql"><img alt="art" src="images/Art_P1640.jpg"/></p>
<a id="p1181"/>
<p class="noindent">This formula is symmetric in <em>k</em> and <em>n</em> − <em>k</em>:</p>
<p class="eqr"><img alt="art" src="images/Art_P1641.jpg"/></p>
<p class="noindent">These numbers are also known as <strong><em><span class="blue1">binomial coefficients</span></em></strong>, due to their appearance in the <strong><em><span class="blue1">binomial theorem</span></em></strong>:</p>
<p class="eqr"><img alt="art" src="images/Art_P1642.jpg"/></p>
<p class="noindent">where <em>n</em> ∈ <span class="double"><span class="font1">ℕ</span></span> and <em>x</em>, <em>y</em> ∈ <span class="double"><span class="font1">ℝ</span></span>. The right-hand side of equation (C.4) is called the <strong><em><span class="blue1">binomial expansion</span></em></strong> of the left-hand side. A special case of the binomial theorem occurs when <em>x</em> = <em>y</em> = 1:</p>
<p class="eql"><img alt="art" src="images/Art_P1643.jpg"/></p>
<p class="noindent">This formula corresponds to counting the 2<em><sup>n</sup></em> binary <em>n</em>-strings by the number of 1s they contain: <img alt="art" src="images/Art_P1644.jpg"/> binary <em>n</em>-strings contain exactly <em>k</em> 1s, since there are <img alt="art" src="images/Art_P1645.jpg"/> ways to choose <em>k</em> out of the <em>n</em> positions in which to place the 1s.</p>
<p>Many identities involve binomial coefficients. The exercises at the end of this section give you the opportunity to prove a few.</p>
<p class="level4"><strong>Binomial bounds</strong></p>
<p class="noindent">You sometimes need to bound the size of a binomial coefficient. For 1 ≤ <em>k</em> ≤ <em>n</em>, we have the lower bound</p>
<p class="eqr"><img alt="art" src="images/Art_P1646.jpg"/></p>
<p class="noindent">Taking advantage of the inequality <em>k</em>! ≥ (<em>k</em>/<em>e</em>)<em><sup>k</sup></em> derived from Stirling’s approximation (3.25) on page 67, we obtain the upper bounds</p>
<p class="eqr"><img alt="art" src="images/Art_P1647.jpg"/></p>
<a id="p1182"/>
<p class="noindent">For all integers <em>k</em> such that 0 ≤ <em>k</em> ≤ <em>n</em>, you can use induction (see Exercise C.1-12) to prove the bound</p>
<p class="eqr"><img alt="art" src="images/Art_P1648.jpg"/></p>
<p class="noindent">where for convenience we assume that 0<sup>0</sup> = 1. For <em>k</em> = <em>λn</em>, where 0 ≤ <em>λ</em> ≤ 1, we can rewrite this bound as</p>
<p class="eql"><img alt="art" src="images/Art_P1649.jpg"/></p>
<p class="noindent">where</p>
<p class="eqr"><img alt="art" src="images/Art_P1650.jpg"/></p>
<p class="noindent">is the <strong><em><span class="blue1">(binary) entropy function</span></em></strong> and where, for convenience, we assume that 0 lg 0 = 0, so that <em>H</em>(0) = <em>H</em>(1) = 0.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level2"><strong><em>C.1-1</em></strong></p>
<p class="noindent">How many <em>k</em>-substrings does an <em>n</em>-string have? (Consider identical <em>k</em>-substrings at different positions to be different.) How many substrings does an <em>n</em>-string have in total?</p>
<p class="level2"><strong><em>C.1-2</em></strong></p>
<p class="noindent">An <em>n</em>-input, <em>m</em>-output <strong><em><span class="blue1">boolean function</span></em></strong> is a function from {0, 1}<em><sup>n</sup></em> to {0, 1}<em><sup>m</sup></em>. How many <em>n</em>-input, 1-output boolean functions are there? How many <em>n</em>-input, <em>m</em>-output boolean functions are there?</p>
<p class="level2"><strong><em>C.1-3</em></strong></p>
<p class="noindent">In how many ways can <em>n</em> professors sit around a circular conference table? Consider two seatings to be the same if one can be rotated to form the other.</p>
<p class="level2"><strong><em>C.1-4</em></strong></p>
<p class="noindent">In how many ways is it possible to choose three distinct numbers from the set {1, 2, … , 99} so that their sum is even?</p>
<a id="p1183"/>
<p class="level2"><strong><em>C.1-5</em></strong></p>
<p class="noindent">Prove the identity</p>
<p class="eqr"><img alt="art" src="images/Art_P1651.jpg"/></p>
<p class="noindent">for 0 &lt; <em>k</em> ≤ <em>n</em>.</p>
<p class="level2"><strong><em>C.1-6</em></strong></p>
<p class="noindent">Prove the identity</p>
<p class="eql"><img alt="art" src="images/Art_P1652.jpg"/></p>
<p class="noindent">for 0 ≤ <em>k</em> &lt; <em>n</em>.</p>
<p class="level2"><strong><em>C.1-7</em></strong></p>
<p class="noindent">To choose <em>k</em> objects from <em>n</em>, you can make one of the objects distinguished and consider whether the distinguished object is chosen. Use this approach to prove that</p>
<p class="eql"><img alt="art" src="images/Art_P1653.jpg"/></p>
<p class="level2"><strong><em>C.1-8</em></strong></p>
<p class="noindent">Using the result of Exercise C.1-7, make a table for <em>n</em> = 0, 1, … , 6 and 0 ≤ <em>k</em> ≤ <em>n</em> of the binomial coefficients <img alt="art" src="images/Art_P1654.jpg"/> with <img alt="art" src="images/Art_P1655.jpg"/> at the top, <img alt="art" src="images/Art_P1656.jpg"/> and <img alt="art" src="images/Art_P1657.jpg"/> on the next line, then <img alt="art" src="images/Art_P1658.jpg"/>, <img alt="art" src="images/Art_P1659.jpg"/>, and <img alt="art" src="images/Art_P1660.jpg"/>, and so forth. Such a table of binomial coefficients is called <strong><em><span class="blue1">Pascal’s triangle</span></em></strong>.</p>
<p class="level2"><strong><em>C.1-9</em></strong></p>
<p class="noindent">Prove that</p>
<p class="eql"><img alt="art" src="images/Art_P1661.jpg"/></p>
<p class="level2"><strong><em>C.1-10</em></strong></p>
<p class="noindent">Show that for any integers <em>n</em> ≥ 0 and 0 ≤ <em>k</em> ≤ <em>n</em>, the expression <img alt="art" src="images/Art_P1662.jpg"/> achieves its maximum value when <em>k</em> = <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> or <em>k</em> = <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span>.</p>
<a id="p1184"/>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-11</em></strong></p>
<p class="noindent">Argue that for any integers <em>n</em> ≥ 0, <em>j</em> ≥ 0, <em>k</em> ≥ 0, and <em>j</em> + <em>k</em> ≤ <em>n</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1663.jpg"/></p>
<p class="noindent">Provide both an algebraic proof and an argument based on a method for choosing <em>j</em> + <em>k</em> items out of <em>n</em>. Give an example in which equality does not hold.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-12</em></strong></p>
<p class="noindent">Use induction on all integers <em>k</em> such that 0 ≤ <em>k</em> ≤ <em>n</em>/2 to prove inequality (C.7), and use equation (C.3) to extend it to all integers <em>k</em> such that 0 ≤ <em>k</em> ≤ <em>n</em>.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-13</em></strong></p>
<p class="noindent">Use Stirling’s approximation to prove that</p>
<p class="eqr"><img alt="art" src="images/Art_P1664.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-14</em></strong></p>
<p class="noindent">By differentiating the entropy function <em>H</em>(<em>λ</em>), show that it achieves its maximum value at <em>λ</em> = 1/2. What is <em>H</em>(1/2)?</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-15</em></strong></p>
<p class="noindent">Show that for any integer <em>n</em> ≥ 0,</p>
<p class="eqr"><img alt="art" src="images/Art_P1665.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.1-16</em></strong></p>
<p class="noindent">Inequality (C.5) provides a lower bound on the binomial coefficient <img alt="art" src="images/Art_P1666.jpg"/>. For small values of <em>k</em>, a stronger bound holds. Prove that</p>
<p class="eqr"><img alt="art" src="images/Art_P1667.jpg"/></p>
<p class="noindent">for <img alt="art" src="images/Art_P1668.jpg"/>.</p>
</section>
<p class="line1"/>
<section title="C.2 Probability">
<a id="Sec_C.2"/>
<p class="level1" id="h1-226"><a href="toc.xhtml#Rh1-226"><strong>C.2 Probability</strong></a></p>
<p class="noindent">Probability is an essential tool for the design and analysis of probabilistic and randomized algorithms. This section reviews basic probability theory.</p>
<a id="p1185"/>
<p>We define probability in terms of a <strong><em><span class="blue1">sample space</span></em></strong> <em>S</em>, which is a set whose elements are called <strong><em><span class="blue1">outcomes</span></em></strong> or <strong><em><span class="blue1">elementary events</span></em></strong>. Think of each outcome as a possible result of an experiment. For the experiment of flipping two distinguishable coins, with each individual flip resulting in a head (<small>H</small>) or a tail (<small>T</small>), you can view the sample space <em>S</em> as consisting of the set of all possible 2-strings over {<small>H</small>, <small>T</small>}:</p>
<p class="eql"><em>S</em> = {<small>HH</small>, <small>HT</small>, <small>TH</small>, <small>TT</small>}.</p>
<p>An <strong><em><span class="blue1">event</span></em></strong> is a subset<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> of the sample space <em>S</em>. For example, in the experiment of flipping two coins, the event of obtaining one head and one tail is {<small>HT</small>, <small>TH</small>}. The event <em>S</em> is called the <strong><em><span class="blue1">certain event</span></em></strong>, and the event ∅ is called the <strong><em><span class="blue1">null event</span></em></strong>. We say that two events <em>A</em> and <em>B</em> are <strong><em><span class="blue1">mutually exclusive</span></em></strong> if <em>A</em> ∩ <em>B</em> = ∅. An outcome <em>s</em> also defines the event {<em>s</em>}, which we sometimes write as just <em>s</em>. By definition, all outcomes are mutually exclusive.</p>
<p class="level4"><strong>Axioms of probability</strong></p>
<p class="noindent">A <strong><em><span class="blue1">probability distribution</span></em></strong> Pr {} on a sample space <em>S</em> is a mapping from events of <em>S</em> to real numbers satisfying the following <strong><em><span class="blue1">probability axioms</span></em></strong>:</p>
<ol class="olnoindent" epub:type="list">
<li>Pr {<em>A</em>} ≥ 0 for any event <em>A</em>.</li>
<li class="litop">Pr {<em>S</em>} = 1.</li>
<li class="litop">Pr {<em>A</em> ∪ <em>B</em>} = Pr {<em>A</em>} + Pr {<em>B</em>} for any two mutually exclusive events <em>A</em> and <em>B</em>. More generally, for any sequence of events <em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, … (finite or countably infinite) that are pairwise mutually exclusive,
<p class="eql"><img alt="art" src="images/Art_P1669.jpg"/></p></li></ol>
<p class="noindent">We call Pr {<em>A</em>} the <strong><em><span class="blue1">probability</span></em></strong> of the event <em>A</em>. Axiom 2 is simply a normalization requirement: there is really nothing fundamental about choosing 1 as the probability of the certain event, except that it is natural and convenient.</p>
<p>Several results follow immediately from these axioms and basic set theory (see <a href="appendix002.xhtml#Sec_B.1">Section B.1</a>). The null event ∅ has probability Pr {∅} = 0. If <em>A</em> ⊆ <em>B</em>, then <a id="p1186"/>Pr {<em>A</em>} ≤ Pr {<em>B</em>}. Using <em><span class="font1">Ā</span></em> to denote the event <em>S</em> − <em>A</em> (the <strong><em><span class="blue1">complement</span></em></strong> of <em>A</em>), we have Pr {<em><span class="font1">Ā</span></em>} = 1 − Pr {<em>A</em>}. For any two events <em>A</em> and <em>B</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1670.jpg"/></p>
<p>In our coin-flipping example, suppose that each of the four outcomes has probability 1/4. Then the probability of getting at least one head is</p>
<table class="table2b">
<tr>
<td class="td2">Pr {<small>HH</small>, <small>HT</small>, <small>TH</small>}</td>
<td class="td2">=</td>
<td class="td2">Pr {<small>HH</small>} + Pr {<small>HT</small>} + Pr {<small>TH</small>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2">3/4.</td>
</tr>
</table>
<p class="noindent">Another way to obtain the same result is to observe that since the probability of getting strictly less than one head is Pr {<small>TT</small>} = 1/4, the probability of getting at least one head is 1 − 1/4 = 3/4.</p>
<p class="level4"><strong>Discrete probability distributions</strong></p>
<p class="noindent">A probability distribution is <strong><em><span class="blue1">discrete</span></em></strong> if it is defined over a finite or countably infinite sample space. Let <em>S</em> be the sample space. Then for any event <em>A</em>,</p>
<p class="eql"><img alt="art" src="images/Art_P1671.jpg"/></p>
<p class="noindent">since outcomes, specifically those in <em>A</em>, are mutually exclusive. If <em>S</em> is finite and every outcome <em>s</em> ∈ <em>S</em> has probability Pr {<em>s</em>} = 1/|<em>S</em>|, then we have the <strong><em><span class="blue1">uniform probability distribution</span></em></strong> on <em>S</em>. In such a case the experiment is often described as “picking an element of <em>S</em> at random.”</p>
<p>As an example, consider the process of flipping a <strong><em><span class="blue1">fair coin</span></em></strong>, one for which the probability of obtaining a head is the same as the probability of obtaining a tail, that is, 1/2. Flipping the coin <em>n</em> times gives the uniform probability distribution defined on the sample space <em>S</em> = {<small>H</small>, <small>T</small>}<em><sup>n</sup></em>, a set of size 2<em><sup>n</sup></em>. We can represent each outcome in <em>S</em> as a string of length <em>n</em> over {<small>H</small>, <small>T</small>}, with each string occurring with probability 1/2<em><sup>n</sup></em>. The event <em>A</em> = {exactly <em>k</em> heads and exactly <em>n</em> − <em>k</em> tails occur} is a subset of <em>S</em> of size <img alt="art" src="images/Art_P1672.jpg"/>, since <img alt="art" src="images/Art_P1673.jpg"/> strings of length <em>n</em> over {<small>H</small>, <small>T</small>} contain exactly <em>k</em><small>H</small>’s. The probability of event <em>A</em> is thus <img alt="art" src="images/Art_P1674.jpg"/>.</p>
<p class="level4"><strong>Continuous uniform probability distribution</strong></p>
<p class="noindent">The continuous uniform probability distribution is an example of a probability distribution in which not all subsets of the sample space are considered to be events. The continuous uniform probability distribution is defined over a closed interval [<em>a</em>, <em>b</em>] of the reals, where <em>a</em> &lt; <em>b</em>. The intuition is that each point in the interval [<em>a</em>, <em>b</em>] should be “equally likely.” Because there are an uncountable number <a id="p1187"/>of points, however, if all points had the same finite, positive probability, axioms 2 and 3 would not be simultaneously satisfied. For this reason, we’d like to associate a probability only with <em>some</em> of the subsets of <em>S</em> in such a way that the axioms are satisfied for these events.</p>
<p>For any closed interval [<em>c</em>, <em>d</em>], where <em>a</em> ≤ <em>c</em> ≤ <em>d</em> ≤ <em>b</em>, the <strong><em><span class="blue1">continuous uniform probability distribution</span></em></strong> defines the probability of the event [<em>c</em>, <em>d</em>] to be</p>
<p class="eql"><img alt="art" src="images/Art_P1675.jpg"/></p>
<p class="noindent">Letting <em>c</em> = <em>d</em> gives that the probability of a single point is 0. Removing the endpoints [<em>c</em>, <em>c</em>] and [<em>d</em>, <em>d</em>] of an interval [<em>c</em>, <em>d</em>] results in the open interval (<em>c</em>, <em>d</em>). Since [<em>c</em>, <em>d</em>] = [<em>c</em>, <em>c</em>] ∪ (<em>c</em>, <em>d</em>) ∪ [<em>d</em>, <em>d</em>], axiom 3 gives Pr {[<em>c</em>, <em>d</em>]} = Pr {(<em>c</em>, <em>d</em>)}. Generally, the set of events for the continuous uniform probability distribution contains any subset of the sample space [<em>a</em>, <em>b</em>] that can be obtained by a finite or countable union of open and closed intervals, as well as certain more complicated sets.</p>
<p class="level4"><strong>Conditional probability and independence</strong></p>
<p class="noindent">Sometimes you have some prior partial knowledge about the outcome of an experiment. For example, suppose that a friend has flipped two fair coins and has told you that at least one of the coins showed a head. What is the probability that both coins are heads? The information given eliminates the possibility of two tails. The three remaining outcomes are equally likely, and so you infer that each occurs with probability 1/3. Since only one of these outcomes shows two heads, the answer is 1/3.</p>
<p>Conditional probability formalizes the notion of having prior partial knowledge of the outcome of an experiment. The <strong><em><span class="blue1">conditional probability</span></em></strong> of an event <em>A</em> given that another event <em>B</em> occurs is defined to be</p>
<p class="eqr"><img alt="art" src="images/Art_P1676.jpg"/></p>
<p class="noindent">whenever Pr {<em>B</em>} ≠ 0. (Read “Pr {<em>A</em> | <em>B</em>}” as “the probability of <em>A</em> given <em>B</em>.”) The idea behind equation (C.16) is that since we are given that event <em>B</em> occurs, the event that <em>A</em> also occurs is <em>A</em> ∩ <em>B</em>. That is, <em>A</em> ∩ <em>B</em> is the set of outcomes in which both <em>A</em> and <em>B</em> occur. Because the outcome is one of the elementary events in <em>B</em>, we normalize the probabilities of all the elementary events in <em>B</em> by dividing them by Pr {<em>B</em>}, so that they sum to 1. The conditional probability of <em>A</em> given <em>B</em> is, therefore, the ratio of the probability of event <em>A</em> ∩ <em>B</em> to the probability of event <em>B</em>. In the example above, <em>A</em> is the event that both coins are heads, and <em>B</em> is the event that at least one coin is a head. Thus, Pr {<em>A</em> | <em>B</em>} = (1/4)/(3/4) = 1/3.</p>
<a id="p1188"/>
<p>Two events are <strong><em><span class="blue1">independent</span></em></strong> if</p>
<p class="eqr"><img alt="art" src="images/Art_P1677.jpg"/></p>
<p class="noindent">which is equivalent, if Pr {<em>B</em>} ≠ 0, to the condition</p>
<p class="eql">Pr {<em>A</em> | <em>B</em>} = Pr {<em>A</em>}.</p>
<p class="noindent">For example, suppose that you flip two fair coins and that the outcomes are independent. Then the probability of two heads is (1/2)(1/2) = 1/4. Now suppose that one event is that the first coin comes up heads and the other event is that the coins come up differently. Each of these events occurs with probability 1/2, and the probability that both events occur is 1/4. Thus, according to the definition of independence, the events are independent—even though you might think that both events depend on the first coin. Finally, suppose that the coins are welded together so that they both fall heads or both fall tails and that the two possibilities are equally likely. Then the probability that each coin comes up heads is 1/2, but the probability that they both come up heads is 1/2 ≠ (1/2)(1/2). Consequently, the event that one comes up heads and the event that the other comes up heads are not independent.</p>
<p>A collection <em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, … , <em>A<sub>n</sub></em> of events is said to be <strong><em><span class="blue1">pairwise independent</span></em></strong> if</p>
<p class="eql">Pr {<em>A<sub>i</sub></em> ∩ <em>A<sub>j</sub></em> } = Pr {<em>A<sub>i</sub></em>} Pr {<em>A<sub>j</sub></em>}</p>
<p class="noindent">for all 1 ≤ <em>i</em> &lt; <em>j</em> ≤ <em>n</em>. We say that the events of the collection are <strong><em><span class="blue1">(mutually) independent</span></em></strong> if every <em>k</em>-subset <img alt="art" src="images/Art_P1678.jpg"/> of the collection, where 2 ≤ <em>k</em> ≤ <em>n</em> and 1 ≤ <em>i</em><sub>1</sub> &lt; <em>i</em><sub>2</sub> &lt; <span class="font1">⋯</span> &lt; <em>i<sub>k</sub></em> ≤ <em>n</em>, satisfies</p>
<p class="eql"><img alt="art" src="images/Art_P1679.jpg"/></p>
<p class="noindent">For example, suppose that you flip two fair coins. Let <em>A</em><sub>1</sub> be the event that the first coin is heads, let <em>A</em><sub>2</sub> be the event that the second coin is heads, and let <em>A</em><sub>3</sub> be the event that the two coins are different. Then,</p>
<table class="table2b">
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>1</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/2,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>2</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/2,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>3</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/2,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>1</sub> ∩ <em>A</em><sub>2</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/4,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>1</sub> ∩ <em>A</em><sub>3</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/4,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>2</sub> ∩ <em>A</em><sub>3</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">1/4,</p></td>
</tr>
<tr>
<td class="td2"><p class="right">Pr {<em>A</em><sub>1</sub> ∩ <em>A</em><sub>2</sub> ∩ <em>A</em><sub>3</sub>}</p></td>
<td class="td2"><p class="center">=</p></td>
<td class="td2"><p class="right">0.</p></td>
</tr>
</table>
<p class="noindent">Since for 1 ≤ <em>i</em> &lt; <em>j</em> ≤ 3, we have Pr {<em>A<sub>i</sub></em> ∩ <em>A<sub>j</sub></em> } = Pr {<em>A<sub>i</sub></em>} Pr {<em>A<sub>j</sub></em>} = 1/4, the events <em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, and <em>A</em><sub>3</sub> are pairwise independent. The events are not mutually independent, however, because Pr {<em>A</em><sub>1</sub> ∩ <em>A</em><sub>2</sub> ∩ <em>A</em><sub>3</sub>} = 0 and Pr {<em>A</em><sub>1</sub>} Pr {<em>A</em><sub>2</sub>} Pr {<em>A</em><sub>3</sub>} = 1/8 ≠ 0.</p>
<a id="p1189"/>
<p class="level4"><strong>Bayes’s theorem</strong></p>
<p class="noindent">From the definition (C.16) of conditional probability and the commutative law <em>A</em> ∩ <em>B</em> = <em>B</em> ∩ <em>A</em>, it follows that for two events <em>A</em> and <em>B</em>, each with nonzero probability,</p>
<p class="eqr"><img alt="art" src="images/Art_P1680.jpg"/></p>
<p class="noindent">Solving for Pr {<em>A</em> | <em>B</em>}, we obtain</p>
<p class="eqr"><img alt="art" src="images/Art_P1681.jpg"/></p>
<p class="noindent">which is known as <strong><em><span class="blue1">Bayes’s theorem</span></em></strong>. The denominator Pr {<em>B</em>} is a normalizing constant, which we can reformulate as follows. Since <em>B</em> = (<em>B</em> ∩ <em>A</em>) ∪ (<em>B</em> ∩ <em><span class="font1">Ā</span></em>), and since <em>B</em> ∩ <em>A</em> and <em>B</em> ∩ <em><span class="font1">Ā</span></em> are mutually exclusive events,</p>
<table class="table2b">
<tr>
<td class="td2">Pr {<em>B</em>}</td>
<td class="td2"><p class="center">=</p></td>
<td class="td2">Pr {<em>B</em> ∩ <em>A</em>} + Pr {<em>B</em> ∩ <em><span class="font1">Ā</span></em>}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center">=</p></td>
<td class="td2">Pr {<em>A</em>} Pr {<em>B</em> | <em>A</em>} + Pr {<em><span class="font1">Ā</span></em>} Pr {<em>B</em> | <em><span class="font1">Ā</span></em>}.</td>
</tr>
</table>
<p class="noindent">Substituting into equation (C.19) produces an equivalent form of Bayes’s theorem:</p>
<p class="eqr"><img alt="art" src="images/Art_P1682.jpg"/></p>
<p>Bayes’s theorem can simplify the computing of conditional probabilities. For example, suppose that you have a fair coin and a biased coin that always comes up heads. Run an experiment consisting of three independent events: choose one of the two coins at random, flip that coin once, and then flip it again. Suppose that the coin you have chosen comes up heads both times. What is the probability that it’s the biased coin?</p>
<p>Bayes’s theorem solves this problem. Let <em>A</em> be the event that you choose the biased coin, and let <em>B</em> be the event that the chosen coin comes up heads both times. We wish to determine Pr {<em>A</em> | <em>B</em>}, knowing that Pr {<em>A</em>} = 1/2, Pr {<em>B</em> | <em>A</em>} = 1, Pr {<em><span class="font1">Ā</span></em>} = 1/2, and Pr {<em>B</em> | <em><span class="font1">Ā</span></em> = 1/4. Thus we have</p>
<p class="eql"><img alt="art" src="images/Art_P1683.jpg"/></p>
<a id="p1190"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level2"><strong><em>C.2-1</em></strong></p>
<p class="noindent">Professor Rosencrantz flips a fair coin twice. Professor Guildenstern flips a fair coin once. What is the probability that Professor Rosencrantz obtains strictly more heads than Professor Guildenstern?</p>
<p class="level2"><strong><em>C.2-2</em></strong></p>
<p class="noindent">Prove <strong><em><span class="blue1">Boole’s inequality</span></em></strong>: For any finite or countably infinite sequence of events <em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, …,</p>
<p class="eqr"><img alt="art" src="images/Art_P1684.jpg"/></p>
<p class="level2"><strong><em>C.2-3</em></strong></p>
<p class="noindent">You shuffle a deck of 10 cards, each bearing a distinct number from 1 to 10, in order to mix the cards thoroughly. You then remove three cards, one at a time, from the deck. What is the probability that the three cards you select are in sorted (increasing) order?</p>
<p class="level2"><strong><em>C.2-4</em></strong></p>
<p class="noindent">Prove that</p>
<p class="eql">Pr {<em>A</em> | <em>B</em>} + Pr {<em><span class="font1">Ā</span></em> | <em>B</em>} = 1.</p>
<p class="level2"><strong><em>C.2-5</em></strong></p>
<p class="noindent">Prove that for any collection of events <em>A</em><sub>1</sub>, <em>A</em><sub>2</sub>, … , <em>A<sub>n</sub></em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1685.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.2-6</em></strong></p>
<p class="noindent">Show how to construct a set of <em>n</em> events that are pairwise independent but such that no subset of <em>k</em> &gt; 2 of them is mutually independent.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.2-7</em></strong></p>
<p class="noindent">Two events <em>A</em> and <em>B</em> are <strong><em><span class="blue1">conditionally independent</span></em></strong>, given <em>C</em>, if</p>
<p class="eql">Pr {<em>A</em> ∩ <em>B</em> | <em>C</em>} = Pr {<em>A</em> | <em>C</em>} · Pr {<em>B</em> | <em>C</em>}.</p>
<p class="noindent">Give a simple but nontrivial example of two events that are not independent but are conditionally independent given a third event.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.2-8</em></strong></p>
<p class="noindent">Professor Gore teaches a music class on rhythm in which three students—Jeff, Tim, and Carmine—are in danger of failing. Professor Gore tells the three that one of <a id="p1191"/>them will pass the course and the other two will fail. Carmine asks Professor Gore privately which of Jeff and Tim will fail, arguing that since he already knows at least one of them will fail, the professor won’t be revealing any information about Carmine’s outcome. In a breach of privacy law, Professor Gore tells Carmine that Jeff will fail. Carmine feels somewhat relieved now, figuring that either he or Tim will pass, so that his probability of passing is now 1/2. Is Carmine correct, or is his chance of passing still 1/3? Explain.</p>
</section>
<p class="line1"/>
<section title="C.3 Discrete random variables">
<a id="Sec_C.3"/>
<p class="level1" id="h1-227"><a href="toc.xhtml#Rh1-227"><strong>C.3 Discrete random variables</strong></a></p>
<p class="noindent">A <strong><em><span class="blue1">(discrete) random variable</span></em></strong> <em>X</em> is a function from a finite or countably infinite sample space <em>S</em> to the real numbers. It associates a real number with each possible outcome of an experiment, which allows us to work with the probability distribution induced on the resulting set of numbers. Random variables can also be defined for uncountably infinite sample spaces, but they raise technical issues that are unnecessary to address for our purposes. Therefore we’ll assume that random variables are discrete.</p>
<p>For a random variable <em>X</em> and a real number <em>x</em>, we define the event <em>X</em> = <em>x</em> to be {<em>s</em> ∈ <em>S</em> : <em>X</em>(<em>s</em>) = <em>x</em>}, and thus</p>
<p class="eql"><img alt="art" src="images/Art_P1686.jpg"/></p>
<p class="noindent">The function</p>
<p class="eql"><em>f</em>(<em>x</em>) = Pr {<em>X</em> = <em>x</em>}</p>
<p class="noindent">is the <strong><em><span class="blue1">probability density function</span></em></strong> of the random variable <em>X</em>. From the probability axioms, Pr {<em>X</em> = <em>x</em>} ≥ 0 and ∑<em><sub>x</sub></em> Pr {<em>X</em> = <em>x</em>} = 1.</p>
<p>As an example, consider the experiment of rolling a pair of ordinary, 6-sided dice. There are 36 possible outcomes in the sample space. Assume that the probability distribution is uniform, so that each outcome <em>s</em> ∈ <em>S</em> is equally likely: Pr {<em>s</em>} = 1/36. Define the random variable <em>X</em> to be the <em>maximum</em> of the two values showing on the dice. We have Pr {<em>X</em> = 3} = 5/36, since <em>X</em> assigns a value of 3 to 5 of the 36 possible outcomes, namely, (1, 3), (2, 3), (3, 3), (3, 2), and (3, 1).</p>
<p>We can define several random variables on the same sample space. If <em>X</em> and <em>Y</em> are random variables, the function</p>
<p class="eql"><em>f</em>(<em>x</em>, <em>y</em>) = Pr {<em>X</em> = <em>x</em> and <em>Y</em> = <em>y</em>}</p>
<p class="noindent">is the <strong><em><span class="blue1">joint probability density function</span></em></strong> of <em>X</em> and <em>Y</em>. For a fixed value <em>y</em>,</p>
<a id="p1192"/>
<p class="eql"><img alt="art" src="images/Art_P1687.jpg"/></p>
<p class="noindent">and similarly, for a fixed value <em>x</em>,</p>
<p class="eql"><img alt="art" src="images/Art_P1688.jpg"/></p>
<p class="noindent">Using the definition (C.16) of conditional probability on page 1187, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1689.jpg"/></p>
<p class="noindent">We define two random variables <em>X</em> and <em>Y</em> to be <strong><em><span class="blue1">independent</span></em></strong> if for all <em>x</em> and <em>y</em>, the events <em>X</em> = <em>x</em> and <em>Y</em> = <em>y</em> are independent or, equivalently, if for all <em>x</em> and <em>y</em>, we have Pr {<em>X</em> = <em>x</em> and <em>Y</em> = <em>y</em>} = Pr {<em>X</em> = <em>x</em>} Pr {<em>Y</em> = <em>y</em>}.</p>
<p>Given a set of random variables defined over the same sample space, we can define new random variables as sums, products, or other functions of the original variables.</p>
<p class="level4"><strong>Expected value of a random variable</strong></p>
<p class="noindent">The simplest, and often the most useful, summary of the distribution of a random variable is the “average” of the values it takes on. The <strong><em><span class="blue1">expected value</span></em></strong> (or, synonymously, <strong><em><span class="blue1">expectation</span></em></strong> or <strong><em><span class="blue1">mean</span></em></strong>) of a discrete random variable <em>X</em> is</p>
<p class="eqr"><img alt="art" src="images/Art_P1690.jpg"/></p>
<p class="noindent">which is well defined if the sum is finite or converges absolutely. Sometimes the expectation of <em>X</em> is denoted by <em>μ<sub>X</sub></em> or, when the random variable is apparent from context, simply by <em>μ</em>.</p>
<p>Consider a game in which you flip two fair coins. You earn $3 for each head but lose $2 for each tail. The expected value of the random variable <em>X</em> representing your earnings is</p>
<table class="table2b">
<tr>
<td class="td2">E[<em>X</em>]</td>
<td class="td2"><p class="center">=</p></td>
<td class="td2">6 · Pr {2 <small>H</small>’s} + 1 · Pr {1 <small>H</small>, 1 <small>T</small>} − 4 · Pr {2 <small>T</small>’s}</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center">=</p></td>
<td class="td2">6 · (1/4) + 1 · (1/2) − 4 · (1/4)</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2"><p class="center">=</p></td>
<td class="td2">1.</td>
</tr>
</table>
<p><strong><em><span class="blue1">Linearity of expectation</span></em></strong> says that the expectation of the sum of two random variables is the sum of their expectations, that is,</p>
<p class="eqr"><img alt="art" src="images/Art_P1691.jpg"/></p>
<a id="p1193"/>
<p class="noindent">whenever E[<em>X</em>] and E[<em>Y</em>] are defined. Linearity of expectation applies to a broad range of situations, holding even when <em>X</em> and <em>Y</em> are not independent. It also extends to finite and absolutely convergent summations of expectations. Linearity of expectation is the key property that enables us to perform probabilistic analyses by using indicator random variables (see <a href="chapter005.xhtml#Sec_5.2">Section 5.2</a>).</p>
<p>If <em>X</em> is any random variable, any function <em>g</em>(<em>x</em>) defines a new random variable <em>g</em>(<em>X</em>). If the expectation of <em>g</em>(<em>X</em>) is defined, then</p>
<p class="eql"><img alt="art" src="images/Art_P1692.jpg"/></p>
<p class="noindent">Letting <em>g</em>(<em>x</em>) = <em>ax</em>, we have for any constant <em>a</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1693.jpg"/></p>
<p class="noindent">Consequently, expectations are linear: for any two random variables <em>X</em> and <em>Y</em> and any constant <em>a</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1694.jpg"/></p>
<p>When two random variables <em>X</em> and <em>Y</em> are independent and each has a defined expectation,</p>
<p class="eql"><img alt="art" src="images/Art_P1695.jpg"/></p>
<p class="noindent">In general, when <em>n</em> random variables <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, … , <em>X<sub>n</sub></em> are mutually independent,</p>
<p class="eqr"><img alt="art" src="images/Art_P1696.jpg"/></p>
<p class="noindent">When a random variable <em>X</em> takes on values from the set of natural numbers <span class="double"><span class="font1">ℕ</span></span> = {0, 1, 2, …}, we have a nice formula for its expectation:</p>
<p class="eqr"><img alt="art" src="images/Art_P1697.jpg"/></p>
<a id="p1194"/>
<p class="noindent">since each term Pr {<em>X</em> ≥ <em>i</em>} is added in <em>i</em> times and subtracted out <em>i</em> − 1 times (except Pr {<em>X</em> ≥ 0}, which is added in 0 times and not subtracted out at all).</p>
<p>A function <em>f</em>(<em>x</em>) is <strong><em><span class="blue1">convex</span></em></strong> if</p>
<p class="eqr"><img alt="art" src="images/Art_P1698.jpg"/></p>
<p class="noindent">for all <em>x</em> and <em>y</em> and for all 0 ≤ <em>λ</em> ≤ 1. <strong><em><span class="blue1">Jensen’s inequality</span></em></strong> says that when a convex function <em>f</em>(<em>x</em>) is applied to a random variable <em>X</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1699.jpg"/></p>
<p class="noindent">provided that the expectations exist and are finite.</p>
<p class="level4"><strong>Variance and standard deviation</strong></p>
<p class="noindent">The expected value of a random variable does not express how “spread out” the variable’s values are. For example, consider random variables <em>X</em> and <em>Y</em> for which Pr {<em>X</em> = 1/4} = Pr {<em>X</em> = 3/4} = 1/2 and Pr {<em>Y</em> = 0} = Pr {<em>Y</em> = 1} = 1/2. Then both E[<em>X</em>] and E[<em>Y</em>] are 1/2, yet the actual values taken on by <em>Y</em> are further from the mean than the actual values taken on by <em>X</em>.</p>
<p>The notion of variance mathematically expresses how far from the mean a random variable’s values are likely to be. The <strong><em><span class="blue1">variance</span></em></strong> of a random variable <em>X</em> with mean E[<em>X</em>] is</p>
<p class="eqr"><img alt="art" src="images/Art_P1700.jpg"/></p>
<p class="noindent">To justify the equation E[E<sup>2</sup>[<em>X</em>]] = E<sup>2</sup>[<em>X</em>], note that because E[<em>X</em>] is a real number and not a random variable, so is E<sup>2</sup>[<em>X</em>]. The equation E[<em>X</em>E[<em>X</em>]] = E<sup>2</sup>[<em>X</em>] follows from equation (C.25), with <em>a</em> = E[<em>X</em>]. Rewriting equation (C.31) yields an expression for the expectation of the square of a random variable:</p>
<p class="eqr"><img alt="art" src="images/Art_P1701.jpg"/></p>
<p>The variance of a random variable <em>X</em> and the variance of <em>aX</em> are related (see Exercise C.3-10):</p>
<p class="eql">Var[<em>aX</em>] = <em>a</em><sup>2</sup>Var[<em>X</em>].</p>
<p class="noindent">When <em>X</em> and <em>Y</em> are independent random variables,</p>
<p class="eql">Var[<em>X</em> + <em>Y</em>] = Var[<em>X</em>] + Var[<em>Y</em>].</p>
<a id="p1195"/>
<p class="noindent">In general, if <em>n</em> random variables <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, … , <em>X<sub>n</sub></em> are pairwise independent, then</p>
<p class="eqr"><img alt="art" src="images/Art_P1702.jpg"/></p>
<p class="noindent">The <strong><em><span class="blue1">standard deviation</span></em></strong> of a random variable <em>X</em> is the nonnegative square root of the variance of <em>X</em>. The standard deviation of a random variable <em>X</em> is sometimes denoted <em>σ<sub>X</sub></em> or simply <em>σ</em> when the random variable <em>X</em> is understood from context. With this notation, the variance of <em>X</em> is denoted <em>σ</em><sup>2</sup>.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level2"><strong><em>C.3-1</em></strong></p>
<p class="noindent">You roll two ordinary, 6-sided dice. What is the expectation of the sum of the two values showing? What is the expectation of the maximum of the two values showing?</p>
<p class="level2"><strong><em>C.3-2</em></strong></p>
<p class="noindent">An array <em>A</em>[1 : <em>n</em>] contains <em>n</em> distinct numbers that are randomly ordered, with each permutation of the <em>n</em> numbers being equally likely. What is the expectation of the index of the maximum element in the array? What is the expectation of the index of the minimum element in the array?</p>
<p class="level2"><strong><em>C.3-3</em></strong></p>
<p class="noindent">A carnival game consists of three dice in a cage. A player can bet a dollar on any of the numbers 1 through 6. The cage is shaken, and the payoff is as follows. If the player’s number doesn’t appear on any of the dice, the player loses the dollar. Otherwise, if the player’s number appears on exactly <em>k</em> of the three dice, for <em>k</em> = 1, 2, 3, the player keeps the dollar and wins <em>k</em> more dollars. What is the expected gain from playing the carnival game once?</p>
<p class="level2"><strong><em>C.3-4</em></strong></p>
<p class="noindent">Argue that if <em>X</em> and <em>Y</em> are nonnegative random variables, then</p>
<p class="eql">E[max {<em>X</em>, <em>Y</em>}] ≤ E[<em>X</em>] + E[<em>Y</em>].</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.3-5</em></strong></p>
<p class="noindent">Let <em>X</em> and <em>Y</em> be independent random variables. Prove that <em>f</em>(<em>X</em>) and <em>g</em>(<em>Y</em>) are independent for any choice of functions <em>f</em> and <em>g</em>.</p>
<a id="p1196"/>
<p class="level2"><span class="font1">★</span> <strong><em>C.3-6</em></strong></p>
<p class="noindent">Let <em>X</em> be a nonnegative random variable, and suppose that E[<em>X</em>] is well defined. Prove <strong><em><span class="blue1">Markov’s inequality</span></em></strong>:</p>
<p class="eqr"><img alt="art" src="images/Art_P1703.jpg"/></p>
<p class="noindent">for all <em>t</em> &gt; 0.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.3-7</em></strong></p>
<p class="noindent">Let <em>S</em> be a sample space, and let <em>X</em> and <em>X</em>′ be random variables such that <em>X</em>(<em>s</em>) ≥ <em>X</em>′(<em>s</em>) for all <em>s</em> ∈ <em>S</em>. Prove that for any real constant <em>t</em>,</p>
<p class="eql">Pr {<em>X</em> ≥ <em>t</em>} ≥ Pr {<em>X</em>′ ≥ <em>t</em>}.</p>
<p class="level2"><strong><em>C.3-8</em></strong></p>
<p class="noindent">Which is larger: the expectation of the square of a random variable, or the square of its expectation?</p>
<p class="level2"><strong><em>C.3-9</em></strong></p>
<p class="noindent">Show that for any random variable <em>X</em> that takes on only the values 0 and 1, we have Var[<em>X</em>] = E[<em>X</em>] E [1 − <em>X</em>].</p>
<p class="level2"><strong><em>C.3-10</em></strong></p>
<p class="noindent">Prove that Var[<em>aX</em>] = <em>a</em><sup>2</sup>Var[<em>X</em>] from the definition (C.31) of variance.</p>
</section>
<p class="line1"/>
<section title="C.4 The geometric and binomial distributions">
<a id="Sec_C.4"/>
<p class="level1" id="h1-228"><a href="toc.xhtml#Rh1-228"><strong>C.4 The geometric and binomial distributions</strong></a></p>
<p class="noindent">A <strong><em><span class="blue1">Bernoulli trial</span></em></strong> is an experiment with only two possible outcomes: <strong><em><span class="blue1">success</span></em></strong>, which occurs with probability <em>p</em>, and <strong><em><span class="blue1">failure</span></em></strong>, which occurs with probability <em>q</em> = 1 − <em>p</em>. A coin flip serves as an example where, depending on your point of view, heads equates to success and tails to failure. When we speak of <strong><em><span class="blue1">Bernoulli trials</span></em></strong> collectively, we mean that the trials are mutually independent and, unless we specifically say otherwise, that each has the same probability <em>p</em> for success. Two important distributions arise from Bernoulli trials: the geometric distribution and the binomial distribution.</p>
<p class="level4"><strong>The geometric distribution</strong></p>
<p class="noindent">Consider a sequence of Bernoulli trials, each with a probability <em>p</em> of success and a probability <em>q</em> = 1 − <em>p</em> of failure. How many trials occur before a success? Define the random variable <em>X</em> to be the number of trials needed to obtain a success. Then <em>X</em> has values in the range {1, 2, …}, and for <em>k</em> ≥ 1,</p>
<a id="p1197"/>
<div class="divimage">
<p class="fig-imga" id="Fig_C-1"><img alt="art" src="images/Art_P1704.jpg"/></p>
<p class="caption"><strong>Figure C.1</strong> A geometric distribution with probability <em>p</em> = 1/3 of success and a probability <em>q</em> = 1 − <em>p</em> of failure. The expectation of the distribution is 1/<em>p</em> = 3.</p>
</div>
<p class="eqr"><img alt="art" src="images/Art_P1705.jpg"/></p>
<p class="noindent">since <em>k</em> − 1 failures occur before the first success. A probability distribution satisfying equation (C.35) is said to be a <strong><em><span class="blue1">geometric distribution</span></em></strong>. <a href="#Fig_C-1">Figure C.1</a> illustrates such a distribution.</p>
<p>Assuming that <em>q</em> &lt; 1, we can calculate the expectation of a geometric distribution:</p>
<p class="eqr"><img alt="art" src="images/Art_P1706.jpg"/></p>
<a id="p1198"/>
<p class="noindent">Thus, on average, it takes 1/<em>p</em> trials before a success occurs, an intuitive result. As Exercise C.4-3 asks you to show, the variance is</p>
<p class="eqr"><img alt="art" src="images/Art_P1707.jpg"/></p>
<p>As an example, suppose that you repeatedly roll two dice until you obtain either a seven or an eleven. Of the 36 possible outcomes, 6 yield a seven and 2 yield an eleven. Thus, the probability of success is <em>p</em> = 8/36 = 2/9, and you’d have to roll 1/<em>p</em> = 9/2 = 4.5 times on average to obtain a seven or eleven.</p>
<p class="level4"><strong>The binomial distribution</strong></p>
<p class="noindent">How many successes occur during <em>n</em> Bernoulli trials, where a success occurs with probability <em>p</em> and a failure with probability <em>q</em> = 1 − <em>p</em>? Define the random variable <em>X</em> to be the number of successes in <em>n</em> trials. Then <em>X</em> has values in the range {0, 1, … , <em>n</em>}, and for <em>k</em> = 0, 1, … , <em>n</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1708.jpg"/></p>
<p class="noindent">since there are <img alt="art" src="images/Art_P1709.jpg"/> ways to pick which <em>k</em> of the <em>n</em> trials are successes, and the probability that each occurs is <em>p</em><sup><em>k</em></sup><em>q</em><sup><em>n</em>−<em>k</em></sup>. A probability distribution satisfying equation (C.38) is said to be a <strong><em><span class="blue1">binomial distribution</span></em></strong>. For convenience, we define the family of binomial distributions using the notation</p>
<p class="eqr"><img alt="art" src="images/Art_P1710.jpg"/></p>
<p class="noindent"><a href="#Fig_C-2">Figure C.2</a> illustrates a binomial distribution. The name “binomial” comes from the right-hand side of equation (C.38) being the <em>k</em>th term of the expansion of (<em>p</em> +<em>q</em>)<em><sup>n</sup></em>. Consequently, since <em>p</em> + <em>q</em> = 1, equation (C.4) on page 1181 gives</p>
<p class="eqr"><img alt="art" src="images/Art_P1711.jpg"/></p>
<p class="noindent">as axiom 2 of the probability axioms requires.</p>
<p>We can compute the expectation of a random variable having a binomial distribution from equations (C.9) and (C.40). Let <em>X</em> be a random variable that follows the binomial distribution <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>), and let <em>q</em> = 1 − <em>p</em>. The definition of expectation gives</p>
<a id="p1199"/>
<div class="divimage">
<p class="fig-imga" id="Fig_C-2"><img alt="art" src="images/Art_P1712.jpg"/></p>
<p class="caption"><strong>Figure C.2</strong> The binomial distribution <em>b</em>(<em>k</em>; 15, 1/3) resulting from <em>n</em> = 15 Bernoulli trials, each with probability <em>p</em> = 1/3 of success. The expectation of the distribution is <em>np</em> = 5.</p>
</div>
<p class="eqr"><img alt="art" src="images/Art_P1713.jpg"/></p>
<p>Linearity of expectation produces the same result with substantially less algebra. Let <em>X<sub>i</sub></em> be the random variable describing the number of successes in the <em>i</em>th trial. Then E[<em>X<sub>i</sub></em>] = <em>p</em> · 1 + <em>q</em> · 0 = <em>p</em>, and the expected number of successes for <em>n</em> trials is</p>
<a id="p1200"/>
<p class="eqr"><img alt="art" src="images/Art_P1714.jpg"/></p>
<p>We can use the same approach to calculate the variance of the distribution. By equation (C.31), <img alt="art" src="images/Art_P1715.jpg"/>. Since <em>X<sub>i</sub></em> takes on only the values 0 and 1, we have <img alt="art" src="images/Art_P1716.jpg"/>, which implies <img alt="art" src="images/Art_P1717.jpg"/>. Hence,</p>
<p class="eqr"><img alt="art" src="images/Art_P1718.jpg"/></p>
<p class="noindent">To compute the variance of <em>X</em>, we take advantage of the independence of the <em>n</em> trials. By equation (C.33), we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1719.jpg"/></p>
<p>As <a href="#Fig_C-2">Figure C.2</a> shows, the binomial distribution <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) increases with <em>k</em> until it reaches the mean <em>np</em>, and then it decreases. To prove that the distribution always behaves in this manner, examine the ratio of successive terms:</p>
<p class="eqr"><img alt="art" src="images/Art_P1720.jpg"/></p>
<a id="p1201"/>
<p class="noindent">This ratio is greater than 1 precisely when (<em>n</em> + 1)<em>p</em> − <em>k</em> is positive. Consequently, <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) &gt; <em>b</em>(<em>k</em> − 1; <em>n</em>, <em>p</em>) for <em>k</em> &lt; (<em>n</em> + 1)<em>p</em> (the distribution increases), and <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) &lt; <em>b</em>(<em>k</em> − 1; <em>n</em>, <em>p</em>) for <em>k</em> &gt; (<em>n</em> + 1)<em>p</em> (the distribution decreases). If (<em>n</em> + 1)<em>p</em> is an integer, then for <em>k</em> = (<em>n</em> + 1)<em>p</em>, the ratio <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>)/<em>b</em>(<em>k</em> − 1; <em>n</em>, <em>p</em>) equals 1, so that <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) = <em>b</em>(<em>k</em> − 1; <em>n</em>, <em>p</em>). In this case, the distribution has two maxima: at <em>k</em> = (<em>n</em>+1)<em>p</em> and at <em>k</em>−1 = (<em>n</em>+1)<em>p</em>−1 = <em>np</em>−<em>q</em>. Otherwise, it attains a maximum at the unique integer <em>k</em> that lies in the range <em>np</em> − <em>q</em> &lt; <em>k</em> &lt; (<em>n</em> + 1)<em>p</em>.</p>
<p>The following lemma provides an upper bound on the binomial distribution.</p>
<p class="lem"><strong><em>Lemma C.1</em></strong></p>
<p class="noindent">Let <em>n</em> ≥ 0, let 0 &lt; <em>p</em> &lt; 1, let <em>q</em> = 1 − <em>p</em>, and let 0 ≤ <em>k</em> ≤ <em>n</em>. Then</p>
<p class="eql"><img alt="art" src="images/Art_P1721.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   We have</p>
<p class="eql"><img alt="art" src="images/Art_P1722.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level2"><strong><em>C.4-1</em></strong></p>
<p class="noindent">Verify axiom 2 of the probability axioms for the geometric distribution.</p>
<p class="level2"><strong><em>C.4-2</em></strong></p>
<p class="noindent">How many times on average do you need to flip six fair coins before obtaining three heads and three tails?</p>
<p class="level2"><strong><em>C.4-3</em></strong></p>
<p class="noindent">Show that the variance of the geometric distribution is <em>q</em>/<em>p</em><sup>2</sup>. (<em>Hint:</em> Use Exercise A.1-6 on page 1144.)</p>
<p class="level2"><strong><em>C.4-4</em></strong></p>
<p class="noindent">Show that <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) = <em>b</em>(<em>n</em> − <em>k</em>; <em>n</em>, <em>q</em>), where <em>q</em> = 1 − <em>p</em>.</p>
<a id="p1202"/>
<p class="level2"><strong><em>C.4-5</em></strong></p>
<p class="noindent">Show that the value of the maximum of the binomial distribution <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) is approximately <img alt="art" src="images/Art_P1723.jpg"/>, where <em>q</em> = 1 − <em>p</em>.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.4-6</em></strong></p>
<p class="noindent">Show that the probability of no successes in <em>n</em> Bernoulli trials, each with probability <em>p</em> = 1/<em>n</em> of success, is approximately 1/<em>e</em>. Show that the probability of exactly one success is also approximately 1/<em>e</em>.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.4-7</em></strong></p>
<p class="noindent">Professor Rosencrantz flips a fair coin <em>n</em> times, and so does Professor Guildenstern. Show that the probability that they get the same number of heads is <img alt="art" src="images/Art_P1724.jpg"/>. (<em>Hint:</em> For Professor Rosencrantz, call a head a success, and for Professor Guildenstern, call a tail a success.) Use your argument to verify the identity</p>
<p class="eql"><img alt="art" src="images/Art_P1725.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.4-8</em></strong></p>
<p class="noindent">Show that for 0 ≤ <em>k</em> ≤ <em>n</em>,</p>
<p class="eql"><em>b</em>(<em>k</em>; <em>n</em>, 1/2) ≤ 2<sup><em>n H</em>(<em>k</em>/<em>n</em>)−<em>n</em></sup>,</p>
<p class="noindent">where <em>H</em>(<em>x</em>) is the entropy function (C.8) on page 1182.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.4-9</em></strong></p>
<p class="noindent">Consider <em>n</em> Bernoulli trials, where for <em>i</em> = 1, 2, … , <em>n</em>, the <em>i</em>th trial has probability <em>p<sub>i</sub></em> of success, and let <em>X</em> be the random variable denoting the total number of successes. Let <em>p</em> ≥ <em>p<sub>i</sub></em> for all <em>i</em> = 1, 2, … , <em>n</em>. Prove that for 1 ≤ <em>k</em> ≤ <em>n</em>,</p>
<p class="eql"><img alt="art" src="images/Art_P1726.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.4-10</em></strong></p>
<p class="noindent">Let <em>X</em> be the random variable for the total number of successes in a set <em>A</em> of <em>n</em> Bernoulli trials, where the <em>i</em>th trial has a probability <em>p<sub>i</sub></em> of success, and let <em>X</em>′ be the random variable for the total number of successes in a second set <em>A</em>′ of <em>n</em> Bernoulli trials, where the <em>i</em>th trial has a probability <img alt="art" src="images/Art_P1727.jpg"/> of success. Prove that for 0 ≤ <em>k</em> ≤ <em>n</em>,</p>
<p class="eql">Pr {<em>X</em>′ ≥ <em>k</em>} ≥ Pr {<em>X</em> ≥ <em>k</em>}.</p>
<p class="noindent">(<em>Hint:</em> Show how to obtain the Bernoulli trials in <em>A</em>′ by an experiment involving the trials of <em>A</em>, and use the result of Exercise C.3-7.)</p>
<a id="p1203"/>
</section>
<p class="line1"/>
<section title="⋆ C.5 The tails of the binomial distribution">
<a id="Sec_C.5"/>
<p class="level1" id="h1-229"><a href="toc.xhtml#Rh1-229"><span class="font1">★</span> <strong>C.5 The tails of the binomial distribution</strong></a></p>
<p class="noindent">The probability of having at least, or at most, <em>k</em> successes in <em>n</em> Bernoulli trials, each with probability <em>p</em> of success, is often of more interest than the probability of having exactly <em>k</em> successes. In this section, we investigate the <strong><em><span class="blue1">tails</span></em></strong> of the binomial distribution: the two regions of the distribution <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>) that are far from the mean <em>np</em>. We’ll prove several important bounds on (the sum of all terms in) a tail.</p>
<p>We first provide a bound on the right tail of the distribution <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>). To determine bounds on the left tail, simply invert the roles of successes and failures.</p>
<p class="theo"><strong><em>Theorem C.2</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em>. Let <em>X</em> be the random variable denoting the total number of successes. Then for 0 ≤ <em>k</em> ≤ <em>n</em>, the probability of at least <em>k</em> successes is</p>
<p class="eql"><img alt="art" src="images/Art_P1728.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   For <em>S</em> ⊆ {1, 2, … , <em>n</em>}, let <em>A<sub>S</sub></em> denote the event that the <em>i</em>th trial is a success for every <em>i</em> ∈ <em>S</em>. Since Pr {<em>A<sub>S</sub></em>} = <em>p<sup>k</sup></em>, where |<em>S</em>| = <em>k</em>, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1729.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The following corollary restates the theorem for the left tail of the binomial distribution. In general, we’ll leave it to you to adapt the proofs from one tail to the other.</p>
<p class="cor"><strong><em>Corollary C.3</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em>. If <em>X</em> is the random variable denoting the total number of successes, then for 0 ≤ <em>k</em> ≤ <em>n</em>, the probability of at most <em>k</em> successes is</p>
<a id="p1204"/>
<p class="eql"><img alt="art" src="images/Art_P1730.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Our next bound concerns the left tail of the binomial distribution. Its corollary shows that, far from the mean, the left tail diminishes exponentially.</p>
<p class="theo"><strong><em>Theorem C.4</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em> and failure with probability <em>q</em> = 1 − <em>p</em>. Let <em>X</em> be the random variable denoting the total number of successes. Then for 0 &lt; <em>k</em> &lt; <em>np</em>, the probability of fewer than <em>k</em> successes is</p>
<p class="eql"><img alt="art" src="images/Art_P1731.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   We bound the series <img alt="art" src="images/Art_P1732.jpg"/> by a geometric series using the technique from <a href="appendix001.xhtml#Sec_A.2">Section A.2</a>, page 1147. For <em>i</em> = 1, 2, … , <em>k</em>, equation (C.45) gives</p>
<p class="eql"><img alt="art" src="images/Art_P1733.jpg"/></p>
<p class="noindent">If we let</p>
<p class="eql"><img alt="art" src="images/Art_P1734.jpg"/></p>
<a id="p1205"/>
<p class="noindent">it follows that</p>
<p class="eql">b(<em>i</em> − 1; <em>n</em>, <em>p</em>) &lt; <em>xb</em>(<em>i</em>; <em>n</em>, <em>p</em>)</p>
<p class="noindent">for 0 &lt; <em>i</em> ≤ <em>k</em>. Iteratively applying this inequality <em>k</em> − <em>i</em> times gives</p>
<p class="eql"><em>b</em>(<em>i</em>; <em>n</em>, <em>p</em>) &lt; <em>x</em><sup><em>k</em>−<em>i</em></sup> <em>b</em>(<em>k</em>; <em>n</em>, <em>p</em>)</p>
<p class="noindent">for 0 ≤ <em>i</em> &lt; <em>k</em>, and hence</p>
<p class="eql"><img alt="art" src="images/Art_P1735.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="cor"><strong><em>Corollary C.5</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em> and failure with probability <em>q</em> = 1 − <em>p</em>. Then for 0 &lt; <em>k</em> ≤ <em>np</em>/2, the probability of fewer than <em>k</em> successes is less than half the probability of fewer than <em>k</em> + 1 successes.</p>
<p class="prof"><strong><em>Proof</em></strong>   Because <em>k</em> ≤ <em>np</em>/2, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1736.jpg"/></p>
<p class="noindent">since <em>q</em> ≤ 1. Letting <em>X</em> be the random variable denoting the number of successes, Theorem C.4 and inequality (C.46) imply that the probability of fewer than <em>k</em> successes is</p>
<a id="p1206"/>
<p class="eql"><img alt="art" src="images/Art_P1737.jpg"/></p>
<p class="noindent">Thus we have</p>
<p class="eql"><img alt="art" src="images/Art_P1738.jpg"/></p>
<p class="noindent">since <img alt="art" src="images/Art_P1739.jpg"/>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Bounds on the right tail follow similarly. Exercise C.5-2 asks you to prove them.</p>
<p class="cor"><strong><em>Corollary C.6</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em>. Let <em>X</em> be the random variable denoting the total number of successes. Then for <em>np</em> &lt; <em>k</em> &lt; <em>n</em>, the probability of more than <em>k</em> successes is</p>
<p class="eql"><img alt="art" src="images/Art_P1740.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="cor"><strong><em>Corollary C.7</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where success occurs with probability <em>p</em> and failure with probability <em>q</em> = 1 − <em>p</em>. Then for (<em>np</em> + <em>n</em>)/2 &lt; <em>k</em> &lt; <em>n</em>, the probability of more than <em>k</em> successes is less than half the probability of more than <em>k</em> − 1 successes.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The next theorem considers <em>n</em> Bernoulli trials, each with a probability <em>p<sub>i</sub></em> of success, for <em>i</em> = 1, 2, … , <em>n</em>. As the subsequent corollary shows, we can use the theorem to provide a bound on the right tail of the binomial distribution by setting <em>p<sub>i</sub></em> = <em>p</em> for each trial.</p>
<p class="theo"><strong><em>Theorem C.8</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where in the <em>i</em>th trial, for <em>i</em> = 1, 2, … , <em>n</em>, success occurs with probability <em>p<sub>i</sub></em> and failure occurs with probability <em>q<sub>i</sub></em> = 1 − <em>p<sub>i</sub></em>. Let <em>X</em> be the random variable describing the total number of successes, and let <em>μ</em> = E[<em>X</em>]. Then for <em>r</em> &gt; <em>μ</em>,</p>
<a id="p1207"/>
<p class="eql"><img alt="art" src="images/Art_P1741.jpg"/></p>
<p class="prof"><strong><em>Proof</em></strong>   Since for any <em>α</em> &gt; 0, the function <em>e<sup>αx</sup></em> strictly increases in <em>x</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1742.jpg"/></p>
<p class="noindent">where we will determine <em>α</em> later. Using Markov’s inequality (C.34), we obtain</p>
<p class="eqr"><img alt="art" src="images/Art_P1743.jpg"/></p>
<p>The bulk of the proof consists of bounding E[<em>e</em><sup><em>α</em>(<em>X</em>−<em>μ</em>)</sup>] and substituting a suitable value for <em>α</em> in inequality (C.48). First, we evaluate E[<em>e</em><sup><em>α</em>(<em>X</em>−<em>μ</em>)</sup>]. Using the technique of indicator random variables (see <a href="chapter005.xhtml#Sec_5.2">Section 5.2</a>), let <em>X<sub>i</sub></em> = I {the <em>i</em>th Bernoulli trial is a success} for <em>i</em> = 1, 2, … , <em>n</em>. That is, <em>X<sub>i</sub></em> is the random variable that is 1 if the <em>i</em>th Bernoulli trial is a success and 0 if it is a failure. Thus, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1744.jpg"/></p>
<p class="noindent">and by linearity of expectation,</p>
<p class="eql"><img alt="art" src="images/Art_P1745.jpg"/></p>
<p class="noindent">which implies</p>
<p class="eql"><img alt="art" src="images/Art_P1746.jpg"/></p>
<p class="noindent">To evaluate E[<em>e</em><sup><em>α</em>(<em>X</em>−<em>μ</em>)</sup>], we substitute for <em>X</em> − <em>μ</em>, obtaining</p>
<p class="eql"><img alt="art" src="images/Art_P1747.jpg"/></p>
<p class="noindent">which follows from equation (C.27), since the mutual independence of the random variables <em>X<sub>i</sub></em> implies the mutual independence of the random variables <img alt="art" src="images/Art_P1748.jpg"/> (see Exercise C.3-5). By the definition of expectation,</p>
<a id="p1208"/>
<p class="eqr"><img alt="art" src="images/Art_P1749.jpg"/></p>
<p class="noindent">where exp(<em>x</em>) denotes the exponential function: exp(<em>x</em>) = <em>e<sup>x</sup></em>. (Inequality (C.49) follows from the inequalities <em>α</em> &gt; 0, <em>q<sub>i</sub></em> ≤ 1, <img alt="art" src="images/Art_P1750.jpg"/>, and <img alt="art" src="images/Art_P1751.jpg"/>. The last line follows from inequality (3.14) on page 66.) Consequently,</p>
<p class="eqr"><img alt="art" src="images/Art_P1752.jpg"/></p>
<p class="noindent">since <img alt="art" src="images/Art_P1753.jpg"/>. Therefore, from equation (C.47) and inequalities (C.48) and (C.50), it follows that</p>
<p class="eqr"><img alt="art" src="images/Art_P1754.jpg"/></p>
<p class="noindent">Choosing <em>α</em> = ln(<em>r</em>/<em>μ</em>) (see Exercise C.5-7), we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P1755.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">When applied to Bernoulli trials in which each trial has the same probability of success, Theorem C.8 yields the following corollary bounding the right tail of a binomial distribution.</p>
<p class="cor"><strong><em>Corollary C.9</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where in each trial success occurs with probability <em>p</em> and failure occurs with probability <em>q</em> = 1 − <em>p</em>. Then for <em>r</em> &gt; <em>np</em>,</p>
<a id="p1209"/>
<p class="eql"><img alt="art" src="images/Art_P1756.jpg"/></p>
<p class="noindent"><strong><em>Proof</em></strong>   By equation (C.41), we have <em>μ</em> = E[<em>X</em>] = <em>np</em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-1</em></strong></p>
<p class="noindent">Which is more likely: getting exactly <em>n</em> heads in 2<em>n</em> flips of a fair coin, or <em>n</em> heads in <em>n</em> flips of a fair coin?</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-2</em></strong></p>
<p class="noindent">Prove Corollaries C.6 and C.7.</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-3</em></strong></p>
<p class="noindent">Show that</p>
<p class="eql"><img alt="art" src="images/Art_P1757.jpg"/></p>
<p class="noindent">for all <em>a</em> &gt; 0 and all <em>k</em> such that 0 &lt; <em>k</em> &lt; <em>na</em>/(<em>a</em> + 1).</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-4</em></strong></p>
<p class="noindent">Prove that if 0 &lt; <em>k</em> &lt; <em>np</em>, where 0 &lt; <em>p</em> &lt; 1 and <em>q</em> = 1 − <em>p</em>, then</p>
<p class="eql"><img alt="art" src="images/Art_P1758.jpg"/></p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-5</em></strong></p>
<p class="noindent">Use Theorem C.8 to show that</p>
<p class="eql"><img alt="art" src="images/Art_P1759.jpg"/></p>
<p class="noindent">for <em>r</em> &gt; <em>n</em> − <em>μ</em>. Similarly, use Corollary C.9 to show that</p>
<p class="eql"><img alt="art" src="images/Art_P1760.jpg"/></p>
<p class="noindent">for <em>r</em> &gt; <em>n</em> − <em>np</em>.</p>
<a id="p1210"/>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-6</em></strong></p>
<p class="noindent">Consider a sequence of <em>n</em> Bernoulli trials, where in the <em>i</em>th trial, for <em>i</em> = 1, 2, … , <em>n</em>, success occurs with probability <em>p<sub>i</sub></em> and failure occurs with probability <em>q<sub>i</sub></em> = 1 − <em>p<sub>i</sub></em>. Let <em>X</em> be the random variable describing the total number of successes, and let <em>μ</em> = E[<em>X</em>]. Show that for <em>r</em> ≥ 0,</p>
<p class="eql"><img alt="art" src="images/Art_P1761.jpg"/></p>
<p class="noindent">(<em>Hint:</em> Prove that <img alt="art" src="images/Art_P1762.jpg"/>. Then follow the outline of the proof of Theorem C.8, using this inequality in place of inequality (C.49).)</p>
<p class="level2"><span class="font1">★</span> <strong><em>C.5-7</em></strong></p>
<p class="noindent">Show that choosing <em>α</em> = ln(<em>r</em>/<em>μ</em>) minimizes the right-hand side of inequality (C.51).</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-230"><strong>Problems</strong></p>
<section title="C-1 The Monty Hall problem">
<p class="level2"><strong><em>C-1 The Monty Hall problem</em></strong></p>
<p class="noindent">Imagine that you are a contestant in the 1960s game show <em>Let’s Make a Deal</em>, hosted by emcee Monty Hall. A valuable prize is hidden behind one of three doors and comparatively worthless prizes behind the other two doors. You will win the valuable prize, typically an automobile or other expensive product, if you select the correct door. After you have picked one door, but before the door has been opened, Monty, who knows which door hides the automobile, directs his assistant Carol Merrill to open one of the other doors, revealing a goat (not a valuable prize). He asks whether you would like to stick with your current choice or to switch to the other closed door. What should you do to maximize your chances of winning the automobile and not the other goat?</p>
<p>The answer to this question—stick or switch?—has been heavily debated, in part because the problem setup is ambiguous. We’ll explore different subtle assumptions.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Suppose that your first pick is random, with probability 1/3 of choosing the right door. Moreover, you know that Monty always gives every contestant (and will give you) the opportunity to switch. Prove that it is better to switch than stick. What is your probability of winning the automobile?</p>
<p class="space-break">This answer is the one typically given, even though the original statement of the problem rarely mentions the assumption that Monty <em>always</em> offers the contestant the opportunity to switch. But, as the remainder of this problem will elucidate, your best strategy may be different if this unstated assumption does not hold. In <a id="p1211"/>fact, in the real game show, after a contestant picked a door, Monty sometimes simply asked Carol to open the door that the contestant had chosen.</p>
<p>Let’s model the interactions between you and Monty as a probabilistic experiment, where you both employ randomized strategies. Specifically, after you pick a door, Monty offers you the opportunity to switch with probability <em>p</em><sub>right</sub> if you picked the right door and with probability <em>p</em><sub>wrong</sub> if you picked the wrong door. Given the opportunity to switch, you randomly choose to switch with probability <em>p</em><sub>switch</sub>. For example, if Monty always offers you the opportunity to switch, then his strategy is given by <em>p</em><sub>right</sub> = <em>p</em><sub>wrong</sub> = 1. If you always switch, then your strategy is given by <em>p</em><sub>switch</sub> = 1.</p>
<p>The game can now be viewed as an experiment consisting of five steps:</p>
<p class="nl-1list-d">1. You pick a door at random, choosing the automobile (right) with probability 1/3 or a goat (wrong) with probability 2/3.</p>
<p class="nl-1list-d">2. Carol opens one of the two closed doors, revealing a goat.</p>
<p class="nl-1list-d">3. Monty offers you the opportunity to switch with probability <em>p</em><sub>right</sub> if your choice is right and with probability <em>p</em><sub>wrong</sub> if your choice is wrong.</p>
<p class="nl-1list-d">4. If Monty makes you an offer in step 3, you switch with probability <em>p</em><sub>switch</sub>.</p>
<p class="nl-1list-d">5. Carol opens the door you’ve chosen, revealing either an automobile (you win) or a goat (you lose).</p>
<p class="space-break">Let’s now analyze this game and understand how the choices of <em>p</em><sub>right</sub>, <em>p</em><sub>wrong</sub>, and <em>p</em><sub>switch</sub> influence the probability of winning.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> What are the six outcomes in the sample space for this game? Which outcomes correspond to you winning the automobile? What are the probabilities in terms of <em>p</em><sub>right</sub>, <em>p</em><sub>wrong</sub>, and <em>p</em><sub>switch</sub> of each outcome? Organize your answers into a table.</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Use the results of your table (or other means) to prove that the probability of winning the automobile is</p>
<p class="nl-1list-dp1"><img alt="art" src="images/Art_P1763.jpg"/></p>
<p class="noindent1-top">Suppose that Monty knows the probability <em>p</em><sub>switch</sub> that you switch, and his goal is to minimize your chance of winning.</p>
<p class="nl-1list-d"><strong><em>d.</em></strong> If <em>p</em><sub>switch</sub> &gt; 0 (you switch with a positive probability), what is Monty’s best strategy, that is, his best choice for <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>?</p>
<p class="nl-1list-d"><strong><em>e.</em></strong> If <em>p</em><sub>switch</sub> = 0 (you always stick), argue that all of Monty’s possible strategies are optimal for him.</p>
<a id="p1212"/>
<p class="noindent1-top">Suppose that now Monty’s strategy is fixed, with particular values for <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>.</p>
<p class="nl-1list-d"><strong><em>f.</em></strong> If you know <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>, what is your best strategy for choosing your probability <em>p</em><sub>switch</sub> of switching as a function of <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>?</p>
<p class="nl-1list-d"><strong><em>g.</em></strong> If you don’t know <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>, what choice of <em>p</em><sub>switch</sub> maximizes the minimum probability of winning over all the choices of <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub>?</p>
<p class="space-break">Let’s return to the original problem as stated, where Monty has given you the option of switching, but you have no knowledge of Monty’s possible motivations or strategies.</p>
<p class="nl-1list-d"><strong><em>h.</em></strong> Argue that the conditional probability of winning the automobile given that Monty offers you the opportunity to switch is</p>
<p class="nl-1list-dp1"><img alt="art" src="images/Art_P1764.jpg"/></p>
<p class="nl-1list-dp1">Explain why <em>p</em><sub>right</sub> + 2<em>p</em><sub>wrong</sub> ≠ 0.</p>
<p class="nl-1list-d"><strong><em>i.</em></strong> What is the value of expression (C.52) when <em>p</em><sub>switch</sub> = 1/2? Show that choosing <em>p</em><sub>switch</sub> &lt; 1/2 or <em>p</em><sub>switch</sub> &gt; 1/2 allows Monty to select values for <em>p</em><sub>right</sub> and <em>p</em><sub>wrong</sub> that yield a lower value for expression (C.52) than choosing <em>p</em><sub>switch</sub> = 1/2.</p>
<p class="nl-1list-d"><strong><em>j.</em></strong> Suppose that you don’t know Monty’s strategy. Explain why choosing to switch with probability 1/2 is a good strategy for the original problem as stated. Summarize what you have learned overall from this problem.</p>
</section>
<section title="C-2 Balls and bins">
<p class="level2"><strong><em>C-2 Balls and bins</em></strong></p>
<p class="noindent">This problem investigates the effect of various assumptions on the number of ways of placing <em>n</em> balls into <em>b</em> distinct bins.</p>
<p class="nl-1list-d"><strong><em>a.</em></strong> Suppose that the <em>n</em> balls are distinct and that their order within a bin does not matter. Argue that the number of ways of placing the balls in the bins is <em>b<sup>n</sup></em>.</p>
<p class="nl-1list-d"><strong><em>b.</em></strong> Suppose that the balls are distinct and that the balls in each bin are ordered. Prove that there are exactly (<em>b</em> + <em>n</em> − 1)!/(<em>b</em> − 1)! ways to place the balls in the bins. (<em>Hint:</em> Consider the number of ways of arranging <em>n</em> distinct balls and <em>b</em> − 1 indistinguishable sticks in a row.)</p>
<p class="nl-1list-d"><strong><em>c.</em></strong> Suppose that the balls are identical, and hence their order within a bin does not matter. Show that the number of ways of placing the balls in the bins is <img alt="art" src="images/Art_P1765.jpg"/>. (<em>Hint:</em> Of the arrangements in part (b), how many are repeated if the balls are made identical?)</p>
<a id="p1213"/>
<p class="nl-1list-d"><strong><em>d.</em></strong> Suppose that the balls are identical and that no bin may contain more than one ball, so that <em>n</em> ≤ <em>b</em>. Show that the number of ways of placing the balls is <img alt="art" src="images/Art_P1766.jpg"/>.</p>
<p class="nl-1list-d"><strong><em>e.</em></strong> Suppose that the balls are identical and that no bin may be left empty. Assuming that <em>n</em> ≥ <em>b</em>, show that the number of ways of placing the balls is <img alt="art" src="images/Art_P1767.jpg"/>.</p>
</section>
</section>
<p class="line1"/>
<section title="Appendix notes">
<p class="level1" id="h1-231"><strong>Appendix notes</strong></p>
<p class="noindent">The first general methods for solving probability problems were discussed in a famous correspondence between B. Pascal and P. de Fermat, which began in 1654, and in a book by C. Huygens in 1657. Rigorous probability theory began with the work of J. Bernoulli in 1713 and A. De Moivre in 1730. Further developments of the theory were provided by P.-S. Laplace, S.-D. Poisson, and C. F. Gauss.</p>
<p>Sums of random variables were originally studied by P. L. Chebyshev and A. A. Markov. A. N. Kolmogorov axiomatized probability theory in 1933. Chernoff [<a epub:type="noteref" href="bibliography001.xhtml#endnote_91">91</a>] and Hoeffding [<a epub:type="noteref" href="bibliography001.xhtml#endnote_222">222</a>] provided bounds on the tails of distributions. Seminal work in random combinatorial structures was done by P. Erd<span class="font1">ő</span>s.</p>
<p>Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>] and Liu [<a epub:type="noteref" href="bibliography001.xhtml#endnote_302">302</a>] are good references for elementary combinatorics and counting. Standard textbooks such as Billingsley [<a epub:type="noteref" href="bibliography001.xhtml#endnote_56">56</a>], Chung [<a epub:type="noteref" href="bibliography001.xhtml#endnote_93">93</a>], Drake [<a epub:type="noteref" href="bibliography001.xhtml#endnote_125">125</a>], Feller [<a epub:type="noteref" href="bibliography001.xhtml#endnote_139">139</a>], and Rozanov [<a epub:type="noteref" href="bibliography001.xhtml#endnote_390">390</a>] offer comprehensive introductions to probability.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> For a general probability distribution, there may be some subsets of the sample space <em>S</em> that are not considered to be events. This situation usually arises when the sample space is uncountably infinite. The main requirement for what subsets are events is that the set of events of a sample space must be closed under the operations of taking the complement of an event, forming the union of a finite or countable number of events, and taking the intersection of a finite or countable number of events. Most of the probability distributions we see in this book are over finite or countable sample spaces, and we generally consider all subsets of a sample space to be events. A notable exception is the continuous uniform probability distribution, which we’ll see shortly.</p>
</section>
</section>
</div>
</body>
</html>