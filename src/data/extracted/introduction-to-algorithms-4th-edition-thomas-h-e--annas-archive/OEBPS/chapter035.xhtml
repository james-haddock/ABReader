<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p1104"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="35 Approximation Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-35"><strong><span class="blue1">35        Approximation Algorithms</span></strong></a></p>
<p class="noindent">Many problems of practical significance are NP-complete, yet they are too important to abandon merely because nobody knows how to find an optimal solution in polynomial time. Even if a problem is NP-complete, there may be hope. You have at least three options to get around NP-completeness. First, if the actual inputs are small, an algorithm with exponential running time might be fast enough. Second, you might be able to isolate important special cases that you can solve in polynomial time. Third, you can try to devise an approach to find a <em>near-optimal</em> solution in polynomial time (either in the worst case or the expected case). In practice, near-optimality is often good enough. We call an algorithm that returns near-optimal solutions an <strong><em><span class="blue">approximation algorithm</span></em></strong>. This chapter presents polynomial-time approximation algorithms for several NP-complete problems.</p>
<p class="level4"><strong>Performance ratios for approximation algorithms</strong></p>
<p class="noindent">Suppose that you are working on an optimization problem in which each potential solution has a positive cost, and you want to find a near-optimal solution. Depending on the problem, you could define an optimal solution as one with maximum possible cost or as one with minimum possible cost, which is to say that the problem might be either a maximization or a minimization problem.</p>
<p>We say that an algorithm for a problem has an <strong><em><span class="blue">approximation ratio</span></em></strong> of <em>ρ</em>(<em>n</em>) if, for any input of size <em>n</em>, the cost <em>C</em> of the solution produced by the algorithm is within a factor of <em>ρ</em>(<em>n</em>) of the cost <em>C</em>* of an optimal solution:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1483.jpg"/></p>
<p class="noindent">If an algorithm achieves an approximation ratio of <em>ρ</em>(<em>n</em>), we call it a <strong><span class="blue"><em>ρ</em>(<em>n</em>)<em>-approximation algorithm</em></span></strong>. The definitions of approximation ratio and <em>ρ</em>(<em>n</em>)-approximation algorithm apply to both minimization and maximization problems. For a maximization problem, 0 &lt; <em>C</em> ≤ <em>C</em>*, and the ratio <em>C</em>*/<em>C</em> gives the factor by which <a id="p1105"/>the cost of an optimal solution is larger than the cost of the approximate solution. Similarly, for a minimization problem, 0 &lt; <em>C</em>* ≤ <em>C</em>, and the ratio <em>C</em>/<em>C</em>* gives the factor by which the cost of the approximate solution is larger than the cost of an optimal solution. Because we assume that all solutions have positive cost, these ratios are always well defined. The approximation ratio of an approximation algorithm is never less than 1, since <em>C</em>/<em>C</em>* ≤ 1 implies <em>C</em>*/<em>C</em> ≥ 1. Therefore, a 1-approximation algorithm<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> produces an optimal solution, and an approximation algorithm with a large approximation ratio may return a solution that is much worse than optimal.</p>
<p>For many problems, we know of polynomial-time approximation algorithms with small constant approximation ratios, although for other problems, the best known polynomial-time approximation algorithms have approximation ratios that grow as functions of the input size <em>n</em>. An example of such a problem is the set-cover problem presented in <a href="chapter035.xhtml#Sec_35.3">Section 35.3</a>.</p>
<p>Some polynomial-time approximation algorithms can achieve increasingly better approximation ratios by using more and more computation time. For such problems, you can trade computation time for the quality of the approximation. An example is the subset-sum problem studied in <a href="chapter035.xhtml#Sec_35.5">Section 35.5</a>. This situation is important enough to deserve a name of its own.</p>
<p>An <strong><em><span class="blue">approximation scheme</span></em></strong> for an optimization problem is an approximation algorithm that takes as input not only an instance of the problem, but also a value <em><span class="font1">ϵ</span></em> &gt; 0 such that for any fixed <em><span class="font1">ϵ</span></em>, the scheme is a (1 + <em><span class="font1">ϵ</span></em>)-approximation algorithm. We say that an approximation scheme is a <strong><em><span class="blue">polynomial-time approximation scheme</span></em></strong> if for any fixed <em><span class="font1">ϵ</span></em> &gt; 0, the scheme runs in time polynomial in the size <em>n</em> of its input instance.</p>
<p>The running time of a polynomial-time approximation scheme can increase very rapidly as <em><span class="font1">ϵ</span></em> decreases. For example, the running time of a polynomial-time approximation scheme might be <em>O</em>(<em>n</em><sup>2/<em><span class="font1">ϵ</span></em></sup>). Ideally, if <em><span class="font1">ϵ</span></em> decreases by a constant factor, the running time to achieve the desired approximation should not increase by more than a constant factor (though not necessarily the same constant factor by which <em><span class="font1">ϵ</span></em> decreased).</p>
<p>We say that an approximation scheme is a <strong><em><span class="blue">fully polynomial-time approximation scheme</span></em></strong> if it is an approximation scheme and its running time is polynomial in both 1/<em><span class="font1">ϵ</span></em> and the size <em>n</em> of the input instance. For example, the scheme might have a running time of <em>O</em>((1/<em><span class="font1">ϵ</span></em>)<sup>2</sup><em>n</em><sup>3</sup>). With such a scheme, any constant-factor decrease in <em><span class="font1">ϵ</span></em> comes with a corresponding constant-factor increase in the running time.</p>
<a id="p1106"/>
<p class="level4"><strong>Chapter outline</strong></p>
<p class="noindent">The first four sections of this chapter present some examples of polynomial-time approximation algorithms for NP-complete problems, and the fifth section gives a fully polynomial-time approximation scheme. We begin in <a href="chapter035.xhtml#Sec_35.1">Section 35.1</a> with a study of the vertex-cover problem, an NP-complete minimization problem that has an approximation algorithm with an approximation ratio of 2. <a href="chapter035.xhtml#Sec_35.2">Section 35.2</a> looks at a version of the traveling-salesperson problem in which the cost function satisfies the triangle inequality and presents an approximation algorithm with an approximation ratio of 2. The section also shows that without the triangle inequality, for any constant <em>ρ</em> ≥ 1, a <em>ρ</em>-approximation algorithm cannot exist unless P = NP. <a href="chapter035.xhtml#Sec_35.3">Section 35.3</a> applies a greedy method as an effective approximation algorithm for the set-covering problem, obtaining a covering whose cost is at worst a logarithmic factor larger than the optimal cost. <a href="chapter035.xhtml#Sec_35.4">Section 35.4</a> uses randomization and linear programming to develop two more approximation algorithms. The section first defines the optimization version of 3-CNF satisfiability and gives a simple randomized algorithm that produces a solution with an expected approximation ratio of 8/7. Then <a href="chapter035.xhtml#Sec_35.4">Section 35.4</a> examines a weighted variant of the vertex-cover problem and exhibits how to use linear programming to develop a 2-approximation algorithm. Finally, <a href="chapter035.xhtml#Sec_35.5">Section 35.5</a> presents a fully polynomial-time approximation scheme for the subset-sum problem.</p>
<p class="line1"/>
<section title="35.1 The vertex-cover problem">
<a id="Sec_35.1"/>
<p class="level1" id="h1-206"><a href="toc.xhtml#Rh1-206"><strong>35.1    The vertex-cover problem</strong></a></p>
<p class="noindent"><a href="chapter034.xhtml#Sec_34.5.2">Section 34.5.2</a> defined the vertex-cover problem and proved it NP-complete. Recall that a <strong><em><span class="blue">vertex cover</span></em></strong> of an undirected graph <em>G</em> = (<em>V</em>, <em>E</em>) is a subset <em>V</em>′ ⊆ <em>V</em> such that if (<em>u</em>, <em>v</em>) is an edge of <em>G</em>, then either <em>u</em> ∈ <em>V</em>′ or <em>v</em> ∈ <em>V</em>′ (or both). The size of a vertex cover is the number of vertices in it.</p>
<p>The <strong><em><span class="blue">vertex-cover problem</span></em></strong> is to find a vertex cover of minimum size in a given undirected graph. We call such a vertex cover an <strong><em><span class="blue">optimal vertex cover</span></em></strong>. This problem is the optimization version of an NP-complete decision problem.</p>
<p>Even though nobody knows how to find an optimal vertex cover in a graph <em>G</em> in polynomial time, there is an efficient algorithm to find a vertex cover that is near-optimal. The approximation algorithm A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> on the facing page takes as input an undirected graph <em>G</em> and returns a vertex cover whose size is guaranteed to be no more than twice the size of an optimal vertex cover.</p>
<p><a href="chapter035.xhtml#Fig_35-1">Figure 35.1</a> illustrates how A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> operates on an example graph. The variable <em>C</em> contains the vertex cover being constructed. Line 1 initializes <em>C</em> to the empty set. Line 2 sets <em>E</em>′ to be a copy of the edge set <em>G.E</em> of the graph. The <strong>while</strong> loop of lines 3–6 repeatedly picks an edge (<em>u</em>, <em>v</em>) from <em>E</em>′, adds <a id="p1107"/>its endpoints <em>u</em> and <em>v</em> into <em>C</em>, and deletes all edges in <em>E</em>′ that <em>u</em> or <em>v</em> covers. Finally, line 7 returns the vertex cover <em>C</em>. The running time of this algorithm is <em>O</em>(<em>V</em> + <em>E</em>), using adjacency lists to represent <em>E</em>′.</p>
<div class="pull-quote1">
<p class="box-heading">A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> (<em>G</em>)</p>
<table class="table1a">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>C</em> = Ø</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><em>E</em>′ = <em>G.E</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>while</strong> <em>E</em>′ ≠ Ø</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2">let (<em>u</em>, <em>v</em>) be an arbitrary edge of <em>E</em>′</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><em>C</em> = <em>C</em> ∪ {<em>u, v</em>}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2">remove from <em>E</em>′ edge (<em>u</em>, <em>v</em>) and every edge incident on either <em>u</em> or <em>v</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>C</em></p></td>
</tr>
</table>
</div>
<p class="theo"><strong><em>Theorem 35.1</em></strong></p>
<p class="noindent">A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> is a polynomial-time 2-approximation algorithm.</p>
<p class="prof"><strong><em>Proof</em></strong>   We have already shown that A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> runs in polynomial time.</p>
<p>The set <em>C</em> of vertices that is returned by A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> is a vertex cover, since the algorithm loops until every edge in <em>G.E</em> has been covered by some vertex in <em>C</em>.</p>
<p>To see that A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> returns a vertex cover that is at most twice the size of an optimal cover, let <em>A</em> denote the set of edges that line 4 of A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> picked. In order to cover the edges in <em>A</em>, any vertex cover—in particular, an optimal cover <em>C</em>*—must include at least one endpoint of each edge in <em>A</em>. No two edges in <em>A</em> share an endpoint, since once an edge is picked in line 4, all other edges that are incident on its endpoints are deleted from <em>E</em>′ in line 6. Thus, no two edges in <em>A</em> are covered by the same vertex from <em>C</em>*, meaning that for every vertex in <em>C</em>*, there is at most one edge in <em>A</em>, giving the lower bound</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1484.jpg"/></p>
<p class="noindent">on the size of an optimal vertex cover. Each execution of line 4 picks an edge for which neither of its endpoints is already in <em>C</em>, yielding an upper bound (an exact upper bound, in fact) on the size of the vertex cover returned:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1485.jpg"/></p>
<p class="noindent">Combining equations (35.2) and (35.3) yields</p>
<table class="table2b">
<tr>
<td class="td2">|<em>C</em>|</td>
<td class="td2">=</td>
<td class="td2">2 |<em>A</em>|</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">≤</td>
<td class="td2">2 |<em>C</em>*|,</td>
</tr>
</table>
<p class="noindent">thereby proving the theorem.</p>
<p class="right"><span class="font1">▪</span></p>
<a id="p1108"/>
<div class="divimage">
<p class="fig-imga" id="Fig_35-1"><img alt="art" src="images/Art_P1486.jpg"/></p>
<p class="caption"><strong>Figure 35.1</strong> The operation of A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small>. <strong>(a)</strong> The input graph <em>G</em>, which has 7 vertices and 8 edges. <strong>(b)</strong> The highlighted edge (<em>b</em>, <em>c</em>) is the first edge chosen by A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small>. Vertices <em>b</em> and <em>c</em>, in blue, are added to the set <em>C</em> containing the vertex cover being created. Dashed edges (<em>a</em>, <em>b</em>), (<em>c</em>, <em>e</em>), and (<em>c</em>, <em>d</em>) are removed since they are now covered by some vertex in <em>C</em>. <strong>(c)</strong> Edge (<em>e</em>, <em>f</em>) is chosen, and vertices <em>e</em> and <em>f</em> are added to <em>C</em>. <strong>(d)</strong> Edge (<em>d</em>, <em>g</em>) is chosen, and vertices <em>d</em> and <em>g</em> are added to <em>C</em>. <strong>(e)</strong> The set <em>C</em>, which is the vertex cover produced by A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small>, contains the six vertices <em>b, c, d, e, f, g</em>. <strong>(f)</strong> The optimal vertex cover for this problem contains only three vertices: <em>b</em>, <em>d</em>, and <em>e</em>.</p>
</div>
<p>Let us reflect on this proof. At first, you might wonder how you can possibly prove that the size of the vertex cover returned by A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> is at most twice the size of an optimal vertex cover, when you don’t even know the size of an optimal vertex cover. Instead of requiring that you know the exact size of an optimal vertex cover, you find a lower bound on the size. As Exercise 35.1-2 asks you to show, the set <em>A</em> of edges that line 4 of A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> selects is actually a maximal matching in the graph <em>G</em>. (A <strong><em><span class="blue">maximal matching</span></em></strong> is a matching to which no edges can be added and still have a matching.) The size of a maximal matching is, as we argued in the proof of Theorem 35.1, a lower bound on the size of an optimal vertex cover. The algorithm returns a vertex cover whose size is at most twice the size of the maximal matching <em>A</em>. The approximation ratio comes from relating the size of the solution returned to the lower bound. We will use this methodology in later sections as well.</p>
<a id="p1109"/>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>35.1-1</em></strong></p>
<p class="noindent">Give an example of a graph for which A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> always yields a suboptimal solution.</p>
<p class="level3"><strong><em>35.1-2</em></strong></p>
<p class="noindent">Prove that the set of edges picked in line 4 of A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> forms a maximal matching in the graph <em>G</em>.</p>
<p class="level3"><span class="font1">★</span> <strong><em>35.1-3</em></strong></p>
<p class="noindent">Consider the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its incident edges. Give an example to show that this heuristic does not provide an approximation ratio of 2. (<em>Hint:</em> Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.)</p>
<p class="level3"><strong><em>35.1-4</em></strong></p>
<p class="noindent">Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time.</p>
<p class="level3"><strong><em>35.1-5</em></strong></p>
<p class="noindent">The proof of Theorem 34.12 on page 1084 illustrates that the vertex-cover problem and the NP-complete clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer.</p>
</section>
<p class="line1"/>
<section title="35.2 The traveling-salesperson problem">
<a id="Sec_35.2"/>
<p class="level1" id="h1-207"><a href="toc.xhtml#Rh1-207"><strong>35.2    The traveling-salesperson problem</strong></a></p>
<p class="noindent">The input to the traveling-salesperson problem, introduced in <a href="chapter034.xhtml#Sec_34.5.4">Section 34.5.4</a>, is a complete undirected graph <em>G</em> = (<em>V</em>, <em>E</em>) that has a nonnegative integer cost <em>c</em>(<em>u, v</em>) associated with each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em>. The goal is to find a hamiltonian cycle (a tour) of <em>G</em> with minimum cost. As an extension of our notation, let <em>c</em>(<em>A</em>) denote the total cost of the edges in the subset <em>A</em> ⊆ <em>E</em>:</p>
<p class="eql"><img alt="art" src="images/Art_P1487.jpg"/></p>
<a id="p1110"/>
<p>In many practical situations, the least costly way to go from a place <em>u</em> to a place <em>w</em> is to go directly, with no intermediate steps. Put another way, cutting out an intermediate stop never increases the cost. Such a cost function <em>c</em> satisfies the <strong><em><span class="blue">triangle inequality</span></em></strong>: for all vertices <em>u, v, w</em> ∈ <em>V</em>,</p>
<p class="eql"><em>c</em>(<em>u, w</em>) ≤ <em>c</em>(<em>u, v</em>) + <em>c</em>(<em>v, w</em>).</p>
<p class="space-break">The triangle inequality seems as though it should naturally hold, and it is automatically satisfied in several applications. For example, if the vertices of the graph are points in the plane and the cost of traveling between two vertices is the ordinary euclidean distance between them, then the triangle inequality is satisfied. Furthermore, many cost functions other than euclidean distance satisfy the triangle inequality.</p>
<p>As Exercise 35.2-2 shows, the traveling-salesperson problem is NP-complete even if you require the cost function to satisfy the triangle inequality. Thus, you should not expect to find a polynomial-time algorithm for solving this problem exactly. Your time would be better spent looking for good approximation algorithms.</p>
<p>In <a href="chapter035.xhtml#Sec_35.2.1">Section 35.2.1</a>, we examine a 2-approximation algorithm for the traveling-salesperson problem with the triangle inequality. In <a href="chapter035.xhtml#Sec_35.2.2">Section 35.2.2</a>, we show that without the triangle inequality, a polynomial-time approximation algorithm with a constant approximation ratio does not exist unless P = NP.</p>
<section title="35.2.1 The traveling-salesperson problem with the triangle inequality">
<p class="level2" id="Sec_35.2.1"><strong>35.2.1    The traveling-salesperson problem with the triangle inequality</strong></p>
<p class="noindent">Applying the methodology of the previous section, start by computing a structure—a minimum spanning tree—whose weight gives a lower bound on the length of an optimal traveling-salesperson tour. Then use the minimum spanning tree to create a tour whose cost is no more than twice that of the minimum spanning tree’s weight, as long as the cost function satisfies the triangle inequality. The procedure A<small>PPROX</small>-TSP-T<small>OUR</small> on the next page implements this approach, calling the minimum-spanning-tree algorithm MST-P<small>RIM</small> on page 596 as a subroutine. The parameter <em>G</em> is a complete undirected graph, and the cost function <em>c</em> satisfies the triangle inequality.</p>
<p>Recall from <a href="chapter012.xhtml#Sec_12.1">Section 12.1</a> that a preorder tree walk recursively visits every vertex in the tree, listing a vertex when it is first encountered, before visiting any of its children.</p>
<p><a href="chapter035.xhtml#Fig_35-2">Figure 35.2</a> illustrates the operation of A<small>PPROX</small>-TSP-T<small>OUR</small>. Part (a) of the figure shows a complete undirected graph, and part (b) shows the minimum spanning tree <em>T</em> grown from root vertex <em>a</em> by MST-P<small>RIM</small>. Part (c) shows how a preorder walk of <em>T</em> visits the vertices, and part (d) displays the corresponding tour, which is the tour returned by A<small>PPROX</small>-TSP-T<small>OUR</small>. Part (e) displays an optimal tour, which is about 23% shorter.</p>
<a id="p1111"/>
<div class="divimage">
<p class="fig-imga" id="Fig_35-2"><img alt="art" class="width100" src="images/Art_P1488.jpg"/></p>
<p class="caption"><strong>Figure 35.2</strong> The operation of A<small>PPROX</small>-TSP-T<small>OUR</small>. <strong>(a)</strong> A complete undirected graph. Vertices lie on intersections of integer grid lines. For example, <em>f</em> is one unit to the right and two units up from <em>h</em>. The cost function between two points is the ordinary euclidean distance. <strong>(b)</strong> A minimum spanning tree <em>T</em> of the complete graph, as computed by MST-P<small>RIM</small>. Vertex <em>a</em> is the root vertex. Only edges in the minimum spanning tree are shown. The vertices happen to be labeled in such a way that they are added to the main tree by MST-P<small>RIM</small> in alphabetical order. <strong>(c)</strong> A walk of <em>T</em>, starting at <em>a</em>. A full walk of the tree visits the vertices in the order <em>a, b, c, b, h, b, a, d, e, f, e, g, e, d, a</em>. A preorder walk of <em>T</em> lists a vertex just when it is first encountered, as indicated by the dot next to each vertex, yielding the ordering <em>a, b, c, h, d, e, f, g</em>. <strong>(d)</strong> A tour obtained by visiting the vertices in the order given by the preorder walk, which is the tour <em>H</em> returned by A<small>PPROX</small>-TSP-T<small>OUR</small>. Its total cost is approximately 19.074. <strong>(e)</strong> An optimal tour <em>H</em>* for the original complete graph. Its total cost is approximately 14.715.</p>
</div>
<div class="pull-quote1">
<p class="box-heading">A<small>PPROX</small>-TSP-T<small>OUR</small> (<em>G</em>, <em>c</em>)</p>
<table class="table1a">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="tdph">select a vertex <em>r</em> ∈ <em>G.V</em> to be a “root” vertex</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="tdph">compute a minimum spanning tree <em>T</em> for <em>G</em> from root <em>r</em><br/>using MST-P<small>RIM</small> (<em>G</em>, <em>c, r</em>)</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="tdph">let <em>H</em> be a list of vertices, ordered according to when they are first visited in a preorder tree walk of <em>T</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="tdph"><strong>return</strong> the hamiltonian cycle <em>H</em></p></td>
</tr>
</table>
</div>
<a id="p1112"/>
<p>By Exercise 21.2-2, even with a simple implementation of MST-P<small>RIM</small>, the running time of A<small>PPROX</small>-TSP-T<small>OUR</small> is Θ(<em>V</em> <sup>2</sup>). We now show that if the cost function for an instance of the traveling-salesperson problem satisfies the triangle inequality, then A<small>PPROX</small>-TSP-T<small>OUR</small> returns a tour whose cost is at most twice the cost of an optimal tour.</p>
<p class="theo"><strong><em>Theorem 35.2</em></strong></p>
<p class="noindent">When the triangle inequality holds, A<small>PPROX</small>-TSP-T<small>OUR</small> is a polynomial-time 2-approximation algorithm for the traveling-salesperson problem.</p>
<p class="prof"><strong><em>Proof</em></strong>   We have already seen that A<small>PPROX</small>-TSP-T<small>OUR</small> runs in polynomial time.</p>
<p>Let <em>H</em>* denote an optimal tour for the given set of vertices. Deleting any edge from a tour yields a spanning tree, and each edge cost is nonnegative. Therefore, the weight of the minimum spanning tree <em>T</em> computed in line 2 of A<small>PPROX</small>-TSP-T<small>OUR</small> provides a lower bound on the cost of an optimal tour:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1489.jpg"/></p>
<p class="noindent">A <strong><em><span class="blue">full walk</span></em></strong> of <em>T</em> lists the vertices when they are first visited and also whenever they are returned to after a visit to a subtree. Let’s call this full walk <em>W</em>. The full walk of our example gives the order</p>
<p class="eql"><em>a, b, c, b, h, b, a, d, e, f, e, g, e, d, a.</em></p>
<p class="noindent">Since the full walk traverses every edge of <em>T</em> exactly twice, by extending the definition of the cost <em>c</em> in the natural manner to handle multisets of edges, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1490.jpg"/></p>
<p class="noindent">Inequality (35.4) and equation (35.5) imply that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1491.jpg"/></p>
<p class="noindent">and so the cost of <em>W</em> is within a factor of 2 of the cost of an optimal tour.</p>
<p>Of course, the full walk <em>W</em> is not a tour, since it visits some vertices more than once. By the triangle inequality, however, deleting a visit to any vertex from <em>W</em> does not increase the cost. (When a vertex <em>v</em> is deleted from <em>W</em> between visits to <em>u</em> and <em>w</em>, the resulting ordering specifies going directly from <em>u</em> to <em>w</em>.) Repeatedly apply this operation on each visit to a vertex after the first time it’s visited in <em>W</em>, so that <em>W</em> is left with only the first visit to each vertex. In our example, this process leaves the ordering</p>
<p class="eql"><em>a, b, c, h, d, e, f, g.</em></p>
<p class="noindent">This ordering is the same as that obtained by a preorder walk of the tree <em>T</em>. Let <em>H</em> be the cycle corresponding to this preorder walk. It is a hamiltonian cycle, since every <a id="p1113"/>vertex is visited exactly once, and in fact it is the cycle computed by A<small>PPROX</small>-TSP-T<small>OUR</small>. Since <em>H</em> is obtained by deleting vertices from the full walk <em>W</em>, we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1492.jpg"/></p>
<p class="noindent">Combining inequalities (35.6) and (35.7) gives <em>c</em>(<em>H</em>) ≤ 2<em>c</em>(<em>H</em>*), which completes the proof.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Despite the small approximation ratio provided by Theorem 35.2, A<small>PPROX</small>-TSP-T<small>OUR</small> is usually not the best practical choice for this problem. There are other approximation algorithms that typically perform much better in practice. (See the references at the end of this chapter.)</p>
</section>
<section title="35.2.2 The general traveling-salesperson problem">
<p class="level2" id="Sec_35.2.2"><strong>35.2.2    The general traveling-salesperson problem</strong></p>
<p class="noindent">When the cost function <em>c</em> does not satisfy the triangle inequality, there is no way to find good approximate tours in polynomial time unless P = NP.</p>
<p class="theo"><strong><em>Theorem 35.3</em></strong></p>
<p class="noindent">If P ≠ NP, then for any constant <em>ρ</em> ≥ 1, there is no polynomial-time approximation algorithm with approximation ratio <em>ρ</em> for the general traveling-salesperson problem.</p>
<p class="prof"><strong><em>Proof</em></strong>   The proof is by contradiction. Suppose to the contrary that for some number <em>ρ</em> ≥ 1, there is a polynomial-time approximation algorithm <em>A</em> with approximation ratio <em>ρ</em>. Without loss of generality, assume that <em>ρ</em> is an integer, by rounding it up if necessary. We will show how to use <em>A</em> to solve instances of the hamiltonian-cycle problem (defined in <a href="chapter034.xhtml#Sec_34.2">Section 34.2</a>) in polynomial time. Since Theorem 34.13 on page 1085 says that the hamiltonian-cycle problem is NP-complete, Theorem 34.4 on page 1063 implies that if it has a polynomial-time algorithm, then P = NP.</p>
<p>Let <em>G</em> = (<em>V</em>, <em>E</em>) be an instance of the hamiltonian-cycle problem. We will show how to determine efficiently whether <em>G</em> contains a hamiltonian cycle by making use of the hypothesized approximation algorithm <em>A</em>. Convert <em>G</em> into an instance of the traveling-salesperson problem as follows. Let <em>G</em>′ = (<em>V</em>, <em>E</em>′) be the complete graph on <em>V</em>, that is,</p>
<p class="eql"><em>E</em>′ = {(<em>u</em>, <em>v</em>) : <em>u, v</em> ∈ <em>V</em> and <em>u</em> ≠ <em>v</em>}.</p>
<p class="noindent">Assign an integer cost to each edge in <em>E</em>′ as follows:</p>
<p class="eql"><img alt="art" src="images/Art_P1493.jpg"/></p>
<a id="p1114"/>
<p class="noindent">Given a representation of <em>G</em>, it takes time polynomial in |<em>V</em>| and |<em>E</em>| to create representations of <em>G</em>′ and <em>c</em>.</p>
<p>Now consider the traveling-salesperson problem (<em>G</em>′, <em>c</em>). If the original graph <em>G</em> has a hamiltonian cycle <em>H</em>, then the cost function <em>c</em> assigns to each edge of <em>H</em> a cost of 1, and so (<em>G</em>′, <em>c</em>) contains a tour of cost |<em>V</em>|. On the other hand, if <em>G</em> does not contain a hamiltonian cycle, then any tour of <em>G</em>′ must use some edge not in <em>E</em>. But any tour that uses an edge not in <em>E</em> has a cost of at least</p>
<table class="table2b">
<tr>
<td class="td2">(<em>ρ</em> |<em>V</em>| + 1) + (|<em>V</em>| − 1)</td>
<td class="td2">=</td>
<td class="td2"><em>ρ</em> |<em>V</em>| + |<em>V</em>|</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">&gt;</td>
<td class="td2"><em>ρ</em> |<em>V</em>|.</td>
</tr>
</table>
<p class="noindent">Because edges not in <em>G</em> are so costly, there is a gap of at least <em>ρ</em>|<em>V</em>| between the cost of a tour that is a hamiltonian cycle in <em>G</em> (cost |<em>V</em>|) and the cost of any other tour (cost at least <em>ρ</em>|<em>V</em>| + |<em>V</em>|). Therefore, the cost of a tour that is not a hamiltonian cycle in <em>G</em> is at least a factor of <em>ρ</em> + 1 greater than the cost of a tour that is a hamiltonian cycle in <em>G</em>.</p>
<p>What happens upon applying the approximation algorithm <em>A</em> to the traveling-salesperson problem (<em>G</em>′, <em>c</em>)? Because <em>A</em> is guaranteed to return a tour of cost no more than <em>ρ</em> times the cost of an optimal tour, if <em>G</em> contains a hamiltonian cycle, then <em>A</em> must return it. If <em>G</em> has no hamiltonian cycle, then <em>A</em> returns a tour of cost more than <em>ρ</em> |<em>V</em>|. Therefore, using <em>A</em> solves the hamiltonian-cycle problem in polynomial time.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The proof of Theorem 35.3 serves as an example of a general technique to prove that no good approximation algorithm exists for a particular problem. Given an NP-hard decision problem <em>X</em>, produce in polynomial time a minimization problem <em>Y</em> such that “yes” instances of <em>X</em> correspond to instances of <em>Y</em> with value at most <em>k</em> (for some <em>k</em>), but that “no” instances of <em>X</em> correspond to instances of <em>Y</em> with value greater than <em>ρk</em>. This technique shows that, unless P = NP, there is no polynomial-time <em>ρ</em>-approximation algorithm for problem <em>Y</em>.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>35.2-1</em></strong></p>
<p class="noindent">Let <em>G</em> = (<em>V</em>, <em>E</em>) be a complete undirected graph containing at least 3 vertices, and let <em>c</em> be a cost function that satisfies the triangle inequality. Prove that <em>c</em>(<em>u, v</em>) ≥ 0 for all <em>u, v</em> ∈ <em>V</em>.</p>
<p class="level3"><strong><em>35.2-2</em></strong></p>
<p class="noindent">Show how in polynomial time to transform one instance of the traveling-salesperson problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why <a id="p1115"/>such a polynomial-time transformation does not contradict Theorem 35.3, assuming that P ≠ NP.</p>
<p class="level3"><strong><em>35.2-3</em></strong></p>
<p class="noindent">Consider the following <strong><em><span class="blue">closest-point heuristic</span></em></strong> for building an approximate traveling-salesperson tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex <em>u</em> that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest <em>u</em> is vertex <em>v</em>. Extend the cycle to include <em>u</em> by inserting <em>u</em> just after <em>v</em>. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour.</p>
<p class="level3"><strong><em>35.2-4</em></strong></p>
<p class="noindent">A solution to the <strong><em><span class="blue">bottleneck traveling-salesperson problem</span></em></strong> is the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio 3 for this problem. (<em>Hint:</em> Show recursively how to visit all the nodes in a bottleneck spanning tree, as discussed in Problem 21-4 on page 601, exactly once by taking a full walk of the tree and skipping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost bounded from above by the cost of the costliest edge in a bottleneck hamiltonian cycle.)</p>
<p class="level3"><strong><em>35.2-5</em></strong></p>
<p class="noindent">Suppose that the vertices for an instance of the traveling-salesperson problem are points in the plane and that the cost <em>c</em>(<em>u, v</em>) is the euclidean distance between points <em>u</em> and <em>v</em>. Show that an optimal tour never crosses itself.</p>
<p class="level3"><strong><em>35.2-6</em></strong></p>
<p class="noindent">Adapt the proof of Theorem 35.3 to show that for any constant <em>c</em> ≥ 0, there is no polynomial-time approximation algorithm with approximation ratio |<em>V</em>|<sup><em>c</em></sup> for the general traveling-salesperson problem.</p>
</section>
</section>
<p class="line1"/>
<section title="35.3 The set-covering problem">
<a id="Sec_35.3"/>
<p class="level1" id="h1-208"><a href="toc.xhtml#Rh1-208"><strong>35.3    The set-covering problem</strong></a></p>
<p class="noindent">The set-covering problem is an optimization problem that models many problems that require resources to be allocated. Its corresponding decision problem generalizes the NP-complete vertex-cover problem and is therefore also NP-hard. The <a id="p1116"/>approximation algorithm developed to handle the vertex-cover problem doesn’t apply here, however. Instead, this section investigates a simple greedy heuristic with a logarithmic approximation ratio. That is, as the size of the instance gets larger, the size of the approximate solution may grow, relative to the size of an optimal solution. Because the logarithm function grows rather slowly, however, this approximation algorithm may nonetheless give useful results.</p>
<p>An instance (<em>X</em>, <em><span class="font1">ℱ</span></em>) of the <strong><em><span class="blue">set-covering problem</span></em></strong> consists of a finite set <em>X</em> and a family <em><span class="font1">ℱ</span></em> of subsets of <em>X</em>, such that every element of <em>X</em> belongs to at least one subset in <em><span class="font1">ℱ</span></em>:</p>
<p class="eql"><img alt="art" src="images/Art_P1494.jpg"/></p>
<p class="noindent">We say that a subfamily <span class="script">C</span> ⊆ <em><span class="font1">ℱ</span></em> <strong><em><span class="blue">covers</span></em></strong> a set of elements <em>U</em> if</p>
<p class="eql"><img alt="art" src="images/Art_P1495.jpg"/></p>
<p class="noindent">The problem is to find a minimum-size subfamily <span class="script">C</span> ⊆ <em><span class="font1">ℱ</span></em> whose members cover all of <em>X</em>:</p>
<p class="eql"><img alt="art" src="images/Art_P1496.jpg"/></p>
<p class="noindent"><a href="chapter035.xhtml#Fig_35-3">Figure 35.3</a> illustrates the set-covering problem. The size of <span class="script">C</span> is the number of sets it contains, rather than the number of individual elements in these sets, since every subfamily <span class="script">C</span> that covers <em>X</em> must contain all |<em>X</em>| individual elements. In <a href="chapter035.xhtml#Fig_35-3">Figure 35.3</a>, the minimum set cover has size 3.</p>
<p>The set-covering problem abstracts many commonly arising combinatorial problems. As a simple example, suppose that <em>X</em> represents a set of skills that are needed to solve a problem and that you have a given set of people available to work on the problem. You wish to form a committee, containing as few people as possible, such that for every requisite skill in <em>X</em>, at least one member of the committee has that skill. The decision version of the set-covering problem asks whether a covering exists with size at most <em>k</em>, where <em>k</em> is an additional parameter specified in the problem instance. The decision version of the problem is NP-complete, as Exercise 35.3-2 asks you to show.</p>
<p class="level4"><strong>A greedy approximation algorithm</strong></p>
<p class="noindent">The greedy method in the procedure G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> on the facing page works by picking, at each stage, the set <em>S</em> that covers the greatest number of remaining elements that are uncovered. In the example of <a href="chapter035.xhtml#Fig_35-3">Figure 35.3</a>, G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> adds to <span class="script">C</span>, in order, the sets <em>S</em><sub>1</sub>, <em>S</em><sub>4</sub>, and <em>S</em><sub>5</sub>, followed by either <em>S</em><sub>3</sub> or <em>S</em><sub>6</sub>.</p>
<a id="p1117"/>
<div class="divimage">
<p class="fig-imga" id="Fig_35-3"><img alt="art" src="images/Art_P1497.jpg"/></p>
<p class="caption"><strong>Figure 35.3</strong> An instance (<em>X</em>, <em><span class="font1">ℱ</span></em>) of the set-covering problem, where <em>X</em> consists of the 12 tan points and <em><span class="font1">ℱ</span></em> = {<em>S</em><sub>1</sub>, <em>S</em><sub>2</sub>, <em>S</em><sub>3</sub>, <em>S</em><sub>4</sub>, <em>S</em><sub>5</sub>, <em>S</em><sub>6</sub>, <em>S</em><sub>4</sub>, <em>S</em><sub>5</sub>}, Each set <em>S<sub>i</sub></em> ∈ <em><span class="font1">ℱ</span></em> is outlined in blue. A minimum-size set cover <span class="script">C</span> = {<em>S</em><sub>3</sub>, <em>S</em><sub>4</sub>, <em>S</em><sub>5</sub>}, with size 3. The greedy algorithm produces a cover of size 4 by selecting either the sets <em>S</em><sub>1</sub>, <em>S</em><sub>4</sub>, <em>S</em><sub>5</sub>, and <em>S</em><sub>3</sub> or the sets <em>S</em><sub>1</sub>, <em>S</em><sub>4</sub>, <em>S</em><sub>5</sub>, and <em>S</em><sub>6</sub>, in order.</p>
</div>
<div class="pull-quote1">
<p class="box-heading">G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> (<em>X</em>, <em><span class="font1">ℱ</span></em>)</p>
<table class="table1a">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>U</em><sub>0</sub> = <em>X</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><span class="script">C</span> = Ø</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><em>i</em> = 0</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="noindent"><strong>while</strong> <em>U<sub>i</sub></em> ≠ Ø</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2">select <em>S</em> ∈ <em><span class="font1">ℱ</span></em> that maximizes |<em>S</em> ∩ <em>U<sub>i</sub></em>|</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="p2"><em>U</em><sub><em>i</em>+1</sub> = <em>U<sub>i</sub></em> − <em>S</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="p2"><span class="script">C</span> = <span class="script">C</span> ∪ {<em>S</em>}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1"><p class="p2"><em>i</em> = <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">9</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <span class="script">C</span></p></td>
</tr>
</table>
</div>
<p>The greedy algorithm works as follows. At the start of each iteration, <em>U<sub>i</sub></em> is a subset of <em>X</em> containing the remaining uncovered elements, with the initial subset <em>U</em><sub>0</sub> containing all the elements in <em>X</em>. The set <span class="script">C</span> contains the subfamily being constructed. Line 5 is the greedy decision-making step, choosing a subset <em>S</em> that covers as many uncovered elements as possible (breaking ties arbitrarily). After <em>S</em> is selected, line 6 updates the set of remaining uncovered elements, denoting it by <em>U</em><sub><em>i</em>+1</sub>, and line 7 places <em>S</em> into <span class="script">C</span>. When the algorithm terminates, <span class="script">C</span> is a subfamily of <em><span class="font1">ℱ</span></em> that covers <em>X</em>.</p>
<p class="level4"><strong>Analysis</strong></p>
<p class="noindent">We now show that the greedy algorithm returns a set cover that is not too much larger than an optimal set cover.</p>
<a id="p1118"/>
<p class="theo"><strong><em>Theorem 35.4</em></strong></p>
<p class="noindent">The procedure G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> run on a set <em>X</em> and family of subsets <em><span class="font1">ℱ</span></em> is a polynomial-time <em>O</em>(lg <em>X</em>)-approximation algorithm.</p>
<p class="prof"><strong><em>Proof</em></strong>   Let’s first show that the algorithm runs in time that is polynomial in |<em>X</em>| and |<em><span class="font1">ℱ</span></em>|. The number of iterations of the loop in lines 4–7 is bounded above by min {|<em>X</em>|, |<em><span class="font1">ℱ</span></em>|} = <em>O</em>(|<em>X</em>| + |<em><span class="font1">ℱ</span></em>|). The loop body can be implemented to run in <em>O</em>(|<em>X</em>|·|<em><span class="font1">ℱ</span></em>|) time. Thus the algorithm runs in <em>O</em>(|<em>X</em>|·|<em><span class="font1">ℱ</span></em>|·(|<em>X</em>|+|<em><span class="font1">ℱ</span></em>|)) time, which is polynomial in the input size. (Exercise 35.3-3 asks for a linear-time algorithm.)</p>
<p>To prove the approximation bound, let <span class="script">C</span>* be an optimal set cover for the original instance (<em>X</em>, <em><span class="font1">ℱ</span></em>), and let <em>k</em> = |<span class="script">C</span>*|. Since <span class="script">C</span>* is also a set cover of each subset <em>U<sub>i</sub></em> of <em>X</em> constructed by the algorithm, we know that any subset <em>U<sub>i</sub></em> constructed by the algorithm can be covered by <em>k</em> sets. Therefore, if (<em>U<sub>i</sub></em>, <em><span class="font1">ℱ</span></em>) is an instance of the set-covering problem, its optimal set cover has size at most <em>k</em>.</p>
<p>If an optimal set cover for an instance (<em>U<sub>i</sub></em>, <em><span class="font1">ℱ</span></em>) has size at most <em>k</em>, at least one of the sets in <span class="script">C</span> covers at least |<em>U<sub>i</sub></em>|/<em>k</em> new elements. Thus, line 5 of G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small>, which chooses a set with the maximum number of uncovered elements, must choose a set in which the number of newly covered elements is at least |<em>U<sub>i</sub></em>|/<em>k</em>. These elements are removed when constructing <em>U</em><sub><em>i</em>+1</sub>, giving</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1498.jpg"/></p>
<p class="noindent">Iterating inequality (35.8) gives</p>
<table class="table2b">
<tr>
<td class="td2">|<em>U</em><sub>0</sub>|</td>
<td class="td2">=</td>
<td class="td2">|<em>X</em>|,</td>
</tr>
<tr>
<td class="td2">|<em>U</em><sub>1</sub>|</td>
<td class="td2">≤</td>
<td class="td2">|<em>U</em><sub>0</sub>| (1 − 1/<em>k</em>),</td>
</tr>
<tr>
<td class="td2">|<em>U</em><sub>2</sub>|</td>
<td class="td2">≤</td>
<td class="td2">|<em>U</em><sub>1</sub>| (1 − 1/<em>k</em>) = |<em>U</em>| (1 − 1/<em>k</em>)<sup>2</sup>,</td>
</tr>
</table>
<p class="noindent">and in general</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1499.jpg"/></p>
<p class="noindent">The algorithm stops when <em>U<sub>i</sub></em> = Ø, which means that |<em>U<sub>i</sub></em>| &lt; 1. Thus an upper bound on the number of iterations of the algorithm is the smallest value of <em>i</em> for which |<em>U<sub>i</sub></em>| &lt; 1.</p>
<p>Since 1 + <em>x</em> ≤ <em>e<sup>x</sup></em> for all real <em>x</em> (see inequality (3.14) on page 66), by letting <em>x</em> = −1/<em>k</em>, we have 1 − 1/<em>k</em> ≤ <em>e</em><sup>−1/<em>k</em></sup>, so that (1 − 1/<em>k</em>)<em><sup>k</sup></em> ≤ (<em>e</em><sup>−1/<em>k</em></sup>)<em><sup>k</sup></em> = 1/<em>e</em>. Denoting the number <em>i</em> of iterations by <em>ck</em> for some nonnegative integer <em>c</em>, we want <em>c</em> such that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1500.jpg"/></p>
<p class="noindent">Multiplying both sides by <em>e<sup>c</sup></em> and then taking the natural logarithm of both sides gives <em>c</em> ≥ ln |<em>X</em>|, so we can choose for <em>c</em> any integer that is at least ln |<em>X</em>|. We <a id="p1119"/>choose <em>c</em> = <span class="font1">⌈</span>ln |<em>X</em>|<span class="font1">⌉</span>. Since <em>i</em> = <em>ck</em> is an upper bound on the number of iterations, which equals the size of <span class="script">C</span>, and <em>k</em> = |<span class="script">C</span>*|, we have |<span class="script">C</span>| ≤ <em>i</em> = <em>ck</em> = <em>c</em> |<span class="script">C</span>*| = |<span class="script">C</span>*| <span class="font1">⌈</span>ln |<em>X</em>|<span class="font1">⌉</span>, and the theorem follows.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>35.3-1</em></strong></p>
<p class="noindent">Consider each of the following words as a set of letters: {<span class="courierfont">arid</span>, <span class="courierfont">dash</span>, <span class="courierfont">drain</span>, <span class="courierfont">heard</span>, <span class="courierfont">lost</span>, <span class="courierfont">nose</span>, <span class="courierfont">shun</span>, <span class="courierfont">slate</span>, <span class="courierfont">snare</span>, <span class="courierfont">thread</span>}. Show which set cover G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> produces when you break ties in favor of the word that appears first in the dictionary.</p>
<p class="level3"><strong><em>35.3-2</em></strong></p>
<p class="noindent">Show that the decision version of the set-covering problem is NP-complete by reducing the vertex-cover problem to it.</p>
<p class="level3"><strong><em>35.3-3</em></strong></p>
<p class="noindent">Show how to implement G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> to run in <em>O</em>(Σ<sub><em>S</em>∈<em><span class="font1">ℱ</span></em></sub> |<em>S</em>|) time.</p>
<p class="level3"><strong><em>35.3-4</em></strong></p>
<p class="noindent">The proof of Theorem 35.4 says that when G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small>, run on the instance (<em>X</em>, <em><span class="font1">ℱ</span></em>), returns the subfamily <span class="script">C</span>, then |<span class="script">C</span>| ≤ |<span class="script">C</span>*| <span class="font1">⌈</span>ln <em>X</em><span class="font1">⌉</span>. Show that the following weaker bound is trivially true:</p>
<p class="eql">|<span class="script">C</span>| ≤ |<span class="script">C</span>*| max {|<em>S</em>| : <em>S</em> ∈ <em><span class="font1">ℱ</span></em>}.</p>
<p class="level3"><strong><em>35.3-5</em></strong></p>
<p class="noindent">G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> can return a number of different solutions, depending on how it breaks ties in line 5. Give a procedure B<small>AD</small>-S<small>ET</small>-C<small>OVER</small>-I<small>NSTANCE</small> (<em>n</em>) that returns an <em>n</em>-element instance of the set-covering problem for which, depending on how line 5 breaks ties, G<small>REEDY</small>-S<small>ET</small>-C<small>OVER</small> can return a number of different solutions that is exponential in <em>n</em>.</p>
</section>
<p class="line1"/>
<section title="35.4 Randomization and linear programming">
<a id="Sec_35.4"/>
<p class="level1" id="h1-209"><a href="toc.xhtml#Rh1-209"><strong>35.4    Randomization and linear programming</strong></a></p>
<p class="noindent">This section studies two useful techniques for designing approximation algorithms: randomization and linear programming. It starts with a simple randomized algorithm for an optimization version of 3-CNF satisfiability, and then it shows how to design an approximation algorithm for a weighted version of the vertex-cover problem based on linear programming. This section only scratches the surface of <a id="p1120"/>these two powerful techniques. The chapter notes give references for further study of these areas.</p>
<p class="level4"><strong>A randomized approximation algorithm for MAX-3-CNF satisfiability</strong></p>
<p class="noindent">Just as some randomized algorithms compute exact solutions, some randomized algorithms compute approximate solutions. We say that a randomized algorithm for a problem has an <strong><em><span class="blue">approximation ratio</span></em></strong> of <em>ρ</em>(<em>n</em>) if, for any input of size <em>n</em>, the <em>expected</em> cost <em>C</em> of the solution produced by the randomized algorithm is within a factor of <em>ρ</em>(<em>n</em>) of the cost <em>C</em>* of an optimal solution:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1501.jpg"/></p>
<p class="noindent">We call a randomized algorithm that achieves an approximation ratio of <em>ρ</em>(<em>n</em>) a <strong><span class="blue"><em>randomized ρ</em>(<em>n</em>)<em>-approximation algorithm.</em></span></strong> In other words, a randomized approximation algorithm is like a deterministic approximation algorithm, except that the approximation ratio is for an expected cost.</p>
<p>A particular instance of 3-CNF satisfiability, as defined in <a href="chapter034.xhtml#Sec_34.4">Section 34.4</a>, may or may not be satisfiable. In order to be satisfiable, there must exist an assignment of the variables so that every clause evaluates to 1. If an instance is not satisfiable, you might instead want to know how “close” to satisfiable it is, that is, find an assignment of the variables that satisfies as many clauses as possible. We call the resulting maximization problem <strong><em><span class="blue">MAX-3-CNF satisfiability</span></em></strong>. The input to MAX-3-CNF satisfiability is the same as for 3-CNF satisfiability, and the goal is to return an assignment of the variables that maximizes the number of clauses evaluating to 1. You might be surprised that randomly setting each variable to 1 with probability 1/2 and to 0 with probability 1/2 yields a randomized 8/7-approximation algorithm, but we’re about to see why. Recall that the definition of 3-CNF satisfiability from <a href="chapter034.xhtml#Sec_34.4">Section 34.4</a> requires each clause to consist of exactly three distinct literals. We now further assume that no clause contains both a variable and its negation. Exercise 35.4-1 asks you to remove this last assumption.</p>
<p class="theo"><strong><em>Theorem 35.5</em></strong></p>
<p class="noindent">Given an instance of MAX-3-CNF satisfiability with <em>n</em> variables <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em> and <em>m</em> clauses, the randomized algorithm that independently sets each variable to 1 with probability 1/2 and to 0 with probability 1/2 is a randomized 8/7-approximation algorithm.</p>
<p class="prof"><strong><em>Proof</em></strong>   Suppose that each variable is independently set to 1 with probability 1/2 and to 0 with probability 1/2. Define, for <em>i</em> = 1, 2, … , <em>m</em>, the indicator random variable</p>
<a id="p1121"/>
<p class="eql"><em>Y</em><sub><em>i</em></sub> = I {clause <em>i</em> is satisfied},</p>
<p class="noindent">so that <em>Y<sub>i</sub></em> = 1 as long as at least one of the literals in the <em>i</em>th clause is set to 1. Since no literal appears more than once in the same clause, and since we assume that no variable and its negation appear in the same clause, the settings of the three literals in each clause are independent. A clause is not satisfied only if all three of its literals are set to 0, and so Pr {clause <em>i</em> is not satisfied} = (1/2)<sup>3</sup> = 1/8. Thus, we have Pr {clause <em>i</em> is satisfied} = 1 − 1/8 = 7/8, and Lemma 5.1 on page 130 gives E [<em>Y<sub>i</sub></em>] = 7/8. Let <em>Y</em> be the number of satisfied clauses overall, so that <em>Y</em> = <em>Y</em><sub>1</sub> + <em>Y</em><sub>2</sub> + <span class="font1">⋯</span> + <em>Y<sub>m</sub></em>. Then, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1502.jpg"/></p>
<p class="noindent">Since <em>m</em> is an upper bound on the number of satisfied clauses, the approximation ratio is at most <em>m</em>/(7<em>m</em>/8) = 8/7.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Approximating weighted vertex cover using linear programming</strong></p>
<p class="noindent">The <strong><em><span class="blue">minimum-weight vertex-cover problem</span></em></strong> takes as input an undirected graph <em>G</em> = (<em>V</em>, <em>E</em>) in which each vertex <em>v</em> ∈ <em>V</em> has an associated positive weight <em>w</em>(<em>v</em>). The weight <em>w</em>(<em>V</em>′) of a vertex cover <em>V</em>′ ⊆ <em>V</em> is the sum of the weights of its vertices: <em>w</em>(<em>V</em>′) = Σ<sub><em>v</em>∈<em>V′</em></sub> <em>w</em>(<em>v</em>). The goal is to find a vertex cover of minimum weight.</p>
<p>The approximation algorithm for unweighted vertex cover from <a href="chapter035.xhtml#Sec_35.1">Section 35.1</a> won’t work here, because the solution it returns could be far from optimal for the weighted problem. Instead, we’ll first compute a lower bound on the weight of the minimum-weight vertex cover, by using a linear program. Then we’ll “round” this solution and use it to obtain a vertex cover.</p>
<p>Start by associating a variable <em>x</em>(<em>v</em>) with each vertex <em>v</em> ∈ <em>V</em>, and require that <em>x</em>(<em>v</em>) equals either 0 or 1 for each <em>v</em> ∈ <em>V</em>. The vertex cover includes <em>v</em> if and only if <em>x</em>(<em>v</em>) = 1. Then the constraint that for any edge (<em>u</em>, <em>v</em>), at least one of <em>u</em> and <em>v</em> must belong to the vertex cover can be expressed as <em>x</em>(<em>u</em>) + <em>x</em>(<em>v</em>) ≥ 1. This view gives rise to the following <strong><em><span class="blue">0-1 integer program</span></em></strong> for finding a minimum-weight vertex cover:</p>
<a id="p1122"/>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1503.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1504.jpg"/></p>
<p>In the special case in which all the weights <em>w</em>(<em>v</em>) equal 1, this formulation is the optimization version of the NP-hard vertex-cover problem. Let’s remove the constraint that <em>x</em>(<em>v</em>) ∈ {0, 1} and replace it by 0 ≤ <em>x</em>(<em>v</em>) ≤ 1, resulting in the following linear program:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1505.jpg"/></p>
<p class="noindent">subject to</p>
<p class="eqr"><img alt="art" src="images/Art_P1506.jpg"/></p>
<p class="noindent">We refer to this linear program as the <strong><em><span class="blue">linear-programming relaxation</span></em></strong>. Any feasible solution to the 0-1 integer program in lines (35.12)–(35.14) is also a feasible solution to its linear-programming relaxation in lines (35.15)–(35.18). Therefore, the value of an optimal solution to the linear-programming relaxation provides a lower bound on the value of an optimal solution to the 0-1 integer program, and hence a lower bound on the optimal weight in the minimum-weight vertex-cover problem.</p>
<p>The procedure A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC on the facing page starts with a solution to the linear-programming relaxation and uses it to construct an approximate solution to the minimum-weight vertex-cover problem. The procedure works as follows. Line 1 initializes the vertex cover to be empty. Line 2 formulates the linear-programming relaxation in lines (35.15)–(35.18) and then solves this linear program. An optimal solution gives each vertex <em>v</em> an associated value <em><span class="overline">x</span></em>(<em>v</em>), where 0 ≤ <em><span class="overline">x</span></em>(<em>v</em>) ≤ 1. The procedure uses this value to guide the choice of which vertices to add to the vertex cover <em>C</em> in lines 3–5: the vertex cover <em>C</em> includes vertex <em>v</em> if and only if <em><span class="overline">x</span></em>(<em>v</em>) ≥ 1/2. In effect, the procedure “rounds” each fractional variable in the solution to the linear-programming relaxation to either 0 or 1 in order to obtain a solution to the 0-1 integer program in lines (35.12)–(35.14). Finally, line 6 returns the vertex cover <em>C</em>.</p>
<p class="theo"><strong><em>Theorem 35.6</em></strong></p>
<p class="noindent">Algorithm A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC is a polynomial-time2-approximation algorithm for the minimum-weight vertex-cover problem.</p>
<a id="p1123"/>
<div class="pull-quote1">
<p class="box-heading">A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC (<em>G</em>, <em>w</em>)</p>
<table class="table1a">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>C</em> = Ø</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent">compute <span class="overline"><em>x</em></span>, an optimal solution to the linear-programming relaxation in lines (35.15)–(35.18)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> each vertex <em>v</em> ∈ <em>V</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <span class="overline"><em>x</em></span>(<em>v</em>) ≥ 1/2</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p3"><em>C</em> = <em>C</em> ∪ {<em>v</em>}</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>C</em></p></td>
</tr>
</table>
</div>
<p class="prof"><strong><em>Proof</em></strong>   Because there is a polynomial-time algorithm to solve the linear program in line 2, and because the <strong>for</strong> loop of lines 3–5 runs in polynomial time, A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC is a polynomial-time algorithm.</p>
<p>It remains to show that A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC is a 2-approximation algorithm. Let <em>C</em>* be an optimal solution to the minimum-weight vertex-cover problem, and let <em>z</em>* be the value of an optimal solution to the linear-programming relaxation in lines (35.15)–(35.18). Since an optimal vertex cover is a feasible solution to the linear-programming relaxation, <em>z</em>* must be a lower bound on <em>w</em>(<em>C</em>*), that is,</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1507.jpg"/></p>
<p class="noindent">Next, we claim that rounding the fractional values of the variables <span class="overline"><em>x</em></span>(<em>v</em>) in lines 3–5 produces a set <em>C</em> that is a vertex cover and satisfies <em>w</em>(<em>C</em>) ≤ 2<em>z</em>*. To see that <em>C</em> is a vertex cover, consider any edge (<em>u</em>, <em>v</em>) ∈ <em>E</em>. By constraint (35.16), we know that <em>x</em>(<em>u</em>) + <em>x</em>(<em>v</em>) ≥ 1, which implies that at least one of <span class="overline"><em>x</em></span>(<em>u</em>) and <span class="overline"><em>x</em></span>(<em>v</em>) is at least 1/2. Therefore, at least one of <em>u</em> and <em>v</em> is included in the vertex cover, and so every edge is covered.</p>
<p>Now we consider the weight of the cover. We have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1508.jpg"/></p>
<a id="p1124"/>
<p class="noindent">Combining inequalities (35.19) and (35.20) gives</p>
<p class="eql"><em>w</em>(<em>C</em>) ≤ 2<em>z</em>* ≤ 2<em>w</em>(<em>C</em>*),</p>
<p class="noindent">and hence A<small>PPROX</small>-M<small>IN</small>-W<small>EIGHT</small>-VC is a 2-approximation algorithm.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>35.4-1</em></strong></p>
<p class="noindent">Show that even if a clause is allowed to contain both a variable and its negation, randomly setting each variable to 1 with probability 1/2 and to 0 with probability 1/2 still yields a randomized 8/7-approximation algorithm.</p>
<p class="level3"><strong><em>35.4-2</em></strong></p>
<p class="noindent">The <strong><em><span class="blue">MAX-CNF satisfiability problem</span></em></strong> is like the MAX-3-CNF satisfiability problem, except that it does not restrict each clause to have exactly three literals. Give a randomized 2-approximation algorithm for the MAX-CNF satisfiability problem.</p>
<p class="level3"><strong><em>35.4-3</em></strong></p>
<p class="noindent">In the MAX-CUT problem, the input is an unweighted undirected graph <em>G</em> = (<em>V</em>, <em>E</em>). We define a cut (<em>S</em>, <em>V</em> − <em>S</em>) as in <a href="chapter021.xhtml">Chapter 21</a> and the <strong><em><span class="blue">weight</span></em></strong> of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that each vertex <em>v</em> is randomly and independently placed into <em>S</em> with probability 1/2 and into <em>V</em> −<em>S</em> with probability 1/2. Show that this algorithm is a randomized 2-approximation algorithm.</p>
<p class="level3"><strong><em>35.4-4</em></strong></p>
<p class="noindent">Show that the constraints in line (35.17) are redundant in the sense that removing them from the linear-programming relaxation in lines (35.15)–(35.18) yields a linear program for which any optimal solution <em>x</em> must satisfy <em>x</em>(<em>v</em>) ≤ 1 for each <em>v</em> ∈ <em>V</em>.</p>
</section>
<p class="line1"/>
<section title="35.5 The subset-sum problem">
<a id="Sec_35.5"/>
<p class="level1" id="h1-210"><a href="toc.xhtml#Rh1-210"><strong>35.5    The subset-sum problem</strong></a></p>
<p class="noindent">Recall from <a href="chapter034.xhtml#Sec_34.5.5">Section 34.5.5</a> that an instance of the subset-sum problem is given by a pair (<em>S</em>, <em>t</em>), where <em>S</em> is a set {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>} of positive integers and <em>t</em> is a positive integer. This decision problem asks whether there exists a subset of <em>S</em> that adds up exactly to the target value <em>t</em>. As we saw in <a href="chapter034.xhtml#Sec_34.5.5">Section 34.5.5</a>, this problem is NP-complete.</p>
<p>The optimization problem associated with this decision problem arises in practical applications. The optimization problem seeks a subset of {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>} <a id="p1125"/>whose sum is as large as possible but not larger than <em>t</em>. For example, consider a truck that can carry no more than <em>t</em> pounds, which is to be loaded with up to <em>n</em> different boxes, the <em>i</em>th of which weighs <em>x<sub>i</sub></em> pounds. How heavy a load can the truck take without exceeding the <em>t</em>-pound weight limit?</p>
<p>We start this section with an exponential-time algorithm to compute the optimal value for this optimization problem. Then we show how to modify the algorithm so that it becomes a fully polynomial-time approximation scheme. (Recall that a fully polynomial-time approximation scheme has a running time that is polynomial in 1/<em><span class="font1">ϵ</span></em> as well as in the size of the input.)</p>
<p class="level4"><strong>An exponential-time exact algorithm</strong></p>
<p class="noindent">Suppose that you compute, for each subset <em>S</em>′ of <em>S</em>, the sum of the elements in <em>S</em>′, and then you select, among the subsets whose sum does not exceed <em>t</em>, the one whose sum is closest to <em>t</em>. This algorithm returns the optimal solution, but it might take exponential time. To implement this algorithm, you can use an iterative procedure that, in iteration <em>i</em>, computes the sums of all subsets of {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>i</sub></em>}, using as a starting point the sums of all subsets of {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x</em><sub><em>i</em>−1</sub>}. In doing so, you would realize that once a particular subset <em>S</em>′ has a sum exceeding <em>t</em>, there is no reason to maintain it, since no superset of <em>S</em>′ can be an optimal solution. Let’s see how to implement this strategy.</p>
<p>The procedure E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small> takes an input set <em>S</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>}, the size <em>n</em> = |<em>S</em>|, and a target value <em>t</em>. This procedure iteratively computes <em>L<sub>i</sub></em>, the list of sums of all subsets of {<em>x</em><sub>1</sub>, … , <em>x<sub>i</sub></em>} that do not exceed <em>t</em>, and then it returns the maximum value in <em>L<sub>n</sub></em>.</p>
<p>If <em>L</em> is a list of positive integers and <em>x</em> is another positive integer, then let <em>L</em> + <em>x</em> denote the list of integers derived from <em>L</em> by increasing each element of <em>L</em> by <em>x</em>. For example, if <em>L</em> = <span class="font1"><span class="font1">〈</span></span>1, 2, 3, 5, 9<span class="font1"><span class="font1">〉</span></span>, then <em>L</em> + 2 = <span class="font1"><span class="font1">〈</span></span>3, 4, 5, 7, 11<span class="font1"><span class="font1">〉</span></span>. This notation extends to sets, so that</p>
<p class="eql"><em>S</em> + <em>x</em> = {<em>s</em> + <em>x</em> : <em>s</em> ∈ <em>S</em>}.</p>
<div class="pull-quote1">
<p class="box-heading">E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small> (<em>S</em>, <em>n, t</em>)</p>
<table class="table1c">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>L</em><sub>0</sub> = <span class="font1"><span class="font1">〈</span></span>0<span class="font1"><span class="font1">〉</span></span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>L<sub>i</sub></em> = M<small>ERGE</small>-L<small>ISTS</small> (<em>L</em><sub><em>i</em>−1</sub>, <em>L</em><sub><em>i</em>−1</sub> + <em>x<sub>i</sub></em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2">remove from <em>L<sub>i</sub></em> every element that is greater than <em>t</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> the largest element in <em>L</em><sub><em>n</em></sub></p></td>
</tr>
</table>
</div>
<p>E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small> invokes an auxiliary procedure M<small>ERGE</small>-L<small>ISTS</small> (<em>L</em>, <em>L</em>′), which returns the sorted list that is the merge of its two sorted input lists <em>L</em> and <em>L</em>′, <a id="p1126"/>with duplicate values removed. Like the M<small>ERGE</small> procedure we used in merge sort on page 36, M<small>ERGE</small>-L<small>ISTS</small> runs in <em>O</em>(|<em>L</em>| + |<em>L</em>′|) time. We omit the pseudocode for M<small>ERGE</small>-L<small>ISTS</small>.</p>
<p>To see how E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small> works, let <em>P<sub>i</sub></em> denote the set of values obtained by selecting each (possibly empty) subset of {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>i</sub></em>} and summing its members. For example, if <em>S</em> = {1, 4, 5}, then</p>
<table class="table2b">
<tr>
<td class="td2"><em>P</em><sub>1</sub></td>
<td class="td2">=</td>
<td class="td2">{0, 1},</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>2</sub></td>
<td class="td2">=</td>
<td class="td2">{0, 1, 4, 5},</td>
</tr>
<tr>
<td class="td2"><em>P</em><sub>3</sub></td>
<td class="td2">=</td>
<td class="td2">{0, 1, 4, 5, 6, 9, 10}.</td>
</tr>
</table>
<p class="noindent">Given the identity</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1509.jpg"/></p>
<p class="noindent">you can prove by induction on <em>i</em> (see Exercise 35.5-1) that the list <em>L<sub>i</sub></em> is a sorted list containing every element of <em>P<sub>i</sub></em> whose value is not more than <em>t</em>. Since the length of <em>L<sub>i</sub></em> can be as much as 2<sup><em>i</em></sup>, E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small> is an exponential-time algorithm in general, although it is a polynomial-time algorithm in the special cases in which <em>t</em> is polynomial in |<em>S</em>| or all the numbers in <em>S</em> are bounded by a polynomial in |<em>S</em>|.</p>
<p class="level4"><strong>A fully polynomial-time approximation scheme</strong></p>
<p class="noindent">The key to devising a fully polynomial-time approximation scheme for the subset-sum problem is to “trim” each list <em>L<sub>i</sub></em> after it is created. Here’s the idea behind trimming: if two values in <em>L</em> are close to each other, then since the goal is just an approximate solution, there is no need to maintain both of them explicitly. More precisely, use a trimming parameter <em>δ</em> such that 0 &lt; <em>δ</em> &lt; 1. When <strong><em><span class="blue">trimming</span></em></strong> a list <em>L</em> by <em>δ</em>, remove as many elements from <em>L</em> as possible, in such a way that if <em>L</em>′ is the result of trimming <em>L</em>, then for every element <em>y</em> that was removed from <em>L</em>, some element <em>z</em> still in <em>L</em>′ approximates <em>y</em>. For <em>z</em> to approximate <em>y</em>, it must be no greater than <em>y</em> and also within a factor of 1 + <em>δ</em> of <em>y</em>, so that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1510.jpg"/></p>
<p class="noindent">You can think of such a <em>z</em> as “representing” <em>y</em> in the new list <em>L</em>′. Each removed element <em>y</em> is represented by a remaining element <em>z</em> satisfying inequality (35.22). For example, suppose that <em>δ</em> = 0.1 and</p>
<p class="eql"><em>L</em> = <span class="font1"><span class="font1">〈</span></span>10, 11, 12, 15, 20, 21, 22, 23, 24, 29<span class="font1"><span class="font1">〉</span></span>.</p>
<p class="noindent">Then trimming <em>L</em> results in</p>
<p class="eql"><em>L</em>′ = <span class="font1"><span class="font1">〈</span></span>10, 12, 15, 20, 23, 29<span class="font1"><span class="font1">〉</span></span>,</p>
<a id="p1127"/>
<p class="noindent">where the deleted value 11 is represented by 10, the deleted values 21 and 22 are represented by 20, and the deleted value 24 is represented by 23. Because every element of the trimmed version of the list is also an element of the original version of the list, trimming can dramatically decrease the number of elements kept while keeping a close (and slightly smaller) representative value in the list for each deleted element.</p>
<p>The procedure T<small>RIM</small> trims list <em>L</em> = <span class="font1"><span class="font1">〈</span></span><em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, … , <em>y<sub>m</sub></em><span class="font1"><span class="font1">〉</span></span> in Θ(<em>m</em>) time, given <em>L</em> and the trimming parameter <em>δ</em>. It assumes that <em>L</em> is sorted into monotonically increasing order. The output of the procedure is a trimmed, sorted list. The procedure scans the elements of <em>L</em> in monotonically increasing order. A number is appended onto the returned list <em>L</em>′ only if it is the first element of <em>L</em> or if it cannot be represented by the most recent number placed into <em>L</em>′.</p>
<div class="pull-quote1">
<p class="box-heading">T<small>RIM</small> (<em>L</em>, <em>δ</em>)</p>
<table class="table1a1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1" colspan="2"><p class="noindent">let <em>m</em> be the length of <em>L</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>L</em>′ = <span class="font1"><span class="font1">〈</span></span><em>y</em><sub>1</sub><span class="font1"><span class="font1">〉</span></span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1" colspan="2"><p class="noindent"><em>last</em> = <em>y</em><sub>1</sub></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>m</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>if</strong> <em>y</em><sub><em>i</em></sub> &gt; <em>last</em> · (1 + <em>δ</em>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> <em>y<sub>i</sub></em> ≥ <em>last</em> because <em>L</em> is sorted</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><p class="p3">append <em>y<sub>i</sub></em> onto the end of <em>L</em>′</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><p class="p3"><em>last</em> = <em>y<sub>i</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">8</span></td>
<td class="td1" colspan="2"><p class="noindent"><strong>return</strong> <em>L</em>′</p></td>
</tr>
</table>
</div>
<p>Given the procedure T<small>RIM</small>, the procedure A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> on the following page implements the approximation scheme. This procedure takes as input a set <em>S</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, … , <em>x<sub>n</sub></em>} of <em>n</em> integers (in arbitrary order), the size <em>n</em> = |<em>S</em>|, the target integer <em>t</em>, and an approximation parameter <em><span class="font1">ϵ</span></em>, where</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1511.jpg"/></p>
<p class="noindent">It returns a value <em>z</em>* whose value is within a factor of 1 + <em><span class="font1">ϵ</span></em> of the optimal solution.</p>
<p>The A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> procedure works as follows. Line 1 initializes the list <em>L</em><sub>0</sub> to be the list containing just the element 0. The <strong>for</strong> loop in lines 2–5 computes <em>L<sub>i</sub></em> as a sorted list containing a suitably trimmed version of the set <em>P<sub>i</sub></em>, with all elements larger than <em>t</em> removed. Since the procedure creates <em>L<sub>i</sub></em> from <em>L</em><sub><em>i</em>−1</sub>, it must ensure that the repeated trimming doesn’t introduce too much compounded inaccuracy. That’s why instead of the trimming parameter being <em><span class="font1">ϵ</span></em> in the call to T<small>RIM</small>, it has the smaller value <em><span class="font1">ϵ</span></em>/2<em>n</em>. We’ll soon see that A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> returns a correct approximation if one exists.</p>
<a id="p1128"/>
<div class="pull-quote1">
<p class="box-heading">A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> (<em>S</em>, <em>n, t, <span class="font1">ϵ</span></em>)</p>
<table class="table1a1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><p class="noindent"><em>L</em><sub>0</sub> = <span class="font1"><span class="font1">〈</span></span>0<span class="font1"><span class="font1">〉</span></span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><em>L<sub>i</sub></em> = M<small>ERGE</small>-L<small>ISTS</small> (<em>L</em><sub><em>i</em>−1</sub>, <em>L</em><sub><em>i</em>−1</sub> + <em>x<sub>i</sub></em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><em>L<sub>i</sub></em> = T<small>RIM</small> (<em>L</em><sub><em>i</em></sub>, <em><span class="font1">ϵ</span></em>/2<em>n</em>)</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2">remove from <em>L<sub>i</sub></em> every element that is greater than <em>t</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><p class="noindent">let <em>z</em>* be the largest value in <em>L<sub>n</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>z</em>*</p></td>
</tr>
</table>
</div>
<p>As an example, suppose that A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> is given</p>
<p class="eql"><em>S</em> = <span class="font1"><span class="font1">〈</span></span>104, 102, 201, 101<span class="font1"><span class="font1">〉</span></span></p>
<p class="noindent">with <em>t</em> = 308 and <em><span class="font1">ϵ</span></em> = 0.40. The trimming parameter <em>δ</em> is <em><span class="font1">ϵ</span></em>/2<em>n</em> = 0.40/80 = 0.05. The procedure computes the following values on the indicated lines:</p>
<table class="table1t">
<tr>
<td class="td1">line 1:</td>
<td class="td1"><em>L</em><sub>0</sub> = <span class="font1"><span class="font1">〈</span></span>0<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1"><p class="noindent1-top">line 3:</p></td>
<td class="td1"><p class="noindent1-top"><em>L</em><sub>1</sub> = <span class="font1"><span class="font1">〈</span></span>0, 104<span class="font1"><span class="font1">〉</span></span>,</p></td>
</tr>
<tr>
<td class="td1">line 4:</td>
<td class="td1"><em>L</em><sub>1</sub> = <span class="font1"><span class="font1">〈</span></span>0, 104<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1">line 5:</td>
<td class="td1"><em>L</em><sub>1</sub> = <span class="font1"><span class="font1">〈</span></span>0, 104<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1"><p class="noindent1-top">line 3:</p></td>
<td class="td1"><p class="noindent1-top"><em>L</em><sub>2</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 104, 206<span class="font1"><span class="font1">〉</span></span>,</p></td>
</tr>
<tr>
<td class="td1">line 4:</td>
<td class="td1"><em>L</em><sub>2</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 206<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1">line 5:</td>
<td class="td1"><em>L</em><sub>2</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 206<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1"><p class="noindent1-top">line 3:</p></td>
<td class="td1"><p class="noindent1-top"><em>L</em><sub>3</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 201, 206, 303, 407<span class="font1"><span class="font1">〉</span></span>,</p></td>
</tr>
<tr>
<td class="td1">line 4:</td>
<td class="td1"><em>L</em><sub>3</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 201, 303, 407<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1">line 5:</td>
<td class="td1"><em>L</em><sub>3</sub> = <span class="font1"><span class="font1">〈</span></span>0, 102, 201, 303<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1"><p class="noindent1-top">line 3:</p></td>
<td class="td1"><p class="noindent1-top"><em>L</em><sub>4</sub> = <span class="font1"><span class="font1">〈</span></span>0, 101, 102, 201, 203, 302, 303, 404<span class="font1"><span class="font1">〉</span></span>,</p></td>
</tr>
<tr>
<td class="td1">line 4:</td>
<td class="td1"><em>L</em><sub>4</sub> = <span class="font1"><span class="font1">〈</span></span>0, 101, 201, 302, 404<span class="font1"><span class="font1">〉</span></span>,</td>
</tr>
<tr>
<td class="td1">line 5:</td>
<td class="td1"><em>L</em><sub>4</sub> = <span class="font1"><span class="font1">〈</span></span>0, 101, 201, 302<span class="font1"><span class="font1">〉</span></span>.</td>
</tr>
</table>
<p class="noindent1-top">The procedure returns <em>z</em>* = 302 as its answer, which is well within <em><span class="font1">ϵ</span></em> = 40% of the optimal answer 307 = 104 + 102 + 101. In fact, it is within 2%.</p>
<p class="theo"><strong><em>Theorem 35.7</em></strong></p>
<p class="noindent">A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> is a fully polynomial-time approximation scheme for the subset-sum problem.</p>
<a id="p1129"/>
<p class="prof"><strong><em>Proof</em></strong>   The operations of trimming <em>L<sub>i</sub></em> in line 4 and removing from <em>L<sub>i</sub></em> every element that is greater than <em>t</em> maintain the property that every element of <em>L<sub>i</sub></em> is also a member of <em>P<sub>i</sub></em>. Therefore, the value <em>z</em>* returned in line 7 is indeed the sum of some subset of <em>S</em>, that is, <em>z</em>* ∈ <em>P<sub>n</sub></em>. Let <em>y</em>* ∈ <em>P<sub>n</sub></em> denote an optimal solution to the subset-sum problem, so that it is the greatest value in <em>P<sub>n</sub></em> that is less than or equal to <em>t</em>. Because line 5 ensures that <em>z</em>* ≤ <em>t</em>, we know that <em>z</em>* ≤ <em>y</em>*. By inequality (35.1), we need to show that <em>y</em>*/<em>z</em>* ≤ 1 + <em><span class="font1">ϵ</span></em>. We must also show that the running time of this algorithm is polynomial in both 1/<em><span class="font1">ϵ</span></em> and the size of the input.</p>
<p>As Exercise 35.5-2 asks you to show, for every element <em>y</em> in <em>P<sub>i</sub></em> that is at most <em>t</em>, there exists an element <em>z</em> ∈ <em>L<sub>i</sub></em> such that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1512.jpg"/></p>
<p class="noindent">Inequality (35.24) must hold for <em>y</em>* ∈ <em>P<sub>n</sub></em>, and therefore there exists an element <em>z</em> ∈ <em>L<sub>n</sub></em> such that</p>
<p class="eql"><img alt="art" src="images/Art_P1513.jpg"/></p>
<p class="noindent">and thus</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1514.jpg"/></p>
<p class="noindent">Since there exists an element <em>z</em> ∈ <em>L<sub>n</sub></em> fulfilling inequality (35.25), the inequality must hold for <em>z</em>*, which is the largest value in <em>L<sub>n</sub></em>, which is to say</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1515.jpg"/></p>
<p>Now we show that <em>y</em>*/<em>z</em>* ≤ 1 + <em><span class="font1">ϵ</span></em>. We do so by showing that (1 +<em><span class="font1">ϵ</span></em>/2<em>n</em>)<em><sup>n</sup></em> ≤ 1 + <em><span class="font1">ϵ</span></em>. First, inequality (35.23), 0 &lt; <em><span class="font1">ϵ</span></em> &lt; 1, implies that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1516.jpg"/></p>
<p class="noindent">Next, from equation (3.16) on page 66, we have lim<sub><em>n</em>→∞</sub>(1 + <em><span class="font1">ϵ</span></em>/2<em>n</em>)<em><sup>n</sup></em> = <em>e</em><sup><em><span class="font1">ϵ</span></em>/2</sup>. Exercise 35.5-3 asks you to show that</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1517.jpg"/></p>
<p class="noindent">Therefore, the function (1 + <em><span class="font1">ϵ</span></em>/2<em>n</em>)<em><sup>n</sup></em> increases with <em>n</em> as it approaches its limit of <em>e</em><sup><em><span class="font1">ϵ</span></em>/2</sup>, and we have</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P1518.jpg"/></p>
<a id="p1130"/>
<p class="noindent">Combining inequalities (35.26) and (35.29) completes the analysis of the approximation ratio.</p>
<p>To show that A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> is a fully polynomial-time approximation scheme, we derive a bound on the length of <em>L<sub>i</sub></em>. After trimming, successive elements <em>z</em> and <em>z</em>′ of <em>L<sub>i</sub></em> must have the relationship <em>z</em>′/<em>z</em> &gt; 1 + <em><span class="font1">ϵ</span></em>/2<em>n</em>. That is, they must differ by a factor of at least 1 + <em><span class="font1">ϵ</span></em>/2<em>n</em>. Each list, therefore, contains the value 0, possibly the value 1, and up to <span class="font1">⌊</span>log<sub>1 + <em><span class="font1">ϵ</span></em>/2<em>n</em></sub> <em>t</em><span class="font1">⌋</span> additional values. The number of elements in each list <em>L<sub>i</sub></em> is at most</p>
<p class="eql"><img alt="art" class="width100" src="images/Art_P1519.jpg"/></p>
<p class="noindent">This bound is polynomial in the size of the input—which is the number of bits lg <em>t</em> needed to represent <em>t</em> plus the number of bits needed to represent the set <em>S</em>, which in turn is polynomial in <em>n</em>—and in 1/<em><span class="font1">ϵ</span></em>. Since the running time of A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> is polynomial in the lengths of the lists <em>L<sub>i</sub></em>, we conclude that A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> is a fully polynomial-time approximation scheme.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>35.5-1</em></strong></p>
<p class="noindent">Prove equation (35.21). Then show that after executing line 4 of E<small>XACT</small>-S<small>UBSET</small>-S<small>UM</small>, <em>L<sub>i</sub></em> is a sorted list containing every element of <em>P<sub>i</sub></em> whose value is not more than <em>t</em>.</p>
<p class="level3"><strong><em>35.5-2</em></strong></p>
<p class="noindent">Using induction on <em>i</em>, prove inequality (35.24).</p>
<p class="level3"><strong><em>35.5-3</em></strong></p>
<p class="noindent">Prove inequality (35.28).</p>
<p class="level3"><strong><em>35.5-4</em></strong></p>
<p class="noindent">How can you modify the approximation scheme presented in this section to find a good approximation to the smallest value not less than <em>t</em> that is a sum of some subset of the given input list?</p>
<p class="level3"><strong><em>35.5-5</em></strong></p>
<p class="noindent">Modify the A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> procedure to also return the subset of <em>S</em> that sums to the value <em>z</em>*.</p>
<a id="p1131"/>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-211"><strong>Problems</strong></p>
<section title="35-1 Bin packing">
<p class="level2"><strong><em>35-1     Bin packing</em></strong></p>
<p class="noindent">You are given a set of <em>n</em> objects, where the size <em>s<sub>i</sub></em> of the <em>i</em>th object satisfies 0 &lt; <em>s</em><sub><em>i</em></sub> &lt; 1. Your goal is to pack all the objects into the minimum number of unit-size bins. Each bin can hold any subset of the objects whose total size does not exceed 1.</p>
<p class="nl"><strong><em>a.</em></strong> Prove that the problem of determining the minimum number of bins required is NP-hard. (<em>Hint:</em> Reduce from the subset-sum problem.)</p>
<p class="noindent1-top">The <strong><em><span class="blue">first-fit</span></em></strong> heuristic takes each object in turn and places it into the first bin that can accommodate it, as follows. It maintains an ordered list of bins. Let <em>b</em> denote the number of bins in the list, where <em>b</em> increases over the course of the algorithm, and let <span class="font1"><span class="font1">〈</span></span><em>B</em><sub>1</sub>, … , <em>B<sub>b</sub></em><span class="font1"><span class="font1">〉</span></span> be the list of bins. Initially <em>b</em> = 0 and the list is empty. The algorithm takes each object <em>i</em> in turn and places it in the lowest-numbered bin that can still accommodate it. If no bin can accommodate object <em>i</em>, then <em>b</em> is incremented and a new bin <em>B<sub>b</sub></em> is opened, containing object <em>i</em>. Let <img alt="art" src="images/Art_P1520.jpg"/>.</p>
<p class="nl"><strong><em>b.</em></strong> Argue that the optimal number of bins required is at least <span class="font1">⌈</span><em>S</em><span class="font1">⌉</span>.</p>
<p class="nl"><strong><em>c.</em></strong> Argue that the first-fit heuristic leaves at most one bin at most half full.</p>
<p class="nl"><strong><em>d.</em></strong> Prove that the number of bins used by the first-fit heuristic never exceeds <span class="font1">⌈</span>2<em>S</em><span class="font1">⌉</span>.</p>
<p class="nl"><strong><em>e.</em></strong> Prove an approximation ratio of 2 for the first-fit heuristic.</p>
<p class="nl"><strong><em>f.</em></strong> Give an efficient implementation of the first-fit heuristic, and analyze its running time.</p>
</section>
<section title="35-2 Approximating the size of a maximum clique">
<p class="level2"><strong><em>35-2     Approximating the size of a maximum clique</em></strong></p>
<p class="noindent">Let <em>G</em> = (<em>V</em>, <em>E</em>) be an undirected graph. For any <em>k</em> ≥ 1, define <em>G</em><sup>(<em>k</em>)</sup> to be the undirected graph (<em>V</em> <sup>(<em>k</em>)</sup>, <em>E</em><sup>(<em>k</em>)</sup>), where <em>V</em> <sup>(<em>k</em>)</sup> is the set of all ordered <em>k</em>-tuples of vertices from <em>V</em> and <em>E</em><sup>(<em>k</em>)</sup> is defined so that (<em>v</em><sub>1</sub>, <em>v</em><sub>2</sub>, … , <em>v<sub>k</sub></em>) is adjacent to (<em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, … , <em>w<sub>k</sub></em>) if and only if for <em>i</em> = 1, 2, … , <em>k</em>, either vertex <em>v<sub>i</sub></em> is adjacent to <em>w<sub>i</sub></em> in <em>G</em>, or else <em>v<sub>i</sub></em> = <em>w<sub>i</sub></em>.</p>
<p class="nl"><strong><em>a.</em></strong> Prove that the size of the maximum clique in <em>G</em><sup>(<em>k</em>)</sup> is equal to the <em>k</em>th power of the size of the maximum clique in <em>G</em>.</p>
<p class="nl"><strong><em>b.</em></strong> Argue that if there is an approximation algorithm that has a constant approximation ratio for finding a maximum-size clique, then there is a polynomial-time approximation scheme for the problem.</p>
<a id="p1132"/>
</section>
<section title="35-3 Weighted set-covering problem">
<p class="level2"><strong><em>35-3     Weighted set-covering problem</em></strong></p>
<p class="noindent">Suppose that sets have weights in the set-covering problem, so that each set <em>S<sub>i</sub></em> in the family <em><span class="font1">ℱ</span></em> has an associated weight <em>w<sub>i</sub></em>. The weight of a cover <span class="script">C</span> is <img alt="art" src="images/Art_P1521.jpg"/>. The goal is wish to determine a minimum-weight cover. (<a href="chapter035.xhtml#Sec_35.3">Section 35.3</a> handles the case in which <em>w<sub>i</sub></em> = 1 for all <em>i</em>.)</p>
<p>Show how to generalize the greedy set-covering heuristic in a natural manner to provide an approximate solution for any instance of the weighted set-covering problem. Letting <em>d</em> be the maximum size of any set <em>S<sub>i</sub></em>, show that your heuristic has an approximation ratio of <img alt="art" src="images/Art_P1522.jpg"/>.</p>
</section>
<section title="35-4 Maximum matching">
<p class="level2"><strong><em>35-4     Maximum matching</em></strong></p>
<p class="noindent">Recall that for an undirected graph <em>G</em>, a matching is a set of edges such that no two edges in the set are incident on the same vertex. <a href="chapter025.xhtml#Sec_25.1">Section 25.1</a> showed how to find a maximum matching in a bipartite graph, that is, a matching such that no other matching in <em>G</em> contains more edges. This problem examines matchings in undirected graphs that are not required to be bipartite.</p>
<p class="nl"><strong><em>a.</em></strong> Show that a maximal matching need not be a maximum matching by exhibiting an undirected graph <em>G</em> and a maximal matching <em>M</em> in <em>G</em> that is not a maximum matching. (<em>Hint:</em> You can find such a graph with only four vertices.)</p>
<p class="nl"><strong><em>b.</em></strong> Consider a connected, undirected graph <em>G</em> = (<em>V</em>, <em>E</em>). Give an <em>O</em>(<em>E</em>)-time greedy algorithm to find a maximal matching in <em>G</em>.</p>
<p class="noindent1-top">This problem concentrates on a polynomial-time approximation algorithm for maximum matching. Whereas the fastest known algorithm for maximum matching takes superlinear (but polynomial) time, the approximation algorithm here will run in linear time. You will show that the linear-time greedy algorithm for maximal matching in part (b) is a 2-approximation algorithm for maximum matching.</p>
<p class="nl"><strong><em>c.</em></strong> Show that the size of a maximum matching in <em>G</em> is a lower bound on the size of any vertex cover for <em>G</em>.</p>
<p class="nl"><strong><em>d.</em></strong> Consider a maximal matching <em>M</em> in <em>G</em> = (<em>V</em>, <em>E</em>). Let <em>T</em> = {<em>v</em> ∈ <em>V</em> : some edge in <em>M</em> is incident on <em>v</em>}. What can you say about the subgraph of <em>G</em> induced by the vertices of <em>G</em> that are not in <em>T</em> ?</p>
<p class="nl"><strong><em>e.</em></strong> Conclude from part (d) that 2 |<em>M</em>| is the size of a vertex cover for <em>G</em>.</p>
<p class="nl"><strong><em>f.</em></strong> Using parts (c) and (e), prove that the greedy algorithm in part (b) is a 2-approximation algorithm for maximum matching.</p>
<a id="p1133"/>
</section>
<section title="35-5 Parallel machine scheduling">
<p class="level2"><strong><em>35-5     Parallel machine scheduling</em></strong></p>
<p class="noindent">In the <strong><em><span class="blue">parallel-machine-scheduling problem</span></em></strong>, the input has two parts: <em>n</em> jobs, <em>J</em><sub>1</sub>, <em>J</em><sub>2</sub>, … , <em>J<sub>n</sub></em>, where each job <em>J<sub>k</sub></em> has an associated nonnegative processing time of <em>p<sub>k</sub></em>, and <em>m</em> identical machines, <em>M</em><sub>1</sub>, <em>M</em><sub>2</sub>, … , <em>M<sub>m</sub></em>. Any job can run on any machine. A <strong><em><span class="blue">schedule</span></em></strong> specifies, for each job <em>J<sub>k</sub></em>, the machine on which it runs and the time period during which it runs. Each job <em>J<sub>k</sub></em> must run on some machine <em>M<sub>i</sub></em> for <em>p<sub>k</sub></em> consecutive time units, and during that time period no other job may run on <em>M<sub>i</sub></em>. Let <em>C<sub>k</sub></em> denote the <strong><em><span class="blue">completion time</span></em></strong> of job <em>J<sub>k</sub></em>, that is, the time at which job <em>J<sub>k</sub></em> completes processing. Given a schedule, define <em>C</em><sub>max</sub> = max {<em>C<sub>j</sub></em> : 1 ≤ <em>j</em> ≤ <em>n</em>} to be the <strong><em><span class="blue">makespan</span></em></strong> of the schedule. The goal is to find a schedule whose makespan is minimum.</p>
<p>For example, consider an input with two machines <em>M</em><sub>1</sub> and <em>M</em><sub>2</sub>, and four jobs <em>J</em><sub>1</sub>, <em>J</em><sub>2</sub>, <em>J</em><sub>3</sub>, and <em>J</em><sub>4</sub> with <em>p</em><sub>1</sub> = 2, <em>p</em><sub>2</sub> = 12, <em>p</em><sub>3</sub> = 4, and <em>p</em><sub>4</sub> = 5. Then one possible schedule runs, on machine <em>M</em><sub>1</sub>, job <em>J</em><sub>1</sub> followed by job <em>J</em><sub>2</sub>, and on machine <em>M</em><sub>2</sub>, job <em>J</em><sub>4</sub> followed by job <em>J</em><sub>3</sub>. For this schedule, <em>C</em><sub>1</sub> = 2, <em>C</em><sub>2</sub> = 14, <em>C</em><sub>3</sub> = 9, <em>C</em><sub>4</sub> = 5, and <em>C</em><sub>max</sub> = 14. An optimal schedule runs job <em>J</em><sub>2</sub> on machine <em>M</em><sub>1</sub> and jobs <em>J</em><sub>1</sub>, <em>J</em><sub>3</sub>, and <em>J</em><sub>4</sub> on machine <em>M</em><sub>2</sub>. For this schedule, we have <em>C</em><sub>1</sub> = 2, <em>C</em><sub>2</sub> = 12, <em>C</em><sub>3</sub> = 6, and <em>C</em><sub>4</sub> = 11, and so <em>C</em><sub>max</sub> = 12.</p>
<p>Given the input to a parallel-machine-scheduling problem, let <img alt="art" src="images/Art_P1522a.jpg"/> denote the makespan of an optimal schedule.</p>
<p class="nl"><strong><em>a.</em></strong> Show that the optimal makespan is at least as large as the greatest processing time, that is,</p>
<p class="eqnl"><img alt="art" src="images/Art_P1522b.jpg"/></p>
<p class="nl"><strong>b.</strong> Show that the optimal makespan is at least as large as the average machine load, that is,</p>
<p class="eqnl"><img alt="art" src="images/Art_P1523.jpg"/></p>
<p class="noindent">Consider the following greedy algorithm for parallel machine scheduling: whenever a machine is idle, schedule any job that has not yet been scheduled.</p>
<p class="nl"><strong><em>c.</em></strong> Write pseudocode to implement this greedy algorithm. What is the running time of your algorithm?</p>
<p class="nl"><strong><em>d.</em></strong> For the schedule returned by the greedy algorithm, show that</p>
<p class="eqnl"><img alt="art" src="images/Art_P1524.jpg"/></p>
<p class="noindent">Conclude that this algorithm is a polynomial-time 2-approximation algorithm.</p>
<a id="p1134"/>
</section>
<section title="35-6 Approximating a maximum spanning tree">
<p class="level2"><strong><em>35-6     Approximating a maximum spanning tree</em></strong></p>
<p class="noindent">Let <em>G</em> = (<em>V</em>, <em>E</em>) be an undirected graph with distinct edge weights <em>w</em>(<em>u, v</em>) on each edge (<em>u</em>, <em>v</em>) ∈ <em>E</em>. For each vertex <em>v</em> ∈ <em>V</em>, denote by max(<em>v</em>) the maximum-weight edge incident on that vertex. Let <em>S</em><sub><em>G</em></sub>= {max(<em>v</em>) : <em>v</em> ∈ <em>V</em> } be the set of maximum-weight edges incident on each vertex, and let <em>T</em><sub><em>G</em></sub> be the maximum-weight spanning tree of <em>G</em>, that is, the spanning tree of maximum total weight. For any subset of edges <em>E</em>′ ⊆ <em>E</em>, define <em>w</em>(<em>E</em>′) = Σ<sub>(<em>u</em>,<em>v</em>)∈<em>E′</em></sub> <em>w</em>(<em>u, v</em>).</p>
<p class="nl"><strong><em>a.</em></strong> Give an example of a graph with at least 4 vertices for which <em>S</em><sub><em>G</em></sub> = <em>T<sub>G</sub></em>.</p>
<p class="nl"><strong><em>b.</em></strong> Give an example of a graph with at least 4 vertices for which <em>S</em><sub><em>G</em></sub> ≠ <em>T<sub>G</sub></em>.</p>
<p class="nl"><strong><em>c.</em></strong> Prove that <em>S</em><sub><em>G</em></sub> ⊆ <em>T</em><sub><em>G</em></sub> for any graph <em>G</em>.</p>
<p class="nl"><strong><em>d.</em></strong> Prove that <em>w</em>(<em>S<sub>G</sub></em>) ≥ <em>w</em>(<em>T<sub>G</sub></em>)/2 for any graph <em>G</em>.</p>
<p class="nl"><strong><em>e.</em></strong> Give an <em>O</em>(<em>V</em> + <em>E</em>)-time algorithm to compute a 2-approximation to the maximum spanning tree.</p>
</section>
<section title="35-7 An approximation algorithm for the 0-1 knapsack problem">
<p class="level2"><strong><em>35-7     An approximation algorithm for the 0-1 knapsack problem</em></strong></p>
<p class="noindent">Recall the knapsack problem from <a href="chapter015.xhtml#Sec_15.2">Section 15.2</a>. The input includes <em>n</em> items, where the <em>i</em>th item is worth <em>v<sub>i</sub></em> dollars and weighs <em>w<sub>i</sub></em> pounds. The input also includes the capacity of a knapsack, which is <em>W</em> pounds. Here, we add the further assumptions that each weight <em>w<sub>i</sub></em> is at most <em>W</em> and that the items are indexed in monotonically decreasing order of their values: <em>v</em><sub>1</sub> ≥ <em>v</em><sub>2</sub> ≥ <span class="font1">⋯</span> ≥ <em>v<sub>n</sub></em>.</p>
<p>In the 0-1 knapsack problem, the goal is to find a subset of the items whose total weight is at most <em>W</em> and whose total value is maximum. The fractional knapsack problem is like the 0-1 knapsack problem, except that a fraction of each item may be put into the knapsack, rather than either all or none of each item. If a fraction <em>x<sub>i</sub></em> of item <em>i</em> goes into the knapsack, where 0 ≤ <em>x<sub>i</sub></em> ≤ 1, it contributes <em>x<sub>i</sub>w<sub>i</sub></em> to the weight of the knapsack and adds value <em>x<sub>i</sub>v<sub>i</sub></em>. The goal of this problem is to develop a polynomial-time 2-approximation algorithm for the 0-1 knapsack problem.</p>
<p>In order to design a polynomial-time algorithm, let’s consider restricted instances of the 0-1 knapsack problem. Given an instance <em>I</em> of the knapsack problem, form restricted instances <em>I<sub>j</sub></em>, for <em>j</em> = 1, 2, … , <em>n</em>, by removing items 1, 2, … , <em>j</em> − 1 and requiring the solution to include item <em>j</em> (all of item <em>j</em> in both the fractional and 0-1 knapsack problems). No items are removed in instance <em>I</em><sub>1</sub>. For instance <em>I<sub>j</sub></em>, let <em>P<sub>j</sub></em> denote an optimal solution to the 0-1 problem and <em>Q<sub>j</sub></em> denote an optimal solution to the fractional problem.</p>
<p class="nl"><strong><em>a.</em></strong> Argue that an optimal solution to instance <em>I</em> of the 0-1 knapsack problem is one of {<em>P</em><sub>1</sub>, <em>P</em><sub>2</sub>, … , <em>P<sub>n</sub></em>}.</p>
<a id="p1135"/>
<p class="nl"><strong><em>b.</em></strong> Prove that to find an optimal solution <em>Q<sub>j</sub></em> to the fractional problem for instance <em>I<sub>j</sub></em>, you can include item <em>j</em> and then use the greedy algorithm in which each step takes as much as possible of the unchosen item with the maximum value per pound <em>v<sub>i</sub></em>/<em>w<sub>i</sub></em> in the set {<em>j</em> + 1, <em>j</em> + 2, … , <em>n</em>}.</p>
<p class="nl"><strong><em>c.</em></strong> Prove that there is always an optimal solution <em>Q<sub>j</sub></em> to the fractional problem for instance <em>I<sub>j</sub></em> that includes at most one item fractionally. That is, for all items except possibly one, either all of the item or none of the item goes into the knapsack.</p>
<p class="nl"><strong><em>d.</em></strong> Given an optimal solution <em>Q<sub>j</sub></em> to the fractional problem for instance <em>I<sub>j</sub></em>, form solution <em>R<sub>j</sub></em> from <em>Q<sub>j</sub></em> by deleting any fractional items from <em>Q<sub>j</sub></em>. Let <em>v</em>(<em>S</em>) denote the total value of items taken in a solution <em>S</em>. Prove that <em>v</em>(<em>R<sub>j</sub></em>) ≥ <em>v</em>(<em>Q<sub>j</sub></em>)/2 ≥ <em>v</em>(<em>P<sub>j</sub></em>)/2.</p>
<p class="nl"><strong><em>e.</em></strong> Give a polynomial-time algorithm that returns a maximum-value solution from the set {<em>R</em><sub>1</sub>, <em>R</em><sub>2</sub>, … , <em>R<sub>n</sub></em>}, and prove that your algorithm is a polynomial-time 2-approximation algorithm for the 0-1 knapsack problem.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-212"><strong>Chapter notes</strong></p>
<p class="noindent">Although methods that do not necessarily compute exact solutions have been known for thousands of years (for example, methods to approximate the value of <em>π</em>), the notion of an approximation algorithm is much more recent. Hochbaum [<a epub:type="noteref" href="bibliography001.xhtml#endnote_221">221</a>] credits Garey, Graham, and Ullman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_175">175</a>] and Johnson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_236">236</a>] with formalizing the concept of a polynomial-time approximation algorithm. The first such algorithm is often credited to Graham [<a epub:type="noteref" href="bibliography001.xhtml#endnote_197">197</a>].</p>
<p>Since this early work, thousands of approximation algorithms have been designed for a wide range of problems, and there is a wealth of literature on this field. Texts by Ausiello et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_29">29</a>], Hochbaum [<a epub:type="noteref" href="bibliography001.xhtml#endnote_221">221</a>], Vazirani [<a epub:type="noteref" href="bibliography001.xhtml#endnote_446">446</a>], and Williamson and Shmoys [<a epub:type="noteref" href="bibliography001.xhtml#endnote_459">459</a>] deal exclusively with approximation algorithms, as do surveys by Shmoys [<a epub:type="noteref" href="bibliography001.xhtml#endnote_409">409</a>] and Klein and Young [<a epub:type="noteref" href="bibliography001.xhtml#endnote_256">256</a>]. Several other texts, such as Garey and Johnson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_176">176</a>] and Papadimitriou and Steiglitz [<a epub:type="noteref" href="bibliography001.xhtml#endnote_353">353</a>], have significant coverage of approximation algorithms as well. Books edited by Lawler, Lenstra, Rinnooy Kan, and Shmoys [<a epub:type="noteref" href="bibliography001.xhtml#endnote_277">277</a>] and by Gutin and Punnen [<a epub:type="noteref" href="bibliography001.xhtml#endnote_204">204</a>] provide extensive treatments of approximation algorithms and heuristics for the traveling-salesperson problem.</p>
<p>Papadimitriou and Steiglitz attribute the algorithm A<small>PPROX</small>-V<small>ERTEX</small>-C<small>OVER</small> to F. Gavril and M. Yannakakis. The vertex-cover problem has been studied extensively (Hochbaum [<a epub:type="noteref" href="bibliography001.xhtml#endnote_221">221</a>] lists 16 different approximation algorithms for this problem), but all the approximation ratios are at least 2 − <em>o</em>(1).</p>
<a id="p1136"/>
<p>The algorithm A<small>PPROX</small>-TSP-T<small>OUR</small> appears in a paper by Rosenkrantz, Stearns, and Lewis [<a epub:type="noteref" href="bibliography001.xhtml#endnote_384">384</a>]. Christofides improved on this algorithm and gave a 3/2-approximation algorithm for the traveling-salesperson problem with the triangle inequality. Arora [<a epub:type="noteref" href="bibliography001.xhtml#endnote_23">23</a>] and Mitchell [<a epub:type="noteref" href="bibliography001.xhtml#endnote_330">330</a>] have shown that if the points lie in the euclidean plane, there is a polynomial-time approximation scheme. Theorem 35.3 is due to Sahni and Gonzalez [<a epub:type="noteref" href="bibliography001.xhtml#endnote_392">392</a>].</p>
<p>The algorithm A<small>PPROX</small>-S<small>UBSET</small>-S<small>UM</small> and its analysis are loosely modeled after related approximation algorithms for the knapsack and subset-sum problems by Ibarra and Kim [<a epub:type="noteref" href="bibliography001.xhtml#endnote_234">234</a>].</p>
<p>Problem 35-7 is a combinatorial version of a more general result on approximating knapsack-type integer programs by Bienstock and McClosky [<a epub:type="noteref" href="bibliography001.xhtml#endnote_55">55</a>].</p>
<p>The randomized algorithm for MAX-3-CNF satisfiability is implicit in the work of Johnson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_236">236</a>]. The weighted vertex-cover algorithm is by Hochbaum [<a epub:type="noteref" href="bibliography001.xhtml#endnote_220">220</a>]. <a href="chapter035.xhtml#Sec_35.4">Section 35.4</a> only touches on the power of randomization and linear programming in the design of approximation algorithms. A combination of these two ideas yields a technique called “randomized rounding,” which formulates a problem as an integer linear program, solves the linear-programming relaxation, and interprets the variables in the solution as probabilities. These probabilities then help guide the solution of the original problem. This technique was first used by Raghavan and Thompson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_374">374</a>], and it has had many subsequent uses. (See Motwani, Naor, and Raghavan [<a epub:type="noteref" href="bibliography001.xhtml#endnote_335">335</a>] for a survey.) Several other notable ideas in the field of approximation algorithms include the primal-dual method (see Goemans and Williamson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_184">184</a>] for a survey), finding sparse cuts for use in divide-and-conquer algorithms [<a epub:type="noteref" href="bibliography001.xhtml#endnote_288">288</a>], and the use of semidefinite programming [<a epub:type="noteref" href="bibliography001.xhtml#endnote_183">183</a>].</p>
<p>As mentioned in the chapter notes for <a href="chapter034.xhtml">Chapter 34</a>, results in probabilistically checkable proofs have led to lower bounds on the approximability of many problems, including several in this chapter. In addition to the references there, the chapter by Arora and Lund [<a epub:type="noteref" href="bibliography001.xhtml#endnote_26">26</a>] contains a good description of the relationship between probabilistically checkable proofs and the hardness of approximating various problems.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> When the approximation ratio is independent of <em>n</em>, we use the terms “approximation ratio of <em>ρ</em>” and “<em>ρ</em>-approximation algorithm,” indicating no dependence on <em>n</em>.</p>
<a id="p1137"/>
</section>
</section>
</div>
</body>
</html>