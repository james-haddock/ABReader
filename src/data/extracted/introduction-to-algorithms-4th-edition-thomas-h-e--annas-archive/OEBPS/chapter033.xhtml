<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p1003"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="33 Machine-Learning Algorithms">
<p class="chapter-title"><a href="toc.xhtml#chap-33"><strong><span class="blue1">33        Machine-Learning Algorithms</span></strong></a></p>
<p class="noindent">Machine learning may be viewed as a subfield of artificial intelligence. Broadly speaking, artificial intelligence aims to enable computers to carry out complex perception and information-processing tasks with human-like performance. The field of AI is vast and uses many different algorithmic methods.</p>
<p>Machine learning is rich and fascinating, with strong ties to statistics and optimization. Technology today produces enormous amounts of data, providing myriad opportunities for machine-learning algorithms to formulate and test hypotheses about patterns within the data. These hypotheses can then be used to make predictions about the characteristics or classifications in new data. Because machine learning is particularly good with challenging tasks involving uncertainty, where observed data follows unknown rules, it has markedly transformed fields such as medicine, advertising, and speech recognition.</p>
<p>This chapter presents three important machine-learning algorithms: <em>k</em>-means clustering, multiplicative weights, and gradient descent. You can view each of these tasks as a learning problem, whereby an algorithm uses the data collected so far to produce a hypothesis that describes the regularities learned and/or makes predictions about new data. The boundaries of machine learning are imprecise and evolving—some might say that the <em>k</em>-means clustering algorithm should be called “data science” and not “machine learning,” and gradient descent, though an immensely important algorithm for machine learning, also has a multitude of applications outside of machine learning (most notably for optimization problems).</p>
<p>Machine learning typically starts with a <span class="blue"><strong><em>training phase</em></strong></span> followed by a <span class="blue"><strong><em>prediction phase</em></strong></span> in which predictions are made about new data. For <span class="blue"><strong><em>online learning</em></strong></span>, the training and prediction phases are intermingled. The training phase takes as input <span class="blue"><strong><em>training data</em></strong></span>, where each input data point has an associated output or <span class="blue"><strong><em>label</em></strong></span>; the label might be a category name or some real-valued attribute. It then produces as an output one or more <span class="blue"><strong><em>hypotheses</em></strong></span> about how the labels depend on the attributes of the input data points. Hypotheses can take many forms, typically some type of formula or algorithm. The learning algorithm used is often a form of gradient <a id="p1004"/>descent. The prediction phase then uses the hypothesis on new data in order to make <span class="blue"><strong><em>predictions</em></strong></span> regarding the labels of new data points.</p>
<p>The type of learning just described is known as <span class="blue"><strong><em>supervised learning</em></strong></span>, since it starts with a set of inputs that are each labeled. As an example, consider a machine-learning algorithm to recognize spam emails. The training data comprises a collection of emails, each of which is labeled either “spam” or “not spam.” The machine-learning algorithm frames a hypothesis, possibly a rule of the form “if an email has one of a set of words, then it is likely to be spam.” Or it might learn rules that assign a spam score to each word and then evaluates a document by the sum of the spam scores of its constituent words, so that a document with a total score above a certain threshold value is classified as spam. The machine-learning algorithm can then predict whether a new email is spam or not.</p>
<p>A second form of machine learning is <span class="blue"><strong><em>unsupervised learning</em></strong></span>, where the training data is unlabeled, as in the clustering problem of <a href="chapter033.xhtml#Sec_33.1">Section 33.1</a>. Here the machine-learning algorithm produces hypotheses regarding the centers of groups of input data points.</p>
<p>A third form of machine learning (not covered further here) is <span class="blue"><strong><em>reinforcement learning</em></strong></span>, where the machine-learning algorithm takes actions in an environment, receives feedback for those actions from the environment, and then updates its model of the environment based on the feedback. The learner is in an environment that has some state, and the actions of the learner have an effect on that state. Reinforcement learning is a natural choice for situations such as game playing or operating a self-driving car.</p>
<p>Sometimes the goal in a supervised machine-learning application is not making accurate predictions of labels for new examples, but rather performing causal <span class="blue"><strong><em>inference</em></strong></span>: finding an explanatory model that describes how the various features of an input data point affect its associated label. Finding a model that fits a given set of training data well can be tricky. It may involve sophisticated optimization methods that need to balance between producing a hypothesis that fits the data well and producing a hypothesis that is simple.</p>
<p>This chapter focuses on three problem domains: finding hypotheses that group the input data points well (using a clustering algorithm), learning which predictors (experts) to rely upon for making predictions in an online learning problem (using the multiplicative-weights algorithm), and fitting a model to data (using gradient descent).</p>
<p><a href="chapter033.xhtml#Sec_33.1">Section 33.1</a> considers the clustering problem: how to divide a given set of <em>n</em> training data points into a given number <em>k</em> of groups, or “clusters,” based on a measure of how similar (or more accurately, how dissimilar) points are to each other. The approach is iterative, beginning with an arbitrary initial clustering and incorporating successive improvements until no further improvements occur. Clustering <a id="p1005"/>is often used as an initial step when working on a machine-learning problem to discover what structure exists in the data.</p>
<p><a href="chapter033.xhtml#Sec_33.2">Section 33.2</a> shows how to make online predictions quite accurately when you have a set of predictors, often called “experts,” to rely on, many of which might be poor predictors, but some of which are good predictors. At first, you do not know which predictors are poor and which are good. The goal is to make predictions on new examples that are nearly as good as the predictions made by the best predictor. We study an effective multiplicative-weights prediction method that associates a positive real weight with each predictor and multiplicatively decreases the weights associated with predictors when they make poor predictions. The model in this section is online (see <a href="chapter027.xhtml">Chapter 27</a>): at each step, we do not know anything about the future examples. In addition, we are able to make predictions even in the presence of adversarial experts, who are collaborating against us, a situation that actually happens in game-playing settings.</p>
<p>Finally, <a href="chapter033.xhtml#Sec_33.3">Section 33.3</a> introduces gradient descent, a powerful optimization technique used to find parameter settings in machine-learning models. Gradient descent also has many applications outside of machine learning. Intuitively, gradient descent finds the value that produces a local minimum for a function by “walking downhill.” In a learning application, a “downhill step” is a step that adjusts hypothesis parameters so that the hypothesis does better on the given set of labeled examples.</p>
<p>This chapter makes extensive use of vectors. In contrast to the rest of the book, vector names in this chapter appear in boldface, such as <strong>x</strong>, to more clearly delineate which quantities are vectors. Components of vectors do not appear in boldface, so if vector <strong>x</strong> has <em>d</em> dimensions, we might write <strong>x</strong> = (<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>d</em></sub>).</p>
<p class="line1"/>
<section title="33.1 Clustering">
<a id="Sec_33.1"/>
<p class="level1" id="h1-194"><a href="toc.xhtml#Rh1-194"><strong>33.1    Clustering</strong></a></p>
<p class="noindent">Suppose that you have a large number of data points (examples), and you wish to group them into classes based on how similar they are to each other. For example, each data point might represent a celestial star, giving its temperature, size, and spectral characteristics. Or, each data point might represent a fragment of recorded speech. Grouping these speech fragments appropriately might reveal the set of accents of the fragments. Once a grouping of the training data points is found, new data can be placed into an appropriate group, facilitating star-type recognition or speech recognition.</p>
<p>These situations, along with many others, fall under the umbrella of clustering. The input to a <span class="blue"><strong><em>clustering</em></strong></span> problem is a set of <em>n</em> examples (objects) and an integer <em>k</em>, with the goal of dividing the examples into at most <em>k</em> disjoint clusters such that <a id="p1006"/>the examples in each cluster are similar to each other. The clustering problem has several variations. For example, the integer <em>k</em> might not be given, but instead arises out of the clustering procedure. In this section we presume that <em>k</em> is given.</p>
<p class="level4"><strong>Feature vectors and similarity</strong></p>
<p class="noindent">Let’s formally define the clustering problem. The input is a set of <em>n</em> <span class="blue"><strong><em>examples</em></strong></span>. Each example has a set of <span class="blue"><strong><em>attributes</em></strong></span> in common with all other examples, though the attribute values may vary among examples. For example, the clustering problem shown in <a href="chapter033.xhtml#Fig_33-1">Figure 33.1</a> clusters <em>n</em> = 49 examples—48 state capitals plus the District of Columbia—into <em>k</em> = 4 clusters. Each example has two attributes: the latitude and longitude of the capital. In a given clustering problem, each example has <em>d</em> attributes, with an example <strong>x</strong> specified by a <em>d</em>-dimensional <span class="blue"><strong><em>feature vector</em></strong></span></p>
<p class="eql"><strong>x</strong> = (<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>d</em></sub>).</p>
<p class="noindent">Here, <em>x</em><sub><em>a</em></sub> for <em>a</em> = 1, 2, …, <em>d</em> is a real number giving the value of attribute <em>a</em> for example <strong>x</strong>. We call <strong>x</strong> the <span class="blue"><strong><em>point</em></strong></span> in <span class="font1">ℝ</span><sup><em>d</em></sup> representing the example. For the example in <a href="chapter033.xhtml#Fig_33-1">Figure 33.1</a>, each capital <strong>x</strong> has its latitude in <em>x</em><sub>1</sub> and its longitude in <em>x</em><sub>2</sub>.</p>
<p>In order to cluster similar points together, we need to define similarity. Instead, let’s define the opposite: the <span class="blue"><strong><em>dissimilarity</em></strong></span> Δ(<strong>x</strong>, <strong>y</strong>) of points <strong>x</strong> and <strong>y</strong> is the squared Euclidean distance between them:</p>
<p class="eqr"><img alt="art" src="images/Art_P1325.jpg"/></p>
<p class="noindent">Of course, for Δ(<strong>x</strong>, <strong>y</strong>) to be well defined, all attribute values must be present. If any are missing, then you might just ignore that example, or you could fill in a missing attribute value with the median value for that attribute.</p>
<p>The attribute values are often “messy” in other ways, so that some “data cleaning” is necessary before the clustering algorithm is run. For example, the scale of attribute values can vary widely across attributes. In the example of <a href="chapter033.xhtml#Fig_33-1">Figure 33.1</a>, the scales of the two attributes vary by a factor of 2, since latitude ranges from −90 to +90 degrees but longitude ranges from −180 to +180 degrees. You can imagine other scenarios where the differences in scales are even greater. If the examples contain information about students, one attribute might be grade-point average but another might be family income. Therefore, the attribute values are usually scaled or normalized, so that no single attribute can dominate the others when computing dissimilarities. One way to do so is by scaling attribute values with a linear transform so that the minimum value becomes 0 and the maximum value becomes 1. If the attribute values are binary values, then no scaling may be needed. Another option is scaling so that the values for each attribute have mean 0 and unit variance. Sometimes it makes sense to choose the same scaling rule for several related attributes (for example, if they are lengths measured to the same scale).</p>
<a id="p1007"/>
<div class="divimage">
<p class="fig-imga" id="Fig_33-1"><img alt="art" class="width100" src="images/Art_P1326.jpg"/></p>
<p class="caption"><strong>Figure 33.1</strong> The iterations of Lloyd’s procedure when clustering the capitals of the lower 48 states and the District of Columbia into <em>k</em> = 4 clusters. Each capital has two attributes: latitude and longitude. Each iteration reduces the value <em>f</em>, measuring the sum of squares of distances of all capitals to their cluster centers, until the value of <em>f</em> does not change. <strong>(a)</strong> The initial four clusters, with the capitals of Arkansas, Kansas, Louisiana, and Tennessee chosen as centers. <strong>(b)–(k)</strong> Iterations of Lloyd’s procedure. <strong>(l)</strong> The 11th iteration results in the same value of <em>f</em> as the 10th iteration in part (k), and so the procedure terminates.</p>
</div>
<a id="p1008"/>
<p>Also, the choice of dissimilarity measure is somewhat arbitrary. The use of the sum of squared differences as in equation (33.1) is not required, but it is a conventional choice and mathematically convenient. For the example of <a href="chapter033.xhtml#Fig_33-1">Figure 33.1</a>, you might use the actual distance between capitals rather than equation (33.1).</p>
<p class="level4"><strong>Clusterings</strong></p>
<p class="noindent">With the notion of similarity (actually, <em>dis</em>similarity) defined, let’s see how to define clusters of similar points. Let <em>S</em> denote the given set of <em>n</em> points in <span class="font1">ℝ</span><sup><em>d</em></sup>. In some applications the points are not necessarily distinct, so that <em>S</em> is a multiset rather than a set.</p>
<p>Because the goal is to create <em>k</em> clusters, we define a <span class="blue"><strong><em>k-clustering</em></strong></span> of <em>S</em> as a decomposition of <em>S</em> into a sequence <span class="font1">〈</span><em>S</em><sup>(1)</sup>, <em>S</em><sup>(2)</sup>, …, <em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span> of <em>k</em> disjoint subsets, or <span class="blue"><strong><em>clusters</em></strong></span>, so that</p>
<p class="eql"><em>S</em> = <em>S</em><sup>(1)</sup> <span class="font1">⋃</span> <em>S</em><sup>(2)</sup> <span class="font1">⋃</span> <span class="font1">⋯</span> <span class="font1">⋃</span> <em>S</em><sup>(<em>k</em>)</sup>.</p>
<p class="noindent">A cluster may be empty, for example if <em>k</em> &gt; 1 but all of the points in <em>S</em> have the same attribute values.</p>
<p>There are many ways to define a <em>k</em>-clustering of <em>S</em> and many ways to evaluate the quality of a given <em>k</em>-clustering. We consider here only <em>k</em>-clusterings of <em>S</em> that are defined by a sequence <em>C</em> of <em>k</em> <span class="blue"><strong><em>centers</em></strong></span></p>
<p class="eql"><em>C</em> = <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span>,</p>
<p class="noindent">where each center is a point in <span class="font1">ℝ</span><sup><em>d</em></sup>, and the <span class="blue"><strong><em>nearest-center rule</em></strong></span> says that a point <strong>x</strong> may belong to cluster <em>S</em><sup>(ℓ)</sup> if the center of no other cluster is closer to <strong>x</strong> than the center <strong>c<sup>(ℓ)</sup></strong> of <em>S</em><sup>(ℓ)</sup>:</p>
<p class="eql"><strong>x</strong> ∈ <em>S</em><sup>(ℓ)</sup> only if Δ(<strong>x</strong>, <strong>c<sup>(ℓ)</sup></strong>) = min {Δ(<strong>x</strong>, <strong>c<sup>(<em>j</em>)</sup></strong>): 1 ≤ <em>j</em> ≤ <em>k</em>}.</p>
<p class="noindent">A center can be anywhere, and not necessarily a point in <em>S</em>.</p>
<p>Ties are possible and must be broken so that each point lies in exactly one cluster. In general, ties may be broken arbitrarily, although we’ll need the property that we never change which cluster a point <strong>x</strong> is assigned to unless the distance from <strong>x</strong> to its new cluster center is <em>strictly smaller</em> than the distance from <strong>x</strong> to its old cluster center. That is, if the current cluster has a center that is one of the closest cluster centers to <strong>x</strong>, then don’t change which cluster <strong>x</strong> is assigned to.</p>
<p>The <span class="blue"><strong><em>k-means problem</em></strong></span> is then the following: given a set <em>S</em> of <em>n</em> points and a positive integer <em>k</em>, find a sequence <strong><em>C</em></strong> = <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span> of <em>k</em> center points <a id="p1009"/>minimizing the sum <em>f</em>(<em>S</em>, <em>C</em>) of the squared distance from each point to its nearest center, where</p>
<p class="eqr"><img alt="art" src="images/Art_P1327.jpg"/></p>
<p class="noindent">In the second line, the <em>k</em>-clustering <span class="font1">〈</span><em>S</em><sup>(1)</sup>,<em>S</em><sup>(2)</sup>,…,<em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span> is defined by the centers <strong><em>C</em></strong> and the nearest-center rule. See Exercise 33.1-1 for an alternative formulation based on pairwise interpoint distances.</p>
<p>Is there a polynomial-time algorithm for the <em>k</em>-means problem? Probably not, because it is NP-hard [<a epub:type="noteref" href="bibliography001.xhtml#endnote_310">310</a>]. As we’ll see in <a href="chapter034.xhtml">Chapter 34</a>, NP-hard problems have no known polynomial-time algorithm, but nobody has ever proven that polynomial-time algorithms for NP-hard problems cannot exist. Although we know of no polynomial-time algorithm that finds the global minimum over all clusterings (according to equation (33.2)), we <em>can</em> find a local minimum.</p>
<p>Lloyd [<a epub:type="noteref" href="bibliography001.xhtml#endnote_304">304</a>] proposed a simple procedure that finds a sequence <strong><em>C</em></strong> of <em>k</em> centers that yields a local minimum of <em>f</em>(<strong><em>S</em></strong>, <strong><em>C</em></strong>). A local minimum in the <em>k</em>-means problem satisfies two simple properties: each cluster has an optimal center (defined below), and each point is assigned to the cluster (or one of the clusters) with the closest center. Lloyd’s procedure finds a good clustering—possibly optimal—that satisfies these two properties. These properties are necessary, but not sufficient, for optimality.</p>
<p class="level4"><strong>Optimal center for a given cluster</strong></p>
<p class="noindent">In an optimal solution to the <em>k</em>-means problem, each center point must be the <span class="blue"><strong><em>centroid</em></strong></span>, or <span class="blue"><strong><em>mean</em></strong></span>, of the points in its cluster. The centroid is a <em>d</em>-dimensional point, where the value in each dimension is the mean of the values of all the points in the cluster in that dimension (that is, the mean of the corresponding attribute values in the cluster). That is, if <strong>c<sup>(ℓ)</sup></strong> is the centroid for cluster <em>S</em><sup>(ℓ)</sup>, then for attributes <em>a</em> = 1, 2, …, <em>d</em>, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1328.jpg"/></p>
<p class="noindent">Over all attributes, we write</p>
<p class="eqr"><img alt="art" src="images/Art_P1329.jpg"/></p>
<a id="p1010"/>
<p class="theo"><strong><em>Theorem 33.1</em></strong></p>
<p class="noindent">Given a nonempty cluster <em>S</em><sup>(ℓ)</sup>, its centroid (or mean) is the unique choice for the cluster center <strong>c<sup>(ℓ)</sup></strong> ∈ <span class="font1">ℝ</span><sup><em>d</em></sup> that minimizes</p>
<p class="eql"><img alt="art" src="images/Art_P1330.jpg"/></p>
<p class="proof"><strong><em>Proof</em></strong>   We wish to minimize, by choosing <strong>c<sup>(ℓ)</sup></strong> ∈ <span class="font1">ℝ</span><sup><em>d</em></sup>, the sum</p>
<p class="eql"><img alt="art" src="images/Art_P1331.jpg"/></p>
<p class="noindent">For each attribute <em>a</em>, the term summed is a convex quadratic function in <img alt="art" src="images/Art_P1332.jpg"/>. To minimize this function, take its derivative with respect to <img alt="art" src="images/Art_P1332.jpg"/> and set it to 0:</p>
<p class="eql"><img alt="art" src="images/Art_P1334.jpg"/></p>
<p class="noindent">or, equivalently,</p>
<p class="eql"><img alt="art" src="images/Art_P1335.jpg"/></p>
<p class="noindent">Since the minimum is obtained uniquely when each coordinate of <img alt="art" src="images/Art_P1332.jpg"/> is the average of the corresponding coordinate for <strong>x</strong> ∈ <em>S</em><sup>(ℓ)</sup>, the overall minimum is obtained when <strong>c<sup>(ℓ)</sup></strong> is the centroid of the points <strong>x</strong>, as in equation (33.3).</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Optimal clusters for given centers</strong></p>
<p class="noindent">The following theorem shows that the nearest-center rule—assigning each point <strong>x</strong> to one of the clusters whose center is nearest to <strong>x</strong>—yields an optimal solution to the <em>k</em>-means problem.</p>
<p class="theo"><strong><em>Theorem 33.2</em></strong></p>
<p class="noindent">Given a set <em>S</em> of <em>n</em> points and a sequence <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span> of <em>k</em> centers, a clustering <span class="font1">〈</span><em>S</em><sup>(1)</sup>, <em>S</em><sup>(2)</sup>, …, <em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span> minimizes</p>
<p class="eqr"><img alt="art" src="images/Art_P1337.jpg"/></p>
<p class="noindent">if and only if it assigns each point <strong>x</strong> ∈ <em>S</em> to a cluster <em>S</em><sup>(ℓ)</sup> that minimizes Δ(<strong>x</strong>, <strong>c<sup>(ℓ)</sup></strong>).</p>
<a id="p1011"/>
<p class="proof"><strong><em>Proof</em></strong>   The proof is straightforward: each point <strong>x</strong> ∈ <em>S</em> contributes exactly once to the sum (33.4), and choosing to put <strong>x</strong> in a cluster whose center is nearest minimizes the contribution from <strong>x</strong>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Lloyd’s procedure</strong></p>
<p class="noindent">Lloyd’s procedure just iterates two operations—assigning points to clusters based on the nearest-center rule, followed by recomputing the centers of clusters to be their centroids—until the results converge. Here is Lloyd’s procedure:</p>
<p class="para-hang1"><strong>Input:</strong> A set <em>S</em> of points in <span class="font1">ℝ</span><sup><em>d</em></sup>, and a positive integer <em>k</em>.</p>
<p class="para-hang1"><strong>Output:</strong> A <em>k</em>-clustering <span class="font1">〈</span><em>S</em><sup>(1)</sup>, <em>S</em><sup>(2)</sup>, …, <em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span> of <em>S</em> with a sequence of centers <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span>.</p>
<ol class="olnoindent" epub:type="list">
<li><strong>Initialize centers:</strong> Generate an initial sequence <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span> of <em>k</em> centers by picking <em>k</em> points independently from <em>S</em> at random. (If the points are not necessarily distinct, see Exercise 33.1-3.) Assign all points to cluster <em>S</em><sup>(1)</sup> to begin.</li>
<li class="litop"><strong>Assign points to clusters:</strong> Use the nearest-center rule to define the clustering <span class="font1">〈</span><em>S</em><sup>(1)</sup>, <em>S</em><sup>(2)</sup>, …, <em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span>. That is, assign each point <strong>x</strong> ∈ <em>S</em> to a cluster <em>S</em><sup>(ℓ)</sup> having a nearest center (breaking ties arbitrarily, but not changing the assignment for a point <strong>x</strong> unless the new cluster center is strictly closer to <strong>x</strong> than the old one).</li>
<li class="litop"><strong>Stop if no change:</strong> If step 2 did not change the assignments of any points to clusters, then stop and return the clustering <span class="font1">〈</span><em>S</em><sup>(1)</sup>, <em>S</em><sup>(2)</sup>, …, <em>S</em><sup>(<em>k</em>)</sup><span class="font1">〉</span> and the associated centers <span class="font1">〈</span><strong>c<sup>(1)</sup></strong>, <strong>c<sup>(2)</sup></strong>, …, <strong>c<sup>(<em>k</em>)</sup></strong><span class="font1">〉</span>. Otherwise, go to step 4.</li>
<li class="litop"><strong>Recompute centers as centroids:</strong> For ℓ = 1, 2, …, <em>k</em>, compute the center <strong>c<sup>(ℓ)</sup></strong> of cluster <em>S</em><sup>(ℓ)</sup> as the centroid of the points in <em>S</em><sup>(ℓ)</sup>. (If <em>S</em><sup>(ℓ)</sup> is empty, let <strong>c<sup>(ℓ)</sup></strong> be the zero vector.) Then go to step 2.</li></ol>
<p class="noindent">It is possible for some of the clusters returned to be empty, particularly if many of the input points are identical.</p>
<p>Lloyd’s procedure always terminates. By Theorem 33.1, recomputing the centers of each cluster as the cluster centroid cannot increase <em>f</em>(<em>S</em>, <em>C</em>). Lloyd’s procedure ensures that a point is reassigned to a different cluster only when such an operation strictly decreases <em>f</em>(<em>S</em>, <em>C</em>). Thus each iteration of Lloyd’s procedure, except the last iteration, must strictly decrease <em>f</em>(<em>S</em>, <em>C</em>). Since there are only a finite number of possible <em>k</em>-clusterings of <em>S</em> (at most <em>k</em><sup><em>n</em></sup>), the procedure must terminate. Furthermore, once one iteration of Lloyd’s procedure yields no decrease in <em>f</em>, further iterations would not change anything, and the procedure can stop at this locally optimum assignment of points to clusters.</p>
<a id="p1012"/>
<p>If Lloyd’s procedure really required <em>k</em><sup><em>n</em></sup> iterations, it would be impractical. In practice, it sometimes suffices to terminate the procedure when the percentage decrease in <em>f</em>(<em>S</em>, <em>C</em>) in the latest iteration falls below a predetermined threshold. Because Lloyd’s procedure is guaranteed to find only a locally optimal clustering, one approach to finding a good clustering is to run Lloyd’s procedure many times with different randomly chosen initial centers, taking the best result.</p>
<p>The running time of Lloyd’s procedure is proportional to the number <em>T</em> of iterations. In one iteration, assigning points to clusters based on the nearest-center rule requires <em>O</em>(<em>dkn</em>) time, and recomputing new centers for each cluster requires <em>O</em>(<em>dn</em>) time (because each point is in one cluster). The overall running time of the <em>k</em>-means procedure is thus <em>O</em>(<em>Tdkn</em>).</p>
<p>Lloyd’s algorithm illustrates an approach common to many machine-learning algorithms:</p>
<ul class="ulnoindent" epub:type="list">
<li>First, define a hypothesis space in terms an appropriate sequence <em>θ</em> of parameters, so that each <em>θ</em> is associated with a specific hypothesis <em>h</em><sub><em>θ</em></sub>. (For the <em>k</em>-means problem, <em>θ</em> is a <em>dk</em>-dimensional vector, equivalent to <em>C</em>, containing the <em>d</em>-dimensional center of each of the <em>k</em> clusters, and <em>h</em><sub><em>θ</em></sub> is the hypothesis that each data point <strong>x</strong> should be grouped with a cluster having a center closest to <strong>x</strong>.)</li>
<li class="litop">Second, define a measure <em>f</em>(<em>E</em>, <em>θ</em>) describing how poorly hypothesis <em>h</em><sub><em>θ</em></sub> fits the given training data <em>E</em>. Smaller values of <em>f</em>(<em>E</em>, <em>θ</em>) are better, and a (locally) optimal solution (locally) minimizes <em>f</em>(<em>E</em>, <em>θ</em>). (For the <em>k</em>-means problem, <em>f</em>(<em>E</em>, <em>θ</em>) is just <em>f</em>(<em>S</em>, <em>C</em>).)</li>
<li class="litop">Third, given a set of training data <em>E</em>, use a suitable optimization procedure to find a value of <em>θ</em>* that minimizes <em>f</em>(<em>E</em>, <em>θ</em>*), at least locally. (For the <em>k</em>-means problem, this value of <em>θ</em>* is the sequence <em>C</em> of <em>k</em> center points returned by Lloyd’s algorithm.)</li>
<li class="litop">Return <em>θ</em>* as the answer.</li></ul>
<p class="noindent">In this framework, we see that optimization becomes a powerful tool for machine learning. Using optimization in this way is flexible. For example, <span class="blue"><strong><em>regularization</em></strong></span> terms can be incorporated in the function to be minimized, in order to penalize hypotheses that are “too complicated” and that “overfit” the training data. (Regularization is a complex topic that isn’t pursued further here.)</p>
<p class="level4"><strong>Examples</strong></p>
<p class="noindent"><a href="chapter033.xhtml#Fig_33-1">Figure 33.1</a> demonstrates Lloyd’s procedure on a set of <em>n</em> = 49 cities: 48 U.S. state capitals and the District of Columbia. Each city has <em>d</em> = 2 dimensions: latitude and longitude. The initial clustering in part (a) of the figure has the initial cluster centers arbitrarily chosen as the capitals of Arkansas, Kansas, Louisiana, <a id="p1013"/>and Tennessee. As the procedure iterates, the value of the function <em>f</em> decreases, until the 11th iteration in part (l), where it remains the same as in the 10th iteration in part (k). Lloyd’s procedure then terminates with the clusters shown in part (l).</p>
<p>As <a href="chapter033.xhtml#Fig_33-2">Figure 33.2</a> shows, Lloyd’s procedure can also apply to “vector quantization.” Here, the goal is to reduce the number of distinct colors required to represent a photograph, thereby allowing the photograph to be greatly compressed (albeit in a lossy manner). In part (a) of the figure, an original photograph 700 pixels wide and 500 pixels high uses 24 bits (three bytes) per pixel to encode a triple of red, green, and blue (RGB) primary color intensities. Parts (b)–(e) of the figure show the results of using Lloyd’s procedure to compress the picture from a initial space of 2<sup>24</sup> possible values per pixel to a space of only <em>k</em> = 4, <em>k</em> = 16, <em>k</em> = 64, or <em>k</em> = 256 possible values per pixel; these <em>k</em> values are the cluster centers. The photograph can then be represented with only 2, 4, 6, or 8 bits per pixel, respectively, instead of the 24-bits per pixel needed by the initial photograph. An auxiliary table, the “palette,” accompanies the compressed image; it holds the <em>k</em> 24-bit cluster centers and is used to map each pixel value to its 24-bit cluster center when the photo is decompressed.</p>
<p class="level4"><strong>Exercises</strong></p>
<p class="level3"><strong><em>33.1-1</em></strong></p>
<p class="noindent">Show that the objective function <em>f</em>(<em>S</em>, <em>C</em>) of equation (33.2) may be alternatively written as</p>
<p class="eql"><img alt="art" src="images/Art_P1338.jpg"/></p>
<p class="level3"><strong><em>33.1-2</em></strong></p>
<p class="noindent">Give an example in the plane with <em>n</em> = 4 points and <em>k</em> = 2 clusters where an iteration of Lloyd’s procedure does not improve <em>f</em>(<em>S</em>, <em>C</em>), yet the <em>k</em>-clustering is not optimal.</p>
<p class="level3"><strong><em>33.1-3</em></strong></p>
<p class="noindent">When the input to Lloyd’s procedure contains many repeated points, a different initialization procedure might be used. Describe a way to pick a number of centers at random that maximizes the number of distinct centers picked. (<em>Hint:</em> See Exercise 5.3-5.)</p>
<p class="level3"><strong><em>33.1-4</em></strong></p>
<p class="noindent">Show how to find an optimal <em>k</em>-clustering in polynomial time when there is just one attribute (<em>d</em> = 1).</p>
<a id="p1014"/>
<div class="divimage">
<p class="fig-imga" id="Fig_33-2"><img alt="art" class="width100" src="images/Art_P1339.jpg"/></p>
<p class="caption"><strong>Figure 33.2</strong> Using Lloyd’s procedure for vector quantization to compress a photo by using fewer colors. <strong>(a)</strong> The original photo has 350,000 pixels (700 × 500), each a 24-bit RGB (red/blue/green) triple of 8-bit values; these pixels (colors) are the “points” to be clustered. Points repeat, so there are only 79,083 distinct colors (less than 2<sup>24</sup>). After compression, only <em>k</em> distinct colors are used, so each pixel is represented by only <span class="font1">⌈</span>1g <em>k</em><span class="font1">⌉</span> bits instead of 24. A “palette” maps these values back to 24-bit RGB values (the cluster centers). <strong>(b)–(e)</strong> The same photo with <em>k</em> = 4, 16, 64, and 256 colors. (Photo from standuppaddle, pixabay.com.)</p>
</div>
<a id="p1015"/>
</section>
<p class="line1"/>
<section title="33.2 Multiplicative-weights algorithms">
<a id="Sec_33.2"/>
<p class="level1" id="h1-195"><a href="toc.xhtml#Rh1-195"><strong>33.2    Multiplicative-weights algorithms</strong></a></p>
<p class="noindent">This section considers problems that require you to make a series of decisions. After each decision you receive feedback as to whether your decision was correct. We will study a class of algorithms that are called <span class="blue"><strong><em>multiplicative-weights algorithms</em></strong></span>. This class of algorithms has a wide variety of applications, including game playing in economics, approximately solving linear-programming and multicommodity-flow problems, and various applications in online machine learning. We emphasize the online nature of the problem here: you have to make a sequence of decisions, but some of the information needed to make the <em>i</em>th decision appears only after you have already made the (<em>i</em> – 1)st decision. In this section, we look at one particular problem, known as “learning from experts,” and develop an example of a multiplicative-weights algorithm, called the weighted-majority algorithm.</p>
<p>Suppose that a series of events will occur, and you want to make predictions about these events. For example, over a series of days, you want to predict whether it is going to rain. Or perhaps you want to predict whether the price of a stock will increase or decrease. One way to approach this problem is to assemble a group of “experts” and use their collective wisdom in order to make good predictions. Let’s denote the experts, <em>n</em> of them, by <em>E</em><sub>1</sub>, <em>E</em><sub>2</sub>, …, <em>E</em><sub><em>n</em></sub>, and let’s say that <em>T</em> events are going to take place. Each event has an outcome of either 0 or 1, with <em>o</em><sup>(<em>t</em>)</sup> denoting the outcome of the <em>t</em>th event. Before event <em>t</em>, each expert <em>E</em><sup>(<em>i</em>)</sup> makes a prediction <img alt="art" src="images/Art_P1340.jpg"/>. You, as the “learner,” then take the set of <em>n</em> expert predictions for event <em>t</em> and produce a single prediction <em>p</em><sup>(<em>t</em>)</sup> ∈ {0, 1} of your own. You base your prediction only on the predictions of the experts and anything you have learned about the experts from their previous predictions. You do not use any additional information about the event. Only after making your prediction do you ascertain the outcome <em>o</em><sup>(<em>t</em>)</sup> of event <em>t</em>. If your prediction <em>p</em><sup>(<em>t</em>)</sup> matches <em>o</em><sup>(<em>t</em>)</sup>, then you were correct; otherwise, you made a mistake. The goal is to minimize the total number <em>m</em> of mistakes, where <img alt="art" src="images/Art_P1341.jpg"/>. You can also keep track of the number of mistakes each expert makes: expert <em>E</em><sub><em>i</em></sub> makes <em>m</em><sub><em>i</em></sub> mistakes, where <img alt="art" src="images/Art_P1342.jpg"/>.</p>
<p>For example, suppose that you are following the price of a stock, and each day you decide whether to invest in it for just that day by buying it at the beginning of the day and selling it at the end of the day. If, on some day, you buy the stock and it goes up, then you made the correct decision, but if the stock goes down, then you made a mistake. Similarly, if on some day, you do not buy the stock and it goes down, then you made the correct decision, but if the stock goes up, then you made a mistake. Since you would like to make as few mistakes as possible, you use the advice of the experts to make your decisions.</p>
<a id="p1016"/>
<p>We’ll assume nothing about the movement of the stock. We’ll also assume nothing about the experts: the experts’ predictions could be correlated, they could be chosen to deceive you, or perhaps some are not really experts after all. What algorithm would you use?</p>
<p>Before designing an algorithm for this problem, we need to consider what is a fair way to evaluate our algorithm. It is reasonable to expect that our algorithm performs better when the expert predictions are better, and that it performs worse when the expert predictions are worse. The goal of the algorithm is to limit the number of mistakes you make to be close to the number of mistakes that the best of the experts makes. At first, this goal might seem impossible, because you do not know until the end which expert is best. We’ll see, however, that by taking the advice provided by all the experts into account, you can achieve this goal. More formally, we use the notion of “regret,” which compares our algorithm to the performance of the best expert (in hindsight) over all. Letting <em>m</em>* = min {<em>m</em><sub><em>i</em></sub> : 1 ≤ <em>i</em> ≤ <em>n</em>} denote the number of mistakes made by the best expert, the <span class="blue"><strong><em>regret</em></strong></span> is <em>m</em> – <em>m</em>*. The goal is to design an algorithm with low regret. (Regret can be negative, although it typically isn’t, since it is rare that you do better than the best expert.)</p>
<p>As a warm-up, let’s consider the case in which one of the experts makes a correct prediction each time. Even without knowing who that expert is, you can still achieve good results.</p>
<p class="lem"><strong><em>Lemma 33.3</em></strong></p>
<p class="noindent">Suppose that out of <em>n</em> experts, there is one who always makes the correct prediction for all <em>T</em> events. Then there is an algorithm that makes at most <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span> mistakes.</p>
<p class="proof"><strong><em>Proof</em></strong>   The algorithm maintains a set <em>S</em> consisting of experts who have not yet made a mistake. Initially, <em>S</em> contains all <em>n</em> experts. The algorithm’s prediction is always the majority vote of the predictions of the experts remaining in set <em>S</em>. In case of a tie, the algorithm makes any prediction. After each outcome is learned, set <em>S</em> is updated to remove all the experts who made an incorrect prediction about that outcome.</p>
<p>We now analyze the algorithm. The expert who always makes the correct prediction will always be in set <em>S</em>. Every time the algorithm makes a mistake, at least half of the experts who were still in <em>S</em> also make a mistake, and these experts are removed from <em>S</em>. If <em>S</em>′ is the set of experts remaining after removing those who made a mistake, we have that |<em>S</em>′| ≤ |<em>S</em>|/2. The size of <em>S</em> can be halved at most <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span> times until |<em>S</em>| = 1. From this point on, we know that the algorithm never makes a mistake, since the set <em>S</em> consists only of the one expert who never makes a mistake. Therefore, overall the algorithm makes at most <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span> mistakes.</p>
<p class="right"><span class="font1">▪</span></p>
<a id="p1017"/>
<p>Exercise 33.2-1 asks you to generalize this result to the case when there is no expert who makes perfect predictions and show that, for any set of experts, there is an algorithm that makes at most <em>m</em>* <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span> mistakes. The generalized algorithm begins in the same way. The set <em>S</em> might become empty at some point, however. If that ever happens, reset <em>S</em> to contain all the experts and continue the algorithm.</p>
<p>You can substantially improve your prediction ability by not just tracking which experts have not made any mistakes, or have not made any mistakes recently, to a more nuanced evaluation of the quality of each expert. The key idea is to use the feedback you receive to update your evaluation of how much trust to put in each expert. As the experts make predictions, you observe whether they were correct and decrease your confidence in the experts who make more mistakes. In this way, you can learn over time which experts are more reliable and which are less reliable, and weight their predictions accordlingly. The change in weights is accomplished via multiplication, hence the term “multiplicative weights.”</p>
<p>The algorithm appears in the procedure W<small>EIGHTED</small>-M<small>AJORITY</small> on the following page, which takes a set <em>E</em> = {<em>E</em><sub>1</sub>, <em>E</em><sub>2</sub>, …, <em>E</em><sub><em>n</em></sub>} of experts, a number <em>T</em> of events, the number <em>n</em> of experts, and a parameter 0 &lt; <em>γ</em> ≤ 1/2 that controls how the weights change. The algorithm maintains weights <img alt="art" src="images/Art_P1343.jpg"/> for <em>i</em> = 1, 2, …, <em>n</em> and <em>t</em> = 1, 2, …, <em>T</em>, where <img alt="art" src="images/Art_P1344.jpg"/>. The <strong>for</strong> loop of lines 1–2 sets the initial weights <img alt="art" src="images/Art_P1345.jpg"/> to 1, capturing the idea that with no knowledge, you trust each expert equally. Each iteration of the main <strong>for</strong> loop of lines 3–18 does the following for an event <em>t</em> = 1, 2, …, <em>T</em>. Each expert <em>E</em><sub><em>i</em></sub> makes a prediction for event <em>t</em> in line 4. Lines 5–8 compute <em>upweight</em><sup>(<em>t</em>)</sup>, the sum of the weights of the experts who predict 1 for event <em>t</em>, and <em>downweight</em><sup>(<em>t</em>)</sup>, the sum of the weights of the experts who predict 0 for the event. Lines 9–11 decide the algorithm’s prediction <em>p</em><sup>(<em>t</em>)</sup> for event <em>t</em> based on whichever weighted sum is larger (breaking ties in favor of deciding 1). The outcome of event <em>t</em> is revealed in line 12. Finally, lines 14–17 decrease the weights of the experts who made an incorrect prediction for event <em>t</em> by multiplying their weights by 1 – <em>γ</em>, leaving alone the weights of the experts who correctly predicted the event’s outcome. Thus, the fewer mistakes each expert makes, the higher that expert’s weight.</p>
<p>The W<small>EIGHTED</small>-M<small>AJORITY</small> procedure doesn’t do much worse than any expert. In particular, it doesn’t do much worse than the best expert. To quantify this claim, let <em>m</em><sup>(<em>t</em>)</sup> be the number of mistakes made by the procedure through event <em>t</em>, and let <img alt="art" src="images/Art_P1346.jpg"/> be the number of mistakes made by expert <em>E</em><sub><em>i</em></sub> through event <em>t</em>. The following theorem is the key.</p>
<a id="p1018"/>
<div class="pull-quote1">
<p class="box-heading">W<small>EIGHTED</small>-M<small>AJORITY</small>(<em>E</em>, <em>T</em>, <em>n</em>, <em>γ</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1" colspan="2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P1347.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> trust each expert equally</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1" colspan="2"><strong>for</strong> <em>t</em> = 1 <strong>to</strong> <em>T</em></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1" colspan="2"><p class="p2">each expert <em>E</em><sub><em>i</em></sub> ∈ <em>E</em> makes a prediction <img alt="art" src="images/Art_P1348.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P1349.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> experts who predicted 1</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P1350.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> sum of weights of who predicted 1</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P1351.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> experts who predicted 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="p2"><img alt="art" src="images/Art_P1352.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> sum of weights of who predicted 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>if</strong> <em>upweight</em><sup>(<em>t</em>)</sup> ≥ <em>downweight</em><sup>(<em>t</em>)</sup></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><p class="p3"><em>p</em><sup>(<em>t</em>)</sup> = 1</p></td>
<td class="td1"><span class="red"><strong>//</strong> algorithm predicts 1</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1"><p class="p2"><strong>else</strong> <em>p</em><sup>(<em>t</em>)</sup> = 0</p></td>
<td class="td1"><span class="red"><strong>//</strong> algorithm predicts 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2"><p class="p2">outcome <em>o</em><sup>(<em>t</em>)</sup> is revealed</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2"><p class="p2"><span class="red"><strong>//</strong> If <em>p</em><sup>(<em>t</em>)</sup> ≠ <em>o</em><sup>(<em>t</em>)</sup>, the algorithm made a mistake.</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1"><p class="p3"><strong>if</strong> <img alt="art" src="images/Art_P1353.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> if expert <em>E</em><sup>(<em>i</em>)</sup> made a mistake …</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">16</span></td>
<td class="td1"><p class="p4"><img alt="art" src="images/Art_P1354.jpg"/></p></td>
<td class="td1"><span class="red"><strong>//</strong> … then decrease that expert’s weight</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">17</span></td>
<td class="td1" colspan="2"><p class="p3"><strong>else</strong> <img alt="art" src="images/Art_P1355.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">18</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>return</strong> <em>p</em><sup>(<em>t</em>)</sup></p></td>
</tr>
</table>
</div>
<p class="theo"><strong><em>Theorem 33.4</em></strong></p>
<p class="noindent">When running W<small>EIGHTED</small>-M<small>AJORITY</small>, we have, for every expert <em>E</em><sub><em>i</em></sub> and every event <em>T</em>′ ≤ <em>T</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1356.jpg"/></p>
<p class="proof"><strong><em>Proof</em></strong>   Every time an expert <em>E</em><sub><em>i</em></sub> makes a mistake, its weight, which is initially 1, is multiplied by 1 – <em>γ</em>, and so we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1357.jpg"/></p>
<p class="noindent">for <em>t</em> = 1, 2, …, <em>T</em>.</p>
<p>We use a potential function <img alt="art" src="images/Art_P1358.jpg"/>, summing the weights for all <em>n</em> experts after iteration <em>t</em> of the <strong>for</strong> loop of lines 3–18. Initially, we have <em>W</em>(0) = <em>n</em> since all <em>n</em> weights start out with the value 1. Because each expert belongs to either the set <em>U</em> or the set <em>D</em> (defined in lines 5 and 7 of W<small>EIGHTED</small>-M<small>AJORITY</small>), we always have <em>W</em>(<em>t</em>) = <em>upweight</em><sup>(<em>t</em>)</sup> + <em>downweight</em><sup>(<em>t</em>)</sup> after each execution of line 8.</p>
<p>Consider an iteration <em>t</em> in which the algorithm makes a mistake in its prediction, which means that either the algorithm predicts 1 and the outcome is 0 or the algorithm <a id="p1019"/>predicts 0 and the outcome is 1. Without loss of generality, assume that the algorithm predicts 1 and the outcome is 0. The algorithm predicted 1 because <em>upweight</em><sup>(<em>t</em>)</sup> ≥ <em>downweight</em><sup>(<em>t</em>)</sup> in line 9, which implies that</p>
<p class="eqr"><img alt="art" src="images/Art_P1359.jpg"/></p>
<p class="noindent">Each expert in <em>U</em> then has its weight multiplied by 1 – <em>γ</em>, and each expert in <em>D</em> has its weight unchanged. Thus, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1360.jpg"/></p>
<p class="noindent">Therefore, for every iteration <em>t</em> in which the algorithm makes a mistake, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1361.jpg"/></p>
<p class="noindent">In an iteration where the algorithm does not make a mistake, some of the weights decrease and some remain unchanged, so that we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1362.jpg"/></p>
<p class="noindent">Since there are <em>m</em><sup>(<em>T</em>′)</sup> mistakes made through iteration <em>T</em>′, and <em>W</em>(1) = <em>n</em>, we can repeatedly apply inequality (33.8) to iterations where the algorithm makes a mistake and inequality (33.9) to iterations where the algorithm does not make a mistake, obtaining</p>
<p class="eqr"><img alt="art" src="images/Art_P1363.jpg"/></p>
<p class="noindent">Because the function <em>W</em> is the sum of the weights and all weights are positive, its value exceeds any single weight. Therefore, using equation (33.6) we have, for any expert <em>E</em><sub><em>i</em></sub> and for any iteration <em>T</em>′ ≤ <em>T</em>,</p>
<p class="eqr"><img alt="art" src="images/Art_P1364.jpg"/></p>
<p class="noindent">Combining inequalities (33.10) and (33.11) gives</p>
<p class="eql"><img alt="art" src="images/Art_P1365.jpg"/></p>
<p class="noindent">Taking the natural logarithm of both sides yields</p>
<p class="eqr"><img alt="art" src="images/Art_P1366.jpg"/></p>
<a id="p1020"/>
<p>We now use the Taylor series expansion to derive upper and lower bounds on the logarithmic factors in inequality (33.12). The Taylor series for ln(1+<em>x</em>) is given in equation (3.22) on page 67. Substituting −<em>x</em> for <em>x</em>, we have that for 0 &lt; <em>x</em> ≤ 1/2,</p>
<p class="eqr"><img alt="art" src="images/Art_P1367.jpg"/></p>
<p class="noindent">Since each term on the right-hand side is negative, we can drop all terms except the first and obtain an upper bound of ln(1 – <em>x</em>) ≤ −<em>x</em>. Since 0 &lt; <em>γ</em> ≤ 1/2, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1368.jpg"/></p>
<p class="noindent">For the lower bound, Exercise 33.2-2 asks you to show that ln(1 – <em>x</em>) ≥ −<em>x</em> − <em>x</em><sup>2</sup> when 0 &lt; <em>x</em> ≤ 1/2, so that</p>
<p class="eqr"><img alt="art" src="images/Art_P1369.jpg"/></p>
<p class="noindent">Thus, we have</p>
<p class="eql"><img alt="art" src="images/Art_P1370.jpg"/></p>
<p class="noindent">so that</p>
<p class="eqr"><img alt="art" src="images/Art_P1371.jpg"/></p>
<p class="noindent">Subtracting ln <em>n</em> from both sides of inequality (33.16) and then multiplying both sides by −2/<em>γ</em> yields <img alt="art" src="images/Art_P1372.jpg"/>, thus proving the theorem.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Theorem 33.4 applies to any expert and any event <em>T</em>′ ≤ <em>T</em>. In particular, we can compare against the best expert after all events have occurred, producing the following corollary.</p>
<p class="cor"><strong><em>Corollary 33.5</em></strong></p>
<p class="noindent">At the end of procedure W<small>EIGHTED</small>-M<small>AJORITY</small>, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1373.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Let’s explore this bound. Assuming that <img alt="art" src="images/Art_P1374.jpg"/>, we can choose <img alt="art" src="images/Art_P1375.jpg"/> and plug into inequality (33.17) to obtain <a id="p1021"/></p>
<p class="eql"><img alt="art" src="images/Art_P1376.jpg"/></p>
<p class="noindent">and so the number of errors is at most twice the number of errors made by the best expert plus a term that is often slower growing than <em>m</em>*. Exercise 33.2-4 shows that you can decrease the bound on the number of errors by a factor of 2 by using randomization, which leads to much stronger bounds. In particular, the upper bound on regret (<em>m</em> – <em>m</em>*) is reduced from (1 + 2<em>γ</em>)<em>m</em>* + (2 ln <em>n</em>)/<em>γ</em> to an expected value of <em><span class="font1">ϵ</span>m</em>* + (ln <em>n</em>)/<em><span class="font1">ϵ</span></em>, where both <em>γ</em> and <em><span class="font1">ϵ</span></em> are at most 1/2. Numerically, we can see that if<em>γ</em> = 1/2, W<small>EIGHTED</small>-M<small>AJORITY</small> makes at most 3 times the number of errors as the best expert, plus 4 ln <em>n</em> errors. As another example, suppose that <em>T</em> = 1000 predictions are being made by <em>n</em> = 20 experts, and the best expert is correct 95% of the time, making 50 errors. Then W<small>EIGHTED</small>-M<small>AJORITY</small> makes at most 100(1+<em>γ</em>)+2 ln 20/<em>γ</em> errors. By choosing <em>γ</em> = 1/4, W<small>EIGHTED</small>-M<small>AJORITY</small> makes at most 149 errors, or a success rate of at least 85%.</p>
<p>Multiplicative weights methods typically refer to a broader class of algorithms that includes W<small>EIGHTED</small>-M<small>AJORITY</small>. The outcomes and predictions need not be only 0 or 1, but can be real numbers, and there can be a loss associated with a particular outcome and prediction. The weights can be updated by a multiplicative factor that depends on the loss, and the algorithm can, given a set of weights, treat them as a distribution on experts and use them to choose an expert to follow in each event. Even in these more general settings, bounds similar to Theorem 33.4 hold.</p>
<p class="level4"><strong>Exercises</strong></p>
<p class="level3"><strong><em>33.2-1</em></strong></p>
<p class="noindent">The proof of Lemma 33.3 assumes that some expert never makes a mistake. It is possible to generalize the algorithm and analysis to remove this assumption. The new algorithm begins in the same way. The set <em>S</em> might become empty at some point, however. If that ever happens, reset <em>S</em> to contain all the experts and continue the algorithm. Show that the number of mistakes that this algorithm makes is at most <em>m</em>* <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span>.</p>
<p class="level3"><strong><em>33.2-2</em></strong></p>
<p class="noindent">Show that ln(1 – <em>x</em>) ≥ −<em>x</em> – <em>x</em><sup>2</sup> when 0 &lt; <em>x</em> ≤ 1/2. (<em>Hint:</em> Start with equation (33.13), group all the terms after the first three, and use equation (A.7) on page 1142.)</p>
<a id="p1022"/>
<p class="level3"><strong><em>33.2-3</em></strong></p>
<p class="noindent">Consider a randomized variant of the algorithm given in the proof of Lemma 33.3, in which some expert never makes a mistake. At each step, choose an expert <em>E</em><sub><em>i</em></sub> uniformly at random from the set <em>S</em> and then make the same predication as <em>E</em><sub><em>i</em></sub>. Show that the expected number of mistakes made by this algorithm is <span class="font1">⌈</span>1g <em>n</em><span class="font1">⌉</span>.</p>
<p class="level3"><strong><em>33.2-4</em></strong></p>
<p class="noindent">Consider a randomized version of W<small>EIGHTED</small>-M<small>AJORITY</small>. The algorithm is the same, except for the prediction step, which interprets the weights as a probability distribution over the experts and chooses an expert <em>E</em><sub><em>i</em></sub> according to that distribution. It then chooses its prediction to be the same as the prediction made by expert <em>E</em><sub><em>i</em></sub>. Show that, for any 0 &lt; <em><span class="font1">ϵ</span></em> &lt; 1/2, the expected number of mistakes made by this algorithm is at most (1 + <em><span class="font1">ϵ</span></em>)<em>m</em>* + (ln <em>n</em>)/<em><span class="font1">ϵ</span></em>.</p>
</section>
<p class="line1"/>
<section title="33.3 Gradient descent">
<a id="Sec_33.3"/>
<p class="level1" id="h1-196"><a href="toc.xhtml#Rh1-196"><strong>33.3    Gradient descent</strong></a></p>
<p class="noindent">Suppose that you have a set {<em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, …, <em>p</em><sub><em>n</em></sub>} of points and you want to find the line that best fits these points. For any line ℓ, there is a distance <em>d</em><sub><em>i</em></sub> between each point <em>p</em><sub><em>i</em></sub> and the line. You want to find the line that minimizes some function <em>f</em>(<em>d</em><sub>1</sub>, …, <em>d</em><sub><em>n</em></sub>). There are many possible choices for the definition of distance and for the function <em>f</em>. For example, the distance can be the projection distance to the line and the function can be the sum of the squares of the distances. This type of problem is common in data science and machine learning—the line is the hypothesis that best describes the data—where the particular definition of best is determined by the definition of distance and the objective <em>f</em>. If the definition of distance and the function <em>f</em> are linear, then we have a linear-programming problem, as discussed in <a href="chapter029.xhtml">Chapter 29</a>. Although the linear-programming framework captures several important problems, many other problems, including various machine-learning problems, have objectives and constraints that are not necessarily linear. We need frameworks and algorithms to solve such problems.</p>
<p>In this section, we consider the problem of optimizing a continuous function and discuss one of the most popular methods to do so: gradient descent. Gradient descent is a general method for finding a local minimum of a function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span>, where informally, a local minimum of a function <em>f</em> is a point <strong>x</strong> for which <em>f</em>(<strong>x</strong>) ≤ <em>f</em>(<strong>x</strong>′) for all <strong>x</strong>′ that are “near” <strong>x</strong>. When the function is convex, it can find a point near the <span class="blue"><strong><em>global minimizer</em></strong></span> of <em>f</em>: an <em>n</em>-vector argument <strong>x</strong> = (<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em></sub>) such that <em>f</em>(<strong>x</strong>) is minimum. For the intuitive idea behind gradient descent, imagine being in a landscape of hills and valleys, and wanting to get to a low point as quickly as possible. You survey the terrain and choose to <a id="p1023"/>move in the direction that takes you downhill the fastest from your current position. You move in that direction, but only for a short while, because as you proceed, the terrain changes and you might need to choose a different direction. So you stop, reevaluate the possible directions and move another short distance in the steepest downhill direction, which might differ from the direction of your previous movement. You continue this process until you reach a point from which all directions lead up. Such a point is a local minimum.</p>
<p>In order to make this informal procedure more formal, we need to define the gradient of a function, which in the analogy above is a measure of the steepness of the various directions. Given a function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span>, its <span class="blue"><strong><em>gradient</em></strong></span> ∇<em>f</em> is a function ∇<em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span><sup><em>n</em></sup> comprising <em>n</em> partial derivatives: <img alt="art" src="images/Art_P1377.jpg"/>. Analogous to the derivative of a function of a single variable, the gradient can be viewed as a direction in which the function value locally increases the fastest, and the rate of that increase. This view is informal; in order to make it formal we would have to define what local means and place certain conditions, such as continuity or existence of derivatives, on the function. Nevertheless, this view motivates the key step of gradient descent—move in the direction opposite to the gradient, by a distance influenced by the magnitude of the gradient.</p>
<p>The general procedure of gradient descent proceeds in steps. You start at some initial point <strong>x<sup>(0)</sup></strong>, which is an <em>n</em>-vector. At each step <em>t</em>, you compute the value of the gradient of <em>f</em> at point <strong>x<sup>(<em>t</em>)</sup></strong>, that is, (∇<em>f</em>)(<strong>x<sup>(<em>t</em>)</sup></strong>), which is also an <em>n</em>-vector. You then move in the direction opposite to the gradient in each dimension at <strong>x<sup>(<em>t</em>)</sup></strong> to arrive at the next point <strong>x<sup>(<em>t</em>+1)</sup></strong>, which again is an <em>n</em>-vector. Because you moved in a monotonically decreasing direction in each dimension, you should have that <em>f</em>(<strong>x<sup>(<em>t</em>+1)</sup></strong>) ≤ <em>f</em>(<strong>x<sup>(<em>t</em>)</sup></strong>). Several details are needed to turn this idea into an actual algorithm. The two main details are that you need an initial point and that you need to decide how far to move in the direction of the negative gradient. You also need to understand when to stop and what you can conclude about the quality of the solution found. We will explore these issues further in this section, for both constrained minimization, where there are additional constraints on the points, and unconstrained minimization, where there are none.</p>
<p class="level4"><strong>Unconstrained gradient descent</strong></p>
<p class="noindent">In order to gain intuition, let’s consider unconstrained gradient descent in just one dimension, that is, when <em>f</em> is a function of a scalar <em>x</em>, so that <em>f</em> : <span class="font1">ℝ</span> → <span class="font1">ℝ</span>. In this case, the gradient ∇<em>f</em> of <em>f</em> is just <em>f</em>′(<em>x</em>), the derivative of <em>f</em> with respect to <em>x</em>. Consider the function <em>f</em> shown in blue in <a href="chapter033.xhtml#Fig_33-3">Figure 33.3</a>, with minimizer <em>x</em>* and starting point <em>x</em><sup>(0)</sup>. The gradient (derivative) <em>f</em>′(<em>x</em><sup>(0)</sup>), shown in orange, has a negative slope, so that a small step from <em>x</em><sup>(0)</sup> in the direction of increasing <em>x</em> results in a point <em>x</em>′ for which <em>f</em>(<em>x</em>′) &lt; <em>f</em>(<em>x</em><sup>(0)</sup>). Too large a step, however, results in a <a id="p1024"/></p>
<div class="divimage">
<p class="fig-img1" id="Fig_33-3"><img alt="art" src="images/Art_P1378.jpg"/></p>
<p class="caption"><strong>Figure 33.3</strong> A function <em>f</em> : <span class="font1">ℝ</span> → <span class="font1">ℝ</span>, shown in blue. Its gradient at point <em>x</em><sup>(0)</sup>, in orange, has a negative slope, and so a small increase in <em>x</em> from <em>x</em><sup>(0)</sup> to <em>x</em>′ results in <em>f</em>(<em>x</em>′) &lt; <em>f</em>(<em>x</em><sup>(0)</sup>). Small increases in <em>x</em> from <em>x</em><sup>(0)</sup> head toward <img alt="art" src="images/Art_P1379.jpg"/>, which gives a local minimum. Too large an increase in <em>x</em> can end up at <em>x</em>″, where <em>f</em>(<em>x</em>″) &gt; <em>f</em>(<em>x</em><sup>(0)</sup>). Small steps starting from <em>x</em><sup>(0)</sup> and going only in the direction of decreasing values of <em>f</em> cannot end up at the global minimizer <em>x</em>*.</p>
</div>
<p class="noindent">point <em>x</em>″ for which <em>f</em>(<em>x</em>″) &gt; <em>f</em>(<em>x</em><sup>(0)</sup>), so this is a bad idea. Restricting ourselves to small steps, where each one has <em>f</em>(<em>x</em>′) &lt; <em>f</em>(<em>x</em>), eventually results in getting close to point <img alt="art" src="images/Art_P1380.jpg"/>, which gives a local minimum. By taking only small downhill steps, however, gradient descent has no chance to get to the global minimizer <em>x</em>*, given the starting point <em>x</em><sup>(0)</sup>.</p>
<p>We draw two observations from this simple example. First, gradient descent converges toward a local minimum, and not necessarily a global minimum. Second, the speed at which it converges and how it behaves are related to properties of the function, to the initial point, and to the step size of the algorithm.</p>
<p>The procedure G<small>RADIENT</small>-D<small>ESCENT</small> on the facing page takes as input a function <em>f</em>, an initial point <strong>x<sup>(0)</sup></strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>, a fixed step-size multiplier <em>γ</em> &gt; 0, and a number <em>T</em> &gt; 0 of steps to take. Each iteration of the <strong>for</strong> loop of lines 2–4 performs a step by computing the <em>n</em>-dimensional gradient at point <strong>x<sup>(<em>t</em>)</sup></strong> and then moving distance <em>γ</em> in the opposite direction in the <em>n</em>-dimensional space. The complexity of computing the gradient depends on the function <em>f</em> and can sometimes be expensive. Line 3 sums the points visited. After the loop terminates, line 6 returns <strong>x-avg</strong>, the average of all the points visited except for the last one, <strong>x<sup>(<em>T</em>)</sup></strong>. It might seem more natural to return <strong>x<sup>(<em>T</em>)</sup></strong>, and in fact, in many circumstances, you might prefer to have the function return <strong>x<sup>(<em>T</em>)</sup></strong>. For the version we will analyze, however, we use <strong>x-avg</strong>.</p>
<a id="p1025"/>
<div class="pull-quote1">
<p class="box-heading">G<small>RADIENT</small>-D<small>ESCENT</small>(<em>f</em>, <strong>x<sup>(0)</sup></strong>, <em>γ</em>, <em>T</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>sum</strong> = 0</td>
<td class="td1"><span class="red"><strong>//</strong> <em>n</em>-dimensional vector, initially all 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><strong>for</strong> <em>t</em> = 0 <strong>to</strong> <em>T</em> – 1</td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><strong>sum</strong> = <strong>sum</strong> + <strong>x<sup>(<em>t</em>)</sup></strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> add each of <em>n</em> dimensions into <strong>sum</strong></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>x<sup>(<em>t</em>+1)</sup> = x<sup>(<em>t</em>)</sup> – γ · (∇<em>f</em>)(x<sup>(<em>t</em>)</sup>)</strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> (∇<em>f</em>)(<strong>x<sup>(<em>t</em>)</sup>), x<sup>(<em>t</em>+1)</sup></strong> are <em>n</em>-dimensional</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><strong>x-avg</strong> = <strong>sum</strong>/<em>T</em></td>
<td class="td1"><span class="red"><strong>//</strong> divide each of <em>n</em> dimensions by <em>T</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1" colspan="2"><strong>return x-avg</strong></td>
</tr>
</table>
</div>
<p><a href="chapter033.xhtml#Fig_33-4">Figure 33.4</a> depicts how gradient descent ideally runs on a convex 1-dimensional function.<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> We’ll define convexity more formally below, but the figure shows that each iteration moves in the direction opposite to the gradient, with the distance moved being proportional to the magnitude of the gradient. As the iterations proceed, the magnitude of the gradient decreases, and thus the distance moved along the horizontal axis decreases. After each iteration, the distance to the optimal point <strong>x*</strong> decreases. This ideal behavior is not guaranteed to occur in general, but the analysis in the remainder of this section formalizes when this behavior occurs and quantifies the number of iterations needed. Gradient descent does not always work, however. We have already seen that if the function is not convex, gradient descent can converge to a local, rather than global, minimum. We have also seen that if the step size is too large, G<small>RADIENT</small>-D<small>ESCENT</small> can overshoot the minimum and wind up farther away. (It is also possible to overshoot the minimum and wind up closer to the optimum.)</p>
<p class="level4"><strong>Analysis of unconstrained gradient descent for convex functions</strong></p>
<p class="noindent">Our analysis of gradient descent focuses on convex functions. Inequality (C.29) on page 1194 defines a convex function of one variable, as shown in <a href="chapter033.xhtml#Fig_33-5">Figure 33.5</a>. We can extend that definition to a function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span> and say that <em>f</em> is <span class="blue"><strong><em>convex</em></strong></span> if for all <strong>x</strong>, <strong>y</strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup> and for all 0 ≤ λ ≤ 1, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1381.jpg"/></p>
<p class="noindent">(Inequalities (33.18) and (C.29) are the same, except for the dimensions of <strong>x</strong> and <strong>y</strong>.) We also assume that our convex functions are closed<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> and differentiable.</p>
<a id="p1026"/>
<div class="divimage">
<p class="fig-img1" id="Fig_33-4"><img alt="art" src="images/Art_P1382.jpg"/></p>
<p class="caption"><strong>Figure 33.4</strong> An example of running gradient descent on a convex function <em>f</em> : <span class="font1">ℝ</span> → <span class="font1">ℝ</span>, shown in blue. Beginning at point <strong>x<sup>(0)</sup></strong>, each iteration moves in the direction opposite to the gradient, and the distance moved is proportional to the magnitude of the gradient. Orange lines represent the negative of the gradient at each point, scaled by the step size <em>γ</em>. As the iterations proceed, the magnitude of the gradient decreases, and the distance moved decreases correspondingly. After each iteration, the distance to the optimal point <strong>x*</strong> decreases.</p>
</div>
<p class="block"/>
<div class="divimage">
<p class="fig-img1" id="Fig_33-5"><img alt="art" src="images/Art_P1383.jpg"/></p>
<p class="caption"><strong>Figure 33.5</strong> A convex function <em>f</em> : <span class="font1">ℝ</span> → <span class="font1">ℝ</span>, shown in blue, with local and global minimizer <strong>x*</strong>. Because <em>f</em> is convex, <em>f</em>(<em>λ</em><strong>x</strong> + (1 – <em>λ</em>)<strong>y</strong>) ≤ <em>λf</em>(<strong>x</strong>) + (1 – <em>λ</em>)<em>f</em>(<strong>y</strong>) for any two values <strong>x</strong> and <strong>y</strong> and all 0 ≤ <em>λ</em> ≤ 1, shown for a particular value of <em>λ</em>. Here, the orange line segment represents all values <em>λf</em>(<strong>x</strong>) + (1 – <em>λ</em>)<em>f</em>(<strong>y</strong>) for 0 ≤ <em>λ</em> ≤ 1, and it is above the blue line.</p>
</div>
<p>A convex function has the property that any local minimum is also a global minimum. To verify this property, consider inequality (33.18), and suppose for the purpose of contradiction that <strong>x</strong> is a local minimum but not a global minimum and <strong>y</strong> ≠ <strong>x</strong> is a global minimum, so <em>f</em>(<strong>y</strong>) &lt; <em>f</em>(<strong>x</strong>). Then we have</p>
<table class="table2b">
<tr>
<td class="td2"><em>f</em>(<em>λ</em><strong>x</strong> + (1 – <em>λ</em>)<strong>y</strong>)</td>
<td class="td2">≤</td>
<td class="td2"><em>λf</em>(<strong>x</strong>) + (1 – <em>λ</em>)<em>f</em>(<strong>y</strong>)</td>
<td class="td2">(by inequality (33.18))</td>
</tr>
<tr>
<td class="td2"/>
<td class="td2">&lt;</td>
<td class="td2"><em>λf</em>(<strong>x</strong>) + (1 – <em>λ</em>)<em>f</em>(<strong>x</strong>)</td>
<td class="td2"/>
</tr>
<tr>
<td class="td2"/>
<td class="td2">=</td>
<td class="td2"><em>f</em>(<strong>x</strong>).</td>
<td class="td2"/>
</tr>
</table>
<a id="p1027"/>
<p class="noindent">Thus, letting approach 1, we see that there is another point near <strong>x</strong>, say <strong>x</strong>′, such that <em>f</em>(<strong>x</strong>′) &lt; <em>f</em>(<strong>x</strong>), so <strong>x</strong> is not a local minimum.</p>
<p>Convex functions have several useful properties. The first property, whose proof we leave as Exercise 33.3-1, says that a convex function always lies above its tangent hyperplane. In the context of gradient descent, angle brackets denote the notation for inner product defined on page 1219 rather than denoting a sequence.</p>
<p class="lem"><strong><em>Lemma 33.6</em></strong></p>
<p class="noindent">For any convex differentiable function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span> and for all <em>x</em>, <em>y</em> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>, we have ≤ <em>f</em>(<strong>x</strong>) ≤ <em>f</em>(<strong>y</strong>) + <span class="font1">〈</span>(∇<em>f</em>)(<strong>x</strong>), <strong>x</strong> – <strong>y</strong><span class="font1">〉</span>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The second property, which Exercise 33.3-2 asks you to prove, is a repeated application of the definition of convexity in inequality (33.18).</p>
<p class="lem"><strong><em>Lemma 33.7</em></strong></p>
<p class="noindent">For any convex function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span>, for any integer <em>T</em> ≥ 1, and for all <strong>x<sup>(0)</sup></strong>, …, <strong>x<sup>(<em>T</em>–1)</sup></strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1384.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">The left-hand side of inequality (33.19) is the value of <em>f</em> at the vector <strong>x-avg</strong> that G<small>RADIENT</small>-D<small>ESCENT</small> returns.</p>
<p>We now proceed to analyze G<small>RADIENT</small>-D<small>ESCENT</small>. It might not return the exact global minimizer <strong>x</strong>*. We use an error bound <em><span class="font1">ϵ</span></em>, and we want to choose <em>T</em> so that <em>f</em>(<strong>x-avg</strong>) – <em>f</em>(<strong>x</strong>*) ≤ <em><span class="font1">ϵ</span></em> at termination. The value of <em><span class="font1">ϵ</span></em> depends on the number <em>T</em> of iterations and two additional values. First, since you expect it to be better to start close to the global minimizer, <em><span class="font1">ϵ</span></em> is a function of</p>
<p class="eqr"><img alt="art" src="images/Art_P1385.jpg"/></p>
<p class="noindent">the euclidean norm (or distance, defined on page 1219) of the difference between <strong>x<sup>(0)</sup></strong> and <strong>x</strong>*. The error bound <em><span class="font1">ϵ</span></em> is also a function of a quantity we call <em>L</em>, which is an upper bound on the magnitude <span class="font1">∥</span>(∇<em>f</em>)(<strong>x</strong>)<span class="font1">∥</span> of the gradient, so that</p>
<p class="eqr"><img alt="art" src="images/Art_P1386.jpg"/></p>
<p class="noindent">where <strong>x</strong> ranges over all the points <strong>x<sup>(0)</sup></strong>, …, <strong>x<sup>(<em>T</em>–1)</sup></strong> whose gradients are computed by G<small>RADIENT</small>-D<small>ESCENT</small>. Of course, we don’t know the values of <em>L</em> and <em>R</em>, but for now let’s assume that we do. We’ll discuss later how to remove these assumptions. The analysis of G<small>RADIENT</small>-D<small>ESCENT</small> is summarized in the following theorem.</p>
<a id="p1028"/>
<p class="theo"><strong><em>Theorem 33.8</em></strong></p>
<p class="noindent">Let <strong>x</strong>* ∈ <span class="font1">ℝ</span><sup><em>n</em></sup> be the minimizer of a convex function <em>f</em>, and suppose that an execution of G<small>RADIENT</small>-D<small>ESCENT</small>(<em>f</em>, <strong>x<sup>(0)</sup></strong>, <em>γ</em>, <em>T</em>) returns <strong>x-avg</strong>, where <img alt="art" src="images/Art_P1387.jpg"/> and <em>R</em> and <em>L</em> are defined in equations (33.20) and (33.21). Let <img alt="art" src="images/Art_P1388.jpg"/>. Then we have <em>f</em>(<strong>x-avg</strong>) – <em>f</em>(<strong>x</strong>*) ≤ <em><span class="font1">ϵ</span></em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We now prove this theorem. We do not give an absolute bound on how much progress each iteration makes. Instead, we use a potential function, as in <a href="chapter016.xhtml#Sec_16.3">Section 16.3</a>. Here, we define a potential Φ(<em>t</em>) after computing <strong>x<sup>(<em>t</em>)</sup></strong>, such that Φ(<em>t</em>) ≥ 0 for <em>t</em> = 0, …, <em>T</em>. We define the <span class="blue"><strong><em>amortized progress</em></strong></span> in the iteration that computes <strong>x<sup>(<em>t</em>)</sup></strong> as</p>
<p class="eqr"><img alt="art" src="images/Art_P1389.jpg"/></p>
<p class="noindent">Along with including the change in potential (Φ(<em>t</em> + 1) – Φ(<em>t</em>)), equation (33.22) also subtracts the minimum value <em>f</em>(<strong>x</strong>*) because ultimately, you care not about the values <em>f</em>(<strong>x<sup>(<em>t</em>)</sup></strong>) but about how close they are to <em>f</em>(<strong>x</strong>*). Suppose that we can show that <em>p</em>(<em>t</em>) ≤ <em>B</em> for some value <em>B</em> and <em>t</em> = 0, …, <em>T</em> – 1. Then we can substitute for <em>p</em>(<em>t</em>) using equation (33.22), giving</p>
<p class="eqr"><img alt="art" src="images/Art_P1390.jpg"/></p>
<p class="noindent">Summing inequality (33.23) over <em>t</em> = 0, …, <em>T</em> – 1 yields</p>
<p class="eql"><img alt="art" src="images/Art_P1391.jpg"/></p>
<p class="noindent">Observing that we have a telescoping series on the right and regrouping terms, we have that</p>
<p class="eql"><img alt="art" src="images/Art_P1392.jpg"/></p>
<p class="noindent">Dividing by <em>T</em> and dropping the positive term Φ(<em>T</em>) gives</p>
<p class="eqr"><img alt="art" src="images/Art_P1393.jpg"/></p>
<p class="noindent">and thus we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1394.jpg"/></p>
<a id="p1029"/>
<p class="noindent">In other words, if we can show that <em>p</em>(<em>t</em>) ≤ <em>B</em> for some value <em>B</em> and choose a potential function where Φ(0) is not too large, then inequality (33.25) tells us how close the function value <em>f</em>(<strong>x-avg</strong>) is to the function value <em>f</em>(<strong>x</strong>*) after <em>T</em> iterations. That is, we can set the error bound <em><span class="font1">ϵ</span></em> to <em>B</em> + Φ(0)/<em>T</em>.</p>
<p>In order to bound the amortized progress, we need to come up with a concrete potential function. Define the potential function Φ(<em>t</em>) by</p>
<p class="eqr"><img alt="art" src="images/Art_P1395.jpg"/></p>
<p class="noindent">that is, the potential function is proportional to the square of the distance between the current point and the minimizer <strong>x</strong>*. With this potential function in hand, the next lemma provides a bound on the amortized progress made in any iteration of G<small>RADIENT</small>-D<small>ESCENT</small>.</p>
<p class="lem"><strong><em>Lemma 33.9</em></strong></p>
<p class="noindent">Let <strong>x</strong>* ∈ <span class="font1">ℝ</span><sup><em>n</em></sup> be the minimizer of a convex function <em>f</em>, and consider an execution of G<small>RADIENT</small>-D<small>ESCENT</small>(<em>f</em>, <strong>x<sup>(0)</sup></strong>, <em>γ</em>, <em>T</em>). Then for each point <strong>x<sup>(<em>t</em>)</sup></strong> computed by the procedure, we have that</p>
<p class="eql"><img alt="art" src="images/Art_P1396.jpg"/></p>
<p class="proof"><strong><em>Proof</em></strong>   We first bound the potential change Φ(<em>t</em> + 1) – Φ(<em>t</em>). Using the definition of Φ(<em>t</em>) from equation (33.26), we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1397.jpg"/></p>
<p class="noindent">From line 4 in G<small>RADIENT</small>-D<small>ESCENT</small>, we know that</p>
<p class="eqr"><img alt="art" src="images/Art_P1398.jpg"/></p>
<p class="noindent">and so we would like to rewrite equation (33.27) to have <strong>x<sup>(<em>t</em>+1)</sup></strong> – <strong>x<sup>(<em>t</em>)</sup></strong> terms. As Exercise 33.3-3 asks you to prove, for any two vectors <strong>a</strong>, <strong>b</strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>, we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1399.jpg"/></p>
<p class="noindent">Letting <strong>a</strong> = <strong>x<sup>(<em>t</em>)</sup></strong> – <strong>x</strong>* and <strong>b</strong> = <strong>x<sup>(<em>t</em>+1)</sup></strong> – <strong>x<sup>(<em>t</em>)</sup></strong>, we can write the right-hand side of equation (33.27) as <img alt="art" src="images/Art_P1400.jpg"/>. Then we can express the potential change as <a id="p1030"/></p>
<p class="eqr"><img alt="art" src="images/Art_P1401.jpg"/></p>
<p class="noindent">and thus we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1402.jpg"/></p>
<p>We can now proceed to bound <em>p</em>(<em>t</em>). By the bound on the potential change from inequality (33.31), and using the definition of <em>L</em> (inequality (33.21)), we have</p>
<p class="eqr"><img alt="art" src="images/Art_P1403.jpg"/></p>
<p class="right"><span class="font1">▪</span></p>sult in the following theorem
<p class="space-break">Having bounded the amortized progress in one step, we now analyze the entire G<small>RADIENT</small>-D<small>ESCENT</small> procedure, completing the proof of Theorem 33.8.</p>
<p class="proof"><strong><em>Proof of Theorem 33.8</em></strong> Inequality (33.25) tells us that if we have an upper bound of <em>B</em> for <em>p</em>(<em>t</em>), then we also have the bound <em>f</em>(<strong>x-avg</strong>) – <em>f</em>(<strong>x</strong>*) ≤ <em>B</em> + Φ(0)/<em>T</em>. By equations (33.20) and (33.26), we have that Φ(0) = <em>R</em><sup>2</sup>/(2<em>γ</em>). Lemma 33.9 gives us the upper bound of <em>B</em> = <em>γL</em><sup>2</sup>/2, and so we have</p>
<p class="eql"><img alt="art" src="images/Art_P1404.jpg"/></p>
<a id="p1031"/>
<p class="noindent">Our choice of <img alt="art" src="images/Art_P1405.jpg"/> in the statement of Theorem 33.8 balances the two terms, and we obtain</p>
<p class="eql"><img alt="art" src="images/Art_P1406.jpg"/></p>
<p class="noindent">Since we chose <img alt="art" src="images/Art_P1407.jpg"/> in the theorem statement, the proof is complete.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">Continuing under the assumption that we know <em>R</em> (from equation (33.20)) and <em>L</em> (from inequality (33.21)), we can think of the analysis in a slightly different way. We can presume that we have a target accuracy <em><span class="font1">ϵ</span></em> and then compute the number of iterations needed. That is, we can solve <img alt="art" src="images/Art_P1408.jpg"/> for <em>T</em>, obtaining <em>T</em> = <em>R</em><sup>2</sup><em>L</em><sup>2</sup>/<em><span class="font1">ϵ</span></em><sup>2</sup>. The number of iterations thus depends on the square of <em>R</em> and <em>L</em> and, most importantly, on 1/<em><span class="font1">ϵ</span></em><sup>2</sup>. (The definition of <em>L</em> from inequality (33.21) depends on <em>T</em>, but we may know an upper bound on <em>L</em> that doesn’t depend on the particular value of <em>T</em>.) Thus, if you want to halve your error bound, you need to run four times as many iterations.</p>
<p>It is quite possible that we don’t really know <em>R</em> and <em>L</em>, since you’d need to know <strong>x</strong>* in order to know <em>R</em> (since <em>R</em> = <span class="font1">∥</span><strong>x<sup>(0)</sup></strong> – <strong>x</strong>*<span class="font1">∥</span>), and you might not have an explicit upper bound on the gradient, which would provide <em>L</em>. You can, however, interpret the analysis of gradient descent as a proof that there is some step size for which the procedure makes progress toward the minimum. You can then compute a step size for which <em>f</em>(<strong>x<sup>(<em>t</em>)</sup></strong>) – <em>f</em>(<strong>x<sup>(<em>t</em>+1)</sup></strong>) is large enough. In fact, not having a fixed step size multiplier can actually help in practice, as you are free to use any step size <em>s</em> that achieves sufficient decrease in the value of <em>f</em>. You can search for a step size that achieves a large decrease via a binary-search-like routine, which is often called <span class="blue"><strong><em>line search</em></strong></span>. For a given function <em>f</em> and step size <em>s</em>, define the function <em>g</em>(<strong>x<sup>(<em>t</em>)</sup></strong>, <em>s</em>) = <em>f</em>(<strong>x<sup>(<em>t</em>)</sup></strong>) – <em>s</em>(∇<em>f</em>)(<strong>x<sup>(<em>t</em>)</sup></strong>). Start with a small step size <em>s</em> for which <em>g</em>(<strong>x<sup>(<em>t</em>)</sup></strong>, <em>s</em>) ≤ <em>f</em>(<strong>x<sup>(<em>t</em>)</sup></strong>). Then repeatedly double <em>s</em> until <em>g</em>(<strong>x<sup>(<em>t</em>)</sup></strong>, 2<em>s</em>) ≥ <em>g</em>(<strong>x<sup>(<em>t</em>)</sup></strong>, <em>s</em>), and then perform a binary search in the interval [<em>s</em>, 2<em>s</em>]. This procedure can produce a step size that achieves a significant decrease in the objective function. In other circumstances, however, you may know good upper bounds on <em>R</em> and <em>L</em>, typically from problem-specific information, which can suffice.</p>
<p>The dominant computational step in each iteration of the <strong>for</strong> loop of lines 2–4 is computing the gradient. The complexity of computing and evaluating a gradient varies widely, depending on the application at hand. We’ll discuss several applications later.</p>
<a id="p1032"/>
<p class="level4"><strong>Constrained gradient descent</strong></p>
<p class="noindent">We can adapt gradient descent for constrained minimization to minimize a closed convex function <em>f</em>(<strong>x</strong>), subject to the additional requirement that <strong>x</strong> ∈ <em>K</em>, where <em>K</em> is a closed convex body. A <span class="blue"><strong><em>body</em></strong></span> <em>K</em> ⊆ <span class="font1">ℝ</span><sup><em>n</em></sup> is <span class="blue"><strong><em>convex</em></strong></span> if for all <strong>x</strong>, <strong>y</strong> ∈ <em>K</em>, the convex combination <em>λ</em><strong>x</strong>+(1–<em>λ</em>)<strong>y</strong> ∈ <em>K</em> for all 0 ≤ <em>λ</em> ≤ 1. A <span class="blue"><strong><em>closed</em></strong></span> convex body contains its limit points. Somewhat surprisingly, restricting to the constrained problem does not significantly increase the number of iterations of gradient descent. The idea is that you run the same algorithm, but in each iteration, check whether the current point <strong>x<sup>(<em>t</em>)</sup></strong> is still within the convex body <em>K</em>. If it is not, just move to the closest point in <em>K</em>. Moving to the closest point is known as <span class="blue"><strong><em>projection</em></strong></span>. We formally define the projection ∏<sub><em>K</em></sub>(<strong>x</strong>) of a point <strong>x</strong> in <em>n</em> dimensions onto a convex body <em>K</em> as the point <strong>y</strong> ∈ <em>K</em> such that <span class="font1">∥</span><strong>x</strong> – <strong>y</strong><span class="font1">∥</span> = min {<span class="font1">∥</span><strong>x</strong> – <strong>z</strong><span class="font1">∥</span> : <em>z</em> ∈ <em>K</em>}. If we have <strong>x</strong> ∈ <em>K</em>, then ∏<sub><em>K</em></sub>(<strong>x</strong>) = <strong>x</strong>.</p>
<p>This one change yields the procedure G<small>RADIENT</small>-D<small>ESCENT</small>-CONSTRAINED, in which line 4 of G<small>RADIENT</small>-D<small>ESCENT</small> is replaced by two lines. It assumes that <strong>x<sup>(0)</sup></strong> ∈ <em>K</em>. Line 4 of G<small>RADIENT</small>-D<small>ESCENT</small>-C<small>ONSTRAINED</small> moves in the direction of the negative gradient, and line 5 projects back onto <em>K</em>. The lemma that follows helps to show that when <strong>x</strong>* ∈ <em>K</em>, if the projection step in line 5 moves from a point outside of <em>K</em> to a point in <em>K</em>, it cannot be moving away from <strong>x</strong>*.</p>
<div class="pull-quote1">
<p class="box-heading">G<small>RADIENT</small>-D<small>ESCENT</small>-C<small>ONSTRAINED</small>(<em>f</em>, <strong>x<sup>(0)</sup></strong>, <em>γ</em>, <em>T</em>, <em>K</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">1</span></td>
<td class="td1"><strong>sum</strong> = 0</td>
<td class="td1"><span class="red"><strong>//</strong> <em>n</em>-dimensional vector, initially all 0</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">2</span></td>
<td class="td1" colspan="2"><strong>for</strong> <em>t</em> = 0 <strong>to</strong> <em>T</em> – 1</td>
</tr>
<tr>
<td class="td1"><span class="x-small">3</span></td>
<td class="td1"><p class="p2"><strong>sum</strong> = <strong>sum</strong> + <strong>x<sup>(<em>t</em>)</sup></strong></p></td>
<td class="td1"><span class="red"><strong>//</strong> add each of <em>n</em> dimensions into <strong>sum</strong></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">4</span></td>
<td class="td1"><p class="p2"><strong>x′</strong><sup>(<em>t</em>+1)</sup> = <strong>x<sup>(<em>t</em>)</sup></strong> – <em>γ</em> · (∇<em>f</em>)(<strong>x<sup>(<em>t</em>)</sup></strong>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> (∇<em>f</em>)(<strong>x<sup>(<em>t</em>)</sup></strong>), <strong>x′<sup>(<em>t</em>+1)</sup></strong> are <em>n</em>-dimensional</span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">5</span></td>
<td class="td1"><p class="p2"><strong>x<sup>(<em>t</em>+1)</sup></strong> = ∏<sub><em>K</em></sub>(<strong>x<sup>(<em>t</em>+1)</sup></strong>)</p></td>
<td class="td1"><span class="red"><strong>//</strong> project onto <em>K</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">6</span></td>
<td class="td1"><strong>x-avg</strong> = <strong>sum</strong>/<em>T</em></td>
<td class="td1"><span class="red"><strong>//</strong> divide each of <em>n</em> dimensions by <em>T</em></span></td>
</tr>
<tr>
<td class="td1"><span class="x-small">7</span></td>
<td class="td1" colspan="2"><strong>return x-avg</strong></td>
</tr>
</table>
</div>
<p class="lem"><strong><em>Lemma 33.10</em></strong></p>
<p class="noindent">Consider a convex body <em>K</em> ⊆ <span class="font1">ℝ</span><sup><em>n</em></sup> and points <strong>a</strong> ∈ <em>K</em> and <strong>b</strong>′ ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>. Let <strong>b</strong> = ∏<sub><em>K</em></sub>(<strong>b</strong>′). Then <span class="font1">∥</span><strong>b</strong> – <strong>a</strong><span class="font1">∥</span><sup>2</sup> ≤ <span class="font1">∥</span><strong>b</strong>′ – <strong>a</strong><span class="font1">∥</span><sup>2</sup>.</p>
<p class="proof"><strong><em>Proof</em></strong>   If <strong>b</strong>′ ∈ <em>K</em>, then <strong>b</strong> = <strong>b</strong>′ and the claim is true. Otherwise, <strong>b</strong>′ ≠ <strong>b</strong>, and as <a href="chapter033.xhtml#Fig_33-6">Figure 33.6</a> shows, we can extend the line segment between <strong>b</strong> and <strong>b</strong>′ to a line ℓ. Let <strong>c</strong> be the projection of <strong>a</strong> onto ℓ. Point <strong>c</strong> may or may not be in <em>K</em>, and if <strong>a</strong> is on the boundary of <em>K</em>, then <strong>c</strong> could coincide with <strong>b</strong>. If <strong>c</strong> coincides with <strong>b</strong> (part (c) of the figure), then <strong>abb</strong>′ is a right triangle, and so <span class="font1">∥</span><strong>b</strong> – <strong>a</strong><span class="font1">∥</span><sup>2</sup> ≤ <span class="font1">∥</span><strong>b</strong>′ – <strong>a</strong><span class="font1">∥</span><sup>2</sup>. <a id="p1033"/>If <strong>c</strong> does not coincide with <strong>b</strong> (parts (a) and (b) of the figure), then because of convexity, the angle ∠<strong>abb</strong>′ must be obtuse. Because angle ∠<strong>abb</strong>′ is obtuse, <strong>b</strong> lies between <strong>c</strong> and <strong>b</strong>′ on ℓ. Furthermore, because <strong>c</strong> is the projection of <strong>a</strong> onto line ℓ, <strong>acb</strong> and <strong>acb</strong>′ must be right triangles. By the Pythagorean theorem, we have that <span class="font1">∥</span>b′ – <strong>a</strong><span class="font1">∥</span><sup>2</sup> = <span class="font1">∥</span><strong>a</strong> – <strong>c</strong><span class="font1">∥</span><sup>2</sup>+<span class="font1">∥</span><strong>c</strong> – <strong>b</strong>′<span class="font1">∥</span><sup>2</sup> and <span class="font1">∥</span><strong>b</strong> – <strong>a</strong><span class="font1">∥</span><sup>2</sup> = <span class="font1">∥</span><strong>a</strong> – <strong>c</strong><span class="font1">∥</span><sup>2</sup>+<span class="font1">∥</span><strong>c</strong> – <strong>b</strong><span class="font1">∥</span><sup>2</sup>. Subtracting these two equations gives <span class="font1">∥</span><strong>b</strong>′ – <strong>a</strong><span class="font1">∥</span><sup>2</sup> – <span class="font1">∥</span><strong>b</strong> – <strong>a</strong><span class="font1">∥</span><sup>2</sup> = <span class="font1">∥</span><strong>c</strong> – <strong>b</strong>′<span class="font1">∥</span><sup>2</sup> – <span class="font1">∥</span><strong>c</strong> – <strong>b</strong><span class="font1">∥</span><sup>2</sup>. Because <strong>b</strong> is between <strong>c</strong> and <strong>b</strong>′, we must have <span class="font1">∥</span><strong>c</strong> – <strong>b</strong>′<span class="font1">∥</span><sup>2</sup> ≥ <span class="font1">∥</span><strong>c</strong> – <strong>b</strong><span class="font1">∥</span><sup>2</sup>, and thus <span class="font1">∥</span><strong>b</strong>′ – <strong>a</strong><span class="font1">∥</span><sup>2</sup> – <span class="font1">∥</span><strong>b</strong> – <strong>a</strong><span class="font1">∥</span><sup>2</sup> ≥ 0. The lemma follows.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_33-6"><img alt="art" src="images/Art_P1409.jpg"/></p>
<p class="caption"><strong>Figure 33.6</strong> Projecting a point <strong>b</strong>′ outside the convex body <em>K</em> to the closest point <strong>b</strong> = ∏<sub><em>K</em></sub>(<strong>b</strong>′) in <em>K</em>. Line ℓ is the line containing <strong>b</strong> and <strong>b</strong>′, and point <strong>c</strong> is the projection of <strong>a</strong> onto ℓ. <strong>(a)</strong> When <strong>c</strong> is in <em>K</em>. <strong>(b)</strong> When <strong>c</strong> is not in <em>K</em>. <strong>(c)</strong> When <strong>a</strong> is on the boundary of <em>K</em> and <strong>c</strong> coincides with <strong>b</strong>.</p>
</div>
<p class="right"><span class="font1">▪</span></p>
<p class="space-break">We can now repeat the entire proof for the unconstrained case and obtain the same bounds. Lemma 33.10 with <strong>a</strong> = <strong>x</strong>*, <strong>b</strong> = <strong>x<sup>(<em>t</em>+1)</sup></strong>, and <strong>b</strong>′ = <strong>x′<sup>(<em>t</em>+1)</sup></strong> tells us that <span class="font1">∥</span><strong>x<sup>(<em>t</em>+1)</sup></strong>–<strong>x</strong>*<span class="font1">∥</span><sup>2</sup> ≤ <span class="font1">∥</span><strong>x′<sup>(<em>t</em>+1)</sup></strong>–<strong>x</strong>*<span class="font1">∥</span><sup>2</sup>. We can therefore derive an upper bound that matches inequality (33.31). We continue to define Φ(<em>t</em>) as in equation (33.26), but noting that <strong>x<sup>(<em>t</em>+1)</sup></strong>, computed in line 5 of G<small>RADIENT</small>-D<small>ESCENT</small>-C<small>ONSTRAINED</small>, has a different meaning here from in inequality (33.31):</p>
<p class="eqr"><img alt="art" src="images/Art_P1410.jpg"/></p>
<a id="p1034"/>
<p class="noindent">With the same upper bound on the change in the potential function as in equation (33.30), the entire proof of Lemma 33.9 can proceed as before. We can therefore conclude that the procedure <small>GRADIENT</small>-<small>DESCENT</small>-<small>CONSTRAINED</small> has the same asymptotic complexity as <small>GRADIENT</small>-<small>DESCENT</small>. We summarize this result in the following theorem.</p>
<p class="theo"><strong><em>Theorem 33.11</em></strong></p>
<p class="noindent">Let <em>K</em> ⊆ <span class="font1">ℝ</span><sup><em>n</em></sup> be a convex body, <strong>x</strong>* ∈ <span class="font1">ℝ</span><sup><em>n</em></sup> be the minimizer of a convex function <em>f</em> over <em>K</em>, and <img alt="art" src="images/Art_P1411.jpg"/>, where <em>R</em> and <em>L</em> are defined in equations (33.20) and (33.21). Suppose that the vector <strong>x-avg</strong> is returned by an execution of G<small>RADIENT</small>-D<small>ESCENT</small>-C<small>ONSTRAINED</small>(<em>f</em>, <strong>x<sup>(0)</sup></strong>, <em>γ</em>, <em>T</em>, <em>K</em>). Let <img alt="art" src="images/Art_P1412.jpg"/>. Then we have <em>f</em>(<strong>x-avg</strong>) – <em>f</em>(<strong>x</strong>*) ≤ <em><span class="font1">ϵ</span></em>.</p>
<p class="right"><span class="font1">▪</span></p>
<p class="level4"><strong>Applications of gradient descent</strong></p>
<p class="noindent">Gradient descent has many applications to minimizing functions and is widely used in optimization and machine learning. Here we sketch how it can be used to solve linear systems. Then we discuss an application to machine learning: prediction using linear regression.</p>
<p>In <a href="chapter028.xhtml">Chapter 28</a>, we saw how to use Gaussian elimination to solve a system of linear equations <em>A</em><strong>x</strong> = <strong>b</strong>, thereby computing <strong>x</strong> = <em>A</em><sup>−1</sup><strong>b</strong>. If <em>A</em> is an <em>n</em> × <em>n</em> matrix and <strong>b</strong> is a length-<em>n</em> vector, then the running time of Gaussian elimination is Θ(<em>n</em><sup>3</sup>), which for large matrices might be prohibitively expensive. If an approximate solution is acceptable, however, you can use gradient descent.</p>
<p>First, let’s see how to use gradient descent as a roundabout—and admittedly inefficient—way to solve for <em>x</em> in the scalar equation <em>ax</em> = <em>b</em>, where <em>a</em>, <em>x</em>, <em>b</em> ∈ <span class="font1">ℝ</span>. This equation is equivalent to <em>ax</em> – <em>b</em> = 0. If <em>ax</em> – <em>b</em> is the derivative of a convex function <em>f</em>(<em>x</em>), then <em>ax</em> – <em>b</em> = 0 for the value of <em>x</em> that minimizes <em>f</em>(<em>x</em>). Given <em>f</em>(<em>x</em>), gradient descent can then determine this minimizer. Of course, <em>f</em>(<em>x</em>) is just the integral of <em>ax</em> – <em>b</em>, that is, <img alt="art" src="images/Art_P1413.jpg"/>, which is convex if <em>a</em> ≥ 0. Therefore, one way to solve <em>ax</em> = <em>b</em> for <em>a</em> ≥ 0 is to find the minimizer for <img alt="art" src="images/Art_P1414.jpg"/> via gradient descent.</p>
<p>We now generalize this idea to higher dimensions, where using gradient descent may actually lead to a faster algorithm. One <em>n</em>-dimensional analog is the function <img alt="art" src="images/Art_P1415.jpg"/>, where <em>A</em> is an <em>n</em> × <em>n</em> matrix. The gradient of <em>f</em> with respect to <strong>x</strong> is the function <em>A</em><strong>x</strong> – <strong>b</strong>. To find the value of <strong>x</strong> that minimizes <em>f</em>, we set the gradient of <em>f</em> to 0 and solve for <strong>x</strong>. Solving <em>A</em><strong>x</strong>–<strong>b</strong> = 0 for <strong>x</strong>, we obtain <strong>x</strong> = <em>A</em><sup>−1</sup><strong>b</strong>, Thus, minimizing <em>f</em>(<strong>x</strong>) is equivalent to solving <em>A</em><strong>x</strong> = <strong>b</strong>. If <em>f</em>(<strong>x</strong>) is convex, then gradient descent can approximately compute this minimum.</p>
<p>A 1-dimensional function is convex when its second derivative is positive. The equivalent definition for a multidimensional function is that it is convex when its <a id="p1035"/>Hessian matrix is positive-semidefinite (see page 1222 for a definition), where the <span class="blue"><strong><em>Hessian matrix</em></strong></span> (∇<sup>2</sup><em>f</em>)(<strong>x</strong>) of a function <em>f</em>(<strong>x</strong>) is the matrix in which entry (<em>i</em>, <em>j</em>) is the partial derivative of <em>f</em> with respect to <em>i</em> and <em>j</em>:</p>
<p class="eql"><img alt="art" src="images/Art_P1416.jpg"/></p>
<p class="noindent">Analogous to the 1-dimensional case, the Hessian of <em>f</em> is just <em>A</em>, and so if <em>A</em> is a positive-semidefinite matrix, then we can use gradient descent to find a point <strong>x</strong> where <em>A</em><strong>x</strong> ≈ <strong>b</strong>. If <em>R</em> and <em>L</em> are not too large, then this method is faster than using Gaussian elimination.</p>
<p class="level4"><strong>Gradient descent in machine learning</strong></p>
<p class="noindent">As a concrete example of supervised learning for prediction, suppose that you want to predict whether a patient will develop heart disease. For each of <em>m</em> patients, you have <em>n</em> different attributes. For example, you might have <em>n</em> = 4 and the four pieces of data are age, height, blood pressure, and number of close family members with heart disease. Denote the data for patient <em>i</em> as a vector <strong>x<sup>(<em>i</em>)</sup></strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>, with <img alt="art" src="images/Art_P1417.jpg"/> giving the <em>j</em>th entry in vector <strong>x<sup>(<em>i</em>)</sup></strong>. The <span class="blue"><strong><em>label</em></strong></span> of patient <em>i</em> is denoted by a scalar <em>y</em><sup>(<em>i</em>)</sup> ∈ <span class="font1">ℝ</span>, signifying the severity of the patient’s heart disease. The hypothesis should capture a relationship between the <strong>x<sup>(<em>i</em>)</sup></strong> values and <em>y</em><sup>(<em>i</em>)</sup>. For this example, we make the modeling assumption that the relationship is linear, and therefore the goal is to compute the “best” linear relationship between the <strong>x<sup>(<em>i</em>)</sup></strong> values and <em>y</em><sup>(<em>i</em>)</sup>: a linear function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span> such that <em>f</em>(<strong>x<sup>(<em>i</em>)</sup></strong>) ≈ <em>y</em><sup>(<em>i</em>)</sup> for each patient <em>i</em>. Of course, no such function may exist, but you would like one that comes as close as possible. A linear function <em>f</em> can be defined by a vector of weights <strong>w</strong> = (<em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, …, <em>w</em><sub><em>n</em></sub>), with</p>
<p class="eqr"><img alt="art" src="images/Art_P1418.jpg"/></p>
<p>When evaluating a machine-learning model, you need to measure how close each value <em>f</em>(<strong>x<sup>(<em>i</em>)</sup></strong>) is to its corresponding label <em>y</em><sup>(<em>i</em>)</sup>. In this example, we define the error <em>e</em><sup>(<em>i</em>)</sup> ∈ <span class="font1">ℝ</span> associated with patient <em>i</em> as <em>e</em><sup>(<em>i</em>)</sup> = <em>f</em>(<strong>x<sup>(<em>i</em>)</sup></strong>) – <em>y</em><sup>(<em>i</em>)</sup>. The objective function we choose is to minimize the sum of squares of the errors, which is</p>
<a id="p1036"/>
<p class="eqr"><img alt="art" src="images/Art_P1419.jpg"/></p>
<p>The objective function is typically called the <span class="blue"><strong><em>loss function</em></strong></span>, and the <span class="blue"><strong><em>least-squares error</em></strong></span> given by equation (33.33) is just one example of many possible loss functions. The goal is then, given the <strong>x<sup>(<em>i</em>)</sup></strong> and <em>y</em><sup>(<em>i</em>)</sup> values, to compute the weights <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, …, <em>w</em><sub><em>n</em></sub> so as to minimize the loss function in equation (33.33). The variables here are the weights <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, …, <em>w</em><sub><em>n</em></sub> and not the <strong>x<sup>(<em>i</em>)</sup></strong> or <em>y</em><sup>(<em>i</em>)</sup> values.</p>
<p>This particular objective is sometimes known as a <span class="blue"><strong><em>least-squares fit</em></strong></span>, and the problem of finding a linear function to fit data and minimize the least-squares error is called <span class="blue"><strong><em>linear regression</em></strong></span>. Finding a least-squares fit is also addressed in <a href="chapter028.xhtml#Sec_28.3">Section 28.3</a>.</p>
<p>When the function <em>f</em> is linear, the loss function defined in equation (33.33) is convex, because it is the sum of squares of linear functions, which are themselves convex. Therefore, we can apply gradient descent to compute a set of weights to approximately minimize the least-squares error. The concrete goal of learning is to be able to make predictions on new data. Informally, if the features are all reported in the same units and are from the same range (perhaps from being normalized), then the weights tend to have a natural interpretation because the features of the data that are better predictors of the label have a larger associated weight. For example, you would expect that, after normalization, the weight associated with the number of family members with heart disease would be larger than the weight associated with height.</p>
<p>The computed weights form a model of the data. Once you have a model, you can make predictions, so that given new data, you can predict its label. In our example, given a new patient <strong>x</strong>′ who is not part of the original training data set, you would still hope to predict the chance that the new patient develops heart disease. You can do so by computing the label <em>f</em>(<strong>x</strong>′), incorporating the weights computed by gradient descent.</p>
<p>For this linear-regression problem, the objective is to minimize the expression in equation (33.33), which is a quadratic in each of the <em>n</em>+1 weights <em>w</em><sub><em>j</em></sub>. Thus, entry <em>j</em> in the gradient is linear in <em>w</em><sub><em>j</em></sub>. Exercise 33.3-5 asks you to explicitly compute the gradient and see that it can be computed in <em>O</em>(<em>nm</em>) time, which is linear in the input size. Compared with the exact method of solving equation (33.33) in <a href="chapter028.xhtml">Chapter 28</a>, which needs to invert a matrix, gradient descent is typically much faster.</p>
<p><a href="chapter033.xhtml#Sec_33.1">Section 33.1</a> briefly discussed regularization—the idea that a complicated hypothesis should be penalized in order to avoid overfitting the training data. Regularization often involves adding a term to the objective function, but it can also <a id="p1037"/>be achieved by adding a constraint. One way to regularize this example would be to explicitly limit the norm of the weights, adding a constraint that <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em> for some bound <em>B</em> &gt; 0. (Recall again that the components of the vector <strong>w</strong> are the variables in the present application.) Adding this constraint controls the complexity of the model, as the number of values <em>w</em><sub><em>j</em></sub> that can have large absolute value is now limited.</p>
<p>In order to run G<small>RADIENT</small>-D<small>ESCENT</small>-C<small>ONSTRAINED</small> for any problem, you need to implement the projection step, as well as to compute bounds on <em>R</em> and <em>L</em>. We conclude this section by describing these calculations for gradient descent with the constraint <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em>. First, consider the projection step in line 5. Suppose that the update in line 4 results in a vector <strong>w</strong>′. The projection is implemented by computing ∏<sub><em>k</em></sub>(<strong>w</strong>′) where <em>K</em> is defined by <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em>. This particular projection can be accomplished by simply scaling <strong>w</strong>′, since we know that closest point in <em>K</em> to <strong>w</strong>′ must be the point along the vector whose norm is exactly <em>B</em>. The amount <em>z</em> by which we need to scale <strong>w</strong>′ to hit the boundary of <em>K</em> is the solution to the equation <em>z</em> <span class="font1">∥</span><strong>w</strong>′<span class="font1">∥</span> = <em>B</em>, which is solved by <em>z</em> = <em>B</em>/<span class="font1">∥</span><strong>w</strong>′<span class="font1">∥</span>. Hence line 5 is implemented by computing <strong>w</strong> = <strong>w</strong>′<em>B</em>/<span class="font1">∥</span><strong>w</strong>′<span class="font1">∥</span>. Because we always have <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em>, Exercise 33.3-6 asks you to show that the upper bound on the magnitude <em>L</em> of the gradient is <em>O</em>(<em>B</em>). We also get a bound on <em>R</em>, as follows. By the constraint <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em>, we know that both <span class="font1">∥</span><strong>w<sup>(0)</sup></strong><span class="font1">∥</span> ≤ <em>B</em> and <span class="font1">∥</span><strong>w</strong>*<span class="font1">∥</span> ≤ <em>B</em>, and thus <span class="font1">∥</span><strong>w<sup>(0)</sup></strong> – <strong>w</strong>*<span class="font1">∥</span> ≤ 2<em>B</em>. Using the definition of <em>R</em> in equation (33.20), we have <em>R</em> = <em>O</em>(<em>B</em>). The bound <img alt="art" src="images/Art_P1420.jpg"/> on the accuracy of the solution after <em>T</em> iterations in Theorem 33.11 becomes <img alt="art" src="images/Art_P1421.jpg"/>.</p>
<p class="level4"><strong>Exercises</strong></p>
<p class="level3"><strong><em>33.3-1</em></strong></p>
<p class="noindent">Prove Lemma 33.6. Start from the definition of a convex function given in equation (33.18). (<em>Hint:</em> You can prove the statement when <em>n</em> = 1 first. The proof for general values of <em>n</em> is similar.)</p>
<p class="level3"><strong><em>33.3-2</em></strong></p>
<p class="noindent">Prove Lemma 33.7.</p>
<p class="level3"><strong><em>33.3-3</em></strong></p>
<p class="noindent">Prove equation (33.29). (<em>Hint:</em> The proof for <em>n</em> = 1 dimension is straightforward. The proof for general values of <em>n</em> dimensions follows along similar lines.)</p>
<p class="level3"><strong><em>33.3-4</em></strong></p>
<p class="noindent">Show that the function <em>f</em> in equation (33.32) is a convex function of the variables <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, …, <em>w</em><sub><em>n</em></sub>.</p>
<a id="p1038"/>
<p class="level3"><strong><em>33.3-5</em></strong></p>
<p class="noindent">Compute the gradient of expression (33.33) and explain how to evaluate the gradient in <em>O</em>(<em>nm</em>) time.</p>
<p class="level3"><strong><em>33.3-6</em></strong></p>
<p class="noindent">Consider the function <em>f</em> defined in equation (33.32), and suppose that you have a bound <span class="font1">∥</span><strong>w</strong><span class="font1">∥</span> ≤ <em>B</em>, as is considered in the discussion on regularization. Show that <em>L</em> = <em>O</em>(<em>B</em>) in this case.</p>
<p class="level3"><strong><em>33.3-7</em></strong></p>
<p class="noindent">Equation (33.2) on page 1009 gives a function that, when minimized, gives an optimal solution to the <em>k</em>-means problem. Explain how to use gradient descent to solve the <em>k</em>-means problem.</p>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-197"><strong>Problems</strong></p>
<section title="33-1 Newton’s method">
<p class="level2"><strong><em>33-1     Newton’s method</em></strong></p>
<p class="noindent">Gradient descent iteratively moves closer to a desired value (the minimum) of a function. Another algorithm in this spirit is known as <span class="blue"><strong><em>Newton’s method</em></strong></span>, which is an iterative algorithm that finds the root of a function. Here, we consider Newton’s method which, given a function <em>f</em> : <span class="font1">ℝ</span> → <span class="font1">ℝ</span>, finds a value <em>x</em>* such that <em>f</em>(<em>x</em>* ) = 0. The algorithm moves through a series of points <em>x</em><sup>(0)</sup>, <em>x</em><sup>(1)</sup>, …. If the algorithm is currently at a point <em>x</em><sup>(<em>t</em>)</sup>, then to find point <em>x</em><sup>(<em>t</em>+1)</sup>, it first takes the equation of the line tangent to the curve at <em>x</em> = <em>x</em><sup>(<em>t</em>)</sup>,</p>
<p class="eql"><em>y</em> = <em>f</em>′(<em>x</em><sup>(<em>t</em>)</sup>)(<em>x</em> – <em>x</em><sup>(<em>t</em>)</sup>) + <em>f</em>(<em>x</em><sup>(<em>t</em>)</sup>).</p>
<p class="noindent">It then uses the <em>x</em>-intercept of this line as the next point <em>x</em><sup>(<em>t</em>+1)</sup>.</p>
<p class="nl"><strong><em>a.</em></strong> Show that the algorithm described above can be summarized by the update rule</p>
<p class="eql"><img alt="art" src="images/Art_P1422.jpg"/></p>
<p class="noindent">We restrict our attention to some domain <em>I</em> and assume that <em>f</em>′(<em>x</em>) ≠ 0 for all <em>x</em> ∈ <em>I</em> and that <em>f</em>″(<em>x</em>) is continuous. We also assume that the starting point <em>x</em><sup>(0)</sup> is sufficiently close to <em>x</em>*, where “sufficiently close” means that we can use only the first two terms of the Taylor expansion of <em>f</em>(<em>x</em>*) about <em>x</em><sup>(0)</sup>, namely</p>
<p class="eqr"><img alt="art" src="images/Art_P1423.jpg"/></p>
<a id="p1039"/>
<p class="noindent">where <em>γ</em><sup>(0)</sup> is some value between <em>x</em><sup>(0)</sup> and <em>x</em>*. If the approximation in equation (33.34) holds for <em>x</em><sup>(0)</sup>, it also holds for any point closer to <em>x</em>*.</p>
<p class="nl"><strong><em>b.</em></strong> Assume that the function <em>f</em> has exactly one point <em>x</em>* for which <em>f</em>(<em>x</em>*) = 0. Let <em><span class="font1">ϵ</span></em><sup>(<em>t</em>)</sup> = |<em>x</em><sup>(<em>t</em>)</sup> – <em>x</em>*|. Using the Taylor expansion in equation (33.34), show that</p>
<p class="eqnl"><img alt="art" src="images/Art_P1424.jpg"/></p>
<p class="nl-parat">where <em>γ</em><sup>(<em>t</em>)</sup> is some value between <em>x</em><sup>(<em>t</em>)</sup> and <em>x</em>*.</p>
<p class="nl"><strong><em>c.</em></strong> If</p>
<p class="eqnl"><img alt="art" src="images/Art_P1425.jpg"/></p>
<p class="nl-parat">for some constant <em>c</em> and <em><span class="font1">ϵ</span></em><sup>(0)</sup> &lt; 1, then we say that the function <em>f</em> has <span class="blue"><strong><em>quadratic convergence</em></strong></span>, since the error decreases quadratically. Assuming that <em>f</em> has quadratic convergence, how many iterations are needed to find a root of <em>f</em>(<em>x</em>) to an accuracy of <em>δ</em>? Your answer should include <em>δ</em>.</p>
<p class="nl"><strong><em>d.</em></strong> Suppose you wish to find a root of the function <em>f</em>(<em>x</em>) = (<em>x</em> – 3)<sup>2</sup>, which is also the minimizer, and you start at <em>x</em><sup>(0)</sup> = 3.5. Compare the number of iterations needed by gradient descent to find the minimizer and Newton’s method to find the root.</p>
</section>
<section title="33-2 Hedge">
<p class="level2"><strong><em>33-2     Hedge</em></strong></p>
<p class="noindent">Another variant in the multiplicative-weights framework is known as H<small>EDGE</small>. It differs from W<small>EIGHTED</small> M<small>AJORITY</small> in two ways. First, H<small>EDGE</small> makes the prediction randomly—in iteration <em>t</em>, it assigns a probability <img alt="art" src="images/Art_P1426.jpg"/> to expert <em>E</em><sub><em>i</em></sub>, where <img alt="art" src="images/Art_P1427.jpg"/>. It then chooses an expert <em>E</em><sub><em>i</em>′</sub> according to this probability distribution and predicts according to <em>E</em><sub><em>i</em>′</sub>. Second, the update rule is different. If an expert makes a mistake, line 16 updates that expert’s weight by the rule <img alt="art" src="images/Art_P1428.jpg"/>, for some 0 &lt; <em><span class="font1">ϵ</span></em> &lt; 1. Show that the expected number of mistakes made by H<small>EDGE</small>, running for <em>T</em> rounds, is at most <em>m</em>* + (ln <em>n</em>)/<em><span class="font1">ϵ</span></em> + <em><span class="font1">ϵ</span>T</em>.</p>
</section>
<section title="33-3 Nonoptimality of Lloyd’s procedure in one dimension">
<p class="level2"><strong><em>33-3     Nonoptimality of Lloyd’s procedure in one dimension</em></strong></p>
<p class="noindent">Give an example to show that even in one dimension, Lloyd’s procedure for finding clusters does not always return an optimum result. That is, Lloyd’s procedure may terminate and return as a result a set <em>C</em> of clusters that does not minimize <em>f</em>(<em>S</em>, <em>C</em>), even when <em>S</em> is a set of points on a line.</p>
<a id="p1040"/>
</section>
<section title="33-4 Stochastic gradient descent">
<p class="level2"><strong><em>33-4     Stochastic gradient descent</em></strong></p>
<p class="noindent">Consider the problem described in <a href="chapter033.xhtml#Sec_33.3">Section 33.3</a> of fitting a line <em>f</em>(<em>x</em>) = <em>ax</em> + <em>b</em> to a given set of point/value pairs <em>S</em> = {(<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>), …, (<em>x</em><sub><em>T</em></sub>, <em>y</em><sub><em>T</em></sub>)} by optimizing the choice of the parameters <em>a</em> and <em>b</em> using gradient descent to find a best least-squares fit. Here we consider the case where <em>x</em> is a real-valued variable, rather than a vector.</p>
<p>Suppose that you are not given the point/value pairs in <em>S</em> all at once, but only one at a time in an online manner. Furthermore, the points are given in random order. That is, you know that there are <em>n</em> points, but in iteration <em>t</em> you are given only (<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>) where <em>i</em> is independently and randomly chosen from {1, …, <em>T</em>}.</p>
<p>You can use gradient descent to compute an estimate to the function. As each point (<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>) is considered, you can update the current values of <em>a</em> and <em>b</em> by taking the derivative with respect to <em>a</em> and <em>b</em> of the term of the objective function depending on (<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>). Doing so gives you a stochastic estimate of the gradient, and you can then take a small step in the opposite direction.</p>
<p>Give pseudcode to implement this variant of gradient descent. What would the expected value of the error be as a function of <em>T</em>, <em>L</em>, and <em>R</em>? (<em>Hint:</em> Replicate the analysis of G<small>RADIENT</small>-D<small>ESCENT</small> in <a href="chapter033.xhtml#Sec_33.3">Section 33.3</a> for this variant.)</p>
<p>This procedure and its variants are known as <span class="blue"><strong><em>stochastic gradient descent</em></strong></span>.</p>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-198"><strong>Chapter notes</strong></p>
<p class="noindent">For a general introduction to artificial intelligence, we recommend Russell and Norvig [<a epub:type="noteref" href="bibliography001.xhtml#endnote_391">391</a>]. For a general introduction to machine learning, we recommend Murphy [<a epub:type="noteref" href="bibliography001.xhtml#endnote_340">340</a>].</p>
<p>Lloyd’s procedure for the <em>k</em>-means problem was first proposed by Lloyd [<a epub:type="noteref" href="bibliography001.xhtml#endnote_304">304</a>] and also later by Forgy [<a epub:type="noteref" href="bibliography001.xhtml#endnote_151">151</a>]. It is sometimes called “Lloyd’s algorithm” or the “Lloyd-Forgy algorithm.” Although Mahajan et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_310">310</a>] showed that finding an optimal clustering is NP-hard, even in the plane, Kanungo et al. [<a epub:type="noteref" href="bibliography001.xhtml#endnote_241">241</a>] have shown that there is an approximation algorithm for the <em>k</em>-means problem with approximation ratio 9 + <em><span class="font1">ϵ</span></em>, for any <em><span class="font1">ϵ</span></em> &gt; 0.</p>
<p>The multiplicative-weights method is surveyed by Arora, Hazan, and Kale [<a epub:type="noteref" href="bibliography001.xhtml#endnote_25">25</a>]. The main idea of updating weights based on feedback has been rediscovered many times. One early use is in game theory, where Brown defined “Fictitious Play” [<a epub:type="noteref" href="bibliography001.xhtml#endnote_74">74</a>] and conjectured its convergence to the value of a zero-sum game. The convergence properties were established by Robinson [<a epub:type="noteref" href="bibliography001.xhtml#endnote_382">382</a>].</p>
<p>In machine learning, the first use of multiplicative weights was by Littlestone in the Winnow algorithm [<a epub:type="noteref" href="bibliography001.xhtml#endnote_300">300</a>], which was later extended by Littlestone and Warmuth to the weighted-majority algorithm described in <a href="chapter033.xhtml#Sec_33.2">Section 33.2</a> [<a epub:type="noteref" href="bibliography001.xhtml#endnote_301">301</a>]. This work is closely connected to the boosting algorithm, originally due to Freund and Shapire <a id="p1041"/>[<a epub:type="noteref" href="bibliography001.xhtml#endnote_159">159</a>]. The multiplicative-weights idea is also closely related to several more general optimization algorithms, including the perceptron algorithm [<a epub:type="noteref" href="bibliography001.xhtml#endnote_328">328</a>] and algorithms for optimization problems such as packing linear programs [<a epub:type="noteref" href="bibliography001.xhtml#endnote_177">177</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_359">359</a>].</p>
<p>The treatment of gradient descent in this chapter draws heavily on the unpublished manuscript of Bansal and Gupta [<a epub:type="noteref" href="bibliography001.xhtml#endnote_35">35</a>]. They emphasize the idea of using a potential function and using ideas from amortized analysis to explain gradient descent. Other presentations and analyses of gradient descent include works by Bubeck [<a epub:type="noteref" href="bibliography001.xhtml#endnote_75">75</a>], Boyd and Vanderberghe [<a epub:type="noteref" href="bibliography001.xhtml#endnote_69">69</a>], and Nesterov [<a epub:type="noteref" href="bibliography001.xhtml#endnote_343">343</a>].</p>
<p>Gradient descent is known to converge faster when functions obey stronger properties than general convexity. For example, a function <em>f</em> is <span class="blue"><strong><em>α-strongly convex</em></strong></span> if <em>f</em>(<strong>y</strong>) ≥ <em>f</em>(<strong>x</strong>) + <span class="font1">〈</span>(∇<em>f</em>)(<strong>x</strong>), (<strong>y</strong> – <strong>x</strong>)<span class="font1">〉</span> + <em>α</em><span class="font1">∥</span><strong>y</strong> – <strong>x</strong><span class="font1">∥</span> for all <strong>x</strong>, <strong>y</strong> ∈ <span class="font1">ℝ</span><sup><em>n</em></sup>. In this case, G<small>RADIENT</small>-D<small>ESCENT</small> can use a variable step size and return <strong>x<sup>(<em>T</em>)</sup></strong>. The step size at step <em>t</em> becomes <em>γ</em><sub><em>t</em></sub> = 1/(<em>α</em>(<em>t</em> + 1)), and the procedure returns a point such that <em>f</em>(<strong>x-avg</strong>) – <em>f</em>(<strong>x</strong>*) ≤ <em>L</em><sup>2</sup>/(<em>α</em>(<em>T</em> + 1)). This convergence is better than that of Theorem 33.8 because the number of iterations needed is linear, rather than quadratic, in the desired error parameter <em><span class="font1">ϵ</span></em>, and because the performance is independent of the initial point.</p>
<p>Another case in which gradient descent can be shown to perform better than the analysis in <a href="chapter033.xhtml#Sec_33.3">Section 33.3</a> suggests is for smooth convex functions. We say that a function is <span class="blue"><strong><em>β-smooth</em></strong></span> if <img alt="art" src="images/Art_P1429.jpg"/>. This inequality goes in the opposite direction from the one for <em>≈</em>-strong convexity. Better bounds on gradient descent are possible here as well.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> Although the curve in <a href="chapter033.xhtml#Fig_33-4">Figure 33.4</a> looks concave, according to the definition of convexity that we’ll see below, the function <em>f</em> in the figure is convex.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> A function <em>f</em> : <span class="font1">ℝ</span><sup><em>n</em></sup> → <span class="font1">ℝ</span> is closed if, for each <em>α</em> ∈ <span class="font1">ℝ</span>, the set {<strong>x</strong> ∈ dom(<em>f</em>) : <em>f</em>(<strong>x</strong>) ≤ <em>α</em>} is closed, where dom(<em>f</em>) is the domain of <em>f</em>.</p>
</section>
</section>
</div>
</body>
</html>