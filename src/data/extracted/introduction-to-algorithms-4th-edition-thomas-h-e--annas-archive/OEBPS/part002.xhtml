<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p156"/>
<p class="line-p"/>
<section epub:type="bodymatter part" title="Part II Sorting and Order Statistics">
<p class="part-title"><a href="toc.xhtml#part-2"><strong><em><span class="blue1">Part II    Sorting and Order Statistics</span></em></strong></a></p>
<a id="p157"/>
<p class="cp"/>
<p class="noindent1-top1"> </p>
<p class="noindent1-top1"> </p>
<section title="Introduction">
<p class="level1a" id="h1-31"><a href="toc.xhtml#Rh1-31"><strong>Introduction</strong></a></p>
<p class="noindent">This part presents several algorithms that solve the following <strong><em><span class="blue1">sorting problem</span></em></strong>:</p>
<p class="para-hang1"><strong>Input:</strong> A sequence of <em>n</em> numbers <span class="font1">〈</span><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em><span class="font1">〉</span>.</p>
<p class="para-hang1"><strong>Output:</strong> A permutation (reordering) <img alt="art" src="images/Art_P297a.jpg"/> of the input sequence such that <img alt="art" src="images/Art_P297b.jpg"/>.</p>
<p class="noindent1-top">The input sequence is usually an <em>n</em>-element array, although it may be represented in some other fashion, such as a linked list.</p>
<section title="The structure of the data">
<p class="level2"><strong>The structure of the data</strong></p>
<p class="noindent">In practice, the numbers to be sorted are rarely isolated values. Each is usually part of a collection of data called a <strong><em><span class="blue1">record</span></em></strong>. Each record contains a <strong><em><span class="blue1">key</span></em></strong>, which is the value to be sorted. The remainder of the record consists of <strong><em><span class="blue1">satellite data</span></em></strong>, which are usually carried around with the key. In practice, when a sorting algorithm permutes the keys, it must permute the satellite data as well. If each record includes a large amount of satellite data, it often pays to permute an array of pointers to the records rather than the records themselves in order to minimize data movement.</p>
<p>In a sense, it is these implementation details that distinguish an algorithm from a full-blown program. A sorting algorithm describes the <em>method</em> to determine the sorted order, regardless of whether what’s being sorted are individual numbers or large records containing many bytes of satellite data. Thus, when focusing on the problem of sorting, we typically assume that the input consists only of numbers. Translating an algorithm for sorting numbers into a program for sorting records is conceptually straightforward, although in a given engineering situation other subtleties may make the actual programming task a challenge.</p>
<a id="p158"/>
</section>
<section title="Why sorting?">
<p class="level2"><strong>Why sorting?</strong></p>
<p class="noindent">Many computer scientists consider sorting to be the most fundamental problem in the study of algorithms. There are several reasons:</p>
<ul class="ulnoindent" epub:type="list">
<li>Sometimes an application inherently needs to sort information. For example, in order to prepare customer statements, banks need to sort checks by check number.</li>
<li class="litop">Algorithms often use sorting as a key subroutine. For example, a program that renders graphical objects which are layered on top of each other might have to sort the objects according to an “above” relation so that it can draw these objects from bottom to top. We will see numerous algorithms in this text that use sorting as a subroutine.</li>
<li class="litop">We can draw from among a wide variety of sorting algorithms, and they employ a rich set of techniques. In fact, many important techniques used throughout algorithm design appear in sorting algorithms that have been developed over the years. In this way, sorting is also a problem of historical interest.</li>
<li class="litop">We can prove a nontrivial lower bound for sorting (as we’ll do in <a href="chapter008.xhtml">Chapter 8</a>). Since the best upper bounds match the lower bound asymptotically, we can conclude that certain of our sorting algorithms are asymptotically optimal. Moreover, we can use the lower bound for sorting to prove lower bounds for various other problems.</li>
<li class="litop">Many engineering issues come to the fore when implementing sorting algorithms. The fastest sorting program for a particular situation may depend on many factors, such as prior knowledge about the keys and satellite data, the memory hierarchy (caches and virtual memory) of the host computer, and the software environment. Many of these issues are best dealt with at the algorithmic level, rather than by “tweaking” the code.</li></ul>
</section>
<section title="Sorting algorithms">
<p class="level2"><strong>Sorting algorithms</strong></p>
<p class="noindent">We introduced two algorithms that sort <em>n</em> real numbers in <a href="chapter002.xhtml">Chapter 2</a>. Insertion sort takes Θ(<em>n</em><sup>2</sup>) time in the worst case. Because its inner loops are tight, however, it is a fast sorting algorithm for small input sizes. Moreover, unlike merge sort, it sorts <strong><em><span class="blue1">in place</span></em></strong>, meaning that at most a constant number of elements of the input array are ever stored outside the array, which can be advantageous for space efficiency. Merge sort has a better asymptotic running time, Θ(<em>n</em> lg <em>n</em>), but the M<small>ERGE</small> procedure it uses does not operate in place. (We’ll see a parallelized version of merge sort in <a href="chapter026.xhtml#Sec_26.3">Section 26.3</a>.)</p>
<a id="p159"/>
<p>This part introduces two more algorithms that sort arbitrary real numbers. Heapsort, presented in <a href="chapter006.xhtml">Chapter 6</a>, sorts <em>n</em> numbers in place in <em>O</em>(<em>n</em> lg <em>n</em>) time. It uses an important data structure, called a heap, which can also implement a priority queue.</p>
<p>Quicksort, in <a href="chapter007.xhtml">Chapter 7</a>, also sorts <em>n</em> numbers in place, but its worst-case running time is Θ(<em>n</em><sup>2</sup>). Its expected running time is Θ(<em>n</em> lg <em>n</em>), however, and it generally outperforms heapsort in practice. Like insertion sort, quicksort has tight code, and so the hidden constant factor in its running time is small. It is a popular algorithm for sorting large arrays.</p>
<p>Insertion sort, merge sort, heapsort, and quicksort are all comparison sorts: they determine the sorted order of an input array by comparing elements. <a href="chapter008.xhtml">Chapter 8</a> begins by introducing the decision-tree model in order to study the performance limitations of comparison sorts. Using this model, we prove a lower bound of Ω(<em>n</em> lg <em>n</em>) on the worst-case running time of any comparison sort on <em>n</em> inputs, thus showing that heapsort and merge sort are asymptotically optimal comparison sorts.</p>
<p><a href="chapter008.xhtml">Chapter 8</a> then goes on to show that we might be able to beat this lower bound of Ω(<em>n</em> lg <em>n</em>) if an algorithm can gather information about the sorted order of the input by means other than comparing elements. The counting sort algorithm, for example, assumes that the input numbers belong to the set {0, 1, … , <em>k</em>}. By using array indexing as a tool for determining relative order, counting sort can sort <em>n</em> numbers in Θ(<em>k</em> + <em>n</em>) time. Thus, when <em>k</em> = <em>O</em>(<em>n</em>), counting sort runs in time that is linear in the size of the input array. A related algorithm, radix sort, can be used to extend the range of counting sort. If there are <em>n</em> integers to sort, each integer has <em>d</em> digits, and each digit can take on up to <em>k</em> possible values, then radix sort can sort the numbers in Θ(<em>d</em>(<em>n</em> + <em>k</em>)) time. When <em>d</em> is a constant and <em>k</em> is <em>O</em>(<em>n</em>), radix sort runs in linear time. A third algorithm, bucket sort, requires knowledge of the probabilistic distribution of numbers in the input array. It can sort <em>n</em> real numbers uniformly distributed in the half-open interval [0, 1) in average-case <em>O</em>(<em>n</em>) time.</p>
<p class="space-break">The table on the following page summarizes the running times of the sorting algorithms from <a href="chapter002.xhtml">Chapters 2</a> and <a href="chapter006.xhtml">6</a>–<a href="chapter008.xhtml">8</a>. As usual, <em>n</em> denotes the number of items to sort. For counting sort, the items to sort are integers in the set {0, 1, … , <em>k</em>}. For radix sort, each item is a <em>d</em>-digit number, where each digit takes on <em>k</em> possible values. For bucket sort, we assume that the keys are real numbers uniformly distributed in the half-open interval [0, 1). The rightmost column gives the average-case or expected running time, indicating which one it gives when it differs from the worst-case running time. We omit the average-case running time of heapsort because we do not analyze it in this book.</p>
<a id="p160"/>
<table class="table2">
<tr>
<td class="th2">Algorithm</td>
<td class="th1"><p class="p2a">Worst-case running time</p></td>
<td class="th1">Average-case/expected running time</td>
</tr>
<tr>
<td class="td1r">Insertion sort</td>
<td class="td2"><p class="p2a">Θ(<em>n</em><sup>2</sup>)</p></td>
<td class="td2">Θ(<em>n</em><sup>2</sup>)</td>
</tr>
<tr>
<td class="td1r">Merge sort</td>
<td class="td2"><p class="p2a">Θ(<em>n</em> lg <em>n</em>)</p></td>
<td class="td2">Θ(<em>n</em> lg <em>n</em>)</td>
</tr>
<tr>
<td class="td1r">Heapsort</td>
<td class="td2"><p class="p2a"><em>O</em>(<em>n</em> lg <em>n</em>)</p></td>
<td class="td2">—</td>
</tr>
<tr>
<td class="td1r">Quicksort</td>
<td class="td2"><p class="p2a">Θ(<em>n</em><sup>2</sup>)</p></td>
<td class="td2">Θ(<em>n</em> lg <em>n</em>) (expected)</td>
</tr>
<tr>
<td class="td1r">Counting sort</td>
<td class="td2"><p class="p2a">Θ(<em>k</em> + <em>n</em>)</p></td>
<td class="td2">Θ(<em>k</em> + <em>n</em>)</td>
</tr>
<tr>
<td class="td1r">Radix sort</td>
<td class="td2"><p class="p2a">Θ(<em>d</em>(<em>n</em> + <em>k</em>))</p></td>
<td class="td2">Θ(<em>d</em>(<em>n</em> + <em>k</em>))</td>
</tr>
<tr>
<td class="td1r">Bucket sort</td>
<td class="td2"><p class="p2a">Θ(<em>n</em><sup>2</sup>)</p></td>
<td class="td2">Θ(<em>n</em>) (average-case)</td>
</tr>
</table>
</section>
<section title="Order statistics">
<p class="level2"><strong>Order statistics</strong></p>
<p class="noindent">The <em>i</em>th order statistic of a set of <em>n</em> numbers is the <em>i</em>th smallest number in the set. You can, of course, select the <em>i</em>th order statistic by sorting the input and indexing the <em>i</em>th element of the output. With no assumptions about the input distribution, this method runs in Ω(<em>n</em> lg <em>n</em>) time, as the lower bound proved in <a href="chapter008.xhtml">Chapter 8</a> shows.</p>
<p><a href="chapter009.xhtml">Chapter 9</a> shows how to find the <em>i</em>th smallest element in <em>O</em>(<em>n</em>) time, even when the elements are arbitrary real numbers. We present a randomized algorithm with tight pseudocode that runs in Θ(<em>n</em><sup>2</sup>) time in the worst case, but whose expected running time is <em>O</em>(<em>n</em>). We also give a more complicated algorithm that runs in <em>O</em>(<em>n</em>) worst-case time.</p>
</section>
<section title="Background">
<p class="level2"><strong>Background</strong></p>
<p class="noindent">Although most of this part does not rely on difficult mathematics, some sections do require mathematical sophistication. In particular, analyses of quicksort, bucket sort, and the order-statistic algorithm use probability, which is reviewed in <a href="appendix003.xhtml">Appendix C</a>, and the material on probabilistic analysis and randomized algorithms in <a href="chapter005.xhtml">Chapter 5</a>.</p>
</section>
</section>
</section>
</div>
</body>
</html>