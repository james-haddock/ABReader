<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
<title>Introduction to Algorithms</title>
<link href="css/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:4a9ccac5-f2db-4081-af1f-a5a376b433e1" name="Adept.expected.resource"/>
</head>
<body>
<div class="body"><a id="p17"/>
<p class="line-c"/>
<section epub:type="bodymatter chapter" title="2 Getting Started">
<p class="chapter-title"><a href="toc.xhtml#chap-2"><strong><span class="blue1">2          Getting Started</span></strong></a></p>
<p class="noindent">This chapter will familiarize you with the framework we’ll use throughout the book to think about the design and analysis of algorithms. It is self-contained, but it does include several references to material that will be introduced in <a href="chapter003.xhtml">Chapters 3</a> and <a href="chapter004.xhtml">4</a>. (It also contains several summations, which <a href="appendix001.xhtml">Appendix A</a> shows how to solve.)</p>
<p>We’ll begin by examining the insertion sort algorithm to solve the sorting problem introduced in <a href="chapter001.xhtml">Chapter 1</a>. We’ll specify algorithms using a pseudocode that should be understandable to you if you have done computer programming. We’ll see why insertion sort correctly sorts and analyze its running time. The analysis introduces a notation that describes how running time increases with the number of items to be sorted. Following a discussion of insertion sort, we’ll use a method called divide-and-conquer to develop a sorting algorithm called merge sort. We’ll end with an analysis of merge sort’s running time.</p>
<p class="line1"/>
<section title="2.1 Insertion sort">
<a id="Sec_2.1"/>
<p class="level1" id="h1-6"><a href="toc.xhtml#Rh1-6"><strong>2.1      Insertion sort</strong></a></p>
<p class="noindent">Our first algorithm, insertion sort, solves the <strong><em><span class="blue1">sorting problem</span></em></strong> introduced in <a href="chapter001.xhtml">Chapter 1</a>:</p>
<p class="para-hang1"><strong>Input:</strong> A sequence of <em>n</em> numbers <span class="font1">〈</span><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em><span class="font1">〉</span>.</p>
<p class="para-hang1"><strong>Output:</strong> A permutation (reordering) <img alt="art" src="images/Art_P6.jpg"/> of the input sequence such that <img alt="art" src="images/Art_P7.jpg"/>.</p>
<p class="noindent1-top">The numbers to be sorted are also known as the <strong><em><span class="blue1">keys</span></em></strong>. Although the problem is conceptually about sorting a sequence, the input comes in the form of an array with <em>n</em> elements. When we want to sort numbers, it’s often because they are the keys associated with other data, which we call <strong><em><span class="blue1">satellite data</span></em></strong>. Together, a key and satellite data form a <strong><em><span class="blue1">record</span></em></strong>. For example, consider a spreadsheet containing student records with many associated pieces of data such as age, grade-point average, and number of courses taken. Any one of these quantities could be a key, but when the <a id="p18"/>spreadsheet sorts, it moves the associated record (the satellite data) with the key. When describing a sorting algorithm, we focus on the keys, but it is important to remember that there usually is associated satellite data.</p>
<p>In this book, we’ll typically describe algorithms as procedures written in a <strong><em><span class="blue1">pseudocode</span></em></strong> that is similar in many respects to C, C++, Java, Python,<sup><a epub:type="footnote" href="#footnote_1" id="footnote_ref_1">1</a></sup> or JavaScript. (Apologies if we’ve omitted your favorite programming language. We can’t list them all.) If you have been introduced to any of these languages, you should have little trouble understanding algorithms “coded” in pseudocode. What separates pseudocode from real code is that in pseudocode, we employ whatever expressive method is most clear and concise to specify a given algorithm. Sometimes the clearest method is English, so do not be surprised if you come across an English phrase or sentence embedded within a section that looks more like real code. Another difference between pseudocode and real code is that pseudocode often ignores aspects of software engineering—such as data abstraction, modularity, and error handling—in order to convey the essence of the algorithm more concisely.</p>
<p>We start with <strong><em><span class="blue1">insertion sort</span></em></strong>, which is an efficient algorithm for sorting a small number of elements. Insertion sort works the way you might sort a hand of playing cards. Start with an empty left hand and the cards in a pile on the table. Pick up the first card in the pile and hold it with your left hand. Then, with your right hand, remove one card at a time from the pile, and insert it into the correct position in your left hand. As <a href="chapter002.xhtml#Fig_2-1">Figure 2.1</a> illustrates, you find the correct position for a card by comparing it with each of the cards already in your left hand, starting at the right and moving left. As soon as you see a card in your left hand whose value is less than or equal to the card you’re holding in your right hand, insert the card that you’re holding in your right hand just to the right of this card in your left hand. If all the cards in your left hand have values greater than the card in your right hand, then place this card as the leftmost card in your left hand. At all times, the cards held in your left hand are sorted, and these cards were originally the top cards of the pile on the table.</p>
<p>The pseudocode for insertion sort is given as the procedure I<small>NSERTION</small>-S<small>ORT</small> on the facing page. It takes two parameters: an array <em>A</em> containing the values to be sorted and the number <em>n</em> of values of sort. The values occupy positions <em>A</em>[1] through <em>A</em>[<em>n</em>] of the array, which we denote by <em>A</em>[1 : <em>n</em>]. When the I<small>NSERTION</small>-S<small>ORT</small> procedure is finished, array <em>A</em>[1 : <em>n</em>] contains the original values, but in sorted order.</p>
<a id="p19"/>
<div class="divimage">
<p class="fig-imga" id="Fig_2-1"><img alt="art" src="images/Art_P8.jpg"/></p>
<p class="caption"><strong>Figure 2.1</strong> Sorting a hand of cards using insertion sort.</p>
</div>
<div class="pull-quote1">
<p class="box-heading">I<small>NSERTION</small>-S<small>ORT</small>(<em>A, n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><em>key</em> = <em>A</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><span class="red"><strong>//</strong> Insert <em>A</em>[<em>i</em>] into the sorted subarray <em>A</em>[1 : <em>i</em> – 1].</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><em>j</em> = <em>i</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p2"><strong>while</strong> <em>j</em> &gt; 0 and <em>A</em>[<em>j</em>] &gt; <em>key</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p3"><em>A</em>[<em>j</em> + 1] = <em>A</em>[<em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p3"><em>j</em> = <em>j</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="p2"><em>A</em>[<em>j</em> + 1] = <em>key</em></p></td>
</tr>
</table>
</div>
<p class="level4"><strong>Loop invariants and the correctness of insertion sort</strong></p>
<p class="noindent"><a href="chapter002.xhtml#Fig_2-2">Figure 2.2</a> shows how this algorithm works for an array <em>A</em> that starts out with the sequence <span class="font1">〈</span>5, 2, 4, 6, 1, 3<span class="font1">〉</span>. The index <em>i</em> indicates the “current card” being inserted into the hand. At the beginning of each iteration of the <strong>for</strong> loop, which is indexed by <em>i</em>, the <strong><em><span class="blue1">subarray</span></em></strong> (a contiguous portion of the array) consisting of elements <em>A</em>[1 : <em>i</em> – 1] (that is, <em>A</em>[1] through <em>A</em>[<em>i</em> – 1]) constitutes the currently sorted hand, and the remaining subarray <em>A</em>[<em>i</em> + 1 : <em>n</em>] (elements <em>A</em>[<em>i</em> + 1] through <em>A</em>[<em>n</em>]) corresponds to the pile of cards still on the table. In fact, elements <em>A</em>[1 : <em>i</em> – 1] are the elements <em>originally</em> in positions 1 through <em>i</em> – 1, but now in sorted order. We state these properties of <em>A</em>[1 : <em>i</em> – 1] formally as a <strong><em><span class="blue1">loop invariant</span></em></strong>:</p>
<a id="p20"/>
<div class="divimage">
<p class="fig-imga" id="Fig_2-2"><img alt="art" src="images/Art_P9.jpg"/></p>
<p class="caption"><strong>Figure 2.2</strong> The operation of I<small>NSERTION</small>-S<small>ORT</small>(<em>A, n</em>), where <em>A</em> initially contains the sequence <span class="font1">〈</span>5, 2, 4, 6, 1, 3<span class="font1">〉</span> and <em>n</em> = 6. Array indices appear above the rectangles, and values stored in the array positions appear within the rectangles. <strong>(a)–(e)</strong> The iterations of the <strong>for</strong> loop of lines 1–8. In each iteration, the blue rectangle holds the key taken from <em>A</em>[<em>i</em>], which is compared with the values in tan rectangles to its left in the test of line 5. Orange arrows show array values moved one position to the right in line 6, and blue arrows indicate where the key moves to in line 8. <strong>(f)</strong> The final sorted array.</p>
</div>
<div class="pull-quote">
<p class="pq-noindent">At the start of each iteration of the <strong>for</strong> loop of lines 1–8, the subarray <em>A</em>[1 : <em>i</em> – 1] consists of the elements originally in <em>A</em>[1 : <em>i</em> – 1], but in sorted order.</p>
</div>
<p>Loop invariants help us understand why an algorithm is correct. When you’re using a loop invariant, you need to show three things:</p>
<p class="para-hang1"><strong>Initialization:</strong> It is true prior to the first iteration of the loop.</p>
<p class="para-hang1"><strong>Maintenance:</strong> If it is true before an iteration of the loop, it remains true before the next iteration.</p>
<p class="para-hang1"><strong>Termination:</strong> The loop terminates, and when it terminates, the invariant—usually along with the reason that the loop terminated—gives us a useful property that helps show that the algorithm is correct.</p>
<p class="noindent1-top">When the first two properties hold, the loop invariant is true prior to every iteration of the loop. (Of course, you are free to use established facts other than the loop invariant itself to prove that the loop invariant remains true before each iteration.) A loop-invariant proof is a form of mathematical induction, where to prove that a property holds, you prove a base case and an inductive step. Here, showing that the invariant holds before the first iteration corresponds to the base case, and showing that the invariant holds from iteration to iteration corresponds to the inductive step.</p>
<p>The third property is perhaps the most important one, since you are using the loop invariant to show correctness. Typically, you use the loop invariant along with the condition that caused the loop to terminate. Mathematical induction typically applies the inductive step infinitely, but in a loop invariant the “induction” stops when the loop terminates.</p>
<a id="p21"/>
<p>Let’s see how these properties hold for insertion sort.</p>
<p class="para-hang1"><strong>Initialization:</strong> We start by showing that the loop invariant holds before the first loop iteration, when <em>i</em> = 2.<sup><a epub:type="footnote" href="#footnote_2" id="footnote_ref_2">2</a></sup> The subarray <em>A</em>[1 : <em>i</em> – 1] consists of just the single element <em>A</em>[1], which is in fact the original element in <em>A</em>[1]. Moreover, this subarray is sorted (after all, how could a subarray with just one value not be sorted?), which shows that the loop invariant holds prior to the first iteration of the loop.</p>
<p class="para-hang1"><strong>Maintenance:</strong> Next, we tackle the second property: showing that each iteration maintains the loop invariant. Informally, the body of the <strong>for</strong> loop works by moving the values in <em>A</em>[<em>i</em> – 1], <em>A</em>[<em>i</em> – 2], <em>A</em>[<em>i</em> – 3], and so on by one position to the right until it finds the proper position for <em>A</em>[<em>i</em>] (lines 4–7), at which point it inserts the value of <em>A</em>[<em>i</em>] (line 8). The subarray <em>A</em>[1 : <em>i</em>] then consists of the elements originally in <em>A</em>[1 : <em>i</em>], but in sorted order. <strong><em><span class="blue1">Incrementing</span></em></strong> <em>i</em> (increasing its value by 1) for the next iteration of the <strong>for</strong> loop then preserves the loop invariant.</p>
<p class="php">A more formal treatment of the second property would require us to state and show a loop invariant for the <strong>while</strong> loop of lines 5–7. Let’s not get bogged down in such formalism just yet. Instead, we’ll rely on our informal analysis to show that the second property holds for the outer loop.</p>
<p class="para-hang1"><strong>Termination:</strong> Finally, we examine loop termination. The loop variable <em>i</em> starts at 2 and increases by 1 in each iteration. Once <em>i</em>’s value exceeds <em>n</em> in line 1, the loop terminates. That is, the loop terminates once <em>i</em> equals <em>n</em> + 1. Substituting <em>n</em> + 1 for <em>i</em> in the wording of the loop invariant yields that the subarray <em>A</em>[1 : <em>n</em>] consists of the elements originally in <em>A</em>[1 : <em>n</em>], but in sorted order. Hence, the algorithm is correct.</p>
<p class="space-break">This method of loop invariants is used to show correctness in various places throughout this book.</p>
<p class="level4"><strong>Pseudocode conventions</strong></p>
<p class="noindent">We use the following conventions in our pseudocode.</p>
<ul class="ulnoindent" epub:type="list">
<li>Indentation indicates block structure. For example, the body of the <strong>for</strong> loop that begins on line 1 consists of lines 2–8, and the body of the <strong>while</strong> loop that <a id="p22"/>begins on line 5 contains lines 6–7 but not line 8. Our indentation style applies to <strong>if-else</strong> statements<sup><a epub:type="footnote" href="#footnote_3" id="footnote_ref_3">3</a></sup> as well. Using indentation instead of textual indicators of block structure, such as <strong>begin</strong> and <strong>end</strong> statements or curly braces, reduces clutter while preserving, or even enhancing, clarity.<sup><a epub:type="footnote" href="#footnote_4" id="footnote_ref_4">4</a></sup></li>
<li class="litop">The looping constructs <strong>while</strong>, <strong>for</strong>, and <strong>repeat-until</strong> and the <strong>if-else</strong> conditional construct have interpretations similar to those in C, C++, Java, Python, and JavaScript.<sup><a epub:type="footnote" href="#footnote_5" id="footnote_ref_5">5</a></sup> In this book, the loop counter retains its value after the loop is exited, unlike some situations that arise in C++ and Java. Thus, immediately after a <strong>for</strong> loop, the loop counter’s value is the value that first exceeded the <strong>for</strong> loop bound.<sup><a epub:type="footnote" href="#footnote_6" id="footnote_ref_6">6</a></sup> We used this property in our correctness argument for insertion sort. The <strong>for</strong> loop header in line 1 is <strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>n</em>, and so when this loop terminates, <em>i</em> equals <em>n</em> + 1. We use the keyword <strong>to</strong> when a <strong>for</strong> loop increments its loop counter in each iteration, and we use the keyword <strong>downto</strong> when a <strong>for</strong> loop <strong><em><span class="blue1">decrements</span></em></strong> its loop counter (reduces its value by 1 in each iteration). When the loop counter changes by an amount greater than 1, the amount of change follows the optional keyword <strong>by</strong>.</li>
<li class="litop">The symbol “<strong><span class="red">//</span></strong>” indicates that the remainder of the line is a comment.</li>
<li class="litop">Variables (such as <em>i</em>, <em>j</em>, and <em>key</em>) are local to the given procedure. We won’t use global variables without explicit indication.</li>
<li class="litop">We access array elements by specifying the array name followed by the index in square brackets. For example, <em>A</em>[<em>i</em>] indicates the <em>i</em>th element of the array <em>A</em>.
<p class="ntop1">Although many programming languages enforce 0-origin indexing for arrays (0 is the smallest valid index), we choose whichever indexing scheme is clearest for human readers to understand. Because people usually start counting at 1, not 0, most—but not all—of the arrays in this book use 1-origin indexing. To be <a id="p23"/>clear about whether a particular algorithm assumes 0-origin or 1-origin indexing, we’ll specify the bounds of the arrays explicitly. If you are implementing an algorithm that we specify using 1-origin indexing, but you’re writing in a programming language that enforces 0-origin indexing (such as C, C++, Java, Python, or JavaScript), then give yourself credit for being able to adjust. You can either always subtract 1 from each index or allocate each array with one extra position and just ignore position 0.</p>
<p class="ntop1">The notation “:” denotes a subarray. Thus, <em>A</em>[<em>i</em> : <em>j</em>] indicates the subarray of <em>A</em> consisting of the elements <em>A</em>[<em>i</em>], <em>A</em>[<em>i</em> + 1], … , <em>A</em>[<em>j</em>].<sup><a epub:type="footnote" href="#footnote_7" id="footnote_ref_7">7</a></sup> We also use this notation to indicate the bounds of an array, as we did earlier when discussing the array <em>A</em>[1 : <em>n</em>].</p>
</li>
<li class="litop">We typically organize compound data into <strong><em><span class="blue1">objects</span></em></strong>, which are composed of <strong><em><span class="blue1">attributes</span></em></strong>. We access a particular attribute using the syntax found in many object-oriented programming languages: the object name, followed by a dot, followed by the attribute name. For example, if an object <em>x</em> has attribute <em>f</em>, we denote this attribute by <em>x.f</em>.
<p class="ntop1">We treat a variable representing an array or object as a pointer (known as a reference in some programming languages) to the data representing the array or object. For all attributes <em>f</em> of an object <em>x</em>, setting <em>y</em> = <em>x</em> causes <em>y.f</em> to equal <em>x.f</em>. Moreover, if we now set <em>x.f</em> = 3, then afterward not only does <em>x.f</em> equal 3, but <em>y.f</em> equals 3 as well. In other words, <em>x</em> and <em>y</em> point to the same object after the assignment <em>y</em> = <em>x</em>. This way of treating arrays and objects is consistent with most contemporary programming languages.</p>
<p class="ntop1">Our attribute notation can “cascade.” For example, suppose that the attribute <em>f</em> is itself a pointer to some type of object that has an attribute <em>g</em>. Then the notation <em>x.f.g</em> is implicitly parenthesized as (<em>x.f</em>).<em>g</em>. In other words, if we had assigned <em>y</em> = <em>x.f</em>, then <em>x.f.g</em> is the same as <em>y.g</em>.</p>
<p class="ntop1">Sometimes a pointer refers to no object at all. In this case, we give it the special value <small>NIL</small>.</p>
</li>
<li class="litop">We pass parameters to a procedure <strong><em><span class="blue1">by value</span></em></strong>: the called procedure receives its own copy of the parameters, and if it assigns a value to a parameter, the change is <em>not</em> seen by the calling procedure. When objects are passed, the pointer to the data representing the object is copied, but the object’s attributes are not. For example, if <em>x</em> is a parameter of a called procedure, the assignment <em>x</em> = <em>y</em> within <a id="p24"/>the called procedure is not visible to the calling procedure. The assignment <em>x.f</em> = 3, however, is visible if the calling procedure has a pointer to the same object as <em>x</em>. Similarly, arrays are passed by pointer, so that a pointer to the array is passed, rather than the entire array, and changes to individual array elements are visible to the calling procedure. Again, most contemporary programming languages work this way.</li>
<li class="litop">A <strong>return</strong> statement immediately transfers control back to the point of call in the calling procedure. Most <strong>return</strong> statements also take a value to pass back to the caller. Our pseudocode differs from many programming languages in that we allow multiple values to be returned in a single <strong>return</strong> statement without having to create objects to package them together.<sup><a epub:type="footnote" href="#footnote_8" id="footnote_ref_8">8</a></sup></li>
<li class="litop">The boolean operators “and” and “or” are <strong><em><span class="blue1">short circuiting</span></em></strong>. That is, evaluate the expression “<em>x</em> and <em>y</em>” by first evaluating <em>x</em>. If <em>x</em> evaluates to <small>FALSE</small>, then the entire expression cannot evaluate to <small>TRUE</small>, and therefore <em>y</em> is not evaluated. If, on the other hand, <em>x</em> evaluates to <small>TRUE</small>, <em>y</em> must be evaluated to determine the value of the entire expression. Similarly, in the expression “<em>x</em> or <em>y</em>” the expression <em>y</em> is evaluated only if <em>x</em> evaluates to <small>FALSE</small>. Short-circuiting operators allow us to write boolean expressions such as “<em>x</em> ≠ <small>NIL</small> and <em>x.f</em> = <em>y</em>” without worrying about what happens upon evaluating <em>x.f</em> when <em>x</em> is <small>NIL</small>.</li>
<li class="litop">The keyword <strong>error</strong> indicates that an error occurred because conditions were wrong for the procedure to have been called, and the procedure immediately terminates. The calling procedure is responsible for handling the error, and so we do not specify what action to take.</li></ul>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>2.1-1</em></strong></p>
<p class="noindent">Using <a href="chapter002.xhtml#Fig_2-2">Figure 2.2</a> as a model, illustrate the operation of I<small>NSERTION</small>-S<small>ORT</small> on an array initially containing the sequence <span class="font1">〈</span>31, 41, 59, 26, 41, 58<span class="font1">〉</span>.</p>
<p class="level3"><strong><em>2.1-2</em></strong></p>
<p class="noindent">Consider the procedure S<small>UM</small>-A<small>RRAY</small> on the facing page. It computes the sum of the <em>n</em> numbers in array <em>A</em>[1 : <em>n</em>]. State a loop invariant for this procedure, and use its initialization, maintenance, and termination properties to show that the S<small>UM</small>-A<small>RRAY</small> procedure returns the sum of the numbers in <em>A</em>[1 : <em>n</em>].</p>
<a id="p25"/>
<div class="pull-quote1">
<p class="box-heading">S<small>UM</small>-A<small>RRAY</small>(<em>A, n</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><em>sum</em> = 0</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><em>sum</em> = <em>sum</em> + <em>A</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>sum</em></p></td>
</tr>
</table>
</div>
<p class="level3"><strong><em>2.1-3</em></strong></p>
<p class="noindent">Rewrite the I<small>NSERTION</small>-S<small>ORT</small> procedure to sort into monotonically decreasing instead of monotonically increasing order.</p>
<p class="level3"><strong><em>2.1-4</em></strong></p>
<p class="noindent">Consider the <strong><em><span class="blue1">searching problem</span></em></strong>:</p>
<p class="para-hang1"><strong>Input:</strong> A sequence of <em>n</em> numbers <span class="font1">〈</span><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em><span class="font1">〉</span> stored in array <em>A</em>[1 : <em>n</em>] and a value <em>x</em>.</p>
<p class="para-hang1"><strong>Output:</strong> An index <em>i</em> such that <em>x</em> equals <em>A</em>[<em>i</em>] or the special value <small>NIL</small> if <em>x</em> does not appear in <em>A</em>.</p>
<p class="space-break">Write pseudocode for <strong><em><span class="blue1">linear search</span></em></strong>, which scans through the array from beginning to end, looking for <em>x</em>. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.</p>
<p class="level3"><strong><em>2.1-5</em></strong></p>
<p class="noindent">Consider the problem of adding two <em>n</em>-bit binary integers <em>a</em> and <em>b</em>, stored in two <em>n</em>-element arrays <em>A</em>[0 : <em>n</em> – 1] and <em>B</em>[0 : <em>n</em> – 1], where each element is either 0 or 1, <img alt="art" src="images/Art_P10.jpg"/>, and <img alt="art" src="images/Art_P11.jpg"/>. The sum <em>c</em> = <em>a</em> + <em>b</em> of the two integers should be stored in binary form in an (<em>n</em> + 1)-element array <em>C</em> [0 : <em>n</em>], where <img alt="art" src="images/Art_P12.jpg"/>. Write a procedure A<small>DD</small>-B<small>INARY</small>-I<small>NTEGERS</small> that takes as input arrays <em>A</em> and <em>B</em>, along with the length <em>n</em>, and returns array <em>C</em> holding the sum.</p>
</section>
<p class="line1"/>
<section title="2.2 Analyzing algorithms">
<a id="Sec_2.2"/>
<p class="level1" id="h1-7"><a href="toc.xhtml#Rh1-7"><strong>2.2      Analyzing algorithms</strong></a></p>
<p class="noindent"><strong><em><span class="blue1">Analyzing</span></em></strong> an algorithm has come to mean predicting the resources that the algorithm requires. You might consider resources such as memory, communication bandwidth, or energy consumption. Most often, however, you’ll want to measure computational time. If you analyze several candidate algorithms for a problem, <a id="p26"/>you can identify the most efficient one. There might be more than just one viable candidate, but you can often rule out several inferior algorithms in the process.</p>
<p>Before you can analyze an algorithm, you need a model of the technology that it runs on, including the resources of that technology and a way to express their costs. Most of this book assumes a generic one-processor, <strong><em><span class="blue1">random-access machine (RAM)</span></em></strong> model of computation as the implementation technology, with the understanding that algorithms are implemented as computer programs. In the RAM model, instructions execute one after another, with no concurrent operations. The RAM model assumes that each instruction takes the same amount of time as any other instruction and that each data access—using the value of a variable or storing into a variable—takes the same amount of time as any other data access. In other words, in the RAM model each instruction or data access takes a constant amount of time—even indexing into an array.<sup><a epub:type="footnote" href="#footnote_9" id="footnote_ref_9">9</a></sup></p>
<p>Strictly speaking, we should precisely define the instructions of the RAM model and their costs. To do so, however, would be tedious and yield little insight into algorithm design and analysis. Yet we must be careful not to abuse the RAM model. For example, what if a RAM had an instruction that sorts? Then you could sort in just one step. Such a RAM would be unrealistic, since such instructions do not appear in real computers. Our guide, therefore, is how real computers are designed. The RAM model contains instructions commonly found in real computers: arithmetic (such as add, subtract, multiply, divide, remainder, floor, ceiling), data movement (load, store, copy), and control (conditional and unconditional branch, subroutine call and return).</p>
<p>The data types in the RAM model are integer, floating point (for storing real-number approximations), and character. Real computers do not usually have a separate data type for the boolean values <small>TRUE</small> and <small>FALSE</small>. Instead, they often test whether an integer value is 0 (<small>FALSE</small>) or nonzero (<small>TRUE</small>), as in C. Although we typically do not concern ourselves with precision for floating-point values in this book (many numbers cannot be represented exactly in floating point), precision is crucial for most applications. We also assume that each word of data has a limit on the number of bits. For example, when working with inputs of size <em>n</em>, we typically <a id="p27"/>assume that integers are represented by <em>c</em> log<sub>2</sub> <em>n</em> bits for some constant <em>c</em> ≥ 1. We require <em>c</em> ≥ 1 so that each word can hold the value of <em>n</em>, enabling us to index the individual input elements, and we restrict <em>c</em> to be a constant so that the word size does not grow arbitrarily. (If the word size could grow arbitrarily, we could store huge amounts of data in one word and operate on it all in constant time—an unrealistic scenario.)</p>
<p>Real computers contain instructions not listed above, and such instructions represent a gray area in the RAM model. For example, is exponentiation a constant-time instruction? In the general case, no: to compute <em>x<sup>n</sup></em> when <em>x</em> and <em>n</em> are general integers typically takes time logarithmic in <em>n</em> (see equation (31.34) on page 934), and you must worry about whether the result fits into a computer word. If <em>n</em> is an exact power of 2, however, exponentiation can usually be viewed as a constant-time operation. Many computers have a “shift left” instruction, which in constant time shifts the bits of an integer by <em>n</em> positions to the left. In most computers, shifting the bits of an integer by 1 position to the left is equivalent to multiplying by 2, so that shifting the bits by <em>n</em> positions to the left is equivalent to multiplying by 2<em><sup>n</sup></em>. Therefore, such computers can compute 2<em><sup>n</sup></em> in 1 constant-time instruction by shifting the integer 1 by <em>n</em> positions to the left, as long as <em>n</em> is no more than the number of bits in a computer word. We’ll try to avoid such gray areas in the RAM model and treat computing 2<em><sup>n</sup></em> and multiplying by 2<em><sup>n</sup></em> as constant-time operations when the result is small enough to fit in a computer word.</p>
<p>The RAM model does not account for the memory hierarchy that is common in contemporary computers. It models neither caches nor virtual memory. Several other computational models attempt to account for memory-hierarchy effects, which are sometimes significant in real programs on real machines. <a href="chapter011.xhtml#Sec_11.5">Section 11.5</a> and a handful of problems in this book examine memory-hierarchy effects, but for the most part, the analyses in this book do not consider them. Models that include the memory hierarchy are quite a bit more complex than the RAM model, and so they can be difficult to work with. Moreover, RAM-model analyses are usually excellent predictors of performance on actual machines.</p>
<p>Although it is often straightforward to analyze an algorithm in the RAM model, sometimes it can be quite a challenge. You might need to employ mathematical tools such as combinatorics, probability theory, algebraic dexterity, and the ability to identify the most significant terms in a formula. Because an algorithm might behave differently for each possible input, we need a means for summarizing that behavior in simple, easily understood formulas.</p>
<p class="level4"><strong>Analysis of insertion sort</strong></p>
<p class="noindent">How long does the I<small>NSERTION</small>-S<small>ORT</small> procedure take? One way to tell would be for you to run it on your computer and time how long it takes to run. Of course, you’d <a id="p28"/>first have to implement it in a real programming language, since you cannot run our pseudocode directly. What would such a timing test tell you? You would find out how long insertion sort takes to run on your particular computer, on that particular input, under the particular implementation that you created, with the particular compiler or interpreter that you ran, with the particular libraries that you linked in, and with the particular background tasks that were running on your computer concurrently with your timing test (such as checking for incoming information over a network). If you run insertion sort again on your computer with the same input, you might even get a different timing result. From running just one implementation of insertion sort on just one computer and on just one input, what would you be able to determine about insertion sort’s running time if you were to give it a different input, if you were to run it on a different computer, or if you were to implement it in a different programming language? Not much. We need a way to predict, given a new input, how long insertion sort will take.</p>
<p>Instead of timing a run, or even several runs, of insertion sort, we can determine how long it takes by analyzing the algorithm itself. We’ll examine how many times it executes each line of pseudocode and how long each line of pseudocode takes to run. We’ll first come up with a precise but complicated formula for the running time. Then, we’ll distill the important part of the formula using a convenient notation that can help us compare the running times of different algorithms for the same problem.</p>
<p>How do we analyze insertion sort? First, let’s acknowledge that the running time depends on the input. You shouldn’t be terribly surprised that sorting a thousand numbers takes longer than sorting three numbers. Moreover, insertion sort can take different amounts of time to sort two input arrays of the same size, depending on how nearly sorted they already are. Even though the running time can depend on many features of the input, we’ll focus on the one that has been shown to have the greatest effect, namely the size of the input, and describe the running time of a program as a function of the size of its input. To do so, we need to define the terms “running time” and “input size” more carefully. We also need to be clear about whether we are discussing the running time for an input that elicits the worst-case behavior, the best-case behavior, or some other case.</p>
<p>The best notion for <strong><em><span class="blue1">input size</span></em></strong> depends on the problem being studied. For many problems, such as sorting or computing discrete Fourier transforms, the most natural measure is the <em>number of items in the input</em>—for example, the number <em>n</em> of items being sorted. For many other problems, such as multiplying two integers, the best measure of input size is the <em>total number of bits</em> needed to represent the input in ordinary binary notation. Sometimes it is more appropriate to describe the size of the input with more than just one number. For example, if the input to an algorithm is a graph, we usually characterize the input size by both the number <a id="p29"/>of vertices and the number of edges in the graph. We’ll indicate which input size measure is being used with each problem we study.</p>
<p>The <strong><em><span class="blue1">running time</span></em></strong> of an algorithm on a particular input is the number of instructions and data accesses executed. How we account for these costs should be independent of any particular computer, but within the framework of the RAM model. For the moment, let us adopt the following view. A constant amount of time is required to execute each line of our pseudocode. One line might take more or less time than another line, but we’ll assume that each execution of the <em>k</em>th line takes <em>c<sub>k</sub></em> time, where <em>c<sub>k</sub></em> is a constant. This viewpoint is in keeping with the RAM model, and it also reflects how the pseudocode would be implemented on most actual computers.<sup><a epub:type="footnote" href="#footnote_10" id="footnote_ref_10">10</a></sup></p>
<p>Let’s analyze the I<small>NSERTION</small>-S<small>ORT</small> procedure. As promised, we’ll start by devising a precise formula that uses the input size and all the statement costs <em>c<sub>k</sub></em>. This formula turns out to be messy, however. We’ll then switch to a simpler notation that is more concise and easier to use. This simpler notation makes clear how to compare the running times of algorithms, especially as the size of the input increases.</p>
<p>To analyze the I<small>NSERTION</small>-S<small>ORT</small> procedure, let’s view it on the following page with the time cost of each statement and the number of times each statement is executed. For each <em>i</em> = 2, 3, … , <em>n</em>, let <em>t<sub>i</sub></em> denote the number of times the <strong>while</strong> loop test in line 5 is executed for that value of <em>i</em>. When a <strong>for</strong> or <strong>while</strong> loop exits in the usual way—because the test in the loop header comes up <small>FALSE</small>—the test is executed one time more than the loop body. Because comments are not executable statements, assume that they take no time.</p>
<p>The running time of the algorithm is the sum of running times for each statement executed. A statement that takes <em>c<sub>k</sub></em> steps to execute and executes <em>m</em> times contributes <em>c<sub>k</sub>m</em> to the total running time.<sup><a epub:type="footnote" href="#footnote_11" id="footnote_ref_11">11</a></sup> We usually denote the running time of an algorithm on an input of size <em>n</em> by <em>T</em> (<em>n</em>). To compute <em>T</em> (<em>n</em>), the running time of I<small>NSERTION</small>-S<small>ORT</small> on an input of <em>n</em> values, we sum the products of the <em>cost</em> and <em>times</em> columns, obtaining</p>
<a id="p30"/>
<div class="pull-quote1">
<table class="table1">
<tr>
<td class="td1" colspan="2"><p class="noindent">I<small>NSERTION</small>-S<small>ORT</small>(<em>A, n</em>)</p></td>
<td class="td1"><p class="noindent"><em>cost</em></p></td>
<td class="td1"><p class="noindent"><em>times</em></p></td>
</tr>
<tr>
<td class="td1w"><p class="ntop1"><span class="x-small">1</span></p></td>
<td class="td1"><p class="ntop1"><strong>for</strong> <em>i</em> = 2 <strong>to</strong> <em>n</em></p></td>
<td class="td1"><p class="ntop1"><em>c</em><sub>1</sub></p></td>
<td class="td1"><p class="ntop1"><em>n</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><em>key</em> = <em>A</em>[<em>i</em>]</p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>2</sub></p></td>
<td class="td1"><p class="noindent"><em>n</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><span class="red"><strong>//</strong> Insert <em>A</em>[<em>i</em>] into the sorted subarray <em>A</em>[1 : <em>i</em> – 1].</span></p></td>
<td class="td1"><p class="noindent">0</p></td>
<td class="td1"><p class="noindent"><em>n</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p2"><em>j</em> = <em>i</em> – 1</p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>4</sub></p></td>
<td class="td1"><p class="noindent"><em>n</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="p2"><strong>while</strong> <em>j</em> &gt; 0 and <em>A</em>[<em>j</em>] &gt; <em>key</em></p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>5</sub></p></td>
<td class="td1"><p class="noindent"><img alt="art" src="images/Art_P13.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1"><p class="p3"><em>A</em>[<em>j</em> + 1] = <em>A</em>[<em>j</em>]</p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>6</sub></p></td>
<td class="td1"><p class="noindent"><img alt="art" src="images/Art_P14.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="p3"><em>j</em> = <em>j</em> – 1</p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>7</sub></p></td>
<td class="td1"><p class="noindent"><img alt="art" src="images/Art_P15.jpg"/></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">8</span></p></td>
<td class="td1"><p class="p2"><em>A</em>[<em>j</em> + 1] = <em>key</em></p></td>
<td class="td1"><p class="noindent"><em>c</em><sub>8</sub></p></td>
<td class="td1"><p class="noindent"><em>n</em> – 1</p></td>
</tr>
</table>
</div>
<p class="eql"><img alt="art" src="images/Art_P16.jpg"/></p>
<p>Even for inputs of a given size, an algorithm’s running time may depend on <em>which</em> input of that size is given. For example, in I<small>NSERTION</small>-S<small>ORT</small>, the best case occurs when the array is already sorted. In this case, each time that line 5 executes, the value of <em>key</em>—the value originally in <em>A</em>[<em>i</em>]—is already greater than or equal to all values in <em>A</em>[1 : <em>i</em> – 1], so that the <strong>while</strong> loop of lines 5–7 always exits upon the first test in line 5. Therefore, we have that <em>t<sub>i</sub></em> = 1 for <em>i</em> = 2, 3, … , <em>n</em>, and the best-case running time is given by</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P17.jpg"/></p>
<p class="noindent">We can express this running time as <em>an</em> + <em>b</em> for <em>constants a</em> and <em>b</em> that depend on the statement costs <em>c<sub>k</sub></em> (where <em>a</em> = <em>c</em><sub>1</sub> + <em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub> + <em>c</em><sub>8</sub> and <em>b</em> = <em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub> + <em>c</em><sub>8</sub>). The running time is thus a <strong><em><span class="blue1">linear function</span></em></strong> of <em>n</em>.</p>
<p>The worst case arises when the array is in reverse sorted order—that is, it starts out in decreasing order. The procedure must compare each element <em>A</em>[<em>i</em>] with each element in the entire sorted subarray <em>A</em>[1 : <em>i</em> – 1], and so <em>t<sub>i</sub></em> = <em>i</em> for <em>i</em> = 2, 3, … , <em>n</em>. (The procedure finds that <em>A</em>[<em>j</em>] &gt; <em>key</em> every time in line 5, and the <strong>while</strong> loop exits only when <em>j</em> reaches 0.) Noting that</p>
<p class="eql"><img alt="art" src="images/Art_P18.jpg"/></p>
<a id="p31"/>
<p class="noindent">and</p>
<p class="eql"><img alt="art" src="images/Art_P19.jpg"/></p>
<p class="noindent">we find that in the worst case, the running time of I<small>NSERTION</small>-S<small>ORT</small> is</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P20.jpg"/></p>
<p class="noindent">We can express this worst-case running time as <em>an</em><sup>2</sup> + <em>bn</em> + <em>c</em> for constants <em>a</em>, <em>b</em>, and <em>c</em> that again depend on the statement costs <em>c<sub>k</sub></em> (now, <em>a</em> = <em>c</em><sub>5</sub>/2 + <em>c</em><sub>6</sub>/2 + <em>c</em><sub>7</sub>/2, <em>b</em> = <em>c</em><sub>1</sub> + <em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub>/2 – <em>c</em><sub>6</sub>/2 – <em>c</em><sub>7</sub>/2 + <em>c</em><sub>8</sub>, and <em>c</em> = –(<em>c</em><sub>2</sub> + <em>c</em><sub>4</sub> + <em>c</em><sub>5</sub> + <em>c</em><sub>8</sub>)). The running time is thus a <strong><em><span class="blue1">quadratic function</span></em></strong> of <em>n</em>.</p>
<p>Typically, as in insertion sort, the running time of an algorithm is fixed for a given input, although we’ll also see some interesting “randomized” algorithms whose behavior can vary even for a fixed input.</p>
<p class="level4"><strong>Worst-case and average-case analysis</strong></p>
<p class="noindent">Our analysis of insertion sort looked at both the best case, in which the input array was already sorted, and the worst case, in which the input array was reverse sorted. For the remainder of this book, though, we’ll usually (but not always) concentrate on finding only the <strong><em><span class="blue1">worst-case running time</span></em></strong>, that is, the longest running time for <em>any</em> input of size <em>n</em>. Why? Here are three reasons:</p>
<ul class="ulnoindent" epub:type="list">
<li>The worst-case running time of an algorithm gives an upper bound on the running time for <em>any</em> input. If you know it, then you have a guarantee that the algorithm never takes any longer. You need not make some educated guess about the running time and hope that it never gets much worse. This feature is especially important for real-time computing, in which operations must complete by a deadline.</li>
<li class="litop">For some algorithms, the worst case occurs fairly often. For example, in searching a database for a particular piece of information, the searching algorithm’s worst case often occurs when the information is not present in the database. In some applications, searches for absent information may be frequent.<a id="p32"/></li>
<li class="litop">The “average case” is often roughly as bad as the worst case. Suppose that you run insertion sort on an array of <em>n</em> randomly chosen numbers. How long does it take to determine where in subarray <em>A</em>[1 : <em>i</em> – 1] to insert element <em>A</em>[<em>i</em>]? On average, half the elements in <em>A</em>[1 : <em>i</em> – 1] are less than <em>A</em>[<em>i</em>], and half the elements are greater. On average, therefore, <em>A</em>[<em>i</em>] is compared with just half of the subarray <em>A</em>[1 : <em>i</em> – 1], and so <em>t<sub>i</sub></em> is about <em>i</em>/2. The resulting average-case running time turns out to be a quadratic function of the input size, just like the worst-case running time.</li></ul>
<p>In some particular cases, we’ll be interested in the <strong><em><span class="blue1">average-case</span></em></strong> running time of an algorithm. We’ll see the technique of <strong><em><span class="blue1">probabilistic analysis</span></em></strong> applied to various algorithms throughout this book. The scope of average-case analysis is limited, because it may not be apparent what constitutes an “average” input for a particular problem. Often, we’ll assume that all inputs of a given size are equally likely. In practice, this assumption may be violated, but we can sometimes use a <strong><em><span class="blue1">randomized algorithm</span></em></strong>, which makes random choices, to allow a probabilistic analysis and yield an <strong><em><span class="blue1">expected</span></em></strong> running time. We explore randomized algorithms more in <a href="chapter005.xhtml">Chapter 5</a> and in several other subsequent chapters.</p>
<p class="level4"><strong>Order of growth</strong></p>
<p class="noindent">In order to ease our analysis of the I<small>NSERTION</small>-S<small>ORT</small> procedure, we used some simplifying abstractions. First, we ignored the actual cost of each statement, using the constants <em>c<sub>k</sub></em> to represent these costs. Still, the best-case and worst-case running times in equations (2.1) and (2.2) are rather unwieldy. The constants in these expressions give us more detail than we really need. That’s why we also expressed the best-case running time as <em>an</em> + <em>b</em> for constants <em>a</em> and <em>b</em> that depend on the statement costs <em>c<sub>k</sub></em> and why we expressed the worst-case running time as <em>an</em><sup>2</sup> + <em>bn</em> + <em>c</em> for constants <em>a</em>, <em>b</em>, and <em>c</em> that depend on the statement costs. We thus ignored not only the actual statement costs, but also the abstract costs <em>c<sub>k</sub></em>.</p>
<p>Let’s now make one more simplifying abstraction: it is the <strong><em><span class="blue1">rate of growth</span></em></strong>, or <strong><em><span class="blue1">order of growth</span></em></strong>, of the running time that really interests us. We therefore consider only the leading term of a formula (e.g., <em>an</em><sup>2</sup>), since the lower-order terms are relatively insignificant for large values of <em>n</em>. We also ignore the leading term’s constant coefficient, since constant factors are less significant than the rate of growth in determining computational efficiency for large inputs. For insertion sort’s worst-case running time, when we ignore the lower-order terms and the leading term’s constant coefficient, only the factor of <em>n</em><sup>2</sup> from the leading term remains. That factor, <em>n</em><sup>2</sup>, is by far the most important part of the running time. For example, suppose that an algorithm implemented on a particular machine takes <em>n</em><sup>2</sup>/100 + 100<em>n</em> + 17 microseconds on an input of size <em>n</em>. Although the coefficients of 1/100 for the <em>n</em><sup>2</sup> term and 100 for the <em>n</em> term differ by four orders of magnitude, the <em>n</em><sup>2</sup>/100 term dominates <a id="p33"/>the 100<em>n</em> term once <em>n</em> exceeds 10,000. Although 10,000 might seem large, it is smaller than the population of an average town. Many real-world problems have much larger input sizes.</p>
<p>To highlight the order of growth of the running time, we have a special notation that uses the Greek letter Θ (theta). We write that insertion sort has a worst-case running time of Θ(<em>n</em><sup>2</sup>) (pronounced “theta of <em>n</em>-squared” or just “theta <em>n</em>-squared”). We also write that insertion sort has a best-case running time of Θ(<em>n</em>) (“theta of <em>n</em>” or “theta <em>n</em>”). For now, think of Θ-notation as saying “roughly proportional when <em>n</em> is large,” so that Θ(<em>n</em><sup>2</sup>) means “roughly proportional to <em>n</em><sup>2</sup> when <em>n</em> is large” and Θ(<em>n</em>) means “roughly proportional to <em>n</em> when <em>n</em> is large” We’ll use Θ-notation informally in this chapter and define it precisely in <a href="chapter003.xhtml">Chapter 3</a>.</p>
<p>We usually consider one algorithm to be more efficient than another if its worst-case running time has a lower order of growth. Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth might take less time for small inputs than an algorithm whose running time has a lower order of growth. But on large enough inputs, an algorithm whose worst-case running time is Θ(<em>n</em><sup>2</sup>), for example, takes less time in the worst case than an algorithm whose worst-case running time is Θ(<em>n</em><sup>3</sup>). Regardless of the constants hidden by the Θ-notation, there is always some number, say <em>n</em><sub>0</sub>, such that for all input sizes <em>n</em> ≥ <em>n</em><sub>0</sub>, the Θ(<em>n</em><sup>2</sup>) algorithm beats the Θ(<em>n</em><sup>3</sup>) algorithm in the worst case.</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>2.2-1</em></strong></p>
<p class="noindent">Express the function <em>n</em><sup>3</sup>/1000 + 100<em>n</em><sup>2</sup> – 100<em>n</em> + 3 in terms of Θ-notation.</p>
<p class="level3"><strong><em>2.2-2</em></strong></p>
<p class="noindent">Consider sorting <em>n</em> numbers stored in array <em>A</em>[1 : <em>n</em>] by first finding the smallest element of <em>A</em>[1 : <em>n</em>] and exchanging it with the element in <em>A</em>[1]. Then find the smallest element of <em>A</em>[2 : <em>n</em>], and exchange it with <em>A</em>[2]. Then find the smallest element of <em>A</em>[3 : <em>n</em>], and exchange it with <em>A</em>[3]. Continue in this manner for the first <em>n</em> – 1 elements of <em>A</em>. Write pseudocode for this algorithm, which is known as <strong><em><span class="blue1">selection sort</span></em></strong>. What loop invariant does this algorithm maintain? Why does it need to run for only the first <em>n</em> – 1 elements, rather than for all <em>n</em> elements? Give the worst-case running time of selection sort in Θ-notation. Is the best-case running time any better?</p>
<p class="level3"><strong><em>2.2-3</em></strong></p>
<p class="noindent">Consider linear search again (see Exercise 2.1-4). How many elements of the input array need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? <a id="p34"/>Using Θ-notation, give the average-case and worst-case running times of linear search. Justify your answers.</p>
<p class="level3"><strong><em>2.2-4</em></strong></p>
<p class="noindent">How can you modify any sorting algorithm to have a good best-case running time?</p>
</section>
<p class="line1"/>
<section title="2.3 Designing algorithms">
<a id="Sec_2.3"/>
<p class="level1" id="h1-8"><a href="toc.xhtml#Rh1-8"><strong>2.3      Designing algorithms</strong></a></p>
<p class="noindent">You can choose from a wide range of algorithm design techniques. Insertion sort uses the <strong><em><span class="blue1">incremental</span></em></strong> method: for each element <em>A</em>[<em>i</em>], insert it into its proper place in the subarray <em>A</em>[1 : <em>i</em>], having already sorted the subarray <em>A</em>[1 : <em>i</em> – 1].</p>
<p>This section examines another design method, known as “divide-and-conquer,” which we explore in more detail in <a href="chapter004.xhtml">Chapter 4</a>. We’ll use divide-and-conquer to design a sorting algorithm whose worst-case running time is much less than that of insertion sort. One advantage of using an algorithm that follows the divide-and-conquer method is that analyzing its running time is often straightforward, using techniques that we’ll explore in <a href="chapter004.xhtml">Chapter 4</a>.</p>
<section title="2.3.1 The divide-and-conquer method">
<p class="level2a" id="Sec_2.3.1"><strong>2.3.1    The divide-and-conquer method</strong></p>
<p class="noindent">Many useful algorithms are <strong><em><span class="blue1">recursive</span></em></strong> in structure: to solve a given problem, they <strong><em><span class="blue1">recurse</span></em></strong> (call themselves) one or more times to handle closely related subproblems. These algorithms typically follow the <strong><em><span class="blue1">divide-and-conquer</span></em></strong> method: they break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.</p>
<p>In the divide-and-conquer method, if the problem is small enough—the <strong><em><span class="blue1">base case</span></em></strong>—you just solve it directly without recursing. Otherwise—the <strong><em><span class="blue1">recursive case</span></em></strong>—you perform three characteristic steps:</p>
<p class="para-hang1"><strong>Divide</strong> the problem into one or more subproblems that are smaller instances of the same problem.</p>
<p class="para-hang1"><strong>Conquer</strong> the subproblems by solving them recursively.</p>
<p class="para-hang1"><strong>Combine</strong> the subproblem solutions to form a solution to the original problem.</p>
<p class="space-break">The <strong><em><span class="blue1">merge sort</span></em></strong> algorithm closely follows the divide-and-conquer method. In each step, it sorts a subarray <em>A</em>[<em>p</em> : <em>r</em>], starting with the entire array <em>A</em>[1 : <em>n</em>] and recursing down to smaller and smaller subarrays. Here is how merge sort operates:</p>
<a id="p35"/>
<p class="para-hang1"><strong>Divide</strong> the subarray <em>A</em>[<em>p</em> : <em>r</em>] to be sorted into two adjacent subarrays, each of half the size. To do so, compute the midpoint <em>q</em> of <em>A</em>[<em>p</em> : <em>r</em>] (taking the average of <em>p</em> and <em>r</em>), and divide <em>A</em>[<em>p</em> : <em>r</em>] into subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>].</p>
<p class="para-hang1"><strong>Conquer</strong> by sorting each of the two subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] recursively using merge sort.</p>
<p class="para-hang1"><strong>Combine</strong> by merging the two sorted subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] back into <em>A</em>[<em>p</em> : <em>r</em>], producing the sorted answer.</p>
<p class="noindent1-top">The recursion “bottoms out”—it reaches the base case—when the subarray <em>A</em>[<em>p</em> : <em>r</em>] to be sorted has just 1 element, that is, when <em>p</em> equals <em>r</em>. As we noted in the initialization argument for I<small>NSERTION</small>-S<small>ORT</small>’s loop invariant, a subarray comprising just a single element is always sorted.</p>
<p>The key operation of the merge sort algorithm occurs in the “combine” step, which merges two adjacent, sorted subarrays. The merge operation is performed by the auxiliary procedure M<small>ERGE</small>(<em>A</em>, <em>p</em>, <em>q</em>, <em>r</em>) on the following page, where <em>A</em> is an array and <em>p</em>, <em>q</em>, and <em>r</em> are indices into the array such that <em>p</em> ≤ <em>q</em> &lt; <em>r</em>. The procedure assumes that the adjacent subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] were already recursively sorted. It <strong><em><span class="blue1">merges</span></em></strong> the two sorted subarrays to form a single sorted subarray that replaces the current subarray <em>A</em>[<em>p</em> : <em>r</em>].</p>
<p>To understand how the M<small>ERGE</small> procedure works, let’s return to our card-playing motif. Suppose that you have two piles of cards face up on a table. Each pile is sorted, with the smallest-value cards on top. You wish to merge the two piles into a single sorted output pile, which is to be face down on the table. The basic step consists of choosing the smaller of the two cards on top of the face-up piles, removing it from its pile—which exposes a new top card—and placing this card face down onto the output pile. Repeat this step until one input pile is empty, at which time you can just take the remaining input pile and flip over the entire pile, placing it face down onto the output pile.</p>
<p>Let’s think about how long it takes to merge two sorted piles of cards. Each basic step takes constant time, since you are comparing just the two top cards. If the two sorted piles that you start with each have <em>n</em>/2 cards, then the number of basic steps is at least <em>n</em>/2 (since in whichever pile was emptied, every card was found to be smaller than some card from the other pile) and at most <em>n</em> (actually, at most <em>n</em> – 1, since after <em>n</em> – 1 basic steps, one of the piles must be empty). With each basic step taking constant time and the total number of basic steps being between <em>n</em>/2 and <em>n</em>, we can say that merging takes time roughly proportional to <em>n</em>. That is, merging takes Θ(<em>n</em>) time.</p>
<p>In detail, the M<small>ERGE</small> procedure works as follows. It copies the two subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] into temporary arrays <em>L</em> and <em>R</em> (“left” and “right”), and then it merges the values in <em>L</em> and <em>R</em> back into <em>A</em>[<em>p</em> : <em>r</em>]. Lines 1 and 2 compute the lengths <em>n<sub>L</sub></em> and <em>n<sub>R</sub></em> of the subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>], respectively. Then <a id="p36"/>line 3 creates arrays <em>L</em>[0 : <em>n<sub>L</sub></em> – 1] and <em>R</em>[0 : <em>n<sub>R</sub></em> – 1] with respective lengths <em>n<sub>L</sub></em> and <em>n<sub>R</sub></em>.<sup><a epub:type="footnote" href="#footnote_12" id="footnote_ref_12">12</a></sup> The <strong>for</strong> loop of lines 4–5 copies the subarray <em>A</em>[<em>p</em> : <em>q</em>] into <em>L</em>, and the <strong>for</strong> loop of lines 6–7 copies the subarray <em>A</em>[<em>q</em> + 1 : <em>r</em>] into <em>R</em>.</p>
<div class="pull-quote1">
<p class="box-heading">M<small>ERGE</small>(<em>A</em>, <em>p</em>, <em>q</em>, <em>r</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><span class="x-small">  1</span></td>
<td class="td1"><p class="noindent"><em>n<sub>L</sub></em> = <em>q</em> – <em>p</em> + 1</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> length of <em>A</em>[<em>p</em> : <em>q</em>]</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  2</span></td>
<td class="td1"><p class="noindent"><em>n<sub>R</sub></em> = <em>r</em> – <em>q</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> length of <em>A</em>[<em>q</em> + 1 : <em>r</em>]</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  3</span></td>
<td class="td1" colspan="2">
<p class="noindent">let <em>L</em>[0 : <em>n<sub>L</sub></em> – 1] and <em>R</em>[0 : <em>n<sub>R</sub></em> – 1] be new arrays</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  4</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 0 <strong>to</strong> <em>n<sub>L</sub></em> – 1</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> copy <em>A</em>[<em>p</em> : <em>q</em>] into <em>L</em>[0 : <em>n<sub>L</sub></em> – 1]</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  5</span></td>
<td class="td1" colspan="2"><p class="p2"><em>L</em>[<em>i</em>] = <em>A</em>[<em>p</em> + <em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  6</span></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>j</em> = 0 <strong>to</strong> <em>n<sub>R</sub></em> – 1</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> copy <em>A</em>[<em>q</em> + 1 : <em>r</em>] into <em>R</em>[0 : <em>n<sub>R</sub></em> – 1]</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  7</span></td>
<td class="td1" colspan="2"><p class="p2"><em>R</em>[<em>j</em>] = <em>A</em>[<em>q</em> + <em>j</em> + 1]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  8</span></td>
<td class="td1"><p class="noindent"><em>i</em> = 0</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> <em>i</em> indexes the smallest remaining element in <em>L</em></span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">  9</span></td>
<td class="td1"><p class="noindent"><em>j</em> = 0</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> <em>j</em> indexes the smallest remaining element in <em>R</em></span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">10</span></td>
<td class="td1"><p class="noindent"><em>k</em> = <em>p</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> <em>k</em> indexes the location in <em>A</em> to fill</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">11</span></td>
<td class="td1" colspan="2">
<p class="noindent"><span class="red"><strong>//</strong> As long as each of the arrays <em>L</em> and <em>R</em> contains an unmerged element,<br/><strong>//</strong>          copy the smallest unmerged element back into <em>A</em>[<em>p</em> : <em>r</em>].</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">12</span></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>while</strong> <em>i</em> &lt; <em>n<sub>L</sub></em> and <em>j</em> &lt; <em>n<sub>R</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">13</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>if</strong> <em>L</em>[<em>i</em>] ≤ <em>R</em>[<em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">14</span></td>
<td class="td1" colspan="2">
<p class="p3"><em>A</em>[<em>k</em>] = <em>L</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">15</span></td>
<td class="td1" colspan="2">
<p class="p3"><em>i</em> = <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">16</span></td>
<td class="td1" colspan="2"><p class="p2"><strong>else</strong> <em>A</em>[<em>k</em>] = <em>R</em>[<em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">17</span></td>
<td class="td1" colspan="2">
<p class="p3"><em>j</em> = <em>j</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">18</span></td>
<td class="td1" colspan="2"><p class="p2"><em>k</em> = <em>k</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">19</span></td>
<td class="td1" colspan="2">
<p class="noindent"><span class="red"><strong>//</strong> Having gone through one of <em>L</em> and <em>R</em> entirely, copy the<br/><strong>//</strong>          remainder of the other to the end of <em>A</em>[<em>p</em> : <em>r</em>].</span></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">20</span></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>while</strong> <em>i</em> &lt; <em>n<sub>L</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">21</span></td>
<td class="td1" colspan="2"><p class="p2"><em>A</em>[<em>k</em>] = <em>L</em>[<em>i</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">22</span></td>
<td class="td1" colspan="2"><p class="p2"><em>i</em> = <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">23</span></td>
<td class="td1" colspan="2"><p class="p2"><em>k</em> = <em>k</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">24</span></td>
<td class="td1" colspan="2">
<p class="noindent"><strong>while</strong> <em>j</em> &lt; <em>n<sub>R</sub></em></p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">25</span></td>
<td class="td1" colspan="2"><p class="p2"><em>A</em>[<em>k</em>] = <em>R</em>[<em>j</em>]</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">26</span></td>
<td class="td1" colspan="2"><p class="p2"><em>j</em> = <em>j</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><span class="x-small">27</span></td>
<td class="td1" colspan="2"><p class="p2"><em>k</em> = <em>k</em> + 1</p></td>
</tr>
</table>
</div>
<p>Lines 8–18, illustrated in <a href="chapter002.xhtml#Fig_2-3">Figure 2.3</a>, perform the basic steps. The <strong>while</strong> loop of lines 12–18 repeatedly identifies the smallest value in <em>L</em> and <em>R</em> that has yet to be copied back into <em>A</em>[<em>p</em> : <em>r</em>] and copies it back in. As the comments indicate, the index <em>k</em> gives the position of <em>A</em> that is being filled in, and the indices <em>i</em> and <em>j</em> give the positions in <em>L</em> and <em>R</em>, respectively, of the smallest remaining values. Eventually, either all of <em>L</em> or all of <em>R</em> is copied back into <em>A</em>[<em>p</em> : <em>r</em>], and this loop terminates. If the loop terminates because all of <em>R</em> has been copied back, that is, because <em>j</em> equals <em>n<sub>R</sub></em>, then <em>i</em> is still less than <em>n<sub>L</sub></em>, so that some of <em>L</em> has yet to be copied back, and these values are the greatest in both <em>L</em> and <em>R</em>. In this case, the <strong>while</strong> loop of lines 20–23 copies these remaining values of <em>L</em> into the last few positions of <em>A</em>[<em>p</em> : <em>r</em>]. Because <em>j</em> equals <em>n<sub>R</sub></em>, the <strong>while</strong> loop of lines 24–27 iterates 0 times. If instead the <strong>while</strong> loop of lines 12–18 terminates because <em>i</em> equals <em>n<sub>L</sub></em>, then all of <em>L</em> has already been copied back into <em>A</em>[<em>p</em> : <em>r</em>], and the <strong>while</strong> loop of lines 24–27 copies the remaining values of <em>R</em> back into the end of <em>A</em>[<em>p</em> : <em>r</em>].</p>
<a id="p37"/>
<div class="divimage">
<p class="fig-imga" id="Fig_2-3"><img alt="art" class="width100" src="images/Art_P21.jpg"/></p>
<p class="caption"><strong>Figure 2.3</strong> The operation of the <strong>while</strong> loop in lines 8–18 in the call M<small>ERGE</small>(<em>A</em>, 9, 12, 16), when the subarray <em>A</em>[9 : 16] contains the values <span class="font1">〈</span>2, 4, 6, 7, 1, 2, 3, 5<span class="font1">〉</span>. After allocating and copying into the arrays <em>L</em> and <em>R</em>, the array <em>L</em> contains <span class="font1">〈</span>2, 4, 6, 7<span class="font1">〉</span>, and the array <em>R</em> contains <span class="font1">〈</span>1, 2, 3, 5<span class="font1">〉</span>. Tan positions in <em>A</em> contain their final values, and tan positions in <em>L</em> and <em>R</em> contain values that have yet to be copied back into <em>A</em>. Taken together, the tan positions always comprise the values originally in <em>A</em>[9 : 16]. Blue positions in <em>A</em> contain values that will be copied over, and dark positions in <em>L</em> and <em>R</em> contain values that have already been copied back into <em>A</em>. <strong>(a)–(g)</strong> The arrays <em>A</em>, <em>L</em>, and <em>R</em>, and their respective indices <em>k</em>, <em>i</em>, and <em>j</em> prior to each iteration of the loop of lines 12–18. At the point in part (g), all values in <em>R</em> have been copied back into <em>A</em> (indicated by <em>j</em> equaling the length of <em>R</em>), and so the <strong>while</strong> loop in lines 12–18 terminates. <strong>(h)</strong> The arrays and indices at termination. The <strong>while</strong> loops of lines 20–23 and 24–27 copied back into <em>A</em> the remaining values in <em>L</em> and <em>R</em>, which are the largest values originally in <em>A</em>[9 : 16]. Here, lines 20–23 copied <em>L</em>[2 : 3] into <em>A</em>[15 : 16], and because all values in <em>R</em> had already been copied back into <em>A</em>, the <strong>while</strong> loop of lines 24–27 iterated 0 times. At this point, the subarray in <em>A</em>[9 : 16] is sorted.</p>
</div>
<a id="p38"/>
<p>To see that the M<small>ERGE</small> procedure runs in Θ(<em>n</em>) time, where <em>n</em> = <em>r</em> – <em>p</em> + 1,<sup><a epub:type="footnote" href="#footnote_13" id="footnote_ref_13">13</a></sup> observe that each of lines 1–3 and 8–10 takes constant time, and the <strong>for</strong> loops of lines 4–7 take Θ(<em>n<sub>L</sub></em> + <em>n<sub>R</sub></em>) = Θ(<em>n</em>) time.<sup><a epub:type="footnote" href="#footnote_14" id="footnote_ref_14">14</a></sup> To account for the three <strong>while</strong> loops of lines 12–18, 20–23, and 24–27, observe that each iteration of these loops copies exactly one value from <em>L</em> or <em>R</em> back into <em>A</em> and that every value is copied back into <em>A</em> exactly once. Therefore, these three loops together make a total of <em>n</em> iterations. Since each iteration of each of the three loops takes constant time, the total time spent in these three loops is Θ(<em>n</em>).</p>
<p>We can now use the M<small>ERGE</small> procedure as a subroutine in the merge sort algorithm. The procedure M<small>ERGE</small>-S<small>ORT</small>(<em>A</em>, <em>p</em>, <em>r</em>) on the facing page sorts the elements in the subarray <em>A</em>[<em>p</em> : <em>r</em>]. If <em>p</em> equals <em>r</em>, the subarray has just 1 element and is therefore already sorted. Otherwise, we must have <em>p</em> &lt; <em>r</em>, and M<small>ERGE</small>-S<small>ORT</small> runs the divide, conquer, and combine steps. The divide step simply computes an index <em>q</em> that partitions <em>A</em>[<em>p</em> : <em>r</em>] into two adjacent subarrays: <em>A</em>[<em>p</em> : <em>q</em>], containing <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> elements, and <em>A</em>[<em>q</em> + 1 : <em>r</em>], containing <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> elements.<sup><a epub:type="footnote" href="#footnote_15" id="footnote_ref_15">15</a></sup> The initial call M<small>ERGE</small>-S<small>ORT</small>(<em>A,</em> 1, <em>n</em>) sorts the entire array <em>A</em>[1 : <em>n</em>].</p>
<p><a href="chapter002.xhtml#Fig_2-4">Figure 2.4</a> illustrates the operation of the procedure for <em>n</em> = 8, showing also the sequence of divide and merge steps. The algorithm recursively divides the array down to 1-element subarrays. The combine steps merge pairs of 1-element subarrays <a id="p39"/>to form sorted subarrays of length 2, merges those to form sorted subarrays of length 4, and merges those to form the final sorted subarray of length 8. If <em>n</em> is not an exact power of 2, then some divide steps create subarrays whose lengths differ by 1. (For example, when dividing a subarray of length 7, one subarray has length 4 and the other has length 3.) Regardless of the lengths of the two subarrays being merged, the time to merge a total of <em>n</em> items is Θ(<em>n</em>).</p>
<div class="pull-quote1">
<p class="box-heading">M<small>ERGE</small>-S<small>ORT</small>(<em>A</em>, <em>p</em>, <em>r</em>)</p>
<table class="table1">
<tr>
<td class="td1w"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>if</strong> <em>p</em> ≥ <em>r</em></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> zero or one element?</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><strong>return</strong></p></td>
<td class="td1"/>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="noindent"><em>q</em> = <span class="font1">⌊</span>(<em>p</em> + <em>r</em>)/2<span class="font1">⌋</span></p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> midpoint of <em>A</em>[<em>p</em> : <em>r</em>]</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="noindent">M<small>ERGE</small>-S<small>ORT</small>(<em>A</em>, <em>p</em>, <em>q</em>)</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> recursively sort <em>A</em>[<em>p</em> : <em>q</em>]</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">5</span></p></td>
<td class="td1"><p class="noindent">M<small>ERGE</small>-S<small>ORT</small>(<em>A</em>, <em>q</em> + 1, <em>r</em>)</p></td>
<td class="td1"><p class="noindent"><span class="red"><strong>//</strong> recursively sort <em>A</em>[<em>q</em> + 1 : <em>r</em>]</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">6</span></p></td>
<td class="td1" colspan="2">
<p class="noindent"><span class="red"><strong>//</strong> Merge <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] into <em>A</em>[<em>p</em> : <em>r</em>].</span></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">7</span></p></td>
<td class="td1"><p class="noindent">M<small>ERGE</small>(<em>A</em>, <em>p</em>, <em>q</em>, <em>r</em>)</p></td>
</tr>
</table>
</div>
</section>
<section title="2.3.2 Analyzing divide-and-conquer algorithms">
<p class="level2a" id="Sec_2.3.2"><strong>2.3.2    Analyzing divide-and-conquer algorithms</strong></p>
<p class="noindent">When an algorithm contains a recursive call, you can often describe its running time by a <strong><em><span class="blue1">recurrence equation</span></em></strong> or <strong><em><span class="blue1">recurrence</span></em></strong>, which describes the overall running time on a problem of size <em>n</em> in terms of the running time of the same algorithm on smaller inputs. You can then use mathematical tools to solve the recurrence and provide bounds on the performance of the algorithm.</p>
<p>A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic method. As we did for insertion sort, let <em>T</em> (<em>n</em>) be the worst-case running time on a problem of size <em>n</em>. If the problem size is small enough, say <em>n</em> &lt; <em>n</em><sub>0</sub> for some constant <em>n</em><sub>0</sub> &gt; 0, the straightforward solution takes constant time, which we write as Θ(1).<sup><a epub:type="footnote" href="#footnote_16" id="footnote_ref_16">16</a></sup> Suppose that the division of the problem yields <em>a</em> subproblems, each with size <em>n</em>/<em>b</em>, that is, 1/<em>b</em> the size of the original. For merge sort, both <em>a</em> and <em>b</em> are 2, but we’ll see other divide-and-conquer algorithms in which <em>a</em> ≠ <em>b</em>. It takes <em>T</em> (<em>n</em>/<em>b</em>) time to solve one subproblem of size <em>n</em>/<em>b</em>, and so it takes <em>aT</em> (<em>n</em>/<em>b</em>) time to solve all <em>a</em> of them. If it takes <em>D</em>(<em>n</em>) time to divide the problem into subproblems and <em>C</em>(<em>n</em>) time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence</p>
<a id="p40"/>
<p class="fig-img"><img alt="art" src="images/Art_P22.jpg"/></p>
<p class="noindent"><a href="chapter004.xhtml">Chapter 4</a> shows how to solve common recurrences of this form.</p>
<div class="divimage">
<p class="fig-imga" id="Fig_2-4"><img alt="art" src="images/Art_P23.jpg"/></p>
<p class="caption"><strong>Figure 2.4</strong> The operation of merge sort on the array <em>A</em> with length 8 that initially contains the sequence <span class="font1">〈</span>12, 3, 7, 9, 14, 6, 11, 2<span class="font1">〉</span>. The indices <em>p</em>, <em>q</em>, and <em>r</em> into each subarray appear above their values. Numbers in italics indicate the order in which the M<small>ERGE</small>-S<small>ORT</small> and M<small>ERGE</small> procedures are called following the initial call of M<small>ERGE</small>-S<small>ORT</small>(<em>A,</em> 1, 8).</p>
</div>
<p>Sometimes, the <em>n</em>/<em>b</em> size of the divide step isn’t an integer. For example, the M<small>ERGE</small>-S<small>ORT</small> procedure divides a problem of size <em>n</em> into subproblems of sizes <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> and <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>. Since the difference between <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> and <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span> is at most 1, <a id="p41"/>which for large <em>n</em> is much smaller than the effect of dividing <em>n</em> by 2, we’ll squint a little and just call them both size <em>n</em>/2. As <a href="chapter004.xhtml">Chapter 4</a> will discuss, this simplification of ignoring floors and ceilings does not generally affect the order of growth of a solution to a divide-and-conquer recurrence.</p>
<p>Another convention we’ll adopt is to omit a statement of the base cases of the recurrence, which we’ll also discuss in more detail in <a href="chapter004.xhtml">Chapter 4</a>. The reason is that the base cases are pretty much always <em>T</em> (<em>n</em>) = Θ(1) if <em>n</em> &lt; <em>n</em><sub>0</sub> for some constant <em>n</em><sub>0</sub> &gt; 0. That’s because the running time of an algorithm on an input of constant size is constant. We save ourselves a lot of extra writing by adopting this convention.</p>
<p class="level4"><strong>Analysis of merge sort</strong></p>
<p class="noindent">Here’s how to set up the recurrence for <em>T</em> (<em>n</em>), the worst-case running time of merge sort on <em>n</em> numbers.</p>
<p class="para-hang1"><strong>Divide:</strong> The divide step just computes the middle of the subarray, which takes constant time. Thus, <em>D</em>(<em>n</em>) = Θ(1).</p>
<p class="para-hang1"><strong>Conquer:</strong> Recursively solving two subproblems, each of size <em>n</em>/2, contributes 2<em>T</em> (<em>n</em>/2) to the running time (ignoring the floors and ceilings, as we discussed).</p>
<p class="para-hang1"><strong>Combine:</strong> Since the M<small>ERGE</small> procedure on an <em>n</em>-element subarray takes Θ(<em>n</em>) time, we have <em>C</em>(<em>n</em>) = Θ(<em>n</em>).</p>
<p class="space-break">When we add the functions <em>D</em>(<em>n</em>) and <em>C</em>(<em>n</em>) for the merge sort analysis, we are adding a function that is Θ(<em>n</em>) and a function that is Θ(1). This sum is a linear function of <em>n</em>. That is, it is roughly proportional to <em>n</em> when <em>n</em> is large, and so merge sort’s dividing and combining times together are Θ(<em>n</em>). Adding Θ(<em>n</em>) to the 2<em>T</em> (<em>n</em>/2) term from the conquer step gives the recurrence for the worst-case running time <em>T</em> (<em>n</em>) of merge sort:</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P24.jpg"/></p>
<p class="noindent"><a href="chapter004.xhtml">Chapter 4</a> presents the “master theorem,” which shows that <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>).<sup><a epub:type="footnote" href="#footnote_17" id="footnote_ref_17">17</a></sup> Compared with insertion sort, whose worst-case running time is Θ(<em>n</em><sup>2</sup>), merge sort trades away a factor of <em>n</em> for a factor of lg <em>n</em>. Because the logarithm function grows more slowly than any linear function, that’s a good trade. For large enough inputs, merge sort, with its Θ(<em>n</em> lg <em>n</em>) worst-case running time, outperforms insertion sort, whose worst-case running time is Θ(<em>n</em><sup>2</sup>).</p>
<a id="p42"/>
<p>We do not need the master theorem, however, to understand intuitively why the solution to recurrence (2.3) is <em>T</em> (<em>n</em>) = Θ(<em>n</em> lg <em>n</em>). For simplicity, assume that <em>n</em> is an exact power of 2 and that the implicit base case is <em>n</em> = 1. Then recurrence (2.3) is essentially</p>
<p class="eqr"><img alt="art" class="width100" src="images/Art_P25.jpg"/></p>
<p class="noindent">where the constant <em>c</em><sub>1</sub> &gt; 0 represents the time required to solve a problem of size 1, and <em>c</em><sub>2</sub> &gt; 0 is the time per array element of the divide and combine steps.<sup><a epub:type="footnote" href="#footnote_18" id="footnote_ref_18">18</a></sup></p>
<p><a href="chapter002.xhtml#Fig_2-5">Figure 2.5</a> illustrates one way of figuring out the solution to recurrence (2.4). Part (a) of the figure shows <em>T</em> (<em>n</em>), which part (b) expands into an equivalent tree representing the recurrence. The <em>c</em><sub>2</sub><em>n</em> term denotes the cost of dividing and combining at the top level of recursion, and the two subtrees of the root are the two smaller recurrences <em>T</em> (<em>n</em>/2). Part (c) shows this process carried one step further by expanding <em>T</em> (<em>n</em>/2). The cost for dividing and combining at each of the two nodes at the second level of recursion is <em>c</em><sub>2</sub><em>n</em>/2. Continue to expand each node in the tree by breaking it into its constituent parts as determined by the recurrence, until the problem sizes get down to 1, each with a cost of <em>c</em><sub>1</sub>. Part (d) shows the resulting <strong><em><span class="blue1">recursion tree</span></em></strong>.</p>
<p>Next, add the costs across each level of the tree. The top level has total cost <em>c</em><sub>2</sub><em>n</em>, the next level down has total cost <em>c</em><sub>2</sub>(<em>n</em>/2) + <em>c</em><sub>2</sub>(<em>n</em>/2) = <em>c</em><sub>2</sub><em>n</em>, the level after that has total cost <em>c</em><sub>2</sub>(<em>n</em>/4) + <em>c</em><sub>2</sub>(<em>n</em>/4) + <em>c</em><sub>2</sub>(<em>n</em>/4) + <em>c</em><sub>2</sub>(<em>n</em>/4) = <em>c</em><sub>2</sub><em>n</em>, and so on. Each level has twice as many nodes as the level above, but each node contributes only half the cost of a node from the level above. From one level to the next, doubling and halving cancel each other out, so that the cost across each level is the same: <em>c</em><sub>2</sub><em>n</em>. In general, the level that is <em>i</em> levels below the top has 2<em><sup>i</sup></em> nodes, each contributing a cost of <em>c</em><sub>2</sub>(<em>n</em>/2<em><sup>i</sup></em>), so that the <em>i</em>th level below the top has total cost 2<em><sup>i</sup></em> · <em>c</em><sub>2</sub>(<em>n</em>/2<em><sup>i</sup></em>) = <em>c</em><sub>2</sub><em>n</em>. The bottom level has <em>n</em> nodes, each contributing a cost of <em>c</em><sub>1</sub>, for a total cost of <em>c</em><sub>1</sub><em>n</em>.</p>
<p>The total number of levels of the recursion tree in <a href="chapter002.xhtml#Fig_2-5">Figure 2.5</a> is lg <em>n</em> + 1, where <em>n</em> is the number of leaves, corresponding to the input size. An informal inductive argument justifies this claim. The base case occurs when <em>n</em> = 1, in which case the tree has only 1 level. Since lg 1 = 0, we have that lg <em>n</em> + 1 gives the correct number of levels. Now assume as an inductive hypothesis that the number of levels of a recursion tree with 2<em><sup>i</sup></em> leaves is lg 2<em><sup>i</sup></em> + 1 = <em>i</em> + 1 (since for any value of <em>i</em>, we have that lg 2<em><sup>i</sup></em> = <em>i</em>). Because we assume that the input size is an exact power of 2, the next input size to consider is 2<sup><em>i</em> + 1</sup>. A tree with <em>n</em> = 2<sup><em>i</em> + 1</sup> leaves has 1 more level than a tree with 2<em><sup>i</sup></em> leaves, and so the total number of levels is (<em>i</em> + 1) + 1 = lg 2<sup><em>i</em> + 1</sup> + 1.</p>
<a id="p43"/>
<div class="divimage">
<p class="fig-imga" id="Fig_2-5"><img alt="art" src="images/Art_P26.jpg"/></p>
<p class="caption"><strong>Figure 2.5</strong> How to construct a recursion tree for the recurrence (2.4). Part <strong>(a)</strong> shows <em>T</em> (<em>n</em>), which progressively expands in <strong>(b)–(d)</strong> to form the recursion tree. The fully expanded tree in part (d) has lg <em>n</em> + 1 levels. Each level above the leaves contributes a total cost of <em>c</em><sub>2</sub><em>n</em>, and the leaf level contributes <em>c</em><sub>1</sub><em>n</em>. The total cost, therefore, is <em>c</em><sub>2</sub><em>n</em> lg <em>n</em> + <em>c</em><sub>1</sub><em>n</em> = Θ(<em>n</em> lg <em>n</em>).</p>
</div>
<a id="p44"/>
<p>To compute the total cost represented by the recurrence (2.4), simply add up the costs of all the levels. The recursion tree has lg <em>n</em> + 1 levels. The levels above the leaves each cost <em>c</em><sub>2</sub><em>n</em>, and the leaf level costs <em>c</em><sub>1</sub><em>n</em>, for a total cost of <em>c</em><sub>2</sub><em>n</em> lg <em>n</em> + <em>c</em><sub>1</sub><em>n</em> = Θ(<em>n</em> lg <em>n</em>).</p>
<p class="exe"><strong>Exercises</strong></p>
<p class="level3"><strong><em>2.3-1</em></strong></p>
<p class="noindent">Using <a href="chapter002.xhtml#Fig_2-4">Figure 2.4</a> as a model, illustrate the operation of merge sort on an array initially containing the sequence <span class="font1">〈</span>3, 41, 52, 26, 38, 57, 9, 49<span class="font1">〉</span>.</p>
<p class="level3"><strong><em>2.3-2</em></strong></p>
<p class="noindent">The test in line 1 of the M<small>ERGE</small>-S<small>ORT</small> procedure reads “<strong>if</strong> <em>p</em> ≥ <em>r</em>” rather than “<strong>if</strong> <em>p</em> ≠ <em>r</em>.” If M<small>ERGE</small>-S<small>ORT</small> is called with <em>p</em> &gt; <em>r</em>, then the subarray <em>A</em>[<em>p</em> : <em>r</em>] is empty. Argue that as long as the initial call of M<small>ERGE</small>-S<small>ORT</small>(<em>A,</em> 1, <em>n</em>) has <em>n</em> ≥ 1, the test “<strong>if</strong> <em>p</em> ≠ <em>r</em>” suffices to ensure that no recursive call has <em>p</em> &gt; <em>r</em>.</p>
<p class="level3"><strong><em>2.3-3</em></strong></p>
<p class="noindent">State a loop invariant for the <strong>while</strong> loop of lines 12–18 of the M<small>ERGE</small> procedure. Show how to use it, along with the <strong>while</strong> loops of lines 20–23 and 24–27, to prove that the M<small>ERGE</small> procedure is correct.</p>
<p class="level3"><strong><em>2.3-4</em></strong></p>
<p class="noindent">Use mathematical induction to show that when <em>n</em> ≥ 2 is an exact power of 2, the solution of the recurrence</p>
<p class="eql"><img alt="art" src="images/Art_P27.jpg"/></p>
<p class="noindent">is <i>T</i>(<i>n</i>) = <i>n</i> lg <i>n</i>.</p>
<p class="level3"><strong><em>2.3-5</em></strong></p>
<p class="noindent">You can also think of insertion sort as a recursive algorithm. In order to sort <em>A</em>[1 : <em>n</em>], recursively sort the subarray <em>A</em>[1 : <em>n</em> – 1] and then insert <em>A</em>[<em>n</em>] into the sorted subarray <em>A</em>[1 : <em>n</em> – 1]. Write pseudocode for this recursive version of insertion sort. Give a recurrence for its worst-case running time.</p>
<p class="level3"><strong><em>2.3-6</em></strong></p>
<p class="noindent">Referring back to the searching problem (see Exercise 2.1-4), observe that if the subarray being searched is already sorted, the searching algorithm can check the midpoint of the subarray against <em>v</em> and eliminate half of the subarray from further <a id="p45"/>consideration. The <strong><em><span class="blue1">binary search</span></em></strong> algorithm repeats this procedure, halving the size of the remaining portion of the subarray each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is Θ(lg <em>n</em>).</p>
<p class="level3"><strong><em>2.3-7</em></strong></p>
<p class="noindent">The <strong>while</strong> loop of lines 5–7 of the I<small>NSERTION</small>-S<small>ORT</small> procedure in <a href="chapter002.xhtml#Sec_2.1">Section 2.1</a> uses a linear search to scan (backward) through the sorted subarray <em>A</em>[1 : <em>j</em> – 1]. What if insertion sort used a binary search (see Exercise 2.3-6) instead of a linear search? Would that improve the overall worst-case running time of insertion sort to Θ(<em>n</em> lg <em>n</em>)?</p>
<p class="level3"><strong><em>2.3-8</em></strong></p>
<p class="noindent">Describe an algorithm that, given a set <em>S</em> of <em>n</em> integers and another integer <em>x</em>, determines whether <em>S</em> contains two elements that sum to exactly <em>x</em>. Your algorithm should take Θ(<em>n</em> lg <em>n</em>) time in the worst case.</p>
</section>
</section>
<p class="line1"/>
<section title="Problems">
<p class="level1" id="h1-9"><strong>Problems</strong></p>
<section title="2-1 Insertion sort on small arrays in merge sort">
<p class="level2"><strong><em>2-1     Insertion sort on small arrays in merge sort</em></strong></p>
<p class="noindent">Although merge sort runs in Θ(<em>n</em> lg <em>n</em>) worst-case time and insertion sort runs in Θ(<em>n</em><sup>2</sup>) worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus it makes sense to <strong><em><span class="blue1">coarsen</span></em></strong> the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which <em>n</em>/<em>k</em> sublists of length <em>k</em> are sorted using insertion sort and then merged using the standard merging mechanism, where <em>k</em> is a value to be determined.</p>
<p class="nl"><strong><em>a.</em></strong> Show that insertion sort can sort the <em>n</em>/<em>k</em> sublists, each of length <em>k</em>, in Θ(<em>nk</em>) worst-case time.</p>
<p class="nl"><strong><em>b.</em></strong> Show how to merge the sublists in Θ(<em>n</em> lg(<em>n</em>/<em>k</em>)) worst-case time.</p>
<p class="nl"><strong><em>c.</em></strong> Given that the modified algorithm runs in Θ(<em>nk</em> + <em>n</em> lg(<em>n</em>/<em>k</em>)) worst-case time, what is the largest value of <em>k</em> as a function of <em>n</em> for which the modified algorithm has the same running time as standard merge sort, in terms of Θ-notation?</p>
<p class="nl"><strong><em>d.</em></strong> How should you choose <em>k</em> in practice?</p>
<a id="p46"/>
</section>
<section title="2-2 Correctness of bubblesort">
<p class="level2"><strong><em>2-2     Correctness of bubblesort</em></strong></p>
<p class="noindent">Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order. The procedure B<small>UBBLESORT</small> sorts array <em>A</em>[1 : <em>n</em>].</p>
<div class="pull-quote1">
<p class="box-heading">B<small>UBBLESORT</small>(<em>A</em>, <em>n</em>)</p>
<table class="table1">
<tr>
<td class="td1"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em> – 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="p2"><strong>for</strong> <em>j</em> = <em>n</em> <strong>downto</strong> <em>i</em> + 1</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p3"><strong>if</strong> <em>A</em>[<em>j</em>] &lt; <em>A</em>[<em>j</em> – 1]</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="p4">exchange <em>A</em>[<em>j</em>] with <em>A</em>[<em>j</em> – 1]</p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>a.</em></strong> Let <em>A</em><sup>′</sup> denote the array <em>A</em> after B<small>UBBLESORT</small>(<em>A</em>, <em>n</em>) is executed. To prove that</p>
<p class="eqr"><img alt="art" src="images/Art_P28.jpg"/></p>
<p class="nl-para">In order to show that B<small>UBBLESORT</small> actually sorts, what else do you need to prove?</p>
<p class="noindent1-top">The next two parts prove inequality (2.5).</p>
<p class="nl"><strong><em>b.</em></strong> State precisely a loop invariant for the <strong>for</strong> loop in lines 2–4, and prove that this loop invariant holds. Your proof should use the structure of the loop-invariant proof presented in this chapter.</p>
<p class="nl"><strong><em>c.</em></strong> Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the <strong>for</strong> loop in lines 1–4 that allows you to prove inequality (2.5). Your proof should use the structure of the loop-invariant proof presented in this chapter.</p>
<p class="nl"><strong><em>d.</em></strong> What is the worst-case running time of B<small>UBBLESORT</small>? How does it compare with the running time of I<small>NSERTION</small>-S<small>ORT</small>?</p>
</section>
<section title="2-3 Correctness of Horner’s rule">
<p class="level2"><strong><em>2-3     Correctness of Horner’s rule</em></strong></p>
<p class="noindent">You are given the coefficents <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em> of a polynomial</p>
<p class="eql"><img alt="art" src="images/Art_P29.jpg"/></p>
<p class="noindent">and you want to evaluate this polynomial for a given value of <em>x</em>. <strong><em><span class="blue1">Horner’s rule</span></em></strong> says to evaluate the polynomial according to this parenthesization:</p>
<a id="p47"/>
<p class="eql"><img alt="art" src="images/Art_P30.jpg"/></p>
<p class="noindent">The procedure H<small>ORNER</small> implements Horner’s rule to evaluate <em>P</em>(<em>x</em>), given the coefficients <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, … , <em>a<sub>n</sub></em> in an array <em>A</em>[0 : <em>n</em>] and the value of <em>x</em>.</p>
<div class="pull-quote1">
<p class="box-heading">H<small>ORNER</small>(<em>A, n, x</em>)</p>
<table class="table1">
<tr>
<td class="td1"><p class="noindent"><span class="x-small">1</span></p></td>
<td class="td1"><p class="noindent"><em>p</em> = 0</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">2</span></p></td>
<td class="td1"><p class="noindent"><strong>for</strong> <em>i</em> = <em>n</em> <strong>downto</strong> 0</p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">3</span></p></td>
<td class="td1"><p class="p2"><em>p</em> = <em>A</em>[<em>i</em>] + <em>x</em> · <em>p</em></p></td>
</tr>
<tr>
<td class="td1"><p class="noindent"><span class="x-small">4</span></p></td>
<td class="td1"><p class="noindent"><strong>return</strong> <em>p</em></p></td>
</tr>
</table>
</div>
<p class="nl"><strong><em>a.</em></strong> In terms of Θ-notation, what is the running time of this procedure?</p>
<p class="nl"><strong><em>b.</em></strong> Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare with H<small>ORNER</small>?</p>
<p class="nl"><strong><em>c.</em></strong> Consider the following loop invariant for the procedure H<small>ORNER</small>:</p>
<p class="nl-para">At the start of each iteration of the <strong>for</strong> loop of lines 2–3,</p>
<p class="eqi"><img alt="art" src="images/Art_P31.jpg"/></p>
<p class="nl-para">Interpret a summation with no terms as equaling 0. Following the structure of the loop-invariant proof presented in this chapter, use this loop invariant to show that, at termination, <img alt="art" src="images/Art_P32.jpg"/>.</p>
</section>
<section title="2-4 Inversions">
<p class="level2"><strong><em>2-4     Inversions</em></strong></p>
<p class="noindent">Let <em>A</em>[1 : <em>n</em>] be an array of <em>n</em> distinct numbers. If <em>i</em> &lt; <em>j</em> and <em>A</em>[<em>i</em>] &gt; <em>A</em>[<em>j</em>], then the pair (<em>i, j</em>) is called an <strong><em><span class="blue1">inversion</span></em></strong> of <em>A</em>.</p>
<p class="nl"><strong><em>a.</em></strong> List the five inversions of the array <span class="font1">〈</span>2, 3, 8, 6, 1<span class="font1">〉</span>.</p>
<p class="nl"><strong><em>b.</em></strong> What array with elements from the set {1, 2, … , <em>n</em>} has the most inversions? How many does it have?</p>
<p class="nl"><strong><em>c.</em></strong> What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.</p>
<p class="nl"><strong><em>d.</em></strong> Give an algorithm that determines the number of inversions in any permutation on <em>n</em> elements in Θ(<em>n</em> lg <em>n</em>) worst-case time. (<em>Hint:</em> Modify merge sort.)</p>
<a id="p48"/>
</section>
</section>
<p class="line1"/>
<section title="Chapter notes">
<p class="level1" id="h1-10"><strong>Chapter notes</strong></p>
<p class="noindent">In 1968, Knuth published the first of three volumes with the general title <em>The Art of Computer Programming</em> [<a epub:type="noteref" href="bibliography001.xhtml#endnote_259">259</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_260">260</a>, <a epub:type="noteref" href="bibliography001.xhtml#endnote_261">261</a>]. The first volume ushered in the modern study of computer algorithms with a focus on the analysis of running time. The full series remains an engaging and worthwhile reference for many of the topics presented here. According to Knuth, the word “algorithm” is derived from the name “al-Khowârizmî,” a ninth-century Persian mathematician.</p>
<p>Aho, Hopcroft, and Ullman [<a epub:type="noteref" href="bibliography001.xhtml#endnote_5">5</a>] advocated the asymptotic analysis of algorithms—using notations that <a href="chapter003.xhtml">Chapter 3</a> introduces, including Θ-notation—as a means of comparing relative performance. They also popularized the use of recurrence relations to describe the running times of recursive algorithms.</p>
<p>Knuth [<a epub:type="noteref" href="bibliography001.xhtml#endnote_261">261</a>] provides an encyclopedic treatment of many sorting algorithms. His comparison of sorting algorithms (page 381) includes exact step-counting analyses, like the one we performed here for insertion sort. Knuth’s discussion of insertion sort encompasses several variations of the algorithm. The most important of these is Shell’s sort, introduced by D. L. Shell, which uses insertion sort on periodic subarrays of the input to produce a faster sorting algorithm.</p>
<p>Merge sort is also described by Knuth. He mentions that a mechanical collator capable of merging two decks of punched cards in a single pass was invented in 1938. J. von Neumann, one of the pioneers of computer science, apparently wrote a program for merge sort on the EDVAC computer in 1945.</p>
<p>The early history of proving programs correct is described by Gries [<a epub:type="noteref" href="bibliography001.xhtml#endnote_200">200</a>], who credits P. Naur with the first article in this field. Gries attributes loop invariants to R. W. Floyd. The textbook by Mitchell [<a epub:type="noteref" href="bibliography001.xhtml#endnote_329">329</a>] is a good reference on how to prove programs correct.</p>
<p class="footnote" id="footnote_1"><a href="#footnote_ref_1"><sup>1</sup></a> If you’re familiar with only Python, you can think of arrays as similar to Python lists.</p>
<p class="footnote1" id="footnote_2"><a href="#footnote_ref_2"><sup>2</sup></a> When the loop is a <strong>for</strong> loop, the loop-invariant check just prior to the first iteration occurs immediately after the initial assignment to the loop-counter variable and just before the first test in the loop header. In the case of I<small>NSERTION</small>-S<small>ORT</small>, this time is after assigning 2 to the variable <em>i</em> but before the first test of whether <em>i</em> ≤ <em>n</em>.</p>
<p class="footnote1" id="footnote_3"><a href="#footnote_ref_3"><sup>3</sup></a> In an <strong>if-else</strong> statement, we indent <strong>else</strong> at the same level as its matching <strong>if</strong>. The first executable line of an <strong>else</strong> clause appears on the same line as the keyword <strong>else</strong>. For multiway tests, we use <strong>elseif</strong> for tests after the first one. When it is the first line in an <strong>else</strong> clause, an <strong>if</strong> statement appears on the line following <strong>else</strong> so that you do not misconstrue it as <strong>elseif</strong>.</p>
<p class="footnote1" id="footnote_4"><a href="#footnote_ref_4"><sup>4</sup></a> Each pseudocode procedure in this book appears on one page so that you do not need to discern levels of indentation in pseudocode that is split across pages.</p>
<p class="footnote1" id="footnote_5"><a href="#footnote_ref_5"><sup>5</sup></a> Most block-structured languages have equivalent constructs, though the exact syntax may differ. Python lacks <strong>repeat-until</strong> loops, and its <strong>for</strong> loops operate differently from the <strong>for</strong> loops in this book. Think of the pseudocode line “<strong>for</strong> <em>i</em> = 1 <strong>to</strong> <em>n</em>” as equivalent to “for i in range(1, n+1)” in Python.</p>
<p class="footnote1" id="footnote_6"><a href="#footnote_ref_6"><sup>6</sup></a> In Python, the loop counter retains its value after the loop is exited, but the value it retains is the value it had during the final iteration of the <strong>for</strong> loop, rather than the value that exceeded the loop bound. That is because a Python <strong>for</strong> loop iterates through a list, which may contain nonnumeric values.</p>
<p class="footnote1" id="footnote_7"><a href="#footnote_ref_7"><sup>7</sup></a> If you’re used to programming in Python, bear in mind that in this book, the subarray <em>A</em>[<em>i</em> : <em>j</em>] includes the element <em>A</em>[<em>j</em>]. In Python, the last element of <em>A</em>[<em>i</em> : <em>j</em>] is <em>A</em>[<em>j</em> – 1]. Python allows negative indices, which count from the back end of the list. This book does not use negative array indices.</p>
<p class="footnote1" id="footnote_8"><a href="#footnote_ref_8"><sup>8</sup></a> Python’s tuple notation allows <strong>return</strong> statements to return multiple values without creating objects from a programmer-defined class.</p>
<p class="footnote1" id="footnote_9"><a href="#footnote_ref_9"><sup>9</sup></a> We assume that each element of a given array occupies the same number of bytes and that the elements of a given array are stored in contiguous memory locations. For example, if array <em>A</em>[1 : <em>n</em>] starts at memory address 1000 and each element occupies four bytes, then element <em>A</em>[<em>i</em>] is at address 1000 + 4(<em>i</em> – 1). In general, computing the address in memory of a particular array element requires at most one subtraction (no subtraction for a 0-origin array), one multiplication (often implemented as a shift operation if the element size is an exact power of 2), and one addition. Furthermore, for code that iterates through the elements of an array in order, an optimizing compiler can generate the address of each element using just one addition, by adding the element size to the address of the preceding element.</p>
<p class="footnote1" id="footnote_10"><a href="#footnote_ref_10"><sup>10</sup></a> There are some subtleties here. Computational steps that we specify in English are often variants of a procedure that requires more than just a constant amount of time. For example, in the R<small>ADIX</small>-S<small>ORT</small> procedure on page 213, one line reads “use a stable sort to sort array <em>A</em> on digit <em>i</em>,” which, as we shall see, takes more than a constant amount of time. Also, although a statement that calls a subroutine takes only constant time, the subroutine itself, once invoked, may take more. That is, we separate the process of <strong><em><span class="blue1">calling</span></em></strong> the subroutine—passing parameters to it, etc.—from the process of <strong><em><span class="blue1">executing</span></em></strong> the subroutine.</p>
<p class="footnote1" id="footnote_11"><a href="#footnote_ref_11"><sup>11</sup></a> This characteristic does not necessarily hold for a resource such as memory. A statement that references <em>m</em> words of memory and is executed <em>n</em> times does not necessarily reference <em>mn</em> distinct words of memory.</p>
<p class="footnote1" id="footnote_12"><a href="#footnote_ref_12"><sup>12</sup></a> This procedure is the rare case that uses both 1-origin indexing (for array <em>A</em>) and 0-origin indexing (for arrays <em>L</em> and <em>R</em>). Using 0-origin indexing for <em>L</em> and <em>R</em> makes for a simpler loop invariant in Exercise 2.3-3.</p>
<p class="footnote1" id="footnote_13"><a href="#footnote_ref_13"><sup>13</sup></a> If you’re wondering where the “+1” comes from, imagine that <em>r</em> = <em>p</em> + 1. Then the subarray <em>A</em>[<em>p</em> : <em>r</em>] consists of two elements, and <em>r</em> – <em>p</em> + 1 = 2.</p>
<p class="footnote1" id="footnote_14"><a href="#footnote_ref_14"><sup>14</sup></a> <a href="chapter003.xhtml">Chapter 3</a> shows how to formally interpret equations containing Θ-notation.</p>
<p class="footnote1" id="footnote_15"><a href="#footnote_ref_15"><sup>15</sup></a> The expression <span class="font1">⌈</span><em>x</em><span class="font1">⌉</span> denotes the least integer greater than or equal to <em>x</em>, and <span class="font1">⌊</span><em>x</em><span class="font1">⌋</span> denotes the greatest integer less than or equal to <em>x</em>. These notations are defined in <a href="chapter003.xhtml#Sec_3.3">Section 3.3</a>. The easiest way to verify that setting <em>q</em> to <span class="font1">⌊</span>(<em>p</em> + <em>r</em>)/2<span class="font1">⌋</span> yields subarrays <em>A</em>[<em>p</em> : <em>q</em>] and <em>A</em>[<em>q</em> + 1 : <em>r</em>] of sizes <span class="font1">⌈</span><em>n</em>/2<span class="font1">⌉</span> and <span class="font1">⌊</span><em>n</em>/2<span class="font1">⌋</span>, respectively, is to examine the four cases that arise depending on whether each of <em>p</em> and <em>r</em> is odd or even.</p>
<p class="footnote1" id="footnote_16"><a href="#footnote_ref_16"><sup>16</sup></a> If you’re wondering where Θ(1) comes from, think of it this way. When we say that <em>n</em><sup>2</sup>/100 is Θ(<em>n</em><sup>2</sup>), we are ignoring the coefficient 1/100 of the factor <em>n</em><sup>2</sup>. Likewise, when we say that a constant <em>c</em> is Θ(1), we are ignoring the coefficient <em>c</em> of the factor 1 (which you can also think of as <em>n</em><sup>0</sup>).</p>
<p class="footnote1" id="footnote_17"><a href="#footnote_ref_17"><sup>17</sup></a> The notation lg <em>n</em> stands for log<sub>2</sub> <em>n</em>, although the base of the logarithm doesn’t matter here, but as computer scientists, we like logarithms base 2. <a href="chapter003.xhtml#Sec_3.3">Section 3.3</a> discusses other standard notation.</p>
<p class="footnote1" id="footnote_18"><a href="#footnote_ref_18"><sup>18</sup></a> It is unlikely that <em>c</em><sub>1</sub> is exactly the time to solve problems of size 1 and that <em>c</em><sub>2</sub><em>n</em> is exactly the time of the divide and combine steps. We’ll look more closely at bounding recurrences in <a href="chapter004.xhtml">Chapter 4</a>, where we’ll be more careful about this kind of detail.</p>
</section>
</section>
</div>
</body>
</html>