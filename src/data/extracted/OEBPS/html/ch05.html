<?xml version="1.0" encoding="UTF-8" standalone="no"?><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Five. Strings</title>
<link href="9780132762564.css" rel="stylesheet" type="text/css"/>
<link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml"/>
<meta name="Adept.resource" value="urn:uuid:7baf5dbb-ffe1-4201-87bd-b993ed04f947"/>
</head>
<body>
<p><a id="ch05"/></p>
<h2><a id="page_694"/>Five. Strings</h2>
<div class="sidebar">
<hr/>
<p><a id="ch05sb01"/></p>
<p class="indenthangingN"><strong><a href="#ch05sec1lev7">5.1</a></strong> <a href="#ch05sec1lev7">String Sorts</a> <a href="#ch05sec1lev7">702</a></p>
<p class="indenthangingN"><strong><a href="#ch05sec1lev8">5.2</a></strong> <a href="#ch05sec1lev8">Tries</a> <a href="#ch05sec1lev8">730</a></p>
<p class="indenthangingN"><strong><a href="#ch05sec1lev9">5.3</a></strong> <a href="#ch05sec1lev9">Substring Search</a> <a href="#ch05sec1lev9">758</a></p>
<p class="indenthangingN"><strong><a href="#ch05sec1lev10">5.4</a></strong> <a href="#ch05sec1lev10">Regular Expressions</a> <a href="#ch05sec1lev10">788</a></p>
<p class="indenthangingN"><strong><a href="#ch05sec1lev11">5.5</a></strong> <a href="#ch05sec1lev11">Data Compression</a> <a href="#ch05sec1lev11">810</a></p>
<hr/>
</div>
<p><a id="page_695"/>We communicate by exchanging strings of characters. Accordingly, numerous important and familiar applications are based on processing strings. In this chapter, we consider classic algorithms for addressing the underlying computational challenges surrounding applications such as the following:</p>
<p><a id="ch05sec1lev1"/></p>
<h4><em>Information processing</em></h4>
<p>When you search for web pages containing a given keyword, you are using a string-processing application. In the modern world, virtually <em>all</em> information is encoded as a sequence of strings, and the applications that process it are string-processing applications of crucial importance.</p>
<p><a id="ch05sec1lev2"/></p>
<h4><em>Genomics</em></h4>
<p>Computational biologists work with a <em>genetic code</em> that reduces DNA to (very long) strings formed from four characters (<code>A</code>, <code>C</code>, <code>T</code>, and <code>G</code>). Vast databases giving codes describing all manner of living organisms have been developed in recent years, so that string processing is a cornerstone of modern research in computational biology.</p>
<p><a id="ch05sec1lev3"/></p>
<h4><em>Communications systems</em></h4>
<p>When you send a text message or an email or download an ebook, you are transmitting a string from one place to another. Applications that process strings for this purpose were an original motivation for the development of string-processing algorithms.</p>
<p><a id="ch05sec1lev4"/></p>
<h4><em>Programming systems</em></h4>
<p>Programs are strings. Compilers, interpreters, and other applications that convert programs into machine instructions are critical applications that use sophisticated string-processing techniques. Indeed, all written languages are expressed as strings, and another motivation for the development of string-processing algorithms was the theory of formal languages, the study of describing sets of strings.</p>
<p>This list of a few significant examples illustrates the diversity and importance of string-processing algorithms.</p>
<p><a id="page_696"/>The plan of this chapter is as follows: After addressing basic properties of strings, we revisit in <a href="#ch05sec1lev7">SECTIONS 5.1</a> AND <a href="#ch05sec1lev8">5.2</a> the sorting and searching APIs from <a href="ch02.html#ch02"><small>CHAPTERS 2</small></a> and <a href="ch03.html#ch03">3</a>. Algorithms that exploit special properties of string keys are faster and more flexible than the algorithms that we considered earlier. In <a href="#ch05sec1lev9"><small>SECTION 5.3</small></a> we consider algorithms for <em>substring search</em>, including a famous algorithm due to Knuth, Morris, and Pratt. In <a href="#ch05sec1lev10"><small>SECTION 5.4</small></a> we introduce <em>regular expressions</em>, the basis of the <em>pattern-matching</em> problem, a generalization of substring search, and a quintessential search tool known as <em>grep</em>. These classic algorithms are based on the related conceptual devices known as <em>formal languages</em> and <em>finite automata</em>. <a href="#ch05sec1lev11"><small>SECTION 5.5</small></a> is devoted to a central application: <em>data compression</em>, where we try to reduce the size of a string as much as possible.</p>
<p><a id="ch05sec1lev5"/></p>
<h3>Rules of the game</h3>
<p>For clarity and efficiency, our implementations are expressed in terms of the Java <code>String</code> class, but we intentionally use as few operations as possible from that class to make it easier to adapt our algorithms for use on other string-like types of data and to other programming languages. We introduced strings in detail in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a> but briefly review here their most important characteristics.</p>
<p><a id="ch05sec2lev1"/></p>
<h4><em>Characters</em></h4>
<p>A <code>String</code> is a sequence of characters. Characters are of type <code>char</code> and can have one of 2<sup>16</sup> possible values. For many decades, programmers restricted attention to characters encoded in 7-bit ASCII (see page <a href="#ch05sec3lev75">815</a> for a conversion table) or 8-bit extended ASCII, but many modern applications call for 16-bit Unicode.</p>
<p><a id="ch05sec2lev2"/></p>
<h4><em>Immutability</em></h4>
<p><code>String</code> objects are immutable, so that we can use them in assignment statements and as arguments and return values from methods without having to worry about their values changing.</p>
<p><a id="ch05sec2lev3"/></p>
<h4><em>Indexing</em></h4>
<p>The operation that we perform most often is <em>extract a specified character from a string</em> that the <code>charAt()</code> method in Java’s <code>String</code> class provides. We expect <code>charAt()</code> to complete its work in <em>constant</em> time, as if the string were stored in a <code>char[]</code> array. As discussed in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, this expectation is quite reasonable.</p>
<p><a id="ch05sec2lev4"/></p>
<h4><em>Length</em></h4>
<p>In Java, the <em>find the length of a string</em> operation is implemented in the <code>length()</code> method in <code>String</code>. Again, we expect <code>length()</code> to complete its work in <em>constant</em> time, and again, this expectation is reasonable, although some care is needed in some programming environments.</p>
<p><a id="ch05sec2lev5"/></p>
<h4><em>Substring</em></h4>
<p>Java’s <code>substring()</code> method implements the <em>extract a specified substring</em> operation. Again, we expect a <em>constant</em>-time implementation of this method, as in Java’s standard implementation. <em>If you are not familiar with</em> <code>substring()</code> <em>and the reason that it is constant-time, be sure to reread our discussion of Java’s standard string implementation</em> in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a> (see page <a href="ch01.html#page_80">80</a> and page <a href="ch01a.html#page_204">204</a>).</p>
<p><a id="ch05sec2lev6"/></p>
<h4><a id="page_697"/><em>Concatenation</em></h4>
<p>In Java, the <em>create a new string formed by appending one string to another</em> operation is a built-in operation (using the <code>+</code> operator) that takes time proportional to the length of the result. For example, we avoid forming a string by appending one character at a time because that is a <em>quadratic</em> process in Java. (Java has a <code>StringBuilder</code> class for that use.)</p>
<p class="image"><img alt="image" src="graphics/05_01-stringbasicops.jpg"/></p>
<p><a id="ch05sec2lev7"/></p>
<h4><em>Character arrays</em></h4>
<p>The Java <code>String</code> is decidedly not a primitive type. The standard implementation provides the operations just described to facilitate client programming. By contrast, many of the algorithms that we consider can work with a low-level representation such as an array of <code>char</code> values, and many clients might prefer such a representation, because it consumes less space and takes less time. For several of the algorithms that we consider, the cost of converting from one representation to the other would be higher than the cost of running the algorithm. As indicated in the table below, the differences in code that processes the two representations are minor (<code>substring()</code> is more complicated and is omitted), so use of one representation or the other is no barrier to understanding the algorithm.</p>
<p><small>UNDERSTANDING THE EFFICIENCY OF THESE OPERATIONS</small> is a key ingredient in understanding the efficiency of several string-processing algorithms. Not all programming languages provide <code>String</code> implementations with these performance characteristics. For example, the substring operation and determining the length of a string take time proportional to the number of characters in the string in the widely used C programming language. Adapting the algorithms that we describe to such languages is always possible (implement an ADT like Java’s <code>String</code>), but also might present different challenges and opportunities.</p>
<p class="image"><img alt="image" src="graphics/t0697-01.jpg"/></p>
<p><a id="page_698"/>We primarily use the <code>String</code> data type in the text, with liberal use of indexing and length and occasional use of substring extraction and concatenation. When appropriate, we also provide on the booksite the corresponding code for <code>char</code> arrays. In performance-critical applications, the primary consideration in choosing between the two for clients is often the cost of accessing a character (<code>a[i]</code> is likely to be much faster than <code>s.charAt(i)</code> in typical Java implementations).</p>
<p><a id="ch05sec1lev6"/></p>
<h3>Alphabets</h3>
<p>Some applications involve strings taken from a restricted alphabet. In such applications, it often makes sense to use an <code>Alphabet</code> class with the following API:</p>
<p class="image"><img alt="image" src="graphics/t0698-01.jpg"/></p>
<p>This API is based on a constructor that takes as argument an <em>R</em>-character string that specifies the alphabet and the <code>toChar()</code> and <code>toIndex()</code> methods for converting (in constant time) between string characters and <code>int</code> values between 0 and <em>R</em>-1. It also includes a <code>contains()</code> method for checking whether a given character is in the alphabet, the methods <code>R()</code> and <code>lgR()</code> for finding the number of characters in the alphabet and the number of bits needed to represent them, and the methods <code>toIndices()</code> and <code>toChars()</code> for converting between strings of characters in the alphabet and <code>int</code> arrays. For convenience, we also include the built-in alphabets in the table at the top of the next page, which you can access with code such as <code>Alphabet.UNICODE</code>. Implementing <code>Alphabet</code> is a straightforward exercise (see <a href="#ch05qa1q12"><small>EXERCISE 5.1.12</small></a>). We will examine a sample client on page <a href="#page_699">699</a>.</p>
<p><a id="ch05sec2lev8"/></p>
<h4><em>Character-indexed arrays</em></h4>
<p>One of the most important reasons to use <code>Alphabet</code> is that many algorithms gain efficiency through the use of character-indexed arrays, where we associate information with each character that we can retrieve with a single array <a id="page_700"/>access. With a Java <code>String</code>, we have to use an array of size 65,536; with <code>Alphabet</code>, we just need an array with one entry for each alphabet character. Some of the algorithms that we consider can produce huge numbers of such arrays, and in such cases, the space for arrays of size 65,536 can be prohibitive. As an example, consider the class <code>Count</code> at the bottom of the previous page, which takes a string of characters from the command line and prints a table of the frequency of occurrence of those characters that appear on standard input. The <code>count[]</code> array that holds the frequencies in <code>Count</code> is an example of a character-indexed array. This calculation may seem to you to be a bit frivolous; actually, it is the basis for a family of fast sorting methods that we will consider in <a href="#ch05sec1lev7"><small>SECTION 5.1</small></a>.</p>
<p class="image"><a id="page_699"/><img alt="image" src="graphics/t0699-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0699-01.jpg"/></p>
<p><a id="ch05sec2lev9"/></p>
<h4><em>Numbers</em></h4>
<p>As you can see from several of the standard <code>Alphabet</code> examples, we often represent numbers as strings. The method <code>toIndices()</code> converts any <code>String</code> over a given <code>Alphabet</code> into a base-<em>R</em> number represented as an <code>int[]</code> array with all values between 0 and <em>R</em> − 1. In some situations, doing this conversion at the start leads to compact code, because any digit can be used as an index in a character-indexed array. For example, if we know that the input consists only of characters from the alphabet, we could replace the inner loop in <code>Count</code> with the more compact code</p>
<p class="programlisting3"><img alt="image" src="graphics/p0700-01.jpg"/></p>
<p>In this context, we refer to <em>R</em> as the <em>radix</em>, the base of the number system. Several of the algorithms that we consider are often referred to as “radix” methods because they work with one digit at a time.</p>
<p class="image"><img alt="image" src="graphics/p0700-02.jpg"/></p>
<p><a id="page_701"/><small>DESPITE THE ADVANTAGES</small> of using a data type such as <code>Alphabet</code> in string-processing algorithms (particularly for small alphabets), we do not develop our implementations in the book for strings taken from a general <code>Alphabet</code> because</p>
<p class="indenthangingB">• The preponderance of clients just use <code>String</code></p>
<p class="indenthangingB">• Conversion to and from indices tends to fall in the inner loop and slow down implementations considerably</p>
<p class="indenthangingB">• The code is more complicated, and therefore more difficult to understand</p>
<p>Accordingly we use <code>String</code>, use the constant <code>R = 256</code> in the code and <em>R</em> as a parameter in the analysis, and discuss performance for general alphabets when appropriate. You can find full <code>Alphabet</code>-based implementations on the booksite.</p>
<p><a id="ch05sec1lev7"/></p>
<h3><a id="page_702"/>5.1 String Sorts</h3>
<p><small>FOR MANY SORTING APPLICATIONS</small>, the keys that define the order are strings. In this section, we look at methods that take advantage of special properties of strings to develop sorts for string keys that are more efficient than the general-purpose sorts that we considered in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>.</p>
<p>We consider two fundamentally different approaches to string sorting. Both of them are venerable methods that have served programmers well for many decades.</p>
<p>The first approach examines the characters in the keys in a right-to-left order. Such methods are generally referred to as least-significant-digit (LSD) string sorts. Use of the term <em>digit</em> instead of <em>character</em> traces back to the application of the same basic method to numbers of various types. Thinking of a string as a base-256 number, considering characters from right to left amounts to considering first the least significant digits. This approach is the method of choice for string-sorting applications where all the keys are the same length.</p>
<p>The second approach examines the characters in the keys in a left-to-right order, working with the most significant character first. These methods are generally referred to as most-significant-digit (MSD) string sorts—we will consider two such methods in this section. MSD string sorts are attractive because they can get a sorting job done without necessarily examining all of the input characters. MSD string sorts are similar to quicksort, because they partition the array to be sorted into independent pieces such that the sort is completed by recursively applying the same method to the subarrays. The difference is that MSD string sorts use just the first character of the sort key to do the partitioning, while quicksort uses comparisons that could involve examining the whole key. The first method that we consider creates a partition for each character value; the second always creates three partitions, for sort keys whose first character is less than, equal to, or greater than the partitioning key’s first character.</p>
<p>The number of characters in the alphabet is an important parameter when analyzing string sorts. Though we focus on extended ASCII strings (<em>R</em> = 256), we will also consider strings taken from much smaller alphabets (such as genomic sequences) and from much larger alphabets (such as the 65,536-character Unicode alphabet that is an international standard for encoding natural languages).</p>
<p><a id="ch05sec2lev10"/></p>
<h4><a id="page_703"/>Key-indexed counting</h4>
<p>As a warmup, we consider a simple method for sorting that is effective whenever the keys are small integers. This method, known as <em>key-indexed counting</em>, is useful in its own right and is also the basis for two of the three string sorts that we consider in this section.</p>
<p class="image"><img alt="image" src="graphics/05_03-keyIndexed0.jpg"/></p>
<p>Consider the following data-processing problem, which might be faced by a teacher maintaining grades for a class with students assigned to sections, which are numbered <code>1</code>, <code>2</code>, <code>3</code>, and so forth. On some occasions, it is necessary to have the class listed by section. Since the section numbers are small integers, sorting by key-indexed counting is appropriate. To describe the method, we assume that the information is kept in an array <code>a[]</code> of items that each contain a name and a section number, that section numbers are integers between <code>0</code> and <code>R-1</code>, and that the code <code>a[i].key()</code> returns the section number for the indicated student. The method breaks down into four steps, which we describe in turn.</p>
<p class="image"><img alt="image" src="graphics/05_02-keyIndexed1.jpg"/></p>
<p><a id="ch05sec3lev1"/></p>
<h5><em>Compute frequency counts</em></h5>
<p>The first step is to count the frequency of occurrence of each key value, using an <code>int</code> array <code>count[]</code>. For each item, we use the key to access an entry in <code>count[]</code> and increment that entry. If the key value is <code>r</code>, we increment <code>count[r+1]</code>. (Why <code>+1</code>? The reason for that will become clear in the next step.) In the example at left, we first increment <code>count[3]</code> because <code>Anderson</code> is in section <code>2</code>, then we increment <code>count[4]</code> twice because <code>Brown</code> and <code>Davis</code> are in section <code>3</code>, and so forth. Note that <code>count[0]</code> is always <code>0</code>, and that <code>count[1]</code> is 0 in this example (no students are in section <code>0</code>).</p>
<p><a id="ch05sec3lev2"/></p>
<h5><a id="page_704"/><em>Transform counts to indices</em></h5>
<p>Next, we use <code>count[]</code> to compute, for each key value, the starting index positions in the sorted order of items with that key. In our example, since there are three items with key <code>1</code> and five items with key <code>2</code>, then the items with key <code>3</code> start at position 8 in the sorted array. In general, to get the starting index for items with any given key value we sum the frequency counts of smaller values. For each key value <code>r</code>, the sum of the counts for key values less than <code>r+1</code> is equal to the sum of the counts for key values less than <code>r</code> plus <code>count[r]</code>, so it is easy to proceed from left to right to transform <code>count[]</code> into an index table that we can use to sort the data.</p>
<p class="image"><img alt="image" src="graphics/05_05-keyIndexed2.jpg"/></p>
<p><a id="ch05sec3lev3"/></p>
<h5><em>Distribute the data</em></h5>
<p>With the <code>count[]</code> array transformed into an index table, we accomplish the actual sort by moving the items to an auxiliary array <code>aux[]</code>. We move each item to the position in <code>aux[]</code> indicated by the <code>count[]</code> entry corresponding to its key, and then increment that entry to maintain the following invariant for <code>count[]</code>: for each key value <code>r</code>, <code>count[r]</code> is the index of the position in <code>aux[]</code> where the next item with key value <code>r</code> (if any) should be placed. This process produces a sorted result with one pass through the data, as illustrated at left. <em>Note</em>: In one of our applications, the fact that this implementation is <em>stable</em> is critical: items with equal keys are brought together but kept in the same relative order.</p>
<p class="image"><img alt="image" src="graphics/05_04-keyIndexed3.jpg"/></p>
<p class="image"><a id="page_705"/><img alt="image" src="graphics/05_06-lsdduring.jpg"/></p>
<p><a id="ch05sec3lev4"/></p>
<h5><em>Copy back</em></h5>
<p>Since we accomplished the sort by moving the items to an auxiliary array, the last step is to copy the sorted result back to the original array.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb02"/></p>
<p><strong>Proposition A.</strong> Key-indexed counting uses 8<em>N</em> + 3 <em>R</em> + 1 array accesses to stably sort <em>N</em> items whose keys are integers between 0 and <em>R</em> − 1.</p>
<p><strong>Proof:</strong> Immediate from the code. Initializing the arrays uses <em>N</em> + <em>R</em> + 1 array accesses. The first loop increments a counter for each of the <em>N</em> items (2<em>N</em> array accesses); the second loop does <em>R</em> additions (2<em>R</em> array accesses); the third loop does <em>N</em> counter increments and <em>N</em> data moves (3<em>N</em> array accesses); and the fourth loop does <em>N</em> data moves (2<em>N</em> array accesses). Both moves preserve the relative order of equal keys.</p>
<hr/>
</div>
<p><small>KEY-INDEXED COUNTING</small> is an extremely effective and often overlooked sorting method for applications where keys are small integers. Understanding how it works is a first step toward understanding string sorting. <a href="#ch05sb02"><small>PROPOSITION A</small></a> implies that key-indexed counting breaks through the <em>N</em> log <em>N</em> lower bound that we proved for sorting. How does it manage to do so? <a href="ch02.html#ch02sb19"><small>PROPOSITION I</small></a> in <a href="ch02.html#ch02sec1lev2"><small>SECTION 2.2</small></a> is a lower bound on the number of <em>compares</em> needed (when data is accessed only through <code>compareTo()</code>)—key-indexed counting does <em>no</em> compares (it accesses data only through <code>key()</code>). When <em>R</em> is within a constant factor of <em>N</em>, we have a linear-time sort.</p>
<p class="image"><img alt="image" src="graphics/p0705-01.jpg"/></p>
<p><a id="ch05sec2lev11"/></p>
<h4><a id="page_706"/>LSD string sort</h4>
<p>The first string-sorting method that we consider is known as <em>least-significant-digit first</em> (LSD) string sort. Consider the following motivating application: Suppose that a highway engineer sets up a device that records the license plate numbers of all vehicles using a busy highway for a given period of time and wants to know the number of <em>different</em> vehicles that used the highway. As you know from <a href="ch02.html#ch02sec1lev1"><small>SECTION 2.1</small></a>, one easy way to solve this problem is to sort the numbers, then make a pass through to count the different values, as in <code>Dedup</code> (page <a href="ch03a.html#page_490">490</a>). License plates are a mixture of numbers and letters, so it is natural to represent them as strings. In the simplest situation (such as the California license plate examples at right) the strings all have the same number of characters. This situation is often found in sort applications—for example, telephone numbers, bank account numbers, and IP addresses are typically fixed-length strings.</p>
<p class="image"><img alt="image" src="graphics/05_07-lsd0.jpg"/></p>
<p>Sorting such strings can be done with key-indexed counting, as shown in <a href="#ch05sb04"><small>ALGORITHM 5.1</small></a> (<code>LSD</code>) and the example below it on the facing page. If the strings are each of length <em>W</em>, we sort the strings <em>W</em> times with key-indexed counting, using each of the positions as the key, proceeding from right to left. It is not easy, at first, to be convinced that the method produces a sorted array—in fact, it does not work at all unless the key-indexed count implementation is stable. Keep this fact in mind and refer to the example when studying this proof of correctness:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb03"/></p>
<p><strong>Proposition B.</strong> LSD string sort stably sorts fixed-length strings.</p>
<p><strong>Proof:</strong> This fact depends crucially on the key-indexed counting implementation being <em>stable</em>, as indicated in <a href="#ch05sb02"><small>PROPOSITION A</small></a>. After sorting keys on their <code>i</code> trailing characters (in a stable manner), we know that any two keys appear in proper order in the array (considering just those characters) either because the first of their <code>i</code> trailing characters is different, in which case the sort on that character puts them in order, or because the first of their <code>i</code>th trailing characters is the same, in which case they are in order because of stability (and by induction, for <code>i-1</code>).</p>
<hr/>
</div>
<p>Another way to state the proof is to think about the future: if the characters that have not been examined for a pair of keys are identical, any difference between the keys is restricted to the characters already examined, so the keys have been properly ordered and will remain so because of stability. If, on the other hand, the characters that have not <a id="page_708"/>been examined are different, the characters already examined do not matter, and a later pass will correctly order the pair based on the more significant differences.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb04"/></p>
<h3><a id="page_707"/>Algorithm 5.1 LSD string sort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0707-01.jpg"/></p>
<p>To sort an array <code>a[]</code> of strings that each have exactly <code>W</code> characters, we do <code>W</code> key-indexed counting sorts: one for each character position, proceeding from right to left.</p>
<p class="image"><img alt="image" src="graphics/05_08-lsdexample.jpg"/></p>
<hr/>
</div>
<p>LSD radix sorting is the method used by the old punched-card-sorting machines that were developed at the beginning of the 20th century and thus predated the use of computers in commercial data processing by several decades. Such machines had the capability of distributing a deck of punched cards among 10 bins, according to the pattern of holes punched in the selected columns. If a deck of cards had numbers punched in a particular set of columns, an operator could sort the cards by running them through the machine on the rightmost digit, then picking up and stacking the output decks in order, then running them through the machine on the next-to-rightmost digit, and so forth, until getting to the first digit. The physical stacking of the cards is a stable process, which is mimicked by key-indexed counting sort. Not only was this version of LSD radix sorting important in commercial applications up through the 1970s, but it was also used by many cautious programmers (and students!), who would have to keep their programs on punched cards (one line per card) and would punch sequence numbers in the final few columns of a program deck so as to be able to put the deck back in order mechanically if it were accidentally dropped. This method is also a neat way to sort a deck of playing cards: deal them into thirteen piles (one for each value), pick up the piles in order, then deal into four piles (one for each suit). The (stable) dealing process keeps the cards in order within each suit, so picking up the piles in suit order yields a sorted deck.</p>
<p class="image"><img alt="image" src="graphics/05_09-lsdplay.jpg"/></p>
<p>In many string-sorting applications (even license plates, for some states), the keys are not all be the same length. It is possible to adapt LSD string sort to work for such applications, but we leave this task for exercises because we will next consider two other methods that are specifically designed for variable-length keys.</p>
<p>From a theoretical standpoint, LSD string sort is significant because it is a <em>linear</em>-time sort for typical applications. No matter how large the value of <em>N</em>, it makes <em>W</em> passes through the data. Specifically:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb05"/></p>
<p><a id="page_709"/><strong>Proposition B (continued).</strong> LSD string sort uses ~7<em>WN</em> + 3<em>WR</em> array accesses and extra space proportional to <em>N</em> + <em>R</em> to sort <em>N</em> items whose keys are <em>W</em>-character strings taken from an <em>R</em>-character alphabet.</p>
<p><strong>Proof:</strong> The method is <em>W</em> passes of key-indexed counting, except that the <code>aux[]</code> array is initialized just once. The total is immediate from the code and <a href="#ch05sb02"><small>PROPOSITION A</small></a>.</p>
<hr/>
</div>
<p>For typical applications, <em>R</em> is far smaller than <em>N</em>, so <a href="#ch05sb03"><small>PROPOSITION B</small></a> implies that the total running time is proportional to <em>WN</em>. An input array of <em>N</em> strings that each have <em>W</em> characters has a total of <em>WN</em> characters, so the running time of LSD string sort is <em>linear</em> in the size of the input.</p>
<p><a id="ch05sec2lev12"/></p>
<h4><a id="page_710"/>MSD string sort</h4>
<p>To implement a general-purpose string sort, where strings are not necessarily all the same length, we consider the characters in left-to-right order. We know that strings that start with <code>a</code> should appear before strings that start with <code>b</code>, and so forth. The natural way to implement this idea is a recursive method known as <em>most-significant-digit-first</em> (MSD) string sort. We use key-indexed counting to sort the strings according to their first character, then (recursively) sort the subarrays corresponding to each character (excluding the first character, which we know to be the same for each string in each subarray). Like quicksort, MSD string sort partitions the array into subarrays that can be sorted independently to complete the job, but it partitions the array into one subarray for each possible value of the first character, instead of the two or three partitions in quicksort.</p>
<p class="image"><img alt="image" src="graphics/05_10-msdplay.jpg"/></p>
<p><a id="ch05sec3lev5"/></p>
<h5><em>End-of-string convention</em></h5>
<p>We need to pay particular attention to reaching the ends of strings in MSD string sort. For a proper sort, we need the subarray for strings whose characters have all been examined to appear as the first subarray, and we do not want to recursively sort this subarray. To facilitate these two parts of the computation we use a private two-argument <code>charAt()</code> method to convert from an indexed string character to an array index that returns <code>-1</code> if the specified character position is past the end of the string. Then, we just add <code>1</code> to each returned value, to get a nonnegative <code>int</code> that we can use to index <code>count[]</code>. This convention means that we have <code>R+1</code> different possible character values at each string position: <code>0</code> to signify <em>end of string</em>, <code>1</code> for the first alphabet character, <code>2</code> for the second alphabet character, and so forth. Since <a id="page_711"/>key-indexed counting already needs one extra position, we use the code <code>int count[] = new int[R+2];</code> to create the array of frequency counts (and set all of its values to <code>0</code>). <em>Note</em>: Some languages, notably C and C++, have a built-in end-of-string convention, so our code needs to be adjusted accordingly for such languages.</p>
<p class="image"><img alt="image" src="graphics/05_11-msdoverview.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_12-msd0.jpg"/></p>
<p><small>WITH THESE PREPARATIONS</small>, the implementation of MSD string sort, in <a href="#ch05sb06"><small>ALGORITHM 5.2</small></a>, requires very little new code. We add a test to cutoff to insertion sort for small subarrays (using a specialized insertion sort that we will consider later), and we add a loop to key-indexed counting to do the recursive calls. As summarized in the table at the bottom of this page, the values in the <code>count[]</code> array (after serving to count the frequencies, transform counts to indices, and distribute the data) give us precisely the information that we need to (recursively) sort the subarrays corresponding to each character value.</p>
<p><a id="ch05sec3lev6"/></p>
<h5><em>Specified alphabet</em></h5>
<p>The cost of MSD string sort depends strongly on the number of possible characters in the alphabet. It is easy to modify our sort method to take an <code>Alphabet</code> as argument, to allow for improved efficiency in clients involving strings taken from relatively small alphabets. The following changes will do the job:</p>
<p class="indenthangingB">• Save the alphabet in an instance variable <code>alpha</code> in the constructor.</p>
<p class="indenthangingB">• Set <code>R</code> to <code>alpha.R()</code> in the constructor.</p>
<p class="indenthangingB">• Replace <code>s.charAt(d)</code> with <code>alpha.toIndex(s.charAt(d))</code> in <code>charAt()</code>.</p>
<p class="image"><img alt="image" src="graphics/t0711-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb06"/></p>
<h3><a id="page_712"/>Algorithm 5.2 MSD string sort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0712-01.jpg"/></p>
<p>To sort an array <code>a[]</code> of strings, we sort them on their first character using key-indexed counting, then (recursively) sort the subarrays corresponding to each first-character value.</p>
<hr/>
</div>
<p><a id="page_713"/>In our running examples, we use strings made up of lowercase letters. It is also easy to extend LSD string sort to provide this feature, but typically with much less impact on performance than for MSD string sort.</p>
<p><small>THE CODE IN</small> <a href="#ch05sb06"><small><small>ALGORITHM 5.2</small></small></a> is deceptively simple, masking a rather sophisticated computation. It is definitely worth your while to study the trace of the top level at the bottom of this page and the trace of recursive calls on the next page, to be sure that you understand the intricacies of the algorithm. This trace uses a cutoff-for-small-subarrays threshold value (<code>M</code>) of 0, so that you can see the sort to completion for this small example. The strings in this example are taken from <code>Alphabet.LOWERCASE</code>, with <code>R = 26</code>; bear in mind that typical applications might use <code>Alphabet.EXTENDED.ASCII</code>, with <code>R = 256</code>, or <code>Alphabet.UNICODE</code>, with <code>R = 65536</code>. For large alphabets, MSD string sort is so simple as to be dangerous—improperly used, it can consume outrageous amounts of time and space. Before considering performance characteristics in detail, we shall discuss three important issues (all of which we have considered before, in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>) that must be addressed in any application.</p>
<p><a id="ch05sec3lev7"/></p>
<h5><em>Small subarrays</em></h5>
<p>The basic idea behind MSD string sort is quite effective: in typical applications, the strings will be in order after examining only a few characters in the key. Put another way, the method quickly divides the array to be sorted into small <a id="page_714"/>subarrays. But this is a double-edged sword: we are certain to have to handle huge numbers of tiny subarrays, so we had better be sure that we handle them efficiently. <em>Small subarrays are of critical importance in the performance of MSD string sort.</em> We have seen this situation for other recursive sorts (quicksort and mergesort), but it is much more dramatic for MSD string sort. For example, suppose that you are sorting millions of ASCII strings (<em>R</em> = 256) that are all different, with no cutoff for small subarrays. Each string eventually finds its way to its own subarray, so you will sort millions of subarrays of size 1. But each such sort involves initializing the 258 entries of the <code>count[]</code> array to 0 and transforming them all to indices. This cost is likely to dominate the rest of the sort. With Unicode (<em>R</em> = 65536) the sort might be <em>thousands</em> of times slower. Indeed, many unsuspecting sort clients have seen their running times explode from minutes to <a id="page_715"/>hours on switching from ASCII to Unicode, for precisely this reason. Accordingly, the switch to insertion sort for small subarrays is a <em>must</em> for MSD string sort. To avoid the cost of reexamining characters that we know to be equal, we use the version of insertion sort given at the top of the page, which takes an extra argument <code>d</code> and assumes that the first <code>d</code> characters of all the strings to be sorted are known to be equal. The efficiency of this code depends on <code>substring()</code> being a constant-time operation. As with quicksort and mergesort, most of the benefit of this improvement is achieved with a small value of the cutoff, but the savings here are much more dramatic. The diagram at right shows the results of experiments where using a cutoff to insertion sort for subarrays of size 10 or less decreases the running time by a factor of 10 for a typical application.</p>
<p class="image"><img alt="image" src="graphics/05_13-msdcount.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_14-msdexample.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0715-01.jpg"/></p>
<p><a id="ch05sec3lev8"/></p>
<h5><em>Equal keys</em></h5>
<p>A second pitfall for MSD string sort is that it can be relatively slow for subarrays containing large numbers of equal keys. If a substring occurs sufficiently often that the cutoff for small subarrays does not apply, then a recursive call is needed for every character in all of the equal keys. Moreover, key-indexed counting is an inefficient way to determine that the characters are all equal: not only does each character need to be examined and each string moved, but all the counts have to be initialized, converted to indices, and so forth. Thus, the worst case for MSD string sorting is when all keys are equal. The same problem arises when large numbers of keys have long common prefixes, a situation often found in applications.</p>
<p class="image"><img alt="image" src="graphics/05_15-msdoptcutoff.jpg"/></p>
<p><a id="ch05sec3lev9"/></p>
<h5><a id="page_716"/><em>Extra space</em></h5>
<p>To do the partitioning, MSD uses two auxiliary arrays: the temporary array for distributing keys (<code>aux[]</code>) and the array that holds the counts that are transformed into partition indices (<code>count[]</code>). The <code>aux[]</code> array is of size <em>N</em> and can be created outside the recursive <code>sort()</code> method. This extra space can be eliminated by sacrificing stability (see <a href="#ch05qa1q17"><small>EXERCISE 5.1.17</small></a>), but it is often not a major concern in practical applications of MSD string sort. Space for the <code>count[]</code> array, on the other hand, can be an important issue (because it <em>cannot</em> be created outside the recursive <code>sort()</code> method) as addressed in <a href="#ch05sb08"><small>PROPOSITION D</small></a> below.</p>
<p><a id="ch05sec3lev10"/></p>
<h5><em>Random string model</em></h5>
<p>To study the performance of MSD string sort, we use a <em>random string model</em>, where each string consists of (independently) random characters, with no bound on their length. Long equal keys are essentially ignored, because they are extremely unlikely. The behavior of MSD string sort in this model is similar to its behavior in a model where we consider random fixed-length keys and also to its performance for typical real data; in all three, MSD string sort tends to examine just a few characters at the beginning of each key, as we will see.</p>
<p class="image"><img alt="image" src="graphics/05_16-msdcost.jpg"/></p>
<p><a id="ch05sec3lev11"/></p>
<h5><em>Performance</em></h5>
<p>The running time of MSD string sort depends on the data. For compare-based methods, we were primarily concerned with the <em>order</em> of the keys; for MSD string sort, the order of the keys is immaterial, but we are concerned with the <em>values</em> of the keys.</p>
<p class="indenthangingB">• For <em>random</em> inputs, MSD string sort examines just enough characters to distinguish among the keys, and the running time is <em>sublinear</em> in the number of characters in the data (it examines a small fraction of the input characters).</p>
<p class="indenthangingB">• For <em>nonrandom</em> inputs, MSD string sort still could be sublinear but might need to examine more characters than in the random case, depending on the data. In particular, it has to examine all the characters in equla keys, so the running time is nearly linear in the number of characters in the data when significant numbers of equal keys are present.</p>
<p class="indenthangingB">• In the <em>worst</em> case, MSD string sort examines all the characters in the keys, so the running time is <em>linear</em> in the number of characters in the data (like LSD string sort). A worst-case input is one with all strings equal.</p>
<p><a id="page_717"/>Some applications involve distinct keys that are well-modeled by the random string model; others have significant numbers of equal keys or long common prefixes, so the sort time is closer to the worst case. Our license-plate-processing application, for example, can fall anywhere between these extremes: if our engineer takes an hour of data from a busy interstate, there will not be many duplicates and the random model will apply; for a week’s worth of data on a local road, there will be numerous duplicates and performance will be closer to the worst case.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb07"/></p>
<p><strong>Proposition C.</strong> To sort <em>N</em> random strings from an <em>R-</em>character alphabet, MSD string sort examines about <em>N</em> log<em><sub>R</sub> N</em> characters, on average.</p>
<p><strong>Proof sketch:</strong> We expect the subarrays to be all about the same size, so the recurrence <em>C<sub>N</sub></em> = <em>RC<sub>N</sub></em><sub>/</sub><em><sub>R</sub></em> + <em>N</em> approximately describes the performance, which leads to the stated result, generalizing our argument for quicksort in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>. Again, this description of the situation is not entirely accurate, because <em>N</em>/<em>R</em> is not necessarily an integer, and the subarrays are the same size only on the average (and because the number of characters in real keys is finite). These effects turn out to be less significant for MSD string sort than for standard quicksort, so the leading term of the running time is the solution to this recurrence. The detailed analysis that proves this fact is a classical example in the analysis of algorithms, first done by Knuth in the early 1970s.</p>
<hr/>
</div>
<p>As food for thought and to indicate why the proof is beyond the scope of this book, note that key length does not play a role. Indeed, the random-string model allows key length to approach infinity. There is a nonzero probability that two keys will match for any specified number of characters, but this probability is so small as to not play a role in our performance estimates.</p>
<p>As we have discussed, the number of characters examined is not the full story for MSD string sort. We also have to take into account the time and space required to count frequencies and turn the counts into indices.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb08"/></p>
<p><strong>Proposition D.</strong> MSD string sort uses between 8<em>N</em> + 3<em>R</em> and ~7<em>wN</em> + 3<em>WR</em> array accesses to sort <em>N</em> strings taken from an <em>R</em>-character alphabet, where <em>w</em> is the average string length.</p>
<p><strong>Proof:</strong> Immediate from the code, <a href="#ch05sb02"><small>PROPOSITION A</small></a>, and <a href="#ch05sb03"><small>PROPOSITION B</small></a>. In the best case MSD sort uses just one pass; in the worst case, it performs like LSD string sort.</p>
<hr/>
</div>
<p><a id="page_718"/>When <em>N</em> is small, the factor of <em>R</em> dominates. Though precise analysis of the total cost becomes difficult and complicated, you can estimate the effect of this cost just by considering small subarrays when keys are distinct. With no cutoff for small subarrays, each key appears in its own subarray, so <em>NR</em> array accesses are needed for just these subarrays. If we cut off to small subarrays of size <em>M</em>, we have about <em>N</em>/<em>M</em> subarrays of size <em>M</em>, so we are trading off <em>NR</em>/<em>M</em> array accesses with <em>NM</em>/4 compares, which tells us that we should choose <em>M</em> to be proportional to the square root of <em>R</em>.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb09"/></p>
<p><strong>Proposition D (continued).</strong> To sort <em>N</em> strings taken from an <em>R</em>-character alphabet, the amount of space needed by MSD string sort is proportional to <em>R</em> times the length of the longest string (plus <em>N</em>), in the worst case.</p>
<p><strong>Proof:</strong> The <code>count[]</code> array must be created within <code>sort()</code>, so the total amount of space needed is proportional to <em>R</em> times the depth of recursion (plus <em>N</em> for the auxiliary array). Precisely, the depth of the recursion is the length of the longest string that is a prefix of two or more of the strings to be sorted.</p>
<hr/>
</div>
<p>As just discussed, equal keys cause the depth of the recursion to be proportional to the length of the keys. The immediate practical lesson to be drawn from <a href="#ch05sb08"><small>PROPOSITION D</small></a> is that it is quite possible for MSD string sort to run out of time or space when sorting long strings taken from large alphabets, particularly if long equal keys are to be expected. For example, with <code>Alphabet.UNICODE</code> and more than <code>M</code> equal 1,000-character strings, <code>MSD.sort()</code> would require space for over 65 million counters!</p>
<p><small>THE MAIN CHALLENGE</small> in getting maximum efficiency from MSD string sort on keys that are long strings is to deal with lack of randomness in the data. Typically, keys may have long stretches of equal data, or parts of them might fall in only a narrow range. For example, an information-processing application for student data might have keys that include graduation year (4 bytes, but one of four different values), state names (perhaps 10 bytes, but one of 50 different values), and gender (1 byte with one of two given values), as well as a person’s name (more similar to random strings, but probably not short, with nonuniform letter distributions, and with trailing blanks in a fixed-length field). Restrictions like these lead to large numbers of empty subarrays during the MSD string sort. Next, we consider a graceful way to adapt to such situations.</p>
<p><a id="ch05sec2lev13"/></p>
<h4><a id="page_719"/>Three-way string quicksort</h4>
<p>We can also adapt quicksort to MSD string sorting by using 3-way partitioning on the leading character of the keys, moving to the next character on only the middle subarray (keys with leading character equal to the partitioning character). This method is not difficult to implement, as you can see in <a href="#ch05sb10"><small>ALGORITHM 5.3</small></a>: we just add an argument to the recursive method in <a href="ch02.html#ch02sb21"><small>ALGORITHM 2.5</small></a> that keeps track of the current character, adapt the 3-way partitioning code to use that character, and appropriately modify the recursive calls.</p>
<p class="image"><img alt="image" src="graphics/05_18-q3overview.jpg"/></p>
<p>Although it does the computation in a different order, 3-way string quicksort amounts to sorting the array on the leading characters of the keys (using quicksort), then applying the method recursively on the remainder of the keys. For sorting strings, the method compares favorably with normal quicksort and with MSD string sort. Indeed, it is a hybrid of these two algorithms.</p>
<p>Three-way string quicksort divides the array into only three parts, so it involves more data movement than MSD string sort when the number of nonempty partitions is large because it has to do a series of 3-way partitions to get the effect of the multiway partition. On the other hand, MSD string sort can create large numbers of (empty) subarrays, whereas 3-way string quicksort always has just three. Thus, 3-way string quicksort adapts well to handling equal keys, keys with long common prefixes, keys that fall into a small range, and small arrays—all situations where MSD string sort runs slowly. Of particular importance is that the <a id="page_721"/>partitioning adapts to different kinds of structure in different parts of the key. Also, like quicksort, 3-way string quicksort does not use extra space (other than the implicit stack to support recursion), which is an important advantage over MSD string sort, which requires space for both frequency counts and an auxiliary array.</p>
<p class="image"><img alt="image" src="graphics/05_17-q30.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb10"/></p>
<h3><a id="page_720"/>Algorithm 5.3 Three-way string quicksort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0720-01.jpg"/></p>
<p>To sort an array <code>a[]</code> of strings, we 3-way partition them on their first character, then (recursively) sort the three resulting subarrays: the strings whose first character is less than the partitioning character, the strings whose first character is equal to the partitioning character (excluding their first character in the sort), and the strings whose first character is greater than the partitioning character.</p>
<hr/>
</div>
<p>The figure at the bottom of this page shows all of the recursive calls that <code>Quick3string</code> makes for our example. Each subarray is sorted using precisely three recursive calls, except when we skip the recursive call on reaching the ends of the (equal) string(s) in the middle subarray.</p>
<p>As usual, in practice, it is worthwhile to consider various standard improvements to the implementation in <a href="#ch05sb10"><small>ALGORITHM 5.3</small></a>:</p>
<p><a id="ch05sec3lev12"/></p>
<h5><em>Small subarrays</em></h5>
<p>In any recursive algorithm, we can gain efficiency by treating small subarrays differently. In this case, we use the insertion sort from page <a href="#page_715">715</a>, which skips the characters that are known to be equal. The improvement due to this change is likely to be significant, though not nearly as important as for MSD string sort.</p>
<p><a id="ch05sec3lev13"/></p>
<h5><em>Restricted alphabet</em></h5>
<p>To handle specialized alphabets, we could add an <code>Alphabet</code> argument <code>alpha</code> to each of the methods and replace <code>s.charAt(d)</code> with <code>alpha.toIndex(s.charAt(d))</code> in <code>charAt()</code>. In this case, there is no benefit to doing so, and adding this code is likely to substantially slow the algorithm down because this code is in the inner loop.</p>
<p class="image"><img alt="image" src="graphics/05_19-q3summary.jpg"/></p>
<p><a id="ch05sec3lev14"/></p>
<h5><a id="page_722"/><em>Randomization</em></h5>
<p>As with any quicksort, it is generally worthwhile to shuffle the array beforehand or to use a random paritioning item by swapping the first item with a random one. The primary reason to do so is to protect against worst-case performance in the case that the array is already sorted or nearly sorted.</p>
<p>For string keys, standard quicksort and all the other sorts in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a> are actually MSD string sorts, because the <code>compareTo()</code> method in <code>String</code> accesses the characters in left-to-right order. That is, <code>compareTo()</code> accesses only the leading characters if they are different, the leading two characters if the first characters are the same and the second different, and so forth. For example, if the first characters of the strings are all different, the standard sorts will examine just those characters, thus automatically realizing some of the same performance gain that we seek in MSD string sorting. The essential idea behind 3-way quicksort is to take special action when the leading characters are equal. Indeed, one way to think of <a href="#ch05sb10"><small>ALGORITHM 5.3</small></a> is as a way for standard quicksort to keep track of leading characters that are known to be equal. In the small subarrays, where most of the compares in the sort are done, the strings are likely to have numerous equal leading characters. The standard algorithm has to scan over all those characters for each compare; the 3-way algorithm avoids doing so.</p>
<p><a id="ch05sec3lev15"/></p>
<h5><em>Performance</em></h5>
<p>Consider a case where the string keys are long (and are all the same length, for simplicity), but most of the leading characters are equal. In such a situation, the running time of standard quicksort is proportional to the string length <em>times</em> 2<em>N</em> ln <em>N</em>, whereas the running time of 3-way string quicksort is proportional to <em>N</em> times the string length (to discover all the leading equal characters) <em>plus</em> 2<em>N</em> ln <em>N</em> character comparisons (to do the sort on the remaining short keys). That is, 3-way string quicksort requires up to a factor of 2ln<em>N</em> fewer character compares than normal quicksort. It is not unusual for keys in practical sorting applications to have characteristics similar to this artificial example.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb11"/></p>
<p><a id="page_723"/><strong>Proposition E.</strong> To sort an array of <em>N</em> random strings, 3-way string quicksort uses <em>~</em>2<em>N</em>ln <em>N</em> character compares, on the average.</p>
<p><strong>Proof:</strong> There are two instructive ways to understand this result. First, considering the method to be equivalent to quicksort partitioning on the leading character, then (recursively) using the same method on the subarrays, we should not be surprised that the total number of operations is about the same as for normal quicksort—but they are single-character compares, not full-key compares. Second, considering the method as replacing key-indexed counting by quicksort, we expect that the <em>N</em> log<em><sub>R</sub> N</em> running time from <a href="#ch05sb08"><small>PROPOSITION D</small></a> should be multiplied by a factor of 2 ln <em>R</em> because it takes quicksort 2<em>R</em> ln <em>R</em> steps to sort <em>R</em> characters, as opposed to <em>R</em> steps for the same characters in the MSD string sort. We omit the full proof.</p>
<hr/>
</div>
<p>As emphasized on page <a href="#page_716">716</a>, considering random strings is instructive, but more detailed analysis is needed to predict performance for practical situations. Researchers have studied this algorithm in depth and have proved that no algorithm can beat 3-way string quicksort (measured by number of character compares) by more than a constant factor, under very general assumptions. To appreciate its versatility, note that 3-way string quicksort has no direct dependencies on the size of the alphabet.</p>
<p><a id="ch05sec3lev16"/></p>
<h5><em>Example: web logs</em></h5>
<p>As an example where 3-way string quicksort shines, we can consider a typical modern data-processing task. Suppose that you have built a website and want to analyze the traffic that it generates. You can have your system administrator supply you with a web log of all transactions on your site. Among the information associated with a transaction is the domain name of the originating machine. For example, the file <code>week.log.txt</code> on the booksite is a log of one week’s transactions on our booksite. Why does 3-way string quicksort do well on such a file? Because the sorted result is replete with long common prefixes that this method does not have to reexamine.</p>
<p><a id="ch05sec2lev14"/></p>
<h4><a id="page_724"/>Which string-sorting algorithm should I use?</h4>
<p>Naturally, we are interested in how the string-sorting methods that we have considered compare to the general-purpose methods that we considered in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>. The following table summarizes the important characteristics of the string-sort algorithms that we have discussed in this section (the rows for quicksort, mergesort, and 3-way quicksort are included from <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>, for comparison).</p>
<p class="image"><img alt="image" src="graphics/t0724-01.jpg"/></p>
<p>As in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>, multiplying these growth rates by appropriate algorithm- and data-dependent constants gives an effective way to predict running time.</p>
<p>As explored in the examples that we have already considered and in many other examples in the exercises, different specific situations call for different methods, with appropriate parameter settings. In the hands of an expert (maybe that’s you, by now), dramatic savings can be realized for certain situations.</p>
<p><a id="ch05sec2lev15"/></p>
<h4><a id="page_725"/>Q&amp;A</h4>
<p><strong>Q.</strong> Does the Java system sort use one of these methods for <code>String</code> sorts?</p>
<p><strong>A.</strong> No, but the standard implementation includes a fast string compare that makes standard sorts competitive with the methods considered here.</p>
<p><strong>Q.</strong> So, I should just use the system sort for <code>String</code> keys?</p>
<p><strong>A.</strong> Probably yes in Java, though if you have huge numbers of strings or need an exceptionally fast sort, you may wish to switch to <code>char</code> arrays instead of <code>String</code> values and use a radix sort.</p>
<p><strong>Q.</strong> What is explanation of the log<sup>2</sup><em>N</em> factors on the table in the previous page?</p>
<p><strong>A.</strong> They reflect the idea that most of the comparisons for these algorithms wind up being between keys with a common prefix of length log <em>N.</em> Recent research has established this fact for random strings with careful mathematical analysis (see booksite for reference).</p>
<p><a id="ch05sec2lev16"/></p>
<h4><a id="page_726"/>Exercises</h4>
<p><a id="ch05qa1q1"/><strong>5.1.1</strong> Develop a sort implementation that counts the number of different key values, then uses a symbol table to apply key-indexed counting to sort the array. (This method is <em>not</em> for use when the number of different key values is large.)</p>
<p><a id="ch05qa1q2"/><strong>5.1.2</strong> Give a trace for LSD string sort for the keys</p>
<p class="programlisting">no is th ti fo al go pe to co to th ai of th pa</p>
<p><a id="ch05qa1q3"/><strong>5.1.3</strong> Give a trace for MSD string sort for the keys</p>
<p class="programlisting">no is th ti fo al go pe to co to th ai of th pa</p>
<p><a id="ch05qa1q4"/><strong>5.1.4</strong> Give a trace for 3-way string quicksort for the keys</p>
<p class="programlisting">no is th ti fo al go pe to co to th ai of th pa</p>
<p><a id="ch05qa1q5"/><strong>5.1.5</strong> Give a trace for MSD string sort for the keys</p>
<p class="programlisting">now is the time for all good people to come to the aid of</p>
<p><a id="ch05qa1q6"/><strong>5.1.6</strong> Give a trace for 3-way string quicksort for the keys</p>
<p class="programlisting">now is the time for all good people to come to the aid of</p>
<p><a id="ch05qa1q7"/><strong>5.1.7</strong> Develop an implementation of key-indexed counting that makes use of an array of <code>Queue</code> objects.</p>
<p><a id="ch05qa1q8"/><strong>5.1.8</strong> Give the number of characters examined by MSD string sort and 3-way string quicksort for a file of <code>N</code> keys <code>a</code>, <code>aa</code>, <code>aaa</code>, <code>aaaa</code>, <code>aaaaa</code>, . . .</p>
<p><a id="ch05qa1q9"/><strong>5.1.9</strong> Develop an implementation of LSD string sort that works for variable-length strings.</p>
<p><a id="ch05qa1q10"/><strong>5.1.10</strong> What is the total number of characters examined by 3-way string quicksort when sorting <em>N</em> fixed-length strings (all of length <em>W</em>), in the worst case?</p>
<p><a id="ch05sec2lev17"/></p>
<h4><a id="page_727"/>Creative Problems</h4>
<p><a id="ch05qa1q11"/><strong>5.1.11</strong> <em>Queue sort.</em> Implement MSD string sorting using queues, as follows: Keep one queue for each bin. On a first pass through the items to be sorted, insert each item into the appropriate queue, according to its leading character value. Then, sort the sublists and stitch together all the queues to make a sorted whole. Note that this method does not involve keeping the <code>count[]</code> arrays within the recursive method.</p>
<p><a id="ch05qa1q12"/><strong>5.1.12</strong> <em>Alphabet.</em> Develop an implementation of the <code>Alphabet</code> API that is given on page <a href="#ch05sec1lev6">698</a> and use it to develop LSD and MSD sorts for general alphabets.</p>
<p><a id="ch05qa1q13"/><strong>5.1.13</strong> <em>Hybrid sort.</em> Investigate the idea of using standard MSD string sort for large arrays, in order to get the advantage of multiway partitioning, and 3-way string quicksort for smaller arrays, in order to avoid the negative effects of large numbers of empty bins.</p>
<p><a id="ch05qa1q14"/><strong>5.1.14</strong> <em>Array sort.</em> Develop a method that uses 3-way string quicksort for keys that are <em>arrays</em> of <code>int</code> values.</p>
<p><a id="ch05qa1q15"/><strong>5.1.15</strong> <em>Sublinear sort.</em> Develop a sort implementation for <code>int</code> values that makes two passes through the array to do an LSD sort on the leading 16 bits of the keys, then does an insertion sort.</p>
<p><a id="ch05qa1q16"/><strong>5.1.16</strong> <em>Linked-list sort.</em> Develop a sort implementation that takes a linked list of nodes with <code>String</code> key values as argument and rearranges the nodes so that they appear in sorted order (returning a link to the node with the smallest key). Use 3-way string quicksort.</p>
<p><a id="ch05qa1q17"/><strong>5.1.17</strong> <em>In-place key-indexed counting.</em> Develop a version of key-indexed counting that uses only a constant amount of extra space. Prove that your version is stable or provide a counterexample.</p>
<p><a id="ch05sec2lev18"/></p>
<h4><a id="page_728"/>Experiments</h4>
<p><a id="ch05qa1q18"/><strong>5.1.18</strong> <em>Random decimal keys.</em> Write a static method <code>randomDecimalKeys</code> that takes <code>int</code> values <code>N</code> and <code>W</code> as arguments and returns an array of <code>N</code> string values that are each <code>W</code>-digit decimal numbers.</p>
<p><a id="ch05qa1q19"/><strong>5.1.19</strong> <em>Random CA license plates.</em> Write a static method <code>randomPlatesCA</code> that takes an <code>int</code> value <code>N</code> as argument and returns an array of <code>N String</code> values that represent CA license plates as in the examples in this section.</p>
<p><a id="ch05qa1q20"/><strong>5.1.20</strong> <em>Random fixed-length words.</em> Write a static method <code>randomFixedLengthWords</code> that takes <code>int</code> values <code>N</code> and <code>W</code> as arguments and returns an array of <code>N</code> string values that are each strings of <code>W</code> characters from the alphabet.</p>
<p><a id="ch05qa1q21"/><strong>5.1.21</strong> <em>Random items.</em> Write a static method <code>randomItems</code> that takes an <code>int</code> value <code>N</code> as argument and returns an array of <code>N</code> string values that are each strings of length between 15 and 30 made up of three fields: a 4-character field with one of a set of 10 fixed strings; a 10-char field with one of a set of 50 fixed strings; a 1-character field with one of two given values; and a 15-byte field with random left-justified strings of letters equally likely to be 4 through 15 characters long.</p>
<p><a id="ch05qa1q22"/><strong>5.1.22</strong> <em>Timings.</em> Compare the running times of MSD string sort and 3-way string quicksort, using various key generators. For fixed-length keys, include LSD string sort.</p>
<p><a id="ch05qa1q23"/><strong>5.1.23</strong> <em>Array accesses.</em> Compare the number of array accesses used by MSD string sort and 3-way string sort, using various key generators. For fixed-length keys, include LSD string sort.</p>
<p><a id="ch05qa1q24"/><strong>5.1.24</strong> <em>Rightmost character accessed.</em> Compare the position of the rightmost character accessed for MSD string sort and 3-way string quicksort, using various key generators.</p>
<p><a id="ch05sec1lev8"/></p>
<h3><a id="page_730"/>5.2 Tries</h3>
<p>As with sorting, we can take advantage of properties of strings to develop search methods (symbol-table implementations) that can be more efficient than the general-purpose methods of <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> for typical applications where search keys are strings.</p>
<p>Specifically, the methods that we consider in this section achieve the following performance characteristics in typical applications, even for huge tables:</p>
<p class="indenthangingB">• Search hits take time proportional to the length of the search key.</p>
<p class="indenthangingB">• Search misses involve examining only a few characters.</p>
<p>On reflection, these performance characteristics are quite remarkable, one of the crowning achievements of algorithmic technology and a primary factor in enabling the development of the computational infrastructure we now enjoy that has made so much information instantly accessible. Moreover, we can extend the symbol-table API to include character-basedoperations defined for string keys (but not necessarily for all <code>Comparable</code> types of keys) that are powerful and quite useful in practice, as in the following API:</p>
<p class="image"><img alt="image" src="graphics/t0730-01.jpg"/></p>
<p><a id="page_731"/>This API differs from the symbol-table API introduced in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> in the following aspects:</p>
<p class="indenthangingB">• We replace the generic type <code>Key</code> with the concrete type <code>String</code>.</p>
<p class="indenthangingB">• We add three new methods, <code>longestPrefixOf()</code>, <code>keysWithPrefix()</code> and <code>keysThatMatch()</code>.</p>
<p>We retain the basic conventions of our symbol-table implementations in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> (no duplicate or null keys and no null values).</p>
<p>As we saw for sorting with string keys, it is often quite important to be able to work with strings from a specified alphabet. Simple and efficient implementations that are the method of choice for small alphabets turn out to be useless for large alphabets because they consume too much space. In such cases, it is certainly worthwhile to add a constructor that allows clients to specify the alphabet. We will consider the implementation of such a constructor later in this section but omit it from the API for now, in order to concentrate on string keys.</p>
<p>The following descriptions of the three new methods use the keys <code>she sells sea shells by the sea shore</code> to give examples:</p>
<p class="indenthangingB">• <code>longestPrefixOf()</code> takes a string as argument and returns the longest key in the symbol table that is a prefix of that string. For the keys above, <code>longestPrefixOf("shell")</code> is <code>she</code> and <code>longestPrefixOf("shellsort")</code> is <code>shells</code>.</p>
<p class="indenthangingB">• <code>keysWithPrefix()</code> takes a string as argument and returns all the keys in the symbol table having that string as prefix. For the keys above, <code>keysWithPrefix("she")</code> is <code>she</code> and <code>shells</code>, and <code>keysWithPrefix("se")</code> is <code>sells</code> and <code>sea</code>.</p>
<p class="indenthangingB">• <code>keysThatMatch()</code> takes a string as argument and returns all the keys in the symbol table that match that string, in the sense that a period (.) in the argument string matches any character. For the keys above, <code>keysThatMatch(".he")</code> returns <code>she</code> and <code>the</code>, and <code>keysThatMatch("s..")</code> returns <code>she</code> and <code>sea</code>.</p>
<p>We will consider in detail implementations and applications of these operations after we have seen the basic symbol-table methods. These particular operations are representative of what is possible with string keys; we discuss several other possibilities in the exercises.</p>
<p>To focus on the main ideas, we concentrate on <code>put()</code>, <code>get()</code>, and the new methods; we assume (as in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>) default implementations of <code>contains()</code> and <code>isEmpty()</code>; and we leave implementations of <code>size()</code> and <code>delete()</code> for exercises. Since strings are <code>Comparable</code>, extending the API to also include the ordered operations defined in the ordered symbol-table API in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> is possible (and worthwhile); we leave those implementations (which are generally straightforward) to exercises and booksite code.</p>
<p><a id="ch05sec2lev19"/></p>
<h4><a id="page_732"/>Tries</h4>
<p>In this section, we consider a search tree known as a <em>trie</em>, a data structure built from the characters of the string keys that allows us to use the characters of the search key to guide the search. The name “trie” is a bit of wordplay introduced by E. Fredkin in 1960 because the data structure is used for re<em>trie</em>val, but we pronounce it “try” to avoid confusion with “tree.” We begin with a high-level description of the basic properties of tries, including search and insert algorithms, and then proceed to the details of the representation and Java implementation.</p>
<p><a id="ch05sec3lev17"/></p>
<h5><em>Basic properties</em></h5>
<p>As with search trees, tries are data structures composed of <em>nodes</em> that contain <em>links</em> that are either <em>null</em> or references to other nodes. Each node is pointed to by just one other node, which is called its <em>parent</em> (except for one node, the <em>root</em>, which has no nodes pointing to it), and each node has <em>R</em> links, where <em>R</em> is the alphabet size. Often, tries have a substantial number of null links, so when we draw a trie, we typically omit null links. Although links point to nodes, we can view each link as pointing to a trie, the trie whose root is the referenced node. Each link corresponds to a character value—since each link points to exactly one node, we label each node with the character value corresponding to the link that points to it (except for the root, which has no link pointing to it). Each node also has a corresponding <em>value</em>, which may be null or the value associated with one of the string keys in the symbol table. Specifically, we store the value associated with each key in the node corresponding to its last character. It is very important to bear in mind the following fact: <em>nodes with null values exist to facilitate search in the trie and do not correspond to keys.</em> An example of a trie is shown at right.</p>
<p class="image"><img alt="image" src="graphics/05_20-trieanatomy.jpg"/></p>
<p><a id="ch05sec3lev18"/></p>
<h5><em>Search in a trie</em></h5>
<p>Finding the value associated with a given string key in a trie is a simple process, guided by the characters in the search key. Each node in the trie has a link corresponding to each possible string character. We start at the root, then follow the link associated with the first character in the key; from that node we follow the link associated with the second character in the key; from that node we follow the link associated with the third character in the key and <a id="page_733"/>so forth, until reaching the last character of the key or a null link. At this point, one of the following three conditions holds (refer to the figure above for examples):</p>
<p class="indenthangingB">• The value at the node corresponding to the last character in the key is not <code>null</code> (as in the searches for <code>shells</code> and <code>she</code> depicted at left above). This result is a <em>search hit</em>—the value associated with the key is the value in the node corresponding to its last character.</p>
<p class="indenthangingB">• The value in the node corresponding to the last character in the key is <code>null</code> (as in the search for <code>shell</code> depicted at top right above). This result is a <em>search miss:</em> the key is not in the table.</p>
<p class="indenthangingB">• The search terminated with a null link (as in the search for <code>shore</code> depicted at bottom right above). This result is also a search miss.</p>
<p class="image"><img alt="image" src="graphics/05_21-triesearch.jpg"/></p>
<p>In all cases, the search is accomplished just by examining nodes along a path from the root to another node in the trie.</p>
<p><a id="ch05sec3lev19"/></p>
<h5><a id="page_734"/><em>Insertion into a trie</em></h5>
<p>As with binary search trees, we insert by first doing a search: in a trie that means using the characters of the key to guide us down the trie until reaching the last character of the key or a null link. At this point, one of the following two conditions holds:</p>
<p class="indenthangingB">• We encountered a null link before reaching the last character of the key. In this case, there is no trie node corresponding to the last character in the key, so we need to create nodes for each of the characters in the key not yet encountered and set the value in the last one to the value to be associated with the key.</p>
<p class="indenthangingB">• We encountered the last character of the key before reaching a null link. In this case, we set that node’s value to the value to be associated with the key (whether or not that value is null), as usual with our associative array convention.</p>
<p>In all cases, we examine or create a node in the trie for each key character. The construction of the trie for our standard indexing client from <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> with the input</p>
<p class="programlisting">she sells sea shells by the sea shore</p>
<p>is shown on the facing page.</p>
<p><a id="ch05sec3lev20"/></p>
<h5><em>Node representation</em></h5>
<p>As mentioned at the outset, our trie diagrams do not quite correspond to the data structures our programs will build, because we do not draw null links. Taking null links into account emphasizes the following important characteristics of tries:</p>
<p class="indenthangingB">• Every node has <em>R</em> links, one for each possible character.</p>
<p class="indenthangingB">• Characters and keys are <em>implicitly</em> stored in the data structure.</p>
<p>For example, the figure below depicts a trie for keys made up of lowercase letters, with each node having a value and 26 links. The first link points to a subtrie for keys beginning with <code>a</code>, the second points to a subtrie for substrings beginning with <code>b</code>, and so forth. <a id="page_736"/>Keys in the trie are implicitly represented by paths from the root that end at nodes with non-null values. For example, the string <code>sea</code> is associated with the value <code>2</code> in the trie because the 19th link in the root (which points to the trie for all keys that start with <code>s</code>) is not null and the 5th link in the node that link refers to (which points to the trie for all keys that start with <code>se</code>) is not null, and the first link in the node that link refers to (which points to the trie for all keys that starts with <code>sea</code>) has the value <code>2</code>. Neither the string <code>sea</code> nor the characters <code>s</code>, <code>e</code>, and <code>a</code> are stored in the data structure. Indeed, the data structure contains no characters or strings, just links and values. Since the parameter <em>R</em> plays such a critical role, we refer to a trie for an <em>R</em>-character alphabet as an <em>R-way trie.</em></p>
<p class="image"><img alt="image" src="graphics/05_22-trierepnew.jpg"/></p>
<p class="image"><a id="page_735"/><img alt="image" src="graphics/05_23-trietrace.jpg"/></p>
<p><small>WITH THESE PREPARATIONS</small>, the symbol-table implementation <code>TrieST</code> on the facing page is straightforward. It uses recursive methods like those that we used for search trees in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>, based on a private <code>Node</code> class with instance variable <code>val</code> for client values and an array <code>next[]</code> of <code>Node</code> references. The methods are compact recursive implementations that are worthy of careful study. Next, we discuss implementations of the constructor that takes an <code>Alphabet</code> as argument and the methods <code>size()</code>, <code>keys()</code>, <code>longestPrefixOf()</code>, <code>keysWithPrefix()</code>, <code>keysThatMatch()</code>, and <code>delete()</code>. These are also easily understood recursive methods, each slightly more complicated than the last.</p>
<p><a id="ch05sec3lev21"/></p>
<h5><em>Size</em></h5>
<p>As for the binary search trees of <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>, three straightforward options are available for implementing <code>size()</code>:</p>
<p class="indenthangingB">• An eager implementation where we maintain the number of keys in an instance variable <code>N</code>.</p>
<p class="indenthangingB">• A very eager implementation where we maintain the number of keys in a subtrie as a node instance variable that we update after the recursive calls in <code>put()</code> and <code>delete()</code>.</p>
<p class="indenthangingB">• A lazy recursive implementation like the one at right. It traverses all of the nodes in the trie, counting the number having a non-null value.</p>
<p class="image"><img alt="image" src="graphics/p0736-01.jpg"/></p>
<p>As with binary search trees, the lazy implementation is instructive but should be avoided because it can lead to performance problems for clients. The eager implementations are explored in the exercises.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb12"/></p>
<h3><a id="page_737"/>Algorithm 5.4 Trie symbol table</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0737-01.jpg"/></p>
<p>This code uses an <em>R</em>-way trie to implement a symbol table. Additional methods in the string symbol-table API of page <a href="#ch05sec1lev8">730</a> are presented in the next several pages. Modifying this code to handle keys from specialized alphabets is straighforward (see page <a href="#page_740">740</a>). The value in <code>Node</code> has to be an <code>Object</code> because Java does not support arrays of generics; we cast values back to <code>Value</code> in <code>get()</code>.</p>
<hr/>
</div>
<p><a id="ch05sec3lev22"/></p>
<h5><a id="page_738"/><em>Collecting keys</em></h5>
<p>Because characters and keys are represented implicitly in tries, providing clients with the ability to iterate through the keys presents a challenge. As with binary search trees, we accumulate the string keys in a <code>Queue</code>, but for tries we need to create explicit representations of all of the string keys, not just find them in the data structure. We do so with a recursive private method <code>collect()</code> that is similar to <code>size()</code> but also maintains a string with the sequence of characters on the path from the root. Each time that we visit a node via a call to <code>collect()</code> with that node as first argument, the second argument is the string associated with that node (the sequence of characters on the path from the root to the node). To visit a node, we add its associated string to the queue if its value is not null, then visit (recursively) all the nodes in its array of links, one for each possible character. To create the key for each call, we append the character corresponding to the link to the current key. We use this <code>collect()</code> method to collect keys for both the <code>keys()</code> and the <code>keysWithPrefix()</code> methods in the API. To implement <code>keys()</code> we call <code>keysWithPrefix()</code> with the empty string as argument; to implement <code>keysWithPrefix()</code>, we call <code>get()</code> to find the trie node corresponding to the given prefix (<code>null</code> if there is no such node), then use the <code>collect()</code> method to complete the job. The diagram at left shows a trace of <code>collect()</code> (or <code>keysWithPrefix("")</code>) for an example trie, giving the value of the second argument key and the contents of the queue for each call to <code>collect()</code>. The diagram at the top of the facing page illustrates the process for <code>keysWithPrefix("sh")</code>.</p>
<p class="image"><img alt="image" src="graphics/p0738-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_24-triekeys.jpg"/></p>
<p class="image"><a id="page_739"/><img alt="image" src="graphics/05_25-trieprefix.jpg"/></p>
<p><a id="ch05sec3lev23"/></p>
<h5><em>Wildcard match</em></h5>
<p>To implement <code>keysThatMatch()</code>, we use a similar process, but add an argument specifying the pattern to <code>collect()</code> and add a test to make a recursive call for all links when the pattern character is a wildcard or only for the link corresponding to the pattern character otherwise, as in the code below. Note also that we do not need to consider keys longer than the pattern.</p>
<p><a id="ch05sec3lev24"/></p>
<h5><em>Longest prefix</em></h5>
<p>To find the longest key that is a prefix of a given string, we use a recursive method like <code>get()</code> that keeps track of the length of the longest key found on the search path (by passing it as a parameter to the recursive method, updating the value <a id="page_740"/>whenever a node with a non-null value is encountered). The search ends when the end of the string or a null link is encountered, whichever comes first.</p>
<p class="image"><img alt="image" src="graphics/p0739-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0740-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_26-trielpo.jpg"/></p>
<p><a id="ch05sec3lev25"/></p>
<h5><em>Deletion</em></h5>
<p>The first step needed to delete a key-value pair from a trie is to use a normal search to find the node corresponding to the key and set the corresponding value to <code>null</code>. If that node has a non-null link to a child, then no more work is required; if all the links are null, we need to remove the node from the data structure. If doing so leaves all the links null in its parent, we need to remove that node, and so forth. The implementation on the facing page demonstrates that this action can be accomplished with remarkably little code, using our standard recursive setup: after the recursive calls for a node <code>x</code>, we return <code>null</code> if the client value and all of the links in a node are <code>null</code>; otherwise we return <code>x</code>.</p>
<p><a id="ch05sec3lev26"/></p>
<h5><a id="page_741"/><em>Alphabet</em></h5>
<p>As usual, <a href="#ch05sb12"><small>ALGORITHM 5.4</small></a> is coded for Java <code>String</code> keys, but it is a simple matter to modify the implementation to handle keys taken from any alphabet, as follows:</p>
<p class="indenthangingB">• Implement a constructor that takes an <code>Alphabet</code> as argument, which sets an <code>Alphabet</code> instance variable to that argument value and the instance variable <code>R</code> to the number of characters in the alphabet.</p>
<p class="indenthangingB">• Use the <code>toIndex()</code> method from <code>Alphabet</code> in <code>get()</code> and <code>put()</code> to convert string characters to indices between 0 and <em>R</em> − 1.</p>
<p class="indenthangingB">• Use the <code>toChar()</code> method from <code>Alphabet</code> to convert indices between 0 and <em>R</em> − 1 to <code>char</code> values. This operation is not needed in <code>get()</code> and <code>put()</code> but is important in the implementations of <code>keys()</code>, <code>keysWithPrefix()</code>, and <code>keysThatMatch()</code>.</p>
<p class="image"><img alt="image" src="graphics/p0741-01.jpg"/></p>
<p>With these changes, you can save a considerable amount of space (use only <em>R</em> links per node) when you know that your keys are taken from a small alphabet, at the cost of the time required to do the conversions between characters and indices.</p>
<p class="image"><img alt="image" src="graphics/05_27-triedelete.jpg"/></p>
<p><a id="page_742"/><small>THE CODE THAT WE HAVE CONSIDERED</small> is a compact and complete implementation of the string symbol-table API that has broadly useful practical applications. Several variations and extensions are discussed in the exercises. Next, we consider basic properties of tries, and some limitations on their utility.</p>
<p><a id="ch05sec2lev20"/></p>
<h4>Properties of tries</h4>
<p>As usual, we are interested in knowing the amount of time and space required to use tries in typical applications. Tries have been extensively studied and analyzed, and their basic properties are relatively easy to understand and to apply.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb13"/></p>
<p><strong>Proposition F.</strong> The linked structure (shape) of a trie is independent of the key insertion/deletion order: there is a unique trie for any given set of keys.</p>
<p><strong>Proof:</strong> Immediate, by induction on the subtries.</p>
<hr/>
</div>
<p>This fundamental fact is a distinctive feature of tries: for all of the other search tree structures that we have considered so far, the tree that we construct depends both on the set of keys and on the order in which we insert those keys.</p>
<p><a id="ch05sec3lev27"/></p>
<h5><em>Worst-case time bound for search and insert</em></h5>
<p>How long does it take to find the value associated with a key? For BSTs, hashing, and other methods in <a href="ch04.html#ch04"><small>CHAPTER 4</small></a>, we needed mathematical analysis to study this question, but for tries it is very easy to answer:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb14"/></p>
<p><strong>Proposition G.</strong> The number of array accesses when searching in a trie or inserting a key into a trie is at most 1 plus the length of the key.</p>
<p><strong>Proof:</strong> Immediate from the code. The recursive <code>get()</code> and <code>put()</code> implementations carry an argument <code>d</code> that starts at <code>0</code>, increments for each call, and is used to stop the recursion when it reaches the key length.</p>
<hr/>
</div>
<p>From a theoretical standpoint, the implication of <a href="#ch05sb14"><small>PROPOSITION G</small></a> is that tries are <em>optimal</em> for search hit—we could not expect to do better than search time proportional to the length of the search key. Whatever algorithm or data structure we are using, we cannot know that we have found a key that we seek without examining all of its characters. From a practical standpoint this guarantee is important because <em>it does not depend on the number of keys</em>: when we are working with 7-character keys like license plate numbers, we know that we need to examine at most 8 nodes to search or insert; when we are working with 20-digit account numbers, we only need to examine at most 21 nodes to search or insert.</p>
<p><a id="ch05sec3lev28"/></p>
<h5><a id="page_743"/><em>Expected time bound for search miss</em></h5>
<p>Suppose that we are searching for a key in a trie and find that the link in the root node that corresponds to its first character is null. In this case, we know that the key is not in the table on the basis of examining just <em>one</em> node. This case is typical: one of the most important properties of tries is that search misses typically require examining just a few nodes. If we assume that the keys are drawn from the random string model (each character is equally likely to have any one of the <em>R</em> different character values) we can prove this fact:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb15"/></p>
<p><strong>Proposition H.</strong> The average number of nodes examined for search miss in a trie built from <em>N</em> random keys over an alphabet of size <em>R</em> is ~log<em><sub>R</sub> N</em>.</p>
<p><strong>Proof sketch</strong> (for readers who are familiar with probabilistic analysis): The probability that each of the <em>N</em> keys in a random trie differs from a random search key in at least one of the leading <em>t</em> characters is (1 − <em>R</em><sup>−<em>t</em></sup>)<em><sup>N</sup></em>. Subtracting this quantity from 1 gives the probability that one of the keys in the trie matches the search key in all of the leading <em>t</em> characters. In other words, 1 − (1 − <em>R</em><sup>−<em>t</em></sup>)<em><sup>N</sup></em> is the probability that the search requires more than <em>t</em> character compares. From probabilistic analysis, the sum for <em>t</em> = 0, 1, 2, ... of the probabilities that an integer random variable is &gt;<em>t</em> is the average value of that random variable, so the average search cost is</p>
<p class="center">1 − (1 − <em>R</em><sup>−1</sup>)<em><sup>N</sup></em> + 1 − (1 − <em>R</em><sup>−2</sup>)<em><sup>N</sup></em> + ... + 1 − (1 − <em>R</em><sup>−<em>t</em></sup>)<em><sup>N</sup></em> + ...</p>
<p>Using the elementary approximation (1−1/<em>x</em>)<em><sup>x</sup></em> ~ <em>e</em><sup>−1</sup>, we find the search cost to be approximately</p>
<p class="center">(1 − <em>e</em><sup>−<em>N/R</em><sup>1</sup></sup>) + (1 − <em>e</em><sup>−<em>N/R</em><sup>2</sup></sup>) + ... + (1 − <em>e</em><sup>−<em>N/R<sup>t</sup></em></sup>) + ...</p>
<p>The summand is extremely close to 1 for approximately ln<em><sub>R</sub> N</em> terms with <em>R<sup>t</sup></em> substantially smaller than <em>N</em>; it is extremely close to 0 for all the terms with <em>R<sup>t</sup></em> substantially greater than <em>N</em>; and it is somewhere between 0 and 1 for the few terms with <em>R<sup>t</sup></em> ≈ <em>N</em>. So the grand total is about log<em><sub>R</sub> N</em>.</p>
<hr/>
</div>
<p>From a practical standpoint, the most important implication of this proposition is that <em>search miss does not depend on the key length.</em> For example, it says that unsuccessful search in a trie built with 1 million random keys will require examining only three or four nodes, whether the keys are 7-digit license plates or 20-digit account numbers. While it is unreasonable to expect truly random keys in practical applications, it <em>is</em> reasonable to hypothesize that the behavior of trie algorithms for keys in typical applications <a id="page_744"/>is described by this model. Indeed, this sort of behavior is widely seen in practice and is an important reason for the widespread use of tries.</p>
<p><a id="ch05sec3lev29"/></p>
<h5><em>Space</em></h5>
<p>How much space is needed for a trie? Addressing this question (and understanding how much space is <em>available</em>) is critical to using tries effectively.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb16"/></p>
<p><strong>Proposition I.</strong> The number of links in a trie is between <em>RN</em> and <em>RNw</em>, where <em>w</em> is the average key length.</p>
<p><strong>Proof:</strong> Every key in the trie has a node containing its associated value that also has <em>R</em> links, so the number of links is at least <em>RN</em>. If the first characters of all the keys are different, then there is a node with <em>R</em> links for every key character, so the number of links is <em>R</em> times the total number of key characters, or <em>RNw</em>.</p>
<hr/>
</div>
<p>The table on the facing page shows the costs for some typical applications that we have considered. It illustrates the following rules of thumb for tries:</p>
<p class="indenthangingB">• When keys are short, the number of links is close to <em>RN</em>.</p>
<p class="indenthangingB">• When keys are long, the number of links is close to <em>RNw</em>.</p>
<p class="indenthangingB">• Therefore, decreasing <em>R</em> can save a huge amount of space.</p>
<p class="image"><img alt="image" src="graphics/05_28-trieoneway.jpg"/></p>
<p>A more subtle message of this table is that it is important to understand the properties of the keys to be inserted before deploying tries in an application.</p>
<p><a id="ch05sec3lev30"/></p>
<h5><em>One-way branching</em></h5>
<p>The primary reason that trie space is excessive for long keys is that long keys tend to have long tails in the trie, with each node having a single link to the next node (and, therefore, <em>R</em>−1 null links). This situation is not difficult to correct (see <a href="#ch05qa2q11"><small>EXERCISE 5.2.11</small></a>). A trie might also have internal one-way branching. For example, two long keys may be equal except for their last character. This situation is a bit more difficult to address (see <a href="#ch05qa2q12"><small>EXERCISE 5.2.12</small></a>). These changes can make trie space usage a less important factor <a id="page_745"/>than for the straightforward implementation that we have considered, but they are not necessarily effective in practical applications. Next, we consider an alternative approach to reducing space usage for tries.</p>
<p><small>THE BOTTOM LINE</small> is this: <em>do not try to use</em> <a href="#ch05sb12"><small>ALGORITHM 5.4</small></a> <em>for large numbers of long keys taken from large alphabets</em>, because it will require space proportional to <em>R</em> times the total number of key characters. Otherwise, if you can afford the space, trie performance is difficult to beat.</p>
<p class="image"><img alt="image" src="graphics/t0745-01.jpg"/></p>
<p><a id="ch05sec2lev21"/></p>
<h4><a id="page_746"/>Ternary search tries (TSTs)</h4>
<p>To help us avoid the excessive space cost associated with <em>R</em>-way tries, we now consider an alternative representation: the <em>ternary search trie</em> (TST). In a TST, each node has a character, <em>three</em> links, and a value. The three links correspond to keys whose current characters are less than, equal to, or greater than the node’s character. In the <em>R</em>-way tries of <a href="#ch05sb12"><small>ALGORITHM 5.4</small></a>, trie nodes are represented by <em>R</em> links, with the character corresponding to each non-null link implictly represented by its index. In the corresponding TST, characters appear <em>explicitly</em> in nodes—we find characters corresponding to keys only when we are traversing the middle links.</p>
<p class="image"><img alt="image" src="graphics/05_29-tstanatomy.jpg"/></p>
<p><a id="ch05sec3lev31"/></p>
<h5><em>Search and insert</em></h5>
<p>The search and insert code for implementing our symbol-table API with TSTs writes itself. To search, we compare the first character in the key with the character at the root. If it is less, we take the left link; if it is greater, we take the right link; and if it is equal, we take the middle link and move to the next search key character. In each case, we apply the algorithm recursively. We terminate with a <em>search miss</em> if we encounter a null link or if the node where the search ends has a null value, and we terminate with a <em>search hit</em> if the node where the search ends has a non-null value. To insert a new key, we search, then add new nodes for the characters in the tail of the key, just as we did for tries. <a href="#ch05sb17"><small>ALGORITHM 5.5</small></a> gives the details of the implementation of these methods.</p>
<p class="image"><img alt="image" src="graphics/05_30-tstsearch.jpg"/></p>
<p>Using this arrangement is equivalent to implementing each <em>R</em>-way trie node as a binary search tree that uses as keys the characters corresponding to non-null links. By contrast, <a href="#ch05sb12"><small>ALGORITHM 5.4</small></a> uses a key-indexed array. A <a id="page_748"/>TST and its corresponding trie are illustrated above. Continuing the correspondence described in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> between binary search trees and sorting algorithms, we see that TSTs correspond to 3-way string quicksort in the same way that BSTs correspond to quicksort and tries correspond to MSD sorting. The figures on page <a href="#page_714">714</a> and <a href="#page_721">721</a>, which show the recursive call structure for MSD and 3-way three-way string quicksort (respectively), correspond precisely to the trie and TST drawn on page <a href="#ch05sec2lev21">746</a> for that set of keys. Space for links in tries corresponds to the space for counters in string sorting; 3-way branching provides an effective solution to both problems.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb17"/></p>
<h3><a id="page_747"/>Algorithm 5.5 TST symbol table</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0747-01.jpg"/></p>
<p>This implementation uses a <code>char</code> value <code>c</code> and three links per node to build string search tries where subtries have keys whose first character is less than <code>c</code> (left), equal to <code>c</code> (middle), and greater than <code>c</code> (right).</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/05_31-tstnoderep.jpg"/></p>
<p><a id="ch05sec2lev22"/></p>
<h4><a id="page_749"/>Properties of TSTs</h4>
<p><small>A TST</small> is a compact representation of an <em>R</em>-way trie, but the two data structures have remarkably different properties. Perhaps the most important difference is that <a href="#ch05sb02">PROPERTY A</a> does not hold for TSTs: the BST representations of each trie node depend on the order of key insertion, as with any other BST.</p>
<p><a id="ch05sec3lev32"/></p>
<h5><em>Space</em></h5>
<p>The most important property of TSTs is that they have just three links in each node, so a TST requires far less space than the corresponding trie.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb18"/></p>
<p><strong>Proposition J.</strong> The number of links in a TST built from <em>N</em> string keys of average length <em>w</em> is between 3<em>N</em> and 3<em>Nw.</em></p>
<p><strong>Proof.</strong> Immediate, by the same argument as for <a href="#ch05sb16"><small>PROPOSITION I</small></a>.</p>
<hr/>
</div>
<p>Actual space usage is generally less than the upper bound of three links per character, because keys with common prefixes share nodes at high levels in the tree.</p>
<p><a id="ch05sec3lev33"/></p>
<h5><em>Search cost</em></h5>
<p>To determine the cost of search (and insert) in a TST, we multiply the cost for the corresponding trie by the cost of traversing the BST representation of each trie node..</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb19"/></p>
<p><strong>Proposition K.</strong> A search miss in a TST built from <em>N</em> random string keys requires ~ln <em>N</em> character compares, on the average. A search hit or an insertion in a TST uses a character compare for each character in the search key.</p>
<p><strong>Proof:</strong> The search hit/insertion cost is immediate from the code. The search miss cost is a consequence of the same arguments discussed in the proof sketch of <a href="#ch05sb15"><small>PROPOSITION H</small></a>. We assume that all but a constant number of the nodes on the search path (a few at the top) act as random BSTs on <em>R</em> character values with average path length ln <em>R</em>, so we multiply the time cost log<em><sub>R</sub> N =</em> ln <em>N</em> / ln <em>R</em> by ln <em>R</em>.</p>
<hr/>
</div>
<p>In the worst case, a node might be a full <em>R</em>-way node that is unbalanced, stretched out like a singly linked list, so we would need to multiply by a factor of <em>R</em>. More typically, we might expect to do ln <em>R</em> or fewer character compares at the first level (since the root node behaves like a random BST on the <em>R</em> different character values) and perhaps at a few other levels (if there are keys with a common prefix and up to <em>R</em> different values on the character following the prefix), and to do only a few compares for most characters (since most trie nodes are sparsely populated with non-null links). Search misses are likely to involve only a few character compares, ending at a null link high in the trie, <a id="page_750"/>and search hits involve only about one compare per search key character, since most of them are in nodes with one-way branching at the bottom of the trie.</p>
<p><a id="ch05sec3lev34"/></p>
<h5><em>Alphabet</em></h5>
<p>The prime virtue of using TSTs is that they adapt gracefully to irregularities in search keys that are likely to appear in practical applications. In particular, note that there is no reason to allow for strings to be built from a client-supplied alphabet, as was crucial for tries. There are two main effects. First, keys in practical applications come from large alphabets, and usage of particular characters in the character sets is far from uniform. With TSTs, we can use a 256-character ASCII encoding or a 65,536-character Unicode encoding without having to worry about the excessive costs of nodes with 256- or 65,536-way branching, and without having to determine which sets of characters are relevant. Unicode strings in non-Roman alphabets can have thousands of characters—TSTs are especially appropriate for standard Java <code>String</code> keys that consist of such characters. Second, keys in practical applications often have a structured format, differing from application to application, perhaps using only letters in one part of the key, only digits in another part of the key. In our CA license plate example, the second, third, and fourth characters are uppercase letter (<em>R</em> = 26) and the other characters are decimal digits (<em>R</em> = 10). In a TST for such keys, some of the trie nodes will be represented as 10-node BSTs (for places where all keys have digits) and others will be represented as 26-node BSTs (for places where all keys have letters). This structure develops automatically, without any need for special analysis of the keys.</p>
<p><a id="ch05sec3lev35"/></p>
<h5><em>Prefix match, collecting keys, and wildcard match</em></h5>
<p>Since a TST represents a trie, implementations of <code>longestPrefixOf()</code>, <code>keys()</code>, <code>keysWithPrefix()</code>, and <code>keysThatMatch()</code> are easily adapted from the corresponding code for tries in the previous section, and a worthwhile exercise for you to cement your understanding of both tries and TSTs (see <a href="#ch05qa2q9"><small>EXERCISE 5.2.9</small></a>). The same tradeoff as for search (linear memory usage but an extra ln <em>R</em> multiplicative factor per character compare) holds.</p>
<p><a id="ch05sec3lev36"/></p>
<h5><em>Deletion</em></h5>
<p>The <code>delete()</code> method for TSTs requires more work. Essentially, each character in the key to be deleted belongs to a BST. In a trie, we could remove the link corresponding to a character by setting the corresponding entry in the array of links to <code>null</code>; in a TST, we have to use BST node deletion to remove the node corresponding to the character.</p>
<p><a id="ch05sec3lev37"/></p>
<h5><em>Hybrid TSTs</em></h5>
<p>An easy improvement to TST-based search is to use a large explicit multiway node at the root. The simplest way to proceed is to keep a table of <em>R</em> TSTs: one for each possible value of the first character in the keys. If <em>R</em> is not large, we might use the first two letters of the keys (and a table of size <em>R</em><sup>2</sup>). For this method to be effective, the leading digits of the keys must be well-distributed. The resulting hybrid search <a id="page_751"/>algorithm corresponds to the way that a human might search for names in a telephone book. The first step is a multiway decision (“Let’s see, it starts with ‘A’,”), followed perhaps by some two-way decisions (“It’s before ‘Andrews,’ but after ‘Aitken,’”), followed by sequential character matching (“‘Algonquin,’ ... No, ‘Algorithms’ isn’t listed, because nothing starts with ‘Algor’!”). These programs are likely to be among the fastest available for searching with string keys.</p>
<p><a id="ch05sec3lev38"/></p>
<h5><em>One-way branching</em></h5>
<p>Just as with tries, we can make TSTs more efficient in their use of space by putting keys in leaves at the point where they are distinguished and by eliminating one-way branching between internal nodes.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb20"/></p>
<p><strong>Proposition L.</strong> A search or an insertion in a TST built from <em>N</em> random string keys with no external one-way branching and <em>R<sup>t</sup></em>-way branching at the root requires roughly ln <em>N</em> − <em>t</em> ln <em>R</em> character compares, on the average.</p>
<p><strong>Proof:</strong> These rough estimates follow from the same argument we used to prove <a href="#ch05sb19"><small>PROPOSITION K</small></a>. We assume that all but a constant number of the nodes on the search path (a few at the top) act as random BSTs on <em>R</em> character values, so we multiply the time cost by ln <em>R</em>.</p>
<hr/>
</div>
<p><small>DESPITE THE TEMPTATION</small> to tune the algorithm to peak performance, we should not lose sight of the fact that one of the most attractive features of TSTs is that they free us from having to worry about application-specific dependencies, often providing good performance without any tuning.</p>
<p><a id="ch05sec2lev23"/></p>
<h4><a id="page_752"/>Which string symbol-table implementation should I use?</h4>
<p>As with string sorting, we are naturally interested in how the string-searching methods that we have considered compare to the general-purpose methods that we considered in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>. The following table summarizes the important characteristics of the algorithms that we have discussed in this section (the rows for BSTs, red-black BSTs and hashing are included from <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>, for comparison). For a particular application, these entries must be taken as indicative, not definitive, since so many factors (such as characteristics of keys and mix of operations) come into play when studying symbol-table implementations.</p>
<p class="image"><img alt="image" src="graphics/t0752-01.jpg"/></p>
<p>If space is available, <em>R</em>-way tries provide the fastest search, essentially completing the job with a <em>constant</em> number of character compares. For large alphabets, where space may not be available for <em>R</em>-way tries, TSTs are preferable, since they use a logarithmic number of <em>character</em> compares, while BSTs use a logarithmic number of <em>key</em> compares. Hashing can be competitive, but, as usual, cannot support ordered symbol-table operations or extended character-based API operations such as prefix or wildcard match.</p>
<p><a id="ch05sec2lev24"/></p>
<h4><a id="page_753"/>Q&amp;A</h4>
<p><strong>Q.</strong> Does the Java system sort use one of these methods for searching with <code>String</code> keys?</p>
<p><strong>A.</strong> No.</p>
<p><a id="ch05sec2lev25"/></p>
<h4><a id="page_754"/>Exercises</h4>
<p><a id="ch05qa2q1"/><strong>5.2.1</strong> Draw the <em>R</em>-way trie that results when the keys</p>
<p class="programlisting">no is th ti fo al go pe to co to th ai of th pa</p>
<p>are inserted in that order into an initially empty trie (do not draw null links).</p>
<p><a id="ch05qa2q2"/><strong>5.2.2</strong> Draw the TST that results when the keys</p>
<p class="programlisting">no is th ti fo al go pe to co to th ai of th pa</p>
<p>are inserted in that order into an initially empty TST.</p>
<p><a id="ch05qa2q3"/><strong>5.2.3</strong> Draw the <em>R</em>-way trie that results when the keys</p>
<p class="programlisting">now is the time for all good people to come to the aid of</p>
<p>are inserted in that order into an initially empty trie (do not draw null links).</p>
<p><a id="ch05qa2q4"/><strong>5.2.4</strong> Draw the TST that results when the keys</p>
<p class="programlisting">now is the time for all good people to come to the aid of</p>
<p>are inserted in that order into an initially empty TST.</p>
<p><a id="ch05qa2q5"/><strong>5.2.5</strong> Develop nonrecursive versions of <code>TrieST</code> and <code>TST</code>.</p>
<p><a id="ch05qa2q6"/><strong>5.2.6</strong> Implement the following API, for a <code>StringSET</code> data type:</p>
<p class="image"><img alt="image" src="graphics/t0754-01.jpg"/></p>
<p><a id="ch05sec2lev26"/></p>
<h4><a id="page_755"/>Creative Problems</h4>
<p><a id="ch05qa2q7"/><strong>5.2.7</strong> <em>Empty string in TSTs.</em> The code in TST does not handle the empty string properly. Explain the problem and suggest a correction.</p>
<p><a id="ch05qa2q8"/><strong>5.2.8</strong> <em>Ordered operations for tries.</em> Implement the <code>floor()</code>, <code>ceil()</code>, <code>rank()</code>, and <code>select()</code> (from our standard ordered <code>ST</code> API from <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>) for <code>TrieST</code>.</p>
<p><a id="ch05qa2q9"/><strong>5.2.9</strong> <em>Extended operations for TSTs.</em> Implement <code>keys()</code> and the extended operations introduced in this section—<code>longestPrefixOf()</code>, <code>keysWithPrefix()</code>, and <code>keysThatMatch()</code>—for <code>TST</code>.</p>
<p><a id="ch05qa2q10"/><strong>5.2.10</strong> <em>Size.</em> Implement very eager <code>size()</code> (that keeps in each node the number of keys in its subtree) for <code>TrieST</code> and <code>TST</code>.</p>
<p><a id="ch05qa2q11"/><strong>5.2.11</strong> <em>External one-way branching.</em> Add code to <code>TrieST</code> and <code>TST</code> to eliminate external one-way branching.</p>
<p><a id="ch05qa2q12"/><strong>5.2.12</strong> <em>Internal one-way branching.</em> Add code to <code>TrieST</code> and <code>TST</code> to eliminate internal one-way branching.</p>
<p><a id="ch05qa2q13"/><strong>5.2.13</strong> <em>Hybrid TST with R</em><sup>2</sup><em>-way branching at the root.</em> Add code to <code>TST</code> to do multiway branching at the first two levels, as described in the text.</p>
<p><a id="ch05qa2q14"/><strong>5.2.14</strong> <em>Unique substrings of length L.</em> Write a <code>TST</code> client that reads in text from standard input and calculates the number of unique substrings of length <em>L</em> that it contains. For example, if the input is <code>cgcgggcgcg</code>, then there are five unique substrings of length 3: <code>cgc</code>, <code>cgg</code>, <code>gcg</code>, <code>ggc</code>, and <code>ggg</code>. <em>Hint</em>: Use the string method <code>substring(i, i + L)</code> to extract the <code>i</code>th substring, then insert it into a symbol table.</p>
<p><a id="ch05qa2q15"/><strong>5.2.15</strong> <em>Unique substrings.</em> Write a <code>TST</code> client that reads in text from standard input and calculates the number of distinct substrings of any length. This can be done very efficiently with a suffix tree—see <a href="ch06.html#ch06"><small>CHAPTER 6</small></a></p>
<p><a id="ch05qa2q16"/><strong>5.2.16</strong> <em>Document similarity.</em> Write a <code>TST</code> client with a static method that takes an <code>int</code> value <code>L</code> and two file names as command-line arguments and computes the <code>L</code>-similarity of the two documents: the Euclidean distance between the frequency vectors defined by the number of occurrences of each trigram divided by the number of trigrams. Include a static method <code>main()</code> that takes an <code>int</code> value <code>L</code> as command-line argument and a list of file names from standard input and prints a matrix showing the <code>L</code>-similarity of all pairs of documents.</p>
<p><a id="page_756"/><a id="ch05qa2q17"/><strong>5.2.17</strong> <em>Spell checking.</em> Write a TST client <code>SpellChecker</code> that takes as command-line argument the name of a file containing a dictionary of words in the English language, and then reads a string from standard input and prints out any word that is not in the dictionary. Use a string set.</p>
<p><a id="ch05qa2q18"/><strong>5.2.18</strong> <em>Whitelist.</em> Write a <code>TST</code> client that solves the whitelisting problem presented in <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a> and revisited in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a> (see page <a href="ch03a.html#page_491">491</a>).</p>
<p><a id="ch05qa2q19"/><strong>5.2.19</strong> <em>Random phone numbers.</em> Write a <code>TrieST</code> client (with <code>R = 10</code>) that takes as command line argument an <code>int</code> value <code>N</code> and prints <code>N</code> random phone numbers of the form <code>(xxx) xxx-xxxx</code>. Use a symbol table to avoid choosing the same number more than once. Use the file <code>AreaCodes.txt</code> from the booksite to avoid printing out bogus area codes.</p>
<p><a id="ch05qa2q20"/><strong>5.2.20</strong> <em>Contains prefix</em>. Add a method <code>containsPrefix()</code> to <code>StringSET</code> (see <a href="#ch05qa2q6"><small>EXERCISE 5.2.6</small></a>) that takes a string <code>s</code> as input and returns <code>true</code> if there is a string in the set that contains <code>s</code> as a prefix.</p>
<p><a id="ch05qa2q21"/><strong>5.2.21</strong> <em>Substring matches.</em> Given a list of (short) strings, your goal is to support queries where the user looks up a string <code>s</code> and your job is to report back all strings in the list that contain <code>s</code>. Design an API for this task and develop a TST client that implements your API. <em>Hint</em>: Insert the suffixes of each word (e.g., <code>string</code>, <code>tring</code>, <code>ring</code>, <code>ing</code>, <code>ng</code>, <code>g</code>) into the TST.</p>
<p><a id="ch05qa2q22"/><strong>5.2.22</strong> <em>Typing monkeys.</em> Suppose that a typing monkey creates random words by appending each of 26 possible letter with probability <em>p</em> to the current word and finishes the word with probability 1 − 26<em>p</em>. Write a program to estimate the frequency distribution of the lengths of words produced. If <code>"abc"</code> is produced more than once, count it only once.</p>
<p><a id="ch05sec2lev27"/></p>
<h4><a id="page_757"/>Experiments</h4>
<p><a id="ch05qa2q23"/><strong>5.2.23</strong> <em>Duplicates (revisited again).</em> Redo <a href="ch03a.html#ch03qa5q30"><small>EXERCISE 3.5.30</small></a> using <code>StringSET</code> (see <a href="#ch05qa2q6"><small>EXERCISE 5.2.6</small></a>) instead of <code>HashSET</code>. Compare the running times of the two approaches. Then use <code>Dedup</code> to run the experiments for <em>N</em> = 10<sup>7</sup>, 10<sup>8</sup>, and 10<sup>9</sup>, repeat the experiments for random <code>long</code> values and discuss the results.</p>
<p><a id="ch05qa2q24"/><strong>5.2.24</strong> <em>Spell checker.</em> Redo <a href="ch03a.html#ch03qa5q31"><small>EXERCISE 3.5.31</small></a>, which uses the file <code>dictionary.txt</code> from the booksite and the <code>BlackFilter</code> client on page <a href="ch03a.html#page_491">491</a> to print all misspelled words in a text file. Compare the performance of <code>TrieST</code> and <code>TST</code> for the file <code>war.txt</code> with this client and discuss the results.</p>
<p><a id="ch05qa2q25"/><strong>5.2.25</strong> <em>Dictionary.</em> Redo <a href="ch03a.html#ch03qa5q32"><small>EXERCISE 3.5.32</small></a>: Study the performance of a client like <code>LookupCSV</code> (using <code>TrieST</code> and <code>TST</code>) in a scenario where performance matters. Specifically, design a query-generation scenario instead of taking commands from standard input, and run performance tests for large inputs and large numbers of queries.</p>
<p><a id="ch05qa2q26"/><strong>5.2.26</strong> <em>Indexing.</em> Redo <a href="ch03a.html#ch03qa5q33"><small>EXERCISE 3.5.33</small></a>: Study a client like <code>LookupIndex</code> (using <code>TrieST</code> and <code>TST</code>) in a scenario where performance matters. Specifically, design a query-generation scenario instead of taking commands from standard input, and run performance tests for large inputs and large numbers of queries.</p>
<p><a id="ch05sec1lev9"/></p>
<h3><a id="page_758"/>5.3 Substring Search</h3>
<p><small>A FUNDAMENTAL OPERATION</small> on strings is <em>substring search</em>: given a <em>text</em> string of length <em>N</em> and a <em>pattern</em> string of length <em>M</em>, find an occurrence of the pattern within the text. Most algorithms for this problem can easily be extended to find all occurrences of the pattern in the text, to count the number of occurrences of the pattern in the text, or to provide context (substrings of the text surrounding each occurrence of the pattern).</p>
<p>When you search for a word while using a text editor or a web browser, you are doing substring search. Indeed, the original motivation for this problem was to support such searches. Another classic application is searching for some important pattern in an intercepted communication. A military leader might be interested in finding the pattern <code>ATTACK AT DAWN</code> somewhere in an intercepted text message; a hacker might be interested in finding the pattern <code>Password:</code> somewhere in your computer’s memory. In today’s world, we are often searching through the vast amount of information available on the web.</p>
<p>To best appreciate the algorithms, think of the pattern as being relatively short (with <em>M</em> equal to, say, 100 or 1,000) and the text as being relatively long (with <em>N</em> equal to, say, 1 million or 1 billion). In substring search, we typically preprocess the pattern in order to be able to support fast searches for that pattern in the text.</p>
<p>Substring search is an interesting and classic problem: several very different (and surprising) algorithms have been discovered that not only provide a spectrum of useful practical methods but also illustrate a spectrum of fundamental algorithm design techniques.</p>
<p class="image"><img alt="image" src="graphics/05_32-example0.jpg"/></p>
<p><a id="ch05sec2lev28"/></p>
<h4><a id="page_759"/>A short history</h4>
<p>The algorithms that we examine have an interesting history; we summarize it here to help place the various methods in perspective.</p>
<p>There is a simple brute-force algorithm for substring search that is in widespread use. While it has a worst-case running time proportional to <em>MN</em>, the strings that arise in many applications lead to a running time that is (except in pathological cases) proportional to <em>M</em> + <em>N</em>. Furthermore, it is well-suited to standard architectural features on most computer systems, so an optimized version provides a standard benchmark that is difficult to beat, even with a clever algorithm.</p>
<p>In 1970, S. Cook proved a theoretical result about a particular type of abstract machine that implied the existence of an algorithm that solves the substring search problem in time proportional to <em>M</em> + <em>N</em> in the worst case. D. E. Knuth and V. R. Pratt laboriously followed through the construction Cook used to prove his theorem (which was not intended to be practical) and refined it into a relatively simple and practical algorithm. This seemed a rare and satisfying example of a theoretical result with immediate (and unexpected) practical applicability. But it turned out that J. H. Morris had discovered virtually the same algorithm as a solution to an annoying problem confronting him when implementing a text editor (he wanted to avoid having to “back up” in the text string). The fact that the same algorithm arose from two such different approaches lends it credibility as a fundamental solution to the problem.</p>
<p>Knuth, Morris, and Pratt didn’t get around to publishing their algorithm until 1976, and in the meantime R. S. Boyer and J. S. Moore (and, independently, R. W. Gosper) discovered an algorithm that is much faster in many applications, since it often examines only a fraction of the characters in the text string. Many text editors use this algorithm to achieve a noticeable decrease in response time for substring search.</p>
<p>Both the Knuth-Morris-Pratt (KMP) and the Boyer-Moore algorithms require some complicated preprocessing on the pattern that is difficult to understand and has limited the extent to which they are used. (In fact, the story goes that an unknown systems programmer found Morris’s algorithm too difficult to understand and replaced it with a brute-force implementation.)</p>
<p>In 1980, M. O. Rabin and R. M. Karp used hashing to develop an algorithm almost as simple as the brute-force algorithm that runs in time proportional to <em>M</em> + <em>N</em> with very high probability. Furthermore, their algorithm extends to two-dimensional patterns and text, which makes it more useful than the others for image processing.</p>
<p>This story illustrates that the search for a better algorithm is still very often justified; indeed, one suspects that there are still more developments on the horizon even for this classic problem.</p>
<p><a id="ch05sec2lev29"/></p>
<h4><a id="page_760"/>Brute-force substring search</h4>
<p>An obvious method for substring search is to check, for each possible position in the text at which the pattern could match, whether it does in fact match. The <code>search()</code> method below operates in this way to find the first occurrence of a pattern string <code>pat</code> in a text string <code>txt</code>. The program keeps one pointer (<code>i</code>) into the text and another pointer (<code>j</code>) into the pattern. For each <code>i</code>, it resets <code>j</code> to <code>0</code> and increments it until finding a mismatch or the end of the pattern (<code>j == M</code>). If we reach the end of the text (<code>i == N-M+1</code>) before the end of the pattern, then there is no match: the pattern does not occur in the text. Our convention is to return the value <code>N</code> to indicate a mismatch.</p>
<p class="image"><img alt="image" src="graphics/p0760-01.jpg"/></p>
<p>In a typical text-processing application, the <code>j</code> index rarely increments so the running time is proportional to <em>N</em>. Nearly all of the compares find a mismatch with the first character of the pattern. For example, suppose that you search for the pattern <code>pattern</code> in the text of this paragraph. There are 191 characters up to the end of the first occurrence of the pattern, only 7 of which are the character <code>p</code> (and there are no occurrences of <code>pa</code>), so the total number of character compares is 191+7, for an average of 1.036 compares per character in the text. On the other hand, there is no guarantee that the algorithm will always be so efficient. For example, a pattern might begin with a long string of <code>A</code>s. If it does, and the text also has long strings of <code>A</code>s, then the substring search will be slow.</p>
<p class="image"><img alt="image" src="graphics/05_33-brutetrace.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb21"/></p>
<p><a id="page_761"/><strong>Proposition M.</strong> Brute-force substring search requires ~<em>NM</em> character compares to search for a pattern of length <em>M</em> in a text of length <em>N</em>, in the worst case.</p>
<p><strong>Proof:</strong> A worst-case input is when both pattern and text are all <code>A</code>s followed by a <code>B</code>. Then for each of the <em>N</em> − <em>M</em> + 1 possible match positions, all the characters in the pattern are checked against the text, for a total cost of <em>M</em>(<em>N</em> − <em>M</em> + 1). Normally <em>M</em> is very small compared to <em>N</em>, so the total is ~<em>NM</em>.</p>
<hr/>
</div>
<p>Such degenerate strings are not likely to appear in English text, but they may well occur in other applications (for example, in binary texts), so we seek better algorithms.</p>
<p class="image"><img alt="image" src="graphics/05_34-brutetraceworst.jpg"/></p>
<p>The alternate implementation at the bottom of this page is instructive. As before, the program keeps one pointer (<code>i</code>) into the text and another pointer (<code>j</code>) into the pattern. As long as they point to matching characters, both pointers are incremented. This code performs precisely the same character compares as the previous implementation. To understand it, note that <code>i</code> in this code maintains the value of <code>i+j</code> in the previous code: it points to the <em>end</em> of the sequence of already-matched characters in the text (where <code>i</code> pointed to the <em>beginning</em> of the sequence before). If <code>i</code> and <code>j</code> point to mismatching characters, then we <em>back up</em> both pointers: <code>j</code> to point to the beginning of the pattern and <code>i</code> to correspond to moving the pattern to the right one position for matching against the text.</p>
<p class="image"><img alt="image" src="graphics/p0761-01.jpg"/></p>
<p><a id="ch05sec2lev30"/></p>
<h4><a id="page_762"/>Knuth-Morris-Pratt substring search</h4>
<p>The basic idea behind the algorithm discovered by Knuth, Morris, and Pratt is this: whenever we detect a mismatch, we already know some of the characters in the text (since they matched the pattern characters prior to the mismatch). We can take advantage of this information to avoid backing up the text pointer over all those known characters.</p>
<p>As a specific example, suppose that we have a two-character alphabet and are searching for the pattern <code>B A A A A A A A A A</code>. Now, suppose that we match five characters in the pattern, with a mismatch on the sixth. When the mismatch is detected, we know that the six previous characters in the text must be <code>B A A A A B</code> (the first five match and the sixth does not), with the text pointer now pointing at the <code>B</code> at the end. The key observation is that we need not back up the text pointer <code>i</code>, since the previous four characters in the text are all <code>A</code>s and do not match the first character in the pattern. Furthermore, the character currently pointed to by <code>i</code> is a <code>B</code> and does match the first character in the pattern, so we can increment <code>i</code> and compare the next character in the text with the second character in the pattern. This argument leads to the observation that, for this pattern, we can change the <code>else</code> clause in the alternate brute-force implementation to just set <code>j = 1</code> (and not decrement <code>i</code>). Since the value of <code>i</code> does not change within the loop, this method does at most <code>N</code> character compares. The practical effect of this particular change is limited to this particular pattern, but the idea is worth thinking about—the Knuth-Morris-Pratt algorithm is a generalization of it. Surprisingly, it is <em>always</em> possible to find a value to set the <code>j</code> pointer to on a mismatch, so that the <code>i</code> pointer is never decremented.</p>
<p class="image"><img alt="image" src="graphics/05_35-kmpez.jpg"/></p>
<p>Fully skipping past all the matched characters when detecting a mismatch will not work when the pattern could match itself at any position overlapping the point of the mismatch. For example, when searching for the pattern <code>A A <span class="pd_red">B</span> A A A</code> in the text <code>A A B A A <span class="pd_red">B</span> A A A A</code>, we first detect the mismatch at position 5, but we had better restart at position 3 to continue the search, since otherwise we would miss the match. The insight of the KMP algorithm is that we can decide ahead of time exactly how to restart the search, because that decision depends only on the pattern.</p>
<p><a id="ch05sec3lev39"/></p>
<h5><a id="page_763"/><em>Backing up the pattern pointer</em></h5>
<p>In KMP substring search, we never back up the text pointer <code>i</code>, and we use an array <code>dfa[][]</code> to record how far to back up the pattern pointer <code>j</code> when a mismatch is detected. For every character <code>c</code>, <code>dfa[c][j]</code> is the pattern position to compare against the next text position after comparing <code>c</code> with <code>pat.charAt(j)</code>. During the search, <code>dfa[txt.charAt(i)][j]</code> is the pattern position to compare with <code>txt.charAt(i+1)</code> after comparing <code>txt.charAt(i)</code> with <code>pat.charAt(j)</code>. For a match, we want to just move on to the next character, so <code>dfa[pat.charAt(j)][j]</code> is always <code>j+1</code>. For a mismatch, we know not just <code>txt.charAt(i)</code>, but also the <code>j-1</code> previous characters in the text: <em>they are the first <code>j-1</code> characters in the pattern</em>. For each character <code>c</code>, imagine that we slide a copy of the pattern over these <code>j</code> characters (the first <code>j-1</code> characters in the pattern followed by <code>c</code>—we are deciding what to do when these characters are <code>txt.charAt(i-j+1..i)</code>), from left to right, stopping when all overlapping characters match (or there are none). This gives the next possible place the pattern could match. The index of the pattern character to compare with <code>txt.charAt(i+1)</code> (<code>dfa[txt.charAt(i)][j]</code>) is precisely the number of overlapping characters.</p>
<p class="image"><img alt="image" src="graphics/05_36-kmpnext0abc.jpg"/></p>
<p><a id="ch05sec3lev40"/></p>
<h5><em>KMP search method</em></h5>
<p>Once we have computed the <code>dfa[][]</code> array, we have the substring search method at the top of the next page: when <code>i</code> and <code>j</code> point to mismatching characters (testing for a pattern match beginning at position <code>i-j+1</code> in the text string), then the next possible position for a pattern match is beginning at position <code>i-dfa[txt.charAt(i)][j]</code>. But by construction, the first <code>dfa[txt.charAt(i)][j]</code> characters at that position match the first <code>dfa[txt.charAt(i)][j]</code> characters of the pattern, so there is no need to back up the <code>i</code> pointer: we can simply set <code>j</code> to <code>dfa[txt.charAt(i)][j]</code> and increment <code>i</code>, which is precisely what we do when <code>i</code> and <code>j</code> point to matching characters.</p>
<p><a id="ch05sec3lev41"/></p>
<h5><a id="page_764"/><em>DFA simulation</em></h5>
<p>A useful way to describe this process is in terms of a <em>deterministic finite-state automaton</em> (DFA). Indeed, as indicated by its name, our <code>dfa[][]</code> array precisely defines a DFA. The graphical DFA represention shown at the bottom of this page consists of states (indicated by circled numbers) and transitions (indicated by labeled lines). There is one state for each character in the pattern, each such state having one transition leaving it for each character in the alphabet. For the substring-matching DFAs that we are considering, one of the transitions is a <em>match</em> transition (going from <code>j</code> to <code>j+1</code> and labeled with <code>pat.charAt(j)</code>) and all the others are <em>mismatch</em> transition (going left). The states correspond to character compares, one for each value of the pattern index. The transitions correspond to changing the value of the pattern index. When examining the text character <code>i</code> when in the state labeled <code>j</code>, the machine does the following: “Take the transition to <code>dfa[txt.charAt(i)][j]</code> and move to the next character (by incrementing <code>i</code>).” For a match transition, we move to the right one position because <code>dfa[pat.charAt(j)][j]</code> is always <code>j+1</code>; for a mismatch transition we move to the left. The automaton reads the text characters one at a time, from left to right, moving to a new state each time it reads a character. We also include a <em>halt</em> state <code>M</code> that has no transitions. We start the machine at state 0: if the machine reaches state <code>M</code>, then a substring of the text matching the pattern has been found (and we say that the DFA <em>recognizes</em> the pattern); if the machine reaches the end of the text before reaching state <code>M</code>, then we know the pattern does not appear as a substring of the text. Each pattern corresponds to an automaton (which is represented by the <code>dfa[][]</code> array that gives the transitions). The KMP substring <code>search()</code> method is a Java program that simulates the operation of such an automaton.</p>
<p class="image"><img alt="image" src="graphics/p0764-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_37-kmpdfa.jpg"/></p>
<p>To get a feeling for the operation of a substring search DFA, consider two of the simplest things that it does. At <a id="page_765"/>the beginning of the process, when started in state <code>0</code> at the beginning of the text, it stays in state 0, scanning text characters, until it finds a text character that is equal to the first pattern character, when it moves to the next state and is off and running. At the end of the process, when it finds a match, it matches pattern characters with the right end of the text, incrementing the state until reaching state <code>M</code>. The trace at the top of this page gives a typical example of the operation of our example DFA. Each match moves the DFA to the next state (which is equivalent to incrementing the pattern index <code>j</code>); each mismatch moves the DFA to an earlier state (which is equivalent to setting the pattern index <code>j</code> to a smaller value). The text index <code>i</code> marches from left to right, one position at a time, while the pattern index <code>j</code> bounces around in the pattern as directed by the DFA.</p>
<p class="image"><img alt="image" src="graphics/05_38-kmptracenew.jpg"/></p>
<p><a id="ch05sec3lev42"/></p>
<h5><em>Constructing the DFA</em></h5>
<p>Now that you understand the mechanism, we are ready to address the key question for the KMP algorithm: How do we compute the <code>dfa[][]</code> array corresponding to a given pattern? Remarkably, the answer to this question lies in the DFA <em>itself</em> (!) using the ingenious (and rather tricky) construction that was developed by Knuth, Morris, and Pratt. When we have a mismatch at <code>pat.charAt(j)</code>, our interest is in knowing in what state the DFA <em>would be</em> if we were to back up the text index and rescan the text characters that we just saw after shifting to the right one position. We do not want to actually do the backup, just restart the DFA <em>as if</em> we had done the backup. <a id="page_766"/>The key observation is that the characters in the text that would need to be rescanned are precisely <code>pat.charAt(1)</code> through <code>pat.charAt(j-1)</code>: we drop the first character to shift right one position and the last character because of the mismatch. These are pattern characters that we know, so we can figure out ahead of time, for each possible mismatch position, the state where we need to restart the DFA. The figure at left shows the possibilities for our example. <em>Be sure that you understand this concept</em>.</p>
<p class="image"><img alt="image" src="graphics/05_39-kmprestarts.jpg"/></p>
<p>What should the DFA do with the next character? Exactly what it would have done if we had backed up, <em>except</em> if it finds a match with <code>pat.charAt(j)</code>, when it should go to state <code>j+1</code>. For example, to decide what the DFA should do when we have a mismatch at <code>j = 5</code> for <code>A B A B A C</code>, we use the DFA to learn that a full backup would leave us in state <code>3</code> for <code>B A B A</code>, so we can copy <code>dfa[][3]</code> to <code>dfa[][5]</code>, then set the entry for <code>C</code> to <code>6</code> because <code>pat.charAt(5)</code> is <code>C</code> (a match). Since we only need to know how the DFA runs for <code>j-1</code> characters when we are building the <code>j</code>th state, we can always get the information that we need from the partially built DFA.</p>
<p>The final crucial detail to the computation is to observe that maintaining the restart position <code>X</code> when working on column <code>j</code> of <code>dfa[][]</code> is easy because <code>X &lt; j</code> so that we can use the partially built DFA to do the job—the next value of <code>X</code> is <code>dfa[pat.charAt(j)][X]</code>). Continuing our example from the previous paragraph, we would update the value of <code>X</code> to <code>dfa['C'][3] = 0</code> (but we do not use that value because the DFA construction is complete).</p>
<p>The discussion above leads to the remarkably compact code below for constructing the DFA corresponding to a given pattern. For each <code>j</code>, it</p>
<p class="indenthangingB">• Copies <code>dfa[][X]</code> to <code>dfa[][j]</code> (for mismatch cases)</p>
<p class="indenthangingB">• Sets <code>dfa[pat.charAt(j)][j]</code> to <code>j+1</code> (for the match case)</p>
<p class="indenthangingB">• Updates <code>X</code></p>
<p>The diagram on the facing page traces this code for our example. To make sure that you understand it, work <a href="#ch05qa3q2"><small>EXERCISE 5.3.2</small></a> and <a href="#ch05qa3q3"><small>EXERCISE 5.3.3</small></a>.</p>
<p class="image"><img alt="image" src="graphics/p0766-01.jpg"/></p>
<p class="image"><a id="page_767"/><img alt="image" src="graphics/05_40-kmpdfabuild.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb22"/></p>
<h3><a id="page_768"/>Algorithm 5.6 Knuth-Morris-Pratt substring search</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0768-01.jpg"/></p>
<p>The constructor in this implementation of the Knuth-Morris-Pratt algorithm for substring search builds a DFA from a pattern string, to support a <code>search()</code> method that can find the pattern in a given text string. This program does the same job as the brute-force method, but it runs faster for patterns that are self-repetitive.</p>
<p class="image"><img alt="image" src="graphics/p0768-02.jpg"/></p>
<hr/>
</div>
<p><a id="page_769"/><a href="#ch05sb22"><small>ALGORITHM 5.6</small></a> on the facing page implements the following API:</p>
<p class="image"><img alt="image" src="graphics/t0769-01.jpg"/></p>
<p>You can see a typical test client at the bottom of this page. The constructor builds a DFA from a pattern that the <code>search()</code> method uses to search for the pattern in a given text.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb23"/></p>
<p><strong>Proposition N.</strong> Knuth-Morris-Pratt substring search accesses no more than <em>M</em> + <em>N</em> characters to search for a pattern of length <em>M</em> in a text of length <em>N</em>.</p>
<p><strong>Proof.</strong> Immediate from the code: we access each pattern character once when computing <code>dfa[][]</code> and each text character once (in the worst case) in <code>search()</code>.</p>
<hr/>
</div>
<p>Another parameter comes into play: for an <em>R</em>-character alphabet, the total running time (and space) required to build the DFA is proportional to <em>MR.</em> It is possible to remove the factor of <em>R</em> by building a DFA where each state has a match transition and a mismatch transition (not transitions for each possible character), though the construction is somewhat more intricate.</p>
<p>The linear-time worst-case guarantee provided by the KMP algorithm is a significant theoretical result. In practice, the speedup over the brute-force method is not often important because few applications involve searching for highly self-repetitive patterns in highly self-repetitive text. Still, the method has the practical advantage that it never backs up in the input. This property makes KMP substring search more convenient for use on an input stream of undetermined length (such as standard input) than algorithms requiring backup, which need some complicated buffering in this situation. Ironically, when backup is easy, we can do significantly better than KMP. Next, we consider a method that generally leads to substantial performance gains precisely because it <em>can</em> back up in the text.</p>
<p class="image"><img alt="image" src="graphics/p0769-01.jpg"/></p>
<p><a id="ch05sec2lev31"/></p>
<h4><a id="page_770"/>Boyer-Moore substring search</h4>
<p>When backup in the text string is not a problem, we can develop a significantly faster substring-searching method by scanning the pattern from <em>right to left</em> when trying to match it against the text. For example, when searching for the substring <code>BAABBAA</code>, if we find matches on the seventh and sixth characters but not on the fifth, then we can immediately slide the pattern seven positions to the right, and check the 14th character in the text next, because our partial match found <code>XAA</code> where <code>X</code> is not <code>B</code>, which does not appear elsewhere in the pattern. In general, the pattern at the end might appear elsewhere, so we need an array of restart positions as for Knuth-Morris-Pratt. We will not explore this approach in further detail because it is quite similar to our implementation of the Knuth-Morris-Pratt method. Instead, we will consider another suggestion by Boyer and Moore that is typically even more effective in right-to-left pattern scanning.</p>
<p>As with our implementation of KMP substring search, we decide what to do next on the basis of the character that caused the mismatch in the <em>text</em> as well as the pattern. The preprocessing step is to decide, for each possible character that could occur in the text, what we would do if that character were to cause the mismatch. The simplest realization of this idea leads immediately to an efficient and useful substring search method.</p>
<p><a id="ch05sec3lev43"/></p>
<h5><em>Mismatched character heuristic</em></h5>
<p>Consider the figure at the bottom of this page, which shows a search for the pattern <code>NEEDLE</code> in the text <code>FINDINAHAYSTACKNEEDLE</code>. Proceeding from right to left to match the pattern, we first compare the rightmost <code>E</code> in the pattern with the <code>N</code> (the character at position 5) in the text. Since <code>N</code> appears in the pattern, we slide the pattern five positions to the right to line up the <code>N</code> in the text with the (rightmost) <code>N</code> in the pattern. Then we compare the rightmost <code>E</code> in the pattern with the <code>S</code> (the character at position 10) in the text. This is also a mismatch, but <code>S</code> <em>does not</em> appear in the pattern, so we can slide the pattern six positions to the right. We match the rightmost <code>E</code> in the pattern against the <code>E</code> at position 16 in the text, then find a mismatch and discover the <code>N</code> at position 15 and slide to the right five positions, as at the beginning. Finally, we verify, moving from right to left starting at position 20, that the pattern is in the text. This method brings us to the match position at a cost of only four character compares (and six more to verify the match)!</p>
<p class="image"><img alt="image" src="graphics/05_41-bmtrace0.jpg"/></p>
<p><a id="ch05sec3lev44"/></p>
<h5><a id="page_771"/><em>Starting point</em></h5>
<p>To implement the mismatched character heuristic, we use an array <code>right[]</code> that gives, for each character in the alphabet, the index of its <em>rightmost occurrence</em> in the pattern (or <code>-1</code> if the character is not in the pattern). This value tells us precisely how far to skip if that character appears in the text and causes a mismatch during the string search. To initialize the <code>right[]</code> array, we set all entries to <code>-1</code> and then, for <code>j</code> from <code>0</code> to <code>M-1</code>, set <code>right[pat.charAt(j)]</code> to <code>j</code>, as shown in the example at right for our example pattern <code>NEEDLE</code>.</p>
<p class="image"><img alt="image" src="graphics/05_42-boyermooretable.jpg"/></p>
<p><a id="ch05sec3lev45"/></p>
<h5><em>Substring search</em></h5>
<p>With the <code>right[]</code> array precomputed, the implementation in <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> is straightforward. We have an index <code>i</code> moving from left to right through the text and an index <code>j</code> moving from right to left through the pattern. The inner loop tests whether the pattern aligns with the text at position <code>i</code>. If <code>txt.charAt(i+j)</code> is equal to <code>pat.charAt(j)</code> for all <code>j</code> from <code>M-1</code> down to <code>0</code>, then there is a match. Otherwise, there is a character mismatch, and we have one of the following three cases:</p>
<p class="indenthangingB">• If the character causing the mismatch is not found in the pattern, we can slide the pattern <code>j+1</code> positions to the right (increment <code>i</code> by <code>j+1</code>). Anything less would align that character with some pattern character. Actually, this move aligns some known characters at the beginning of the pattern with known characters at the end of the pattern so that we could further increase <code>i</code> by precomputing a KMP-like table (see example at right).</p>
<p class="image"><img alt="image" src="graphics/05_43-boyermoorecases1.jpg"/></p>
<p class="indenthangingB">• If the character <code>c</code> causing the mismatch is found in the pattern, we use the <code>right[]</code> array to line up the pattern with the text so that character will match its rightmost occurrence in the pattern. To do so, we increment <code>i</code> by <code>j</code> minus <code>right[c]</code>. Again, anything less would align that text character with a pattern character it could not match (one to the right of its rightmost occurrence). Again, there is a possibility that we could do better with a KMP-like table, as indicated in the top example in the figure on page <a href="#page_773">773</a>.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb24"/></p>
<h3><a id="page_772"/>Algorithm 5.7 Boyer-Moore substring search (mismatched character heuristic)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0772-01.jpg"/></p>
<p>The constructor in this substring search algorithm builds a table giving the rightmost occurrence in the pattern of each possible character. The search method scans from right to left in the pattern, skipping to align any character causing a mismatch with its rightmost occurrence in the pattern.</p>
<hr/>
</div>
<p class="indenthangingB"><a id="page_773"/>• If this computation would not increase <code>i</code>, we just increment <code>i</code> instead, to make sure that the pattern always slides at least one position to the right. The bottom example in the figure at right illustrates this situation.</p>
<p class="image"><img alt="image" src="graphics/05_44-boyermoorecases.jpg"/></p>
<p><a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> is a straightforward implementation of this process. Note that the convention of using <code>-1</code> in the <code>right[]</code> array entries corresponding to characters that do not appear in the pattern unifies the first two cases (increment <code>i</code> by <code>j - right[txt.charAt(i+j)]</code>).</p>
<p>The full Boyer-Moore algorithm takes into account precomputed mismatches of the pattern with itself (in a manner similar to the KMP algorithm) and provides a linear-time worst-case guarantee (whereas <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> can take time proportional to <em>NM</em> in the worst case—see <a href="#ch05qa3q19"><small>EXERCISE 5.3.19</small></a>). We omit this computation because the mismatched character heuristic controls the performance in typical practical applications.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb25"/></p>
<p><strong>Property O.</strong> On typical inputs, substring search with the Boyer-Moore mismatched character heuristic uses ~<em>N</em>/<em>M</em> character compares to search for a pattern of length <em>M</em> in a text of length <em>N</em>.</p>
<p><strong>Discussion:</strong> This result can be proved for various random string models, but such models tend to be unrealistic, so we shall skip the details. In many practical situations it is true that all but a few of the alphabet characters appear nowhere in the pattern, so nearly all compares lead to <em>M</em> characters being skipped, which gives the stated result.</p>
<hr/>
</div>
<p><a id="ch05sec2lev32"/></p>
<h4><a id="page_774"/>Rabin-Karp fingerprint search</h4>
<p>The method developed by M. O. Rabin and R. A. Karp is a completely different approach to substring search that is based on hashing. We compute a hash function for the pattern and then look for a match by using the same hash function for each possible <em>M</em>-character substring of the text. If we find a text substring with the same hash value as the pattern, we can check for a match. This process is equivalent to storing the pattern in a hash table, then doing a search for each substring of the text, but we do not need to reserve the memory for the hash table because it would have just one entry. A straightforward implementation based on this description would be much slower than a brute-force search (since computing a hash function that involves every character is likely to be much more expensive than just comparing characters), but Rabin and Karp showed that it is easy to compute hash functions for <em>M</em>-character substrings in <em>constant</em> time (after some preprocessing), which leads to a <em>linear</em>-time substring search in practical situations.</p>
<p><a id="ch05sec3lev46"/></p>
<h5><em>Basic plan</em></h5>
<p>A string of length <code>M</code> corresponds to an <code>M</code>-digit base-<code>R</code> number. To use a hash table of size <code>Q</code> for keys of this type, we need a hash function to convert an <code>M</code>-digit base-<code>R</code> number to an <code>int</code> value between <code>0</code> and <code>Q-1</code>. Modular hashing (see <a href="ch03.html#ch03sec1lev4"><small>SECTION 3.4</small></a>) provides an answer: take the remainder when dividing the number by <code>Q</code>. In practice, we use a random prime <code>Q</code>, taking as large a value as possible while avoiding overflow (because we do not actually need to store a hash table). The method is simplest to understand for small <code>Q</code> and <code>R = 10</code>, shown in the example below. To find the pattern <code>26535</code> in the text <code>3141592653589793</code>, we choose a table size <code>Q</code> (<code>997</code> in the example), compute the hash value <code>26535 % 997 = 613</code>, and then look for a match by computing hash values for each five-digit substring in the text. In the example, we get the hash values <code>508</code>, <code>201</code>, <code>715</code>, <code>971</code>, <code>442</code>, and <code>929</code> before finding the match <code>613</code>.</p>
<p class="image"><img alt="image" src="graphics/05_45-rktrace0.jpg"/></p>
<p><a id="ch05sec3lev47"/></p>
<h5><em>Computing the hash function</em></h5>
<p>With five-digit values, we could just do all the necessary calculations with <code>int</code> values, but what do we do when <code>M</code> is 100 or 1,000? A simple application of Horner’s method, precisely like the method that we examined in <a href="ch03.html#ch03sec1lev4"><small>SECTION 3.4</small></a> for strings and other types of keys with multiple values, <a id="page_775"/>leads to the code shown at right, which computes the hash function for an <code>M</code>-digit base-<code>R</code> number represented as a <code>char</code> array in time proportional to <code>M</code>. (We pass <code>M</code> as an argument so that we can use the method for both the pattern and the text, as you will see.) For each digit in the number, we multiply by <code>R</code>, add the digit, and take the remainder when divided by <code>Q</code>. For example, computing the hash function for our pattern using this process is shown at the bottom of the page. The same method can work for computing the hash functions in the text, but the cost for the substring search would be a multiplication, addition, and remainder calculation for each text character, for a total of <em>NM</em> operations in the worst case, no improvement over the brute-force method.</p>
<p class="image"><img alt="image" src="graphics/p0775-01.jpg"/></p>
<p><a id="ch05sec3lev48"/></p>
<h5><em>Key idea</em></h5>
<p>The Rabin-Karp method is based on efficiently computing the hash function for position <code>i+1</code> in the text, given its value for position <code>i</code>. It follows directly from a simple mathematical formulation. Using the notation <em>t<sub>i</sub></em> for <code>txt.charAt(i)</code>, the number corresponding to the <em>M</em>-character substring of <code>txt</code> that starts at position <code>i</code> is</p>
<p class="center"><em>x<sub>i</sub></em> <code>=</code> <em>t<sub>i</sub> R</em><sup><em>M</em>−1</sup> + <em>t</em><sub><em>i</em>+1</sub> <em>R</em><sup><em>M</em>−2</sup> + . . . + <em>t</em><sub><em>i+M</em>−1</sub><em>R</em><sup>0</sup></p>
<p>and we can assume that we know the value of <em>h</em>(<em>x<sub>i</sub></em>) = <em>x<sub>i</sub></em> mod <em>Q</em>. Shifting one position right in the text corresponds to replacing <em>x<sub>i</sub></em> by</p>
<p class="center"><em>x</em><sub><em>i+</em>1</sub> <code>=</code> (<em>x<sub>i</sub></em> − <em>t<sub>i</sub> R</em><sup><em>M</em>−1</sup>) <em>R</em> + <em>t<sub>i+M</sub></em>.</p>
<p>We subtract off the leading digit, multiply by <code>R</code>, then add the trailing digit. Now, the crucial point is that we do not have to maintain the values of the numbers, just the values of their remainders when divided by <code>Q</code>. A fundamental property of the modulus operation is that if we take the remainder when divided by <code>Q</code> after each arithmetic operation, then we get the same answer as if we were to perform all of the arithmetic operations, then take the remainder when divided by <code>Q</code>. We took advantage of this property once before, when implementing modular hashing with Horner’s method (see page <a href="ch03.html#page_460">460</a>). The result is that we can effectively move right one position in the text in <em>constant</em> time, whether <code>M</code> is 5 or 100 or 1,000.</p>
<p class="image"><img alt="image" src="graphics/05_46-rktracepat.jpg"/></p>
<p><a id="ch05sec3lev49"/></p>
<h5><a id="page_776"/><em>Implementation</em></h5>
<p>This discussion leads directly to the substring search implementation in <a href="#ch05sb26"><small>ALGORITHM 5.8</small></a>. The constructor computes a hash value <code>patHash</code> for the pattern; it also computes the value of <em>R</em><sup><em>M</em>−1</sup>mod <em>Q</em> in the variable <code>RM</code>. The <code>hashSearch()</code> method begins by computing the hash function for the first <code>M</code> characters of the text and comparing that value against the hash value for the pattern. If that is not a match, it proceeds through the text string, using the technique above to maintain the hash function for the <code>M</code> characters starting at position <code>i</code> for each <code>i</code> in a variable <code>txtHash</code> and comparing each new hash value to <code>patHash</code>. (An extra <code>Q</code> is added during the <code>txtHash</code> calculation to make sure that everything stays positive so that the remainder operation works as it should.)</p>
<p class="image"><img alt="image" src="graphics/05_47-rkcomputation.jpg"/></p>
<p><a id="ch05sec3lev50"/></p>
<h5><em>A trick: Monte Carlo correctness</em></h5>
<p>After finding a hash value for an <em>M</em>-character substring of <code>txt</code> that matches the pattern hash value, you might expect to see code to compare those characters with the pattern to ensure that we have a true match, not just a hash collision. We do not do that test because using it requires backup in the text string. Instead, we make the hash table “size” <code>Q</code> as large as we wish, since we are not actually building a hash table, just testing for a collision with one key, our pattern. We will use a <code>long</code> value greater than 10<sup>20</sup>, making the probability that a random key hashes to the <a id="page_778"/>same value as our pattern less than 10<sup>–20</sup>, an exceedingly small value. If that value is not small enough for you, you could run the algorithms again to get a probability of failure of less than 10<sup>–40</sup>. This algorithm is an early and famous example of a <em>Monte Carlo</em> algorithm that has a guaranteed completion time but fails to output a correct answer with a small probability. The alternative method of checking for a match could be slow (it might amount to the brute-force algorithm, with a very small probability) but is guaranteed correct. Such an algorithm is known as a <em>Las Vegas</em> algorithm.</p>
<p class="image"><img alt="image" src="graphics/05_48-rktrace1.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb26"/></p>
<h3><a id="page_777"/>Algorithm 5.8 Rabin-Karp fingerprint substring search</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0777-01.jpg"/></p>
<p>This substring search algorithm is based on hashing. It computes a hash value for the pattern in the constructor, then searches through the text looking for a hash match.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb27"/></p>
<p><strong>Property P.</strong> The Monte Carlo version of Rabin-Karp substring search is linear-time and extremely likely to be correct, and the Las Vegas version of Rabin-Karp substring search is correct and extremely likely to be linear-time.</p>
<p><strong>Discussion:</strong> The use of the very large value of <code>Q</code>, made possible by the fact that we need not maintain an actual hash table, makes it extremely unlikely that a collision will occur. Rabin and Karp showed that when <code>Q</code> is properly chosen, we get a hash collision for random strings with probability 1/<code>Q</code>, which implies that, for practical values of the variables, there are no hash matches when there are no substring matches and only one hash match if there is a substring match. Theoretically, a text position could lead to a hash collision and not a substring match, but in practice it can be relied upon to find a match.</p>
<hr/>
</div>
<p>If your belief in probability theory (or in the random string model and the code we use to generate random numbers) is more half-hearted than resolute, you can add to <code>check()</code> the code to check that the text matches the pattern, which turns <a href="#ch05sb26"><small>ALGORITHM 5.8</small></a> into the Las Vegas version of the algorithm (see <a href="#ch05qa3q12"><small>EXERCISE 5.3.12</small></a>). If you also add a check to see whether that code is ever executed, you might develop more faith in probability theory as time wears on.</p>
<p><small>RABIN-KARP SUBSTRING SEARCH</small> is known as a <em>fingerprint</em> search because it uses a small amount of information to represent a (potentially very large) pattern. Then it looks for this fingerprint (the hash value) in the text. The algorithm is efficient because the fingerprints can be efficiently computed and compared.</p>
<p><a id="ch05sec2lev33"/></p>
<h4><a id="page_779"/>Summary</h4>
<p>The table at the bottom of the page summarizes the algorithms that we have discussed for substring search. As is often the case when we have several algorithms for the same task, each of them has attractive features. Brute-force search is easy to implement and works well in typical cases (Java’s <code>indexOf()</code> method in <code>String</code> uses brute-force search); Knuth-Morris-Pratt is guaranteed linear-time with no backup in the input; Boyer-Moore is sublinear (by a factor of <em>M</em>) in typical situations; and Rabin-Karp is linear. Each also has drawbacks: brute-force might require time proportional to <em>MN;</em> Knuth-Morris-Pratt and Boyer-Moore use extra space; and Rabin-Karp has a relatively long inner loop (several arithmetic operations, as opposed to character compares in the other methods). These characteristics are summarized in the table below.</p>
<p class="image"><img alt="image" src="graphics/t0779-01.jpg"/></p>
<p><a id="ch05sec2lev34"/></p>
<h4><a id="page_780"/>Q&amp;A</h4>
<p><strong>Q.</strong> This substring search problem seems like a bit of a toy problem. Do I really need to understand these complicated algorithms?</p>
<p><strong>A.</strong> Well, the factor of <em>M</em> speedup available with Boyer-Moore can be quite impressive in practice. Also, the ability to stream input (no backup) leads to many practical applications for KMP and Rabin-Karp. Beyond these direct practical applications, this topic provides an interesting introduction to the use of abstract machines and randomization in algorithm design.</p>
<p><strong>Q.</strong> Why not simplify things by converting each character to binary, treating all text as binary text?</p>
<p><strong>A.</strong> That idea is not quite effective because of false matches across character boundaries.</p>
<p><a id="ch05sec2lev35"/></p>
<h4><a id="page_781"/>Exercises</h4>
<p><a id="ch05qa3q1"/><strong>5.3.1</strong> Develop a brute-force substring search implementation <code>Brute</code>, using the same API as <a href="#ch05sb22"><small>ALGORITHM 5.6</small></a>.</p>
<p><a id="ch05qa3q2"/><strong>5.3.2</strong> Give the <code>dfa[][]</code> array for the Knuth-Morris-Pratt algorithm for the pattern <code>A A A A A A A A A</code>, and draw the DFA, in the style of the figures in the text.</p>
<p><a id="ch05qa3q3"/><strong>5.3.3</strong> Give the <code>dfa[][]</code> array for the Knuth-Morris-Pratt algorithm for the pattern <code>A B R A C A D A B R A</code>, and draw the DFA, in the style of the figures in the text.</p>
<p><a id="ch05qa3q4"/><strong>5.3.4</strong> Write an efficient method that takes a string <code>txt</code> and an integer <code>M</code> as arguments and returns the position of the first occurrence of <code>M</code> consecutive blanks in the string, <code>txt.length</code> if there is no such occurrence. Estimate the number of character compares used by your method, on typical text and in the worst case.</p>
<p><a id="ch05qa3q5"/><strong>5.3.5</strong> Develop a brute-force substring search implementation <code>BruteForceRL</code> that processes the pattern from right to left (a simplified version of <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a>).</p>
<p><a id="ch05qa3q6"/><strong>5.3.6</strong> Give the <code>right[]</code> array computed by the constructor in <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> for the pattern <code>A B R A C A D A B R A</code>.</p>
<p><a id="ch05qa3q7"/><strong>5.3.7</strong> Add to our brute-force implementation of substring search a <code>count()</code> method to count occurrences and a <code>searchAll()</code> method to print all occurrences.</p>
<p><a id="ch05qa3q8"/><strong>5.3.8</strong> Add to <code>KMP</code> a <code>count()</code> method to count occurrences and a <code>searchAll()</code> method to print all occurrences.</p>
<p><a id="ch05qa3q9"/><strong>5.3.9</strong> Add to <code>BoyerMoore</code> a <code>count()</code> method to count occurrences and a <code>searchAll()</code> method to print all occurrences.</p>
<p><a id="ch05qa3q10"/><strong>5.3.10</strong> Add to <code>RabinKarp</code> a <code>count()</code> method to count occurrences and a <code>searchAll()</code> method to print all occurrences.</p>
<p><a id="ch05qa3q11"/><strong>5.3.11</strong> Construct a worst-case example for the Boyer-Moore implementation in <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> (which demonstrates that it is not linear-time).</p>
<p><a id="ch05qa3q12"/><strong>5.3.12</strong> Add the code to <code>check()</code> in <code>RabinKarp</code> (<a href="#ch05sb26"><small>ALGORITHM 5.8</small></a>) that turns it into a Las Vegas algorithm (check that the pattern matches the text at the position given as argument).</p>
<p><a id="ch05qa3q13"/><strong>5.3.13</strong> In the Boyer-Moore implementation in <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a>, show that you can set <a id="page_782"/><code>right[c]</code> to the penultimate occurrence of <code>c</code> when <code>c</code> is the last character in the pattern.</p>
<p><a id="ch05qa3q14"/><strong>5.3.14</strong> Develop versions of the substring search implementations in this section that use <code>char[]</code> instead of <code>String</code> to represent the pattern and the text.</p>
<p><a id="ch05qa3q15"/><strong>5.3.15</strong> Design a brute-force substring search algorithm that scans the pattern from right to left.</p>
<p><a id="ch05qa3q16"/><strong>5.3.16</strong> Show the trace of the brute-force algorithm in the style of the figures in the text for the following pattern and text strings</p>
<p class="indenthangingN"><em>a.</em> pattern: <code>AAAAAAAB</code>   text: <code>AAAAAAAAAAAAAAAAAAAAAAAAB</code></p>
<p class="indenthangingN"><em>b.</em> pattern: <code>ABABABAB</code>   text: <code>ABABABABAABABABABAAAAAAAA</code></p>
<p><a id="ch05qa3q17"/><strong>5.3.17</strong> Draw the KMP DFA for the following pattern strings.</p>
<p class="indenthangingN"><em>a.</em> <code>AAAAAAB</code></p>
<p class="indenthangingN"><em>b.</em> <code>AACAAAB</code></p>
<p class="indenthangingN"><em>c.</em> <code>ABABABAB</code></p>
<p class="indenthangingN"><em>d.</em> <code>ABAABAAABAAAB</code></p>
<p class="indenthangingN"><em>e.</em> <code>ABAABCABAABCB</code></p>
<p><a id="ch05qa3q18"/><strong>5.3.18</strong> Suppose that the pattern and text are <em>random</em> strings over an alphabet of size <em>R</em> (which is at least 2). Show that the expected number of character compares for the brute-force method is (<em>N</em> − <em>M</em> + 1) (1 − <em>R</em><sup>−<em>M</em></sup>) / (1 − <em>R</em><sup>−1</sup>) ≤ 2(<em>N</em> − <em>M</em> + 1).</p>
<p><a id="ch05qa3q19"/><strong>5.3.19</strong> Construct an example where the Boyer-Moore algorithm (with only the mismatched character heuristic) performs poorly.</p>
<p><a id="ch05qa3q20"/><strong>5.3.20</strong> How would you modify the Rabin-Karp algorithm to determine whether any of a subset of <em>k</em> patterns (say, all of the same length) is in the text?</p>
<p><em>Solution</em>: Compute the hashes of the <em>k</em> patterns and store the hashes in a <code>StringSET</code> (see <a href="#ch05qa2q6"><small>EXERCISE 5.2.6</small></a>).</p>
<p><a id="ch05qa3q21"/><strong>5.3.21</strong> How would you modify the Rabin-Karp algorithm to search for a given pattern with the additional proviso that the middle character is a “wildcard” (any text character <a id="page_783"/>at all can match it).</p>
<p><a id="ch05qa3q22"/><strong>5.3.22</strong> How would you modify the Rabin-Karp algorithm to search for an <em>H</em>-by-<em>V</em> pattern in an <em>N</em>-by-<em>N</em> text?</p>
<p><a id="ch05qa3q23"/><strong>5.3.23</strong> Write a program that reads characters one at a time and reports at each instant if the current string is a palindrome. <em>Hint</em>: Use the Rabin-Karp hashing idea.</p>
<p><a id="ch05sec2lev36"/></p>
<h4><a id="page_784"/>Creative Problems</h4>
<p><a id="ch05qa3q24"/><strong>5.3.24</strong> <em>Find all occurrences.</em> Add a method <code>findAll()</code> to each of the four substring search algorithms given in the text that returns an <code>Iterable&lt;Integer&gt;</code> that allows clients to iterate through all offsets of the pattern in the text.</p>
<p><a id="ch05qa3q25"/><strong>5.3.25</strong> <em>Streaming.</em> Add a <code>search()</code> method to <code>KMP</code> that takes variable of type <code>In</code> as argument, and searches for the pattern in the specified input stream <em>without</em> using any extra instance variables. Then do the same for <code>RabinKarp</code>.</p>
<p><a id="ch05qa3q26"/><strong>5.3.26</strong> <em>Cyclic rotation check.</em> Write a program that, given two strings, determines whether one is a cyclic rotation of the other, such as <code>example</code> and <code>ampleex</code>.</p>
<p><a id="ch05qa3q27"/><strong>5.3.27</strong> <em>Tandem repeat search.</em> A tandem repeat of a base string <code>b</code> in a string <code>s</code> is a substring of <code>s</code> having at least two consecutive copies <code>b</code> (nonoverlapping). Develop and implement a linear-time algorithm that, given two strings <code>b</code> and <code>s</code>, returns the index of the beginning of the longest tandem repeat of <code>b</code> in <code>s</code>. For example, your program should return <code>3</code> when <code>b</code> is <code>abcab</code> and <code>s</code> is <code>abc<span class="pd_red">abcababcababcab</span>abcab</code>.</p>
<p><a id="ch05qa3q28"/><strong>5.3.28</strong> <em>Buffering in brute-force search.</em> Add a <code>search()</code> method to your solution to <a href="#ch05qa3q1"><small>EXERCISE 5.3.1</small></a> that takes an input stream (of type <code>In</code>) as argument and searches for the pattern in the given input stream. <em>Note</em>: You need to maintain a buffer that can keep at least the previous <code>M</code> characters in the input stream. Your challenge is to write efficient code to initialize, update, and clear the buffer for any input stream.</p>
<p><a id="ch05qa3q29"/><strong>5.3.29</strong> <em>Buffering in Boyer-Moore.</em> Add a <code>search()</code> method to <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a> that takes an input stream (of type <code>In</code>) as argument and searches for the pattern in the given input stream.</p>
<p><a id="ch05qa3q30"/><strong>5.3.30</strong> <em>Two-dimensional search.</em> Implement a version of the Rabin-Karp algorithm to search for patterns in two-dimensional text. Assume both pattern and text are rectangles of characters.</p>
<p><a id="ch05qa3q31"/><strong>5.3.31</strong> <em>Random patterns.</em> How many character compares are needed to do a substring search for a random pattern of length 100 in a given text?</p>
<p><em>Answer</em>: None. The method</p>
<p class="programlisting">public boolean search(char[] txt)<br/>
{  return false; }</p>
<p><a id="page_785"/>is quite effective for this problem, since the chances of a random pattern of length 100 appearing in any text are so low that you may consider it to be 0.</p>
<p><a id="ch05qa3q32"/><strong>5.3.32</strong> <em>Unique substrings.</em> Solve <a href="#ch05qa2q14"><small>EXERCISE 5.2.14</small></a> using the idea behind the Rabin-Karp method.</p>
<p><a id="ch05qa3q33"/><strong>5.3.33</strong> <em>Random primes.</em> Implement <code>longRandomPrime()</code> for <code>RabinKarp</code> (<a href="#ch05sb26"><small>ALGORITHM 5.8</small></a>). <em>Hint</em>: A random <em>n</em>-digit number is prime with probability proportional to 1/<em>n</em>.</p>
<p><a id="ch05qa3q34"/><strong>5.3.34</strong> <em>Straight-line code.</em> The Java Virtual Machine (and your computer’s assembly language) support a <code>goto</code> instruction so that the search can be “wired in” to machine code, like the program at right (which is exactly equivalent to simulating the DFA for the pattern as in <code>KMPdfa</code>, but likely to be much more efficient). To avoid checking whether the end of the text has been reached each time <code>i</code> is incremented, we assume that the pattern itself is stored at the end of the text as a sentinel, as the last <code>M</code> characters of the text. The <code>goto</code> labels in this code correspond precisely to the <code>dfa[]</code> array. Write a static method that takes a pattern as input and produces as output a straight-line program like this that searches for the pattern.</p>
<p class="image"><img alt="image" src="graphics/p0785-01.jpg"/></p>
<p><a id="ch05qa3q35"/><strong>5.3.35</strong> <em>Boyer-Moore in binary strings.</em> The mismatched character heuristic does not help much for binary strings, because there are only two possibilities for characters that cause the mismatch (and these are both likely to be in the pattern). Develop a substring search class for binary strings that groups bits together to make “characters” that can be used exactly as in <a href="#ch05sb24"><small>ALGORITHM 5.7</small></a>. <em>Note</em>: If you take <em>b</em> bits at a time, then you need a <code>right[]</code> array with 2<em><sup>b</sup></em> entries. The value of <em>b</em> should be chosen small enough so that this table is not too large, but large enough that most <em>b</em>-bit sections of the text are not likely to be in the pattern—there are <em>M</em>−<em>b</em>+1 different <em>b</em>-bit sections in the pattern (one starting at each bit position from 1 through <em>M</em>−<em>b</em>+1), so we want <em>M</em>−<em>b</em>+1 to be significantly less than 2<em><sup>b</sup></em>. For example, if you take 2<em><sup>b</sup></em> to be about lg (4<em>M</em>), then the <code>right[]</code> array will be more than three-quarters filled with <code>-1</code> entries, but do not let <em>b</em> become less than <em>M</em>/2, since otherwise you could miss the pattern entirely, if it were split between two <em>b</em>-bit text sections.</p>
<p><a id="ch05sec2lev37"/></p>
<h4><a id="page_786"/>Experiments</h4>
<p><a id="ch05qa3q36"/><strong>5.3.36</strong> <em>Random text.</em> Write a program that takes integers <code>M</code> and <code>N</code> as arguments, generates a random binary text string of length <code>N</code>, then counts the number of other occurrences of the last <code>M</code> bits elsewhere in the string. <em>Note</em>: Different methods may be appropriate for different values of <code>M</code>.</p>
<p><a id="ch05qa3q37"/><strong>5.3.37</strong> <em>KMP for random text.</em> Write a client that takes integers <code>M</code>, <code>N</code>, and <code>T</code> as input and runs the following experiment <code>T</code> times: Generate a random pattern of length <code>M</code> and a random text of length <code>N</code>, counting the number of character compares used by <code>KMP</code> to search for the pattern in the text. Instrument <code>KMP</code> to provide the number of compares, and print the average count for the <code>T</code> trials.</p>
<p><a id="ch05qa3q38"/><strong>5.3.38</strong> <em>Boyer-Moore for random text.</em> Answer the previous exercise for <code>BoyerMoore</code>.</p>
<p><a id="ch05qa3q39"/><strong>5.3.39</strong> <em>Timings.</em> Write a program that times the four methods for the task of searchng for the substring</p>
<p class="programlisting">it is a far far better thing that i do than i have ever done</p>
<p>in the text of <em>Tale of Two Cities</em> (<code>tale.txt</code>). Discuss the extent to which your results validate the hypthotheses about performance that are stated in the text.</p>
<p><a id="ch05sec1lev10"/></p>
<h3><a id="page_788"/>5.4 Regular Expressions</h3>
<p><small>IN MANY APPLICATIONS</small>, we need to do substring searching with somewhat less than complete information about the pattern to be found. A user of a text editor may wish to specify only part of a pattern, or to specify a pattern that could match a few different words, or to specify that any one of a number of patterns would do. A biologist might search for a genomic sequence satisfying certain conditions. In this section we will consider how pattern matching of this type can be done efficiently.</p>
<p>The algorithms in the previous section fundamentally depend on complete specification of the pattern, so we have to consider different methods. The basic mechanisms we will consider make possible a very powerful string-searching facility that can match complicated <em>M</em>-character patterns in <em>N</em>-character text strings in time proportional to <em>MN</em> in the worst case, and much faster for typical applications.</p>
<p>First, we need a way to describe the patterns: a rigorous way to specify the kinds of partial-substring-searching problems suggested above. This specification needs to involve more powerful primitive operations than the “check if the <em>i</em>th character of the text string matches the <em>j</em>th character of the pattern” operation used in the previous section. For this purpose, we use <em>regular expressions</em>, which describe patterns in combinations of three natural, basic, and powerful operations.</p>
<p>Programmers have used regular expressions for decades. With the explosive growth of search opportunities on the web, their use is becoming even more widespread. We will discuss a number of specific applications at the beginning of the section, not only to give you a feeling for their utility and power, but also to enable you to become more familiar with their basic properties.</p>
<p>As with the KMP algorithm in the previous section, we consider the three basic operations in terms of an abstract machine that can search for patterns in a text string. Then, as before, our pattern-matching algorithm will construct such a machine and then simulate its operation. Naturally, pattern-matching machines are typically more complicated than KMP DFAs, but not as complicated as you might expect.</p>
<p>As you will see, the solution we develop to the pattern-matching problem is intimately related to fundamental processes in computer science. For example, the method we will use in our program to perform the string-searching task implied by a given pattern description is akin to the method used by the Java system to transform a given Java program into a machine-language program for your computer. We also encounter the concept of <em>nondeterminism</em>, which plays a critical role in the search for efficient algorithms (see <a href="ch06.html#ch06"><small>CHAPTER 6</small></a>).</p>
<p><a id="ch05sec2lev38"/></p>
<h4><a id="page_789"/>Describing patterns with regular expressions</h4>
<p>We focus on pattern descriptions made up of characters that serve as operands for three fundamental operations. In this context, we use the word <em>language</em> specifically to refer to a set of strings (possibly infinite) and the word <em>pattern</em> to refer to a language specification. The rules that we consider are quite analogous to familiar rules for specifying arithmetic expressions.</p>
<p><a id="ch05sec3lev51"/></p>
<h5><em>Concatenation</em></h5>
<p>The first fundamental operation is the one used in the last section. When we write <code>AB</code>, we are specifying the language {<code>AB</code>} that has one two-character string, formed by concatenating <code>A</code> and <code>B</code>.</p>
<p><a id="ch05sec3lev52"/></p>
<h5><em>Or</em></h5>
<p>The second fundamental operation allows us to specify alternatives in the pattern. If we have an <em>or</em> between two alternatives, then both are in the language. We will use the vertical bar symbol <code>|</code> to denote this operation. For example, <code>A|B</code> specifies the language {<code>A</code>, <code>B</code>} and <code>A|E|I|O|U</code> specifies the language {<code>A</code>, <code>E</code>, <code>I</code>, <code>O</code>, <code>U</code>}. Concatenation has higher precedence than <em>or</em>, so <code>AB|BCD</code> specifies the language {<code>AB</code>, <code>BCD</code>}.</p>
<p><a id="ch05sec3lev53"/></p>
<h5><em>Closure</em></h5>
<p>The third fundamental operation allows parts of the pattern to be repeated arbitrarily. The <em>closure</em> of a pattern is the language of strings formed by concatenating the pattern with itself any number of times (including zero). We denote closure by placing a <code>*</code> after the pattern to be repeated. Closure has higher precedence than concatenation, so <code>AB*</code> specifies the language consisting of strings with an <code>A</code> followed by <code>0</code> or more <code>B</code>s, while <code>A*B</code> specifies the language consisting of strings with <code>0</code> or more <code>A</code>s followed by a <code>B</code>. The <em>empty string</em>, which we denote by <img alt="image" src="graphics/isin.jpg"/>, is found in every text string (and in <code>A*</code>).</p>
<p><a id="ch05sec3lev54"/></p>
<h5><em>Parentheses</em></h5>
<p>We use parentheses to override the default precedence rules. For example, <code>C(AC|B)D</code> specifies the language {<code>CACD</code>, <code>CBD</code>}; <code>(A|C)((B|C)D)</code> specifies the language {<code>ABD</code>, <code>CBD</code>, <code>ACD</code>, <code>CCD</code>}; and <code>(AB)*</code> specifies the language of strings formed by concatenating any number of occurrences of <code>AB</code>, including no occurrences: {<img alt="image" src="graphics/isin.jpg"/>, <code>AB</code>, <code>ABAB</code>, . . .}.</p>
<p class="image"><img alt="image" src="graphics/t0789-01.jpg"/></p>
<p>These simple rules allow us to write down REs that, while complicated, clearly and completely describe languages (see the table at right for a few examples). Often, a language can be simply described in some other way, but discovering such a description can be a challenge. For example, the RE in the bottom row of the table specifies the subset of <code>(A|B)*</code> with an even number of <code>B</code>s.</p>
<p><a id="page_790"/><small>REGULAR EXPRESSIONS ARE EXTREMELY SIMPLE</small> formal objects, even simpler than the arithmetic expressions that you learned in grade school. Indeed, we will take advantage of their simplicity to develop compact and efficient algorithms for processing them. Our starting point will be the following formal definition:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb28"/></p>
<p><strong>Definition.</strong> A regular expression (RE) is either</p>
<p class="indenthangingB">• The empty set Ø.</p>
<p class="indenthangingB">• The empty string <img alt="image" src="graphics/isin.jpg"/></p>
<p class="indenthangingB">• A single character</p>
<p class="indenthangingB">• A regular expression enclosed in parentheses</p>
<p class="indenthangingB">• Two or more <em>concatenated</em> regular expressions</p>
<p class="indenthangingB">• Two or more regular expressions separated by the <em>or</em> operator (<code>|</code>)</p>
<p class="indenthangingB">• A regular expression followed by the <em>closure</em> operator (<code>*</code>)</p>
<hr/>
</div>
<p>This definition describes the <em>syntax</em> of regular expressions, telling us what constitutes a legal regular expression. The <em>semantics</em> that tells us the meaning of a given regular expression is the point of the informal descriptions that we have given in this section. For review, we summarize these by continuing the formal definition:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb29"/></p>
<p><strong>Definition (continued).</strong> Each RE represents a set of strings, defined as follows:</p>
<p class="indenthangingB">• The empty set Ø represents the set of strings with 0 elements.</p>
<p class="indenthangingB">• The empty string <img alt="image" src="graphics/isin.jpg"/> represents the set of strings with one element, the string with zero characters.</p>
<p class="indenthangingB">• A single character represents the set of strings with one element, itself.</p>
<p class="indenthangingB">• An RE enclosed in parentheses represents the same set of strings as the RE without the parentheses.</p>
<p class="indenthangingB">• The RE consisting of two <em>concatenated</em> REs represents the <em>cross product</em> of the sets of strings represented by the individual components (all possible strings that can be formed by taking one string from each and concatenating them, in the same order as the REs).</p>
<p class="indenthangingB">• The RE consisting of the <em>or</em> of two REs represents the <em>union</em> of the sets represented by the individual components.</p>
<p class="indenthangingB">• The RE consisting of the <em>closure</em> of an RE represents <img alt="image" src="graphics/isin.jpg"/> or the union of the sets represented by the concatenation of any number of copies of the RE.</p>
<hr/>
</div>
<p>There are many different ways to describe each language: we must try to specify succinct patterns just as we try to write compact programs and implement effi cient algorithms.</p>
<p><a id="ch05sec2lev39"/></p>
<h4><a id="page_791"/>Shortcuts</h4>
<p>Typical applications adopt various additions to these basic rules to enable us to develop succinct descriptions of languages of practical interest. From a theoretical standpoint, these are each simply a shortcut for a sequence of operations involving many operands; from a practical standpoint, they are a quite useful extention to the basic operations that enable us to develop compact patterns.</p>
<p><a id="ch05sec3lev55"/></p>
<h5><em>Set-of-characters descriptors</em></h5>
<p>It is often convenient to be able to use a single character or a short sequence to directly specify sets of characters. The dot character (.) is a <em>wildcard</em> that represents any single character. A sequence of characters within square brackets represents any one of those characters. The sequence may also be specified as a range of characters. If preceded by a <code>^</code>, a sequence within square brackets represents any character <em>but</em> one of those characters. These notations are simply shortcuts for a sequence of <em>or</em> operations.</p>
<p class="image"><img alt="image" src="graphics/t0791-01.jpg"/></p>
<p><a id="ch05sec3lev56"/></p>
<h5><em>Closure shortcuts</em></h5>
<p>The closure operator specifies any number of copies of its operand. In practice, we want the flexibility to specify the number of copies, or a range on the number. In particular, we use the plus sign (<code>+</code>) to specify at least one copy, the question mark (<code>?</code>) to specify zero or one copy, and a count or a range within braces (<code>{}</code>) to specify a given number of copies. Again, these notations are shortcuts for a sequence of the basic concatenation, <em>or</em>, and closure operations.</p>
<p><a id="ch05sec3lev57"/></p>
<h5><em>Escape sequences</em></h5>
<p>Some characters, such as <code>\</code>, <code>.</code>, <code>|</code>, <code>*</code>, <code>(</code>, and <code>)</code>, are <em>metacharacters</em> that we use to form regular expressions. We use <em>escape sequences</em> that begin with a backslash character <code>\</code> separating metacharacters from characters in the alphabet. An escape sequence may be a <code>\</code> followed by a single metacharacter (which represents that character). For example, <code>\\</code> represents <code>\.</code> Other escape sequences represent special characters and whitespace. For example, <code>\t</code> represents a tab character, <code>\n</code> represents a newline, and <code>\s</code> represents any whitespace character.</p>
<p class="image"><img alt="image" src="graphics/t0791-02.jpg"/></p>
<p><a id="ch05sec2lev40"/></p>
<h4><a id="page_792"/>REs in applications</h4>
<p>REs have proven to be remarkably versatile in describing languages that are relevant in practical applications. Accordingly, REs are heavily used and have been heavily studied. To familiarize you with regular expressions while at the same time giving you some appreciation for their utility, we consider a number of practical applications before addressing the RE pattern-matching algorithm. REs also play an important role in theoretical computer science. Discussing this role to the extent it deserves is beyond the scope of this book, but we sometimes allude to relevant fundamental theoretical results.</p>
<p><a id="ch05sec3lev58"/></p>
<h5><em>Substring search</em></h5>
<p>Our general goal is to develop an algorithm that determines whether a given text string is in the set of strings described by a given regular expression. If a text is in the language described by a pattern, we say that the text <em>matches</em> the pattern. Pattern matching with REs vastly generalizes the substring search problem of <a href="#ch05sec1lev9"><small>SECTION 5.3</small></a>. Precisely, to search for a substring <code>pat</code> in a text string <code>txt</code> is to check whether <code>txt</code> is in the language described by the pattern <code>.*pat.*</code> or not.</p>
<p><a id="ch05sec3lev59"/></p>
<h5><em>Validity checking</em></h5>
<p>You frequently encounter RE matching when you use the web. When you type in a date or an account number on a commercial website, the input-processing program has to check that your response is in the right format. One approach to performing such a check is to write code that checks all the cases: if you were to type in a dollar amount, the code might check that the first symbol is a <code>$</code>, that the <code>$</code> is followed by a set of digits, and so forth. A better approach is to define an RE that describes the set of all legal inputs. Then, checking whether your input is legal is precisely the pattern-matching problem: is your input in the language described by the RE? Libraries of REs for common checks have sprung up on the web as this type of checking has come into widespread use. Typically, an RE is a much more precise and concise expression of the set of all valid strings than would be a program that checks all the cases.</p>
<p class="image"><img alt="image" src="graphics/t0792-01.jpg"/></p>
<p><a id="ch05sec3lev60"/></p>
<h5><a id="page_793"/><em>Programmer’s toolbox</em></h5>
<p>The origin of regular expression pattern matching is the Unix command <code>grep</code>, which prints all lines matching a given RE. This capability has proven invaluable for generations of programmers, and REs are built into many modern programming systems, from <code>awk</code> and <code>emacs</code> to Perl, Python, and JavaScript. For example, suppose that you have a directory with dozens of <code>.java</code> files, and you want to know which of them has code that uses <code>StdIn</code>. The command</p>
<p class="programlisting">% grep StdIn *.java</p>
<p>will immediately give the answer. It prints all lines that match <code>.*StdIn.*</code> for each file.</p>
<p><a id="ch05sec3lev61"/></p>
<h5><em>Genomics</em></h5>
<p>Biologists use REs to help address important scientific problems. For example, the human gene sequence has a region that can be described with the RE <code>gcg(cgg)*ctg</code>, where the number of repeats of the <code>cgg</code> pattern is highly variable among individuals, and a certain genetic disease that can cause mental retardation and other symptoms is known to be associated with a high number of repeats.</p>
<p><a id="ch05sec3lev62"/></p>
<h5><em>Search</em></h5>
<p>Web search engines support REs, though not always in their full glory. Typically, if you want to specify alternatives with <code>|</code> or repetition with <code>*</code>, you can do so.</p>
<p><a id="ch05sec3lev63"/></p>
<h5><em>Possibilities</em></h5>
<p>A first introduction to theoretical computer science is to think about the set of languages that can be specified with an RE. For example, you might be surprised to know that you can implement the modulus operation with an RE: for example, <code>(0 | 1(01*0)*1)*</code> describes all strings of <code>0</code>s and <code>1</code>s that are the binary representatons of numbers that are multiples of three (!): <code>11</code>, <code>110</code>, <code>1001</code>, and <code>1100</code> are in the language, but <code>10</code>, <code>1011</code>, and <code>10000</code> are not.</p>
<p><a id="ch05sec3lev64"/></p>
<h5><em>Limitations</em></h5>
<p>Not all languages can be specified with REs. A thought-provoking example is that no RE can describe the set of all strings that specify legal REs. Simpler versions of this example are that we cannot use REs to check whether parentheses are balanced or to check whether a string has an equal number of <code>A</code>s and <code>B</code>s.</p>
<p><small>THESE EXAMPLES JUST SCRATCH THE SURFACE</small>. Suffice it to say that REs are a useful part of our computational infrastructure and have played an important role in our understanding of the nature of computation. As with KMP, the algorithm that we describe next is a byproduct of the search for that understanding.</p>
<p><a id="ch05sec2lev41"/></p>
<h4><a id="page_794"/>Nondeterministic finite-state automata</h4>
<p>Recall that we can view the Knuth-Morris-Pratt algorithm as a finite-state machine constructed from the search pattern that scans the text. For regular expression pattern matching, we will generalize this idea.</p>
<p>The finite-state automaton for KMP changes from state to state by looking at a character from the text string and then changing to another state, depending on the character. The automaton reports a match if and only if it reaches the accept state. The algorithm itself is a simulation of the automaton. The characteristic of the machine that makes it easy to simulate is that it is <em>deterministic</em>: each state transition is completely determined by the next character in the text.</p>
<p>To handle regular expressions, we consider a more powerful abstract machine. Because of the <em>or</em> operation, the automaton cannot determine whether or not the pattern could occur at a given point by examining just one character; indeed, because of closure, it cannot even determine <em>how many</em> characters might need to be examined before a mismatch is discovered. To overcome these problems, we will endow the automaton with the power of <em>nondeterminism</em>: when faced with more than one way to try to match the pattern, the machine can “guess” the right one! This power might seem to you to be impossible to realize, but we will see that it is easy to write a program to build a <em>nondeterministic finite-state automaton</em> (NFA) and to efficiently simulate its operation. The overview of our RE pattern matching algorithm is the nearly the same as for KMP:</p>
<p class="indenthangingB">• Build the NFA corresponding to the given RE.</p>
<p class="indenthangingB">• Simulate the operation of that NFA on the given text.</p>
<p><em>Kleene’s Theorem</em>, a fundamental result of theoretical computer science, asserts that there is an NFA corresponding to any given RE (and vice versa). We will consider a constructive proof of this fact that will demonstrate how to transform any RE into an NFA; then we simulate the operation of the NFA to complete the job.</p>
<p>Before we consider how to build pattern-matching NFAs, we will consider an example that illustrates their properties and the basic rules for operating them. Consider the figure below, which shows an NFA that determines whether a text string is in the language described by the RE <code>((A*B|AC)D)</code>. As illustrated in this example, the NFAs that we define have the following characteristics:</p>
<p class="indenthangingB">• The NFA corresponding to an RE of length <em>M</em> has exactly one state per pattern character, starts at state 0, and has a (virtual) accept state <em>M</em>.</p>
<p class="image"><img alt="image" src="graphics/05_49-nfa.jpg"/></p>
<p class="indenthangingB"><a id="page_795"/>• States corresponding to a character from the alphabet have an outgoing edge that goes to the state corresponding to the next character in the pattern (black edges in the diagram).</p>
<p class="indenthangingB">• States corresponding to the metacharacters <code>(</code>, <code>)</code>, <code>|</code>, and <code>*</code> have at least one outgoing edge (red edges in the diagram), which may go to any other state.</p>
<p class="indenthangingB">• Some states have multiple outgoing edges, but no state has more than one outgoing black edge.</p>
<p>By convention, we enclose all patterns in parentheses, so the first state corresponds to a left parenthesis and the final state corresponds to a right parenthesis (and has a transition to the accept state).</p>
<p>As with the DFAs of the previous section, we start the NFA at state 0, reading the first character of a text. The NFA moves from state to state, sometimes reading text characters, one at a time, from left to right. However, there are some basic differences from DFAs:</p>
<p class="indenthangingB">• Characters appear in the nodes, not the edges, in the diagrams.</p>
<p class="indenthangingB">• Our NFA recognizes a text string only after explicitly reading all its characters, whereas our DFA recognizes a pattern in a text without necessarily reading all the text characters.</p>
<p>These differences are not critical—we have picked the version of each machine that is best suited to the algorithms that we are studying.</p>
<p>Our focus now is on checking whether the text matches the pattern—for that, we need the machine to reach its accept state and consume all the text. The rules for moving from one state to another are also different than for DFAs—an NFA can do so in one of two ways:</p>
<p class="indenthangingB">• If the current state corresponds to a character in the alphabet <em>and</em> the current character in the text string matches the character, the automaton can scan past the character in the text string and take the (black) transition to the next state. We refer to such a transition as a <em>match transition</em>.</p>
<p class="indenthangingB">• The automaton can follow any red edge to another state without scanning any text character. We refer to such a transition as an <img alt="image" src="graphics/isin.jpg"/>-<em>transition</em>, referring to the idea that it corresponds to “matching” the empty string <img alt="image" src="graphics/isin.jpg"/>.</p>
<p class="image"><img alt="image" src="graphics/05_50-nfatrace0.jpg"/></p>
<p><a id="page_796"/>For example, suppose that our NFA for <code>( ( A * B | A C ) D )</code> is started (at state 0) with the text <code>A A A A B D</code> as input. The figure at the bottom of the previous page shows a sequence of state transitions ending in the accept state. This sequence demonstrates that the text is in the set of strings described by the RE—the text <em>matches</em> the pattern. With respect to the NFA, we say that the NFA <em>recognizes</em> that text.</p>
<p class="image"><img alt="image" src="graphics/05_51-nfatrace1.jpg"/></p>
<p>The examples shown at left illustrate that it is also possible to find transition sequences that cause the NFA to stall, even for input text such as <code>A A A A B D</code> that it should recognize. For example, if the NFA takes the transition to state 4 before scanning all the <code>A</code>s, it is left with nowhere to go, since the only way out of state 4 is to match a <code>B</code>. These two examples demonstrate the nondeterministic nature of the automaton. After scanning an <code>A</code> and finding itself in state 3, the NFA has two choices: it could go on to state 4 or it could go back to state 2. The choices make the difference between getting to the accept state (as in the first example just discussed) or stalling (as in the second example just discussed). This NFA also has a choice to make at state 1 (whether to take an <img alt="image" src="graphics/isin.jpg"/>-transition to state 2 or to state 6).</p>
<p>These examples illustrate the key difference between NFAs and DFAs: since an NFA may have multiple edges leaving a given state, the transition from such a state is <em>not deterministic</em>—it might take one transition at one point in time and a different transition at a different point in time, without scanning past any text character. To make some sense of the operation of such an automaton, imagine that an NFA has the power to <em>guess</em> which transition (if any) will lead to the accept state for the given text string. In other words, we say that <em>an NFA recognizes a text string if and only if there is some sequence of transitions that scans all the text characters and ends in the accept state when started at the beginning of the text in state 0</em>. Conversely, an NFA does not recognize a text string if and only if there is no sequence of match transitions and <img alt="image" src="graphics/isin.jpg"/>-transitions that can scan all the text characters and lead to the accept state for that string.</p>
<p>As with DFAs, we have been tracing the operation of the NFA on a text string simply by listing the sequence of state changes, ending in the final state. Any such sequence is a proof that the machine recognizes the text string (there may be other proofs). But how do we find such a sequence for a given text string? And how do we prove that there is no such sequence for another given text string? The answers to these questions are easier than you might think: we systematically try all possibilities.</p>
<p><a id="ch05sec2lev42"/></p>
<h4><a id="page_797"/>Simulating an NFA</h4>
<p>The idea of an automaton that can guess the state transitions it needs to get to the accept state is like writing a program that can guess the right answer to a problem: it seems ridiculous. On reflection, you will see that the task is conceptually not at all difficult: we make sure that we check all possible sequences of state transitions, so if there is one that gets to the accept state, we will find it.</p>
<p><a id="ch05sec3lev65"/></p>
<h5><em>Representation</em></h5>
<p>To begin, we need an NFA representation. The choice is clear: the RE itself gives the state names (the integers between <code>0</code> and <code>M</code>, where <code>M</code> is the number of characters in the RE). We keep the RE itself in an array <code>re[]</code> of <code>char</code> values that defines the match transitions (if <code>re[i]</code> is in the alphabet, then there is a match transition from <code>i</code> to <code>i+1</code>). The natural representation for the <img alt="image" src="graphics/isin.jpg"/>-transitions is a <em>digraph—</em>they are directed edges (red edges in our diagrams) connecting vertices between <code>0</code> and <code>M</code> (one for each state). Accordingly, we represent all the <img alt="image" src="graphics/isin.jpg"/>-transitions as a digraph <code>G</code>. We will consider the task of building the digraph associated with a given RE after we consider the simulation process. For our example, the digraph consists of the nine edges</p>
<p class="indenthanging"><code>0</code> → <code>1 1</code> → <code>2 1</code> → <code>6 2</code> → <code>3 3</code> → <code>2 3</code> → <code>4 5</code> → <code>8 8</code> → <code>9 10</code> → <code>11</code></p>
<p><a id="ch05sec3lev66"/></p>
<h5><em>NFA simulation and reachability</em></h5>
<p>To simulate an NFA, we keep track of the <em>set</em> of states that could possibly be encountered while the automaton is examining the current input character. The key computation is the familiar <em>multiple-source reachability</em> computation that we addressed in <a href="ch04.html#ch04sb21"><small>ALGORITHM 4.4</small></a> (page <a href="ch04.html#ch04sb21">571</a>). To initialize this set, we find the set of states reachable via <img alt="image" src="graphics/isin.jpg"/>-transitions from state 0. For each such state, we check whether a match transition for the first input character is possible. This check gives us the set of possible states for the NFA just after matching the first input character. To this set, we add all states that could be reached via <img alt="image" src="graphics/isin.jpg"/>-transitions from one of the states in the set. Given the set of possible states for the NFA just after matching the first character in the input, the solution to the multiple-source reachability problem in the <img alt="image" src="graphics/isin.jpg"/>-transition digraph gives the set of states that could lead to match transitions for the <em>second</em> character in the input. For example, the initial set of states for our example NFA is <code>0 1 2 3 4 6</code>; if the first character is an <code>A</code>, the NFA could take a match transition to <code>3</code> or <code>7</code>; then it could take <img alt="image" src="graphics/isin.jpg"/>-transitions from <code>3</code> to <code>2</code> or <code>3</code> to <code>4</code>, so the set of possible states that could lead to a match transition for the second character is <code>2 3 4 7</code>. Iterating this process until all text characters are exhausted leads to one of two outcomes:</p>
<p class="indenthangingB">• The set of possible states contains the accept state.</p>
<p class="indenthangingB">• The set of possible states does not contain the accept state.</p>
<p>The first of these outcomes indicates that there is some sequence of transitions that takes the NFA to the accept state, so we report success. The second of these outcomes indicates that the NFA always stalls on that input, so we report failure. With our <code>SET</code> <a id="page_799"/>data type and the <code>DirectedDFS</code> class just described for computing multiple-source reachability in a digraph, the NFA simulation code given below is a straightforward translation of the English-language description just given. You can check your understanding of the code by following the trace on the facing page, which illustrates the full simulation for our example.</p>
<p class="image"><a id="page_798"/><img alt="image" src="graphics/05_52-nfatraceall.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb30"/></p>
<p><strong>Proposition Q.</strong> Determining whether an <em>N</em>-character text string is recognized by the NFA corresponding to an <em>M</em>-character RE takes time proportional to <em>NM</em> in the worst case.</p>
<p><strong>Proof:</strong> For each of the <em>N</em> text characters, we iterate through a set of states of size no more than <em>M</em> and run a DFS on the digraph of <img alt="image" src="graphics/isin.jpg"/>-transitions. The construction that we will consider next establishes that the number of edges in that digraph is no more than 2<em>M</em>, so the worst-case time for each DFS is proportional to <em>M</em>.</p>
<hr/>
</div>
<p>Take a moment to reflect on this remarkable result. This worst-case cost, the product of the text and pattern lengths, is <em>the same</em> as the worst-case cost of finding an exact substring match using the elementary algorithm that we started with at the beginning of <a href="#ch05sec1lev9"><small>SECTION 5.3</small></a>.</p>
<p class="image"><img alt="image" src="graphics/p0799-01.jpg"/></p>
<p><a id="ch05sec2lev43"/></p>
<h4><a id="page_800"/>Building an NFA corresponding to an RE</h4>
<p>From the similarity between regular expressions and familiar arithmetic expressions, you may not be surprised to find that translating an RE to an NFA is somewhat similar to the process of evaluating an arithmetic expression using Dijkstra’s two-stack algorithm, which we considered in <a href="ch01a.html#ch01sec1lev5"><small>SECTION 1.3</small></a>. The process is a bit different because</p>
<p class="indenthangingB">• REs do not have an explicit operator for concatenation</p>
<p class="indenthangingB">• REs have a unary operator, for closure (<code>*</code>)</p>
<p class="indenthangingB">• REs have only one binary operator, for <em>or</em> (<code>|</code>)</p>
<p>Rather than dwell on the differences and similarities, we will consider an implementation that is tailored for REs. For example, we need only one stack, not two.</p>
<p>From the discussion of the representation at the beginning of the previous subsection, we need only build the digraph <code>G</code> that consists of all the <img alt="image" src="graphics/isin.jpg"/>-transitions. The RE itself and the formal definitions that we considered at the beginning of this section provide precisely the information that we need. Taking a cue from Dijkstra’s algorithm, we will use a stack to keep track of the positions of left parentheses and <em>or</em> operators.</p>
<p><a id="ch05sec3lev67"/></p>
<h5><em>Concatenation</em></h5>
<p>In terms of the NFA, the concatenation operation is the simplest to implement. Match transitions for states corresponding to characters in the alphabet explicitly implement concatenation.</p>
<p><a id="ch05sec3lev68"/></p>
<h5><em>Parentheses</em></h5>
<p>We push the RE index of each left parenthesis on the stack. Each time we encounter a right parenthesis, we eventually pop the corresponding left parentheses from the stack in the manner described below. As in Dijkstra’s algorithm, the stack enables us to handle nested parentheses in a natural manner.</p>
<p><a id="ch05sec3lev69"/></p>
<h5><em>Closure</em></h5>
<p>A closure (*) operator must occur either (<em>i</em>) after a single character, when we add <img alt="image" src="graphics/isin.jpg"/>-transitions to and from the character, or (<em>ii</em>) after a right parenthesis, when we add <img alt="image" src="graphics/isin.jpg"/>-transitions to and from the corresponding left parenthesis, the one at the top of the stack.</p>
<p><a id="ch05sec3lev70"/></p>
<h5><em>Or expression</em></h5>
<p>We process an RE of the form <code>(A | B)</code> where <code>A</code> and <code>B</code> are both REs by adding two <img alt="image" src="graphics/isin.jpg"/>-transitions: one from the state corresponding to the left parenthesis to the state corresponding to the first character of <code>B</code> and one from the state corresponding to the | operator to the state corresponding to the right parenthesis. We push the RE index corresponding the | operator onto the stack (as well as the index corresponding to the left parenthesis, as described above) so that the information we need is at the top of the stack when needed, at the time we reach the right parenthesis. These <img alt="image" src="graphics/isin.jpg"/>-transitions allow the NFA to choose one of the two alternatives. We do not add an <img alt="image" src="graphics/isin.jpg"/>-transition from the state corresponding to the | operator to the state with the next higher index, as we have for all other states—the only way for the NFA to leave such a state is to take a transition to the state corresponding to the right parenthesis.</p>
<p><a id="page_801"/><small>THESE SIMPLE RULES SUFFICE</small> TO build NFAs corresponding to arbitrarily complicated REs. <a href="#ch05sb31"><small>ALGORITHM 5.9</small></a> is an implementation whose constructor builds the <img alt="image" src="graphics/isin.jpg"/>-transition digraph corresponding to a given RE, and a trace of the construction for our example appears on the following page. You can find other examples at the bottom of this page and in the exercises and are encouraged to enhance your understanding of the process by working your own examples. For brevity and for clarity, a few details (handling metacharacters, set-of-character descriptors, closure shortcuts, and multiway or operations) are left for exercises (see <a href="#ch05qa4q16"><small>EXERCISES 5.4.16</small></a> through <a href="#ch05qa4q21">5.4.21</a>). Otherwise, the construction requires remarkably little code and represents one of the most ingenious algorithms that we have seen.</p>
<p class="image"><img alt="image" src="graphics/05_53-kmpdfabuild.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb31"/></p>
<h3><a id="page_802"/>Algorithm 5.9 Regular expression pattern matching (grep)</h3>
<p class="programlisting2"><img alt="image" src="graphics/t0802-01.jpg"/></p>
<p class="programlisting2"><img alt="image" src="graphics/t0802-02.jpg"/></p>
<p>This constructor builds an NFA corresponding to a given RE by creating a digraph of <img alt="image" src="graphics/isin.jpg"/>-transitions.</p>
<hr/>
</div>
<p class="image"><a id="page_803"/><img alt="image" src="graphics/05_54-nfaconstruct.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb32"/></p>
<p><a id="page_804"/><strong>Proposition R.</strong> Building the NFA corresponding to an <em>M</em>-character RE takes time and space proportional to <em>M</em> in the worst case.</p>
<p><strong>Proof.</strong> For each of the <em>M</em> RE characters in the regular expression, we add at most three <img alt="image" src="graphics/isin.jpg"/>-transitions and perhaps execute one or two stack operations.</p>
<hr/>
</div>
<p>The classic <code>GREP</code> client for pattern matching, illustrated in the code at left, takes an RE as argument and prints the lines from standard input having some <em>substring</em> that is in the language described by the RE. This client was a feature in the early implementations of Unix and has been an indispensable tool for generations of programmers.</p>
<p class="image"><img alt="image" src="graphics/p0804-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0804-02.jpg"/></p>
<p><a id="ch05sec2lev44"/></p>
<h4><a id="page_805"/>Q&amp;A</h4>
<p><strong>Q.</strong> What is the difference between <em>null</em> and <img alt="image" src="graphics/isin.jpg"/>?</p>
<p><strong>A.</strong> The former denotes an empty <em>set</em>; the latter denotes an empty <em>string</em>. You can have a set that contains one element, <img alt="image" src="graphics/isin.jpg"/>, and is therefore not <em>null</em>.</p>
<p><a id="ch05sec2lev45"/></p>
<h4><a id="page_806"/>Exercises</h4>
<p><a id="ch05qa4q1"/><strong>5.4.1</strong> Give regular expressions that describe all strings that contain</p>
<p class="indenthangingB">• Exactly four consecutive <code>A</code>s</p>
<p class="indenthangingB">• No more than four consecutive <code>A</code>s</p>
<p class="indenthangingB">• At least one occurrence of four consecutive <code>A</code>s</p>
<p><a id="ch05qa4q2"/><strong>5.4.2</strong> Give a brief English description of each of the following REs:</p>
<p class="indenthangingN"><em>a.</em> <code>.*</code></p>
<p class="indenthangingN"><em>b.</em> <code>A.*A | A</code></p>
<p class="indenthangingN"><em>c.</em> <code>.*ABBABBA.*</code></p>
<p class="indenthangingN"><em>d.</em> .* <code>A.*A.*A.*A.*</code></p>
<p><a id="ch05qa4q3"/><strong>5.4.3</strong> What is the maximum number of different strings that can be described by a regular expression with <em>M or</em> operators and no closure operators (parentheses and concatenation are allowed)?</p>
<p><a id="ch05qa4q4"/><strong>5.4.4</strong> Draw the NFA corresponding to the pattern <code>(((A|B)*|CD*|EFG)*)*</code>.</p>
<p><a id="ch05qa4q5"/><strong>5.4.5</strong> Draw the digraph of <img alt="image" src="graphics/isin.jpg"/>-transitions for the NFA from <a href="#ch05qa4q4"><small>EXERCISE 5.4.4</small></a>.</p>
<p><a id="ch05qa4q6"/><strong>5.4.6</strong> Give the sets of states reachable by your NFA from <a href="#ch05qa4q4"><small>EXERCISE 5.4.4</small></a> after each character match and susbsequent <img alt="image" src="graphics/isin.jpg"/>-transitions for the input <code>ABBACEFGEFGCAAB</code>.</p>
<p><a id="ch05qa4q7"/><strong>5.4.7</strong> Modify the <code>GREP</code> client on page <a href="#page_804">804</a> to be a client <code>GREPmatch</code> that encloses the pattern in parentheses but does <em>not</em> add .* before and after the pattern, so that it prints out only those lines that are strings in the language described by the given RE. Give the result of typing each of the following commands:</p>
<p class="indenthangingN"><em>a.</em> <code>% java GREPmatch "(A|B)(C|D)" &lt; tinyL.txt</code></p>
<p class="indenthangingN"><em>b.</em> <code>% java GREPmatch "A(B|C)*D" &lt; tinyL.txt</code></p>
<p class="indenthangingN"><em>c.</em> <code>% java GREPmatch "(A*B|AC)D" &lt; tinyL.txt</code></p>
<p><a id="ch05qa4q8"/><strong>5.4.8</strong> Write a regular expression for each of the following sets of binary strings:</p>
<p class="indenthangingN"><em>a.</em> Contains at least three consecutive <code>1</code>s</p>
<p class="indenthangingN"><em>b.</em> Contains the substring <code>110</code></p>
<p class="indenthangingN"><em>c.</em> Contains the substring <code>1101100</code></p>
<p class="indenthangingN"><em>d.</em> Does not contain the substring <code>110</code></p>
<p><a id="page_807"/><a id="ch05qa4q9"/><strong>5.4.9</strong> Write a regular expression for binary strings with at least two 0s but not consecutive 0s.</p>
<p><a id="ch05qa4q10"/><strong>5.4.10</strong> Write a regular expression for each of the following sets of binary strings:</p>
<p class="indenthangingN"><em>a.</em> Has at least 3 characters, and the third character is <code>0</code></p>
<p class="indenthangingN"><em>b.</em> Number of <code>0</code>s is a multiple of 3</p>
<p class="indenthangingN"><em>c.</em> Starts and ends with the same character</p>
<p class="indenthangingN"><em>d.</em> Odd length</p>
<p class="indenthangingN"><em>e.</em> Starts with <code>0</code> and has odd length, or starts with <code>1</code> and has even length</p>
<p class="indenthangingN"><em>f.</em> Length is at least <code>1</code> and at most 3</p>
<p><a id="ch05qa4q11"/><strong>5.4.11</strong> For each of the following regular expressions, indicate how many bitstrings of length exactly 1,000 match:</p>
<p class="indenthangingN"><em>a.</em> <code>0(0 | 1)*1</code></p>
<p class="indenthangingN"><em>b.</em> <code>0*101*</code></p>
<p class="indenthangingN"><em>c.</em> <code>(1 | 01)*</code></p>
<p><a id="ch05qa4q12"/><strong>5.4.12</strong> Write a Java regular expression for each of the following:</p>
<p class="indenthangingN"><em>a.</em> Phone numbers, such as <code>(609) 555-1234</code></p>
<p class="indenthangingN"><em>b.</em> Social Security numbers, such as <code>123-45-6789</code></p>
<p class="indenthangingN"><em>c.</em> Dates, such as <code>December 31, 1999</code></p>
<p class="indenthangingN"><em>d.</em> IP addresses of the form <code>a.b.c.d</code> where each letter can represent one, two, or three digits, such as <code>196.26.155.241</code></p>
<p class="indenthangingN"><em>e.</em> License plates that start with four digits and end with two uppercase letters</p>
<p><a id="ch05sec2lev46"/></p>
<h4><a id="page_808"/>Creative Problems</h4>
<p><a id="ch05qa4q13"/><strong>5.4.13</strong> <em>Challenging REs.</em> Construct an RE that describes each of the following sets of strings over the binary alphabet:</p>
<p class="indenthangingN"><em>a.</em> All strings except <code>11</code> or <code>111</code></p>
<p class="indenthangingN"><em>b.</em> Strings with <code>1</code> in every odd-number bit position</p>
<p class="indenthangingN"><em>c.</em> Strings with at least two <code>0</code>s and at most one <code>1</code></p>
<p class="indenthangingN"><em>d.</em> Strings with no two consecutive <code>1</code>s</p>
<p><a id="ch05qa4q14"/><strong>5.4.14</strong> <em>Binary divisibility.</em> Construct an RE that describes all binary strings that when interpreted as a binary number are</p>
<p class="indenthangingN"><em>a.</em> Divisible by 2</p>
<p class="indenthangingN"><em>b.</em> Divisible by 3</p>
<p class="indenthangingN"><em>c.</em> Divisible by 123</p>
<p><a id="ch05qa4q15"/><strong>5.4.15</strong> <em>One-level REs.</em> Construct a Java RE that describes the set of strings that are legal REs over the binary alphabet, but with no occurrence of parentheses within parentheses. For example, <code>(0.*1)* or (1.*0)*</code> is in this language, but <code>(1(0 or 1)1)*</code> is not.</p>
<p><a id="ch05qa4q16"/><strong>5.4.16</strong> <em>Multiway or.</em> Add multiway <em>or</em> to <code>NFA</code>. Your code should produce the machine drawn below for the pattern <code>(.*AB((C|D|E)F)*G)</code>.</p>
<p class="image"><img alt="image" src="graphics/05_55-nfalinear.jpg"/></p>
<p><a id="page_809"/><a id="ch05qa4q17"/><strong>5.4.17</strong> <em>Wildcard.</em> Add to <code>NFA</code> the capability to handle wildcards.</p>
<p><a id="ch05qa4q18"/><strong>5.4.18</strong> <em>One or more.</em> Add to <code>NFA</code> the capability to handle the <code>+</code> closure operator.</p>
<p><a id="ch05qa4q19"/><strong>5.4.19</strong> <em>Specified set.</em> Add to <code>NFA</code> the capability to handle specified-set descriptors.</p>
<p><a id="ch05qa4q20"/><strong>5.4.20</strong> <em>Range.</em> Add to <code>NFA</code> the capability to handle range descriptors.</p>
<p><a id="ch05qa4q21"/><strong>5.4.21</strong> <em>Complement.</em> Add to <code>NFA</code> the capability to handle complement descriptors.</p>
<p><a id="ch05qa4q22"/><strong>5.4.22</strong> <em>Proof.</em> Develop a version of <code>NFA</code> that prints a <em>proof</em> that a given string is in the language recognized by the NFA (a sequence of state transitions that ends in the accept state).</p>
<p><a id="ch05sec1lev11"/></p>
<h3><a id="page_810"/>5.5 Data Compression</h3>
<p>The world is awash with data, and algorithms designed to represent data efficiently play an important role in the modern computational infrastructure. There are two primary reasons to compress data: to save storage when saving information and to save time when communicating information. Both of these reasons have remained important through many generations of technology and are familiar today to anyone needing a new storage device or waiting for a long download.</p>
<p>You have certainly encountered compression when working with digital images, sound, movies, and all sorts of other data. The algorithms we will examine save space by exploiting the fact that most data files have a great deal of redundancy: For example, text files have certain character sequences that appear much more often than others; bitmap files that encode pictures have large homogeneous areas; and files for the digital representation of images, movies, sound, and other analog signals have large repeated patterns.</p>
<p>We will look at an elementary algorithm and two advanced methods that are widely used. The compression achieved by these methods varies depending on characteristics of the input. Savings of 20 to 50 percent are typical for text, and savings of 50 to 90 percent might be achieved in some situations. As you will see, the effectiveness of any data compression method is quite dependent on characteristics of the input. <em>Note</em>: Usually, in this book, we are referring to time when we speak of performance; with data compression we normally are referring to the compression they can achieve, although we will also pay attention to the time required to do the job.</p>
<p>On the one hand, data-compression techniques are less important than they once were because the cost of computer storage devices has dropped dramatically and far more storage is available to the typical user than in the past. On the other hand, data-compression techniques are more important than ever because, since so much storage is in use, the savings they make possible are greater. Indeed, data compression has come into widespread use with the emergence of the internet, because it is a low-cost way to reduce the time required to transmit large amounts of data.</p>
<p>Data compression has a rich history (we will only be providing a brief introduction to the topic), and contemplating its role in the future is certainly worthwhile. Every student of algorithms can benefit from studying data compression because the algorithms are classic, elegant, interesting, and effective.</p>
<p><a id="ch05sec2lev47"/></p>
<h4><a id="page_811"/>Rules of the game</h4>
<p>All of the types of data that we process with modern computer systems have something in common: <em>they are ultimately represented in binary.</em> We can consider each of them to be simply a sequence of bits (or bytes). For brevity, we use the term <em>bitstream</em> in this section to refer to a sequence of bits and <em>bytestream</em> when we are referring to the bits being considered as a sequence of fixed-size bytes. A bitstream or a bytestream might be stored as a file on your computer, or it might be a message being transmitted on the internet.</p>
<p><a id="ch05sec3lev71"/></p>
<h5><em>Basic model</em></h5>
<p>Accordingly, our basic model for data compression is quite simple, having two primary components, each a black box that reads and writes bitstreams:</p>
<p class="indenthangingB">• A <em>compress</em> box that transforms a bitstream <em>B</em> into a compressed version <em>C</em> (<em>B</em>)</p>
<p class="indenthangingB">• An <em>expand</em> box that transforms <em>C</em> (<em>B</em>) back into <em>B</em></p>
<p>Using the notation | <em>B</em> | to denote the number of bits in a bitstream, we are interested in minimizing the quantity | <em>C</em> (<em>B</em>) | <em>/</em> | <em>B</em> |, which is known as the <em>compression ratio</em>.</p>
<p class="image"><img alt="image" src="graphics/05_56-basicmodel.jpg"/></p>
<p>This model is known as <em>lossless compression</em>—we insist that no information be lost, in the specific sense that the result of compressing and expanding a bitstream must match the original, bit for bit. Lossless compression is required for many types of files, such as numerical data or executable code. For some types of files (such as images, videos, or music), it is reasonable to consider compression methods that are allowed to lose some information, so the decoder only produces an approximation of the original file. Lossy methods have to be evaluated in terms of a subjective quality standard in addition to the compression ratio. We do not address lossy compression in this book.</p>
<p><a id="ch05sec2lev48"/></p>
<h4>Reading and writing binary data</h4>
<p>A full description of how information is encoded on your computer is system-dependent and is beyond our scope, but with a few basic assumptions and two simple APIs, we can separate our implementations from these details. These APIs, <code>BinaryStdIn</code> and <code>BinaryStdOut</code>, are modeled on the <code>StdIn</code> and <code>StdOut</code> APIs that you have been using, but their purpose is to read and write <em>bits</em>, where <code>StdIn</code> and <code>StdOut</code> are oriented toward <em>character streams</em> encoded in Unicode. An <code>int</code> value on <code>StdOut</code> is a sequence of characters (its decimal representation); an <code>int</code> value on <code>BinaryStdOut</code> is a sequence of bits (its binary representation).</p>
<p><a id="ch05sec3lev72"/></p>
<h5><a id="page_812"/><em>Binary input and output</em></h5>
<p>Most systems nowadays, including Java, base their I/O on 8-bit bytestreams, so we might decide to read and write bytestreams to match I/O formats with the internal representations of primitive types, encoding an 8-bit <code>char</code> with 1 byte, a 16-bit <code>short</code> with 2 bytes, a 32-bit <code>int</code> with 4 bytes, and so forth. Since <em>bitstreams</em> are the primary abstraction for data compression, we go a bit further to allow clients to read and write individual <em>bits</em>, intermixed with data of primitive types. The goal is to minimize the necessity for type conversion in client programs and also to take care of operating system conventions for representing data. We use the following API for reading a bitstream from standard input:</p>
<p class="image"><img alt="image" src="graphics/t0812-01.jpg"/></p>
<p>A key feature of the abstraction is that, in marked constrast to <code>StdIn</code>, <em>the data on standard input is not necessarily aligned on byte boundaries</em>. If the input stream is a single byte, a client could read it 1 bit at a time with eight calls to <code>readBoolean()</code>. The <code>close()</code> method is not essential, but, for clean termination, clients should call <code>close()</code> to indicate that no more bits are to be read. As with <code>StdIn/StdOut</code>, we use the following complementary API for writing bitstreams to standard output:</p>
<p class="image"><img alt="image" src="graphics/t0812-02.jpg"/></p>
<p><a id="page_813"/>For output, the <code>close()</code> method <em>is</em> essential: clients must call <code>close()</code> to ensure that all of the bits specified in prior <code>write()</code> calls make it to the bitstream and that the final byte is padded with 0s to byte-align the output for compatibility with the file system. As with the <code>In</code> and <code>Out</code> APIs associated with <code>StdIn</code> and <code>StdOut</code>, we also have available <code>BinaryIn</code> and <code>BinaryOut</code> that allows us to reference binary-encoded files directly.</p>
<p><a id="ch05sec3lev73"/></p>
<h5><em>Example</em></h5>
<p>As a simple example, suppose that you have a data type where a date is represented as three <code>int</code> values (month, day, year). Using <code>StdOut</code> to write those values in the format <code>12/31/1999</code> requires 10 characters, or 80 bits. If you write the values directly with <code>BinaryStdOut</code>, you would produce 96 bits (32 bits for each of the 3 <code>int</code> values); if you use a more economical representation that uses <code>byte</code> values for the month and day and a <code>short</code> value for the year, you would produce 32 bits. With <code>BinaryStdOut</code> you could also write a 4-bit field, a 5-bit field, and a 12-bit field, for a total of 21 bits (24 bits, actually, because files must be an integral number of 8-bit bytes, so <code>close()</code> adds three 0 bits at the end). <em>Important note</em>: Such economy, in itself, is a crude form of data compression.</p>
<p><a id="ch05sec3lev74"/></p>
<h5><em>Binary dumps</em></h5>
<p>How can we examine the contents of a bitstream or a bytestream while debugging? This question faced early programmers when the only way to find a bug was to examine each of the bits in memory, and the term <em>dump</em> has been used since the early days of computing to describe a human-readable view of a bitstream. If you try to open <a id="page_814"/>a file with an editor or view it in the manner in which you view text files (or just run a program that uses <code>BinaryStdOut</code>), you are likely to see gibberish, depending on the system you use. <code>BinaryStdIn</code> allows us to avoid such system dependencies by writing our own programs to convert bitstreams such that we can see them with our standard tools. For example, the program <code>BinaryDump</code> at left is a <code>BinaryStdIn</code> client that prints out the bits from standard input, encoded with the characters <code>0</code> and <code>1</code>. This program is useful for debugging when working with small inputs. The similar client <code>HexDump</code> groups the data into 8-bit bytes and prints each as two hexadecimal digits that each represent 4 bits. The client <code>PictureDump</code> displays the bits in a <code>Picture</code> with 0 bits represented as white pixels and 1 bits represented as black pixels. This pictorial representation is often useful in identifying patterns in a bitstream. You can download <code>BinaryDump</code>, <code>HexDump</code>, and <code>PictureDump</code> from the booksite. Typically, we use piping and redirection at the command-line level when working with binary files: we can pipe the output of an encoder to <code>BinaryDump</code>, <code>HexDump</code>, or <code>PictureDump</code>, or redirect it to a file.</p>
<p class="image"><img alt="image" src="graphics/05_57-bytealignment.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0814-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_58-dumps.jpg"/></p>
<p><a id="ch05sec3lev75"/></p>
<h5><a id="page_815"/><em>ASCII encoding</em></h5>
<p>When you <code>HexDump</code> a bit-stream that contains ASCII-encoded characters, the table at right is useful for reference. Given a two digit hex number, use the first hex digit as a row index and the second hex digit as a column index to find the character that it encodes. For example, <code>31</code> encodes the digit <code>1</code>, <code>4A</code> encodes the letter <code>J</code>, and so forth. This table is for 7-bit ASCII, so the first hex digit must be <code>7</code> or less. Hex numbers starting with 0 and 1 (and the numbers <code>20</code> and <code>7F</code>) correspond to non-printing control characters. Many of the control characters are left over from the days when physical devices such as typewriters were controlled by ASCII input; the table highlights a few that you might see in dumps. For example, <code>SP</code> is the space character, <code>NUL</code> is the null character, <code>LF</code> is line feed, and <code>CR</code> is carriage return.</p>
<p class="image"><img alt="image" src="graphics/05_59-lsdexample.jpg"/></p>
<p><small>IN SUMMARY</small>, working with data compression requires us to reorient our thinking about standard input and standard output to include binary encoding of data. <code>BinaryStdIn</code> and <code>BinaryStdOut</code> provide the methods that we need. They provide a way for you to make a clear distinction in your client programs between writing out information intended for file storage and data transmission (that will be read by programs) and printing information (that is likely to be read by humans).</p>
<p><a id="ch05sec2lev49"/></p>
<h4><a id="page_816"/>Limitations</h4>
<p>To appreciate data-compression algorithms, you need to understand fundamental limitations. Researchers have developed a thorough and important theoretical basis for this purpose, which we will consider briefly at the end of this section, but a few ideas will help us get started.</p>
<p><a id="ch05sec3lev76"/></p>
<h5><em>Universal data compression</em></h5>
<p>Armed with the algorithmic tools that have proven so useful for so many problems, you might think that our goal should be <em>universal data compression</em>: an algorithm that can make any bitstream smaller. Quite to the contrary, we have to adopt more modest goals because universal data compression is impossible.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb33"/></p>
<p><strong>Proposition S.</strong> No algorithm can compress every bitstream.</p>
<p><strong>Proof:</strong> We consider two proofs that each provide some insight. The first is by contradiction: Suppose that you have an algorithm that does compress every bitstream. Then you could use that algorithm to compress its output to get a still shorter bitstream, and continue until you have a bistream of length 0! The conclusion that your algorithm compresses every bit-stream to 0 bits is absurd, and so is the assumption that it can compress every bitstream.</p>
<p>The second proof is a counting argument. Suppose that you have an algorithm that claims lossless compression for every 1,000-bit stream. That is, every such stream must map to a different shorter one. But there are only 1 + 2 + 4 + ... + 2<sup>998</sup> + 2<sup>999</sup> = 2<sup>1000</sup>−1 bitstreams with fewer than 1,000 bits and 2<sup>1000</sup> bitstreams with 1,000 bits, so your algorithm cannot compress all of them. This argument becomes more persuasive if we consider stronger claims. Say your goal is to achieve better than a 50 percent compression ratio. You have to know that you will be successful for only about 1 out of 2<sup>500</sup> of the 1,000-bit bitstreams!</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/05_60-universal.jpg"/></p>
<p>Put another way, you have at most a 1 in 2<sup>500</sup> chance of being able to compress by half a random 1,000-bit stream with any data-compression algorithm. When you run across a new lossless compression algorithm, it is a sure bet that it will not achieve significant compression for a random bitstream. The insight that we cannot hope to compress random streams is a start to understanding data compression. We regularly process strings of millions or billions of bits but will never process even the tiniest fraction of all possible such strings, so we need <a id="page_817"/>not be discouraged by this theoretical result. Indeed, the bitstrings that we regularly process are typically highly structured, a fact that we can exploit for compression.</p>
<p class="image"><img alt="image" src="graphics/05_61-million.jpg"/></p>
<p><a id="ch05sec3lev77"/></p>
<h5><em>Undecidability</em></h5>
<p>Consider the million-bit string pictured at the top of this page. This string appears to be random, so you are not likely to find a lossless compression algorithm that will compress it. But there is a way to represent that string with just a few thousand bits, because it was produced by the program below. (This program is an example of a pseudo-random number generator, like Java’s <code>Math.random()</code> method.) A compression algorithm that compresses by writing the program in ASCII and expands by reading the program and then running it achieves a .3 percent compression ratio, which is difficult to beat (and we can drive the ratio arbitrarily low by writing more bits). To compress such a file is to discover the program that produced it. This example is not so far-fetched as it first appears: when you compress a video or an old book that was digitized with a scanner or any of countless other types of files from the web, you are discovering something about the program that produced the file. The realization that much of the data that we process is produced by a program leads to deep issues in the theory of computation and also gives insight into the challenges of data compression. For example, it is possible to prove that optimal data compression (find the shortest program to produce a given string) is an <em>undecidable</em> problem: not only can we not have an algorithm that compresses every bit-stream, but also we cannot have a strategy for developing the best algorithm!</p>
<p class="image"><img alt="image" src="graphics/p0817-01.jpg"/></p>
<p><a id="page_818"/>The practical impact of these limitations is that lossless compression methods must be oriented toward taking advantage of <em>known</em> structure in the bitstreams to be compressed. The four methods that we consider exploit, in turn, the following structural characteristics:</p>
<p class="indenthangingB">• Small alphabets</p>
<p class="indenthangingB">• Long sequences of identical bits/characters</p>
<p class="indenthangingB">• Frequently used characters</p>
<p class="indenthangingB">• Long reused bit/character sequences</p>
<p>If you know that a given bitstream exhibits one or more of these characteristics, you can compress it with one of the methods that you are about to learn; if not, trying them each is probably still worth the effort, since the underlying structure of your data may not be obvious, and these methods are widely applicable. As you will see, each method has parameters and variations that may need to be tuned for best compression of a particular bitstream. The first and last recourse is to learn something about the structure of your data yourself and exploit that knowledge to compress it, perhaps using one of the techniques we are about to consider.</p>
<p><a id="ch05sec2lev50"/></p>
<h4><a id="page_819"/>Warmup: genomics</h4>
<p>As preparation for more complicated data-compression algorithms, we now consider an elementary (but very important) data-compression task. All of our implementations will use the same conventions that we will now introduce in the context of this example.</p>
<p><a id="ch05sec3lev78"/></p>
<h5><em>Genomic data</em></h5>
<p>As a first example of data compression, consider this string:</p>
<p class="programlisting">ATAGATGCATAGCGCATAGCTAGATGTGCTAGCAT</p>
<p>Using standard ASCII encoding (1 byte, or 8 bits per character), this string is a bitstream of length 8×35 = 280. Strings of this sort are extremely important in modern biology, because biologists use the letters <code>A</code>, <code>C</code>, <code>T</code>, and <code>G</code> to represent the four nucleotides in the DNA of living organisms. A <em>genome</em> is a sequence of nucleotides. Scientists know that understanding the properties of genomes is a key to understanding the processes that manifest themselves in living organisms, including life, death, and disease. Genomes for many living things are known, and scientists are writing programs to study the structure of these sequences.</p>
<p class="image"><img alt="image" src="graphics/p0819-01.jpg"/></p>
<p><a id="ch05sec3lev79"/></p>
<h5><em>2-bit code compression</em></h5>
<p>One simple property of genomes is that they contain only four different characters, so each can be encoded with just 2 bits per character, as in the <code>compress()</code> method shown at right. Even though we know the input stream to be character-encoded, we use <code>BinaryStdIn</code> to read the input, to emphasize adherence to the standard data-compression model (bitstream to bitstream). We include the number of encoded characters in the compressed file, to ensure proper decoding if the last bit does not fall at the end of a byte. Since it converts each 8-bit character to a 2-bit code and just adds 32 bits for the length, this program approaches a 25 percent compression ratio as the number of characters increases.</p>
<p><a id="ch05sec3lev80"/></p>
<h5><em>2-bit code expansion</em></h5>
<p>The <code>expand()</code> method at the top of the next page expands a bitstream produced by this <code>compress()</code> method. As with compression, this method reads a bitstream and writes a bitstream, in accordance with the basic data-compression model. The bitstream that we produce as output is the original input.</p>
<p><a id="page_820"/><small>THE SAME APPROACH</small> works for other fixed-size alphabets, but we leave this generalization for an (easy) exercise (see <a href="#ch05qa5q25"><small>EXERCISE 5.5.25</small></a>).</p>
<p class="image"><img alt="image" src="graphics/p0820-01.jpg"/></p>
<p>These methods do not quite adhere to the standard data-compression model, because the compressed bitstream does not contain all the information needed to decode it. The fact that the alphabet is one of the letters <code>A</code>, <code>C</code>, <code>T</code>, or <code>G</code> is agreed upon by the two methods. Such a convention is reasonable in an application such as genomics, where the same code is widely reused. Other situations might require including the alphabet in the encoded message (see <a href="#ch05qa5q25"><small>EXERCISE 5.5.25</small></a>). The norm in data compression is to include such costs when comparing methods.</p>
<p>In the early days of genomics, learning a genomic sequence was a long and arduous task, so sequences were relatively short and scientists used standard ASCII encoding to store and exchange them. The experimental process has been vastly streamlined, to the point where known genomes are numerous and lengthy (the human genome is over 10<sup>10</sup> bits), and the 75 percent savings achieved by these simple methods is very significant. Is there room for further compression? That is a very interesting question to contemplate, because it is a <em>scientific</em> question: the ability to compress implies the existence of some structure in the data, and a prime focus of modern genomics is to discover structure in genomic data. Standard data-compression methods like the ones we will consider are ineffective with (2-bit-encoded) genomic data, as with random data.</p>
<p>We package <code>compress()</code> and <code>expand()</code> as static methods in the same class, along with a simple driver, as shown at right. To test your understanding of the rules of the game and the basic tools that we use for data compression, make sure that you understand the various commands on the facing page that invoke <code>Genome.compress()</code> and <code>Genome.expand()</code> on our sample data (and their consequences).</p>
<p class="image"><img alt="image" src="graphics/p0820-02.jpg"/></p>
<p class="image"><a id="page_821"/><img alt="image" src="graphics/05_62-genomevirus.jpg"/></p>
<p><a id="ch05sec2lev51"/></p>
<h4><a id="page_822"/>Run-length encoding</h4>
<p>The simplest type of redundancy in a bitstream is long runs of repeated bits. Next, we consider a classic method known as <em>run-length encoding</em> for taking advantage of this redundancy to compress data. For example, consider the following 40-bit string:</p>
<p class="programlisting">0000000000000001111111000000011111111111</p>
<p>This string consists of 15 <code>0</code>s, then 7 <code>1</code>s, then 7 <code>0</code>s, then 11 <code>1</code>s, so we can encode the bitstring with the numbers 15, 7, 7, and 11. All bitstrings are composed of alternating runs of 0s and 1s; we just encode the length of the runs. In our example, if we use 4 bits to encode the numbers and start with a run of <code>0</code>s, we get the 16-bit string</p>
<p class="programlisting">1111011101111011</p>
<p>(15 = <code>1111</code>, then 7 = <code>0111</code>, then 7 = <code>0111</code>, then 11 = <code>1011</code>) for a compression ratio of 16/40 = 40 percent. In order to turn this description into an effective data compression method, we have to consider the following issues:</p>
<p class="indenthangingB">• How many bits do we use to store the counts?</p>
<p class="indenthangingB">• What do we do when encountering a run that is longer than the maximum count implied by this choice?</p>
<p class="indenthangingB">• What do we do about runs that are shorter than the number of bits needed to store their length?</p>
<p>We are primarily interested in long bitstreams with relatively few short runs, so we address these questions by making the following choices:</p>
<p class="indenthangingB">• Counts are between 0 and 255, all encoded with 8 bits.</p>
<p class="indenthangingB">• We make all run lengths less than 256 by including runs of length 0 if needed.</p>
<p class="indenthangingB">• We encode short runs, even though doing so might lengthen the output.</p>
<p>These choices are very easy to implement and also very effective for several kinds of bitstreams that are commonly encountered in practice. They are <em>not</em> effective when short runs are numerous—we save bits on a run only when the length of the run is more than the number of bits needed to represent itself in binary.</p>
<p><a id="ch05sec3lev81"/></p>
<h5><em>Bitmaps</em></h5>
<p>As an example of the effectiveness of run-length encoding, we consider <em>bitmaps</em>, which are widely use to represent pictures and scanned documents. For brevity and simplicity, we consider binary-valued bitmaps organized as bitstreams formed by taking the pixels in row-major order. To view the contents of a bitmap, we use <code>PictureDump</code>. Writing a program to convert an image from one of the many common lossless image formats that have been defined for “screen shots” or scanned documents into a bitmap is a simple matter (see <small>EXERCISE 5.5.</small>X). Our example to demonstrate the effectiveness of run-length encoding comes from screen shots of this book: a letter <em>q</em> (at various resolutions). We focus on a binary dump of a 32-by-48-pixel screen <a id="page_823"/>shot, shown at right along with run lengths for each row. Since each row starts and ends with a 0, there is an odd number of run lengths on each row; since the end of one row is followed by the beginning of the next, the corresponding run length in the bitstream is the sum of the last run length in each row and the first run length in the next (with extra additions corresponding to rows that are all 0).</p>
<p class="image"><img alt="image" src="graphics/05_63-rleq.jpg"/></p>
<p><a id="ch05sec3lev82"/></p>
<h5><em>Implementation</em></h5>
<p>The informal description just given leads immediately to the <code>compress()</code> and <code>expand()</code> implementations on the next page. As usual, the <code>expand()</code> implementation is the simpler of the two: read a run length, print that many copies of the current bit, complement the current bit, and continue until the input is exhausted. The <code>compress()</code> method is not much more difficult, consisting of the following steps while there are bits in the input stream:</p>
<p class="indenthangingB">• Read a bit.</p>
<p class="indenthangingB">• If it differs from the last bit read, write the current count and reset the count to 0.</p>
<p class="indenthangingB">• If it is the same as the last bit read, and the count is a maximum, write the count, write a 0 count, and reset the count to 0.</p>
<p class="indenthangingB">• Increment the count.</p>
<p>When the input stream empties, writing the count (length of the last run) completes the process.</p>
<p><a id="ch05sec3lev83"/></p>
<h5><em>Increasing resolution in bitmaps</em></h5>
<p>The primary reason that run-length encoding is widely used for bitmaps is that its effectiveness increases dramatically as resolution increases. It is easy to see why this is true. Suppose that we double the resolution for our example. Then the following facts are evident:</p>
<p class="indenthangingB">• The number of bits increases by a factor of 4.</p>
<p class="indenthangingB">• The number of runs increases by about a factor of 2.</p>
<p class="indenthangingB">• The run lengths increase by about a factor of 2.</p>
<p class="indenthangingB">• The number of bits in the compressed version increases by about a factor of 2.</p>
<p class="indenthangingB">• Therefore, the compression ratio is halved!</p>
<p><a id="page_824"/>Without run-length encoding, space requirements increase by a factor of 4 when the resolution is doubled; with run-length encoding, space requirements for the compressed bitstream just double when the resolution is doubled. That is, space grows and the compression ratio drops linearly with resolution. For example, our (low-resolution) letter <em>q</em> yields just a 74 percent compression ratio; if we increase the resolution to 64 by 96, the ratio drops to 37 percent. This change is graphically evident in the <code>PictureDump</code> outputs shown in the figure on the facing page. The higher-resolution letter takes four times the space of the lower resolution letter (double in both dimensions), but the compressed version takes just twice the space (double in one dimension). If we further increase the resolution to 128-by-192 (closer to what is needed for print), the ratio drops to 18 percent (see <a href="#ch05qa5q5"><small>EXERCISE 5.5.5</small></a>).</p>
<p class="image"><img alt="image" src="graphics/p0824-01.jpg"/></p>
<p class="image1"><img alt="image" src="graphics/p0824-02.jpg"/></p>
<p><small>RUN-LENGTH ENCODING IS VERY EFFECTIVE</small> in many situations, but there are plenty of cases where the bitstream we wish to compress (for example, typical English-language text) may have no long runs at all. Next, we consider two methods that are effective for a broad variety of files. They are widely used, and you likely have used one or both of these methods when downloading from the web.</p>
<p class="image"><a id="page_825"/><img alt="image" src="graphics/05_64-q64x96.rle.jpg"/></p>
<p><a id="ch05sec2lev52"/></p>
<h4><a id="page_826"/>Huffman compression</h4>
<p>We now examine a data-compression technique that can save a substantial amount of space in natural language files (and many other kinds of files). The idea is to abandon the way in which text files are usually stored: instead of using the usual 7 or 8 bits for each character, we use fewer bits for characters that appear often than for those that appear rarely.</p>
<p>To introduce the basic ideas, we start with a small example. Suppose we wish to encode the string <code>ABRACADABRA</code>! Encoding it in 7-bit ASCII gives this bitstring:</p>
<p class="center"><code>100000110000101010010100000110000111000001-</code><br/>
<code>100010010000011000010101001010000010100001.</code></p>
<p>To decode this bitstring, we simply read off 7 bits at a time and convert according to the ASCII coding table on page <a href="#page_815">815</a>. In this standard code the <code>D</code>, which appears only once, requires the same number of bits as the <code>A</code>, which appears five times. Huffman compression is based on the idea that we can save bits by encoding frequently used characters with fewer bits than rarely used characters, thereby lowering the total number of bits used.</p>
<p><a id="ch05sec3lev84"/></p>
<h5><em>Variable-length prefix-free codes</em></h5>
<p>A <em>code</em> associates each character with a bitstring: a symbol table with characters as keys and bitstrings as values. As a start, we might try to assign the shortest bitstrings to the most commonly used letters, encoding <code>A</code> with <code>0</code>, <code>B</code> with <code>1</code>, <code>R</code> with <code>00</code>, <code>C</code> with <code>01</code>, <code>D</code> with <code>10</code>, and <code>!</code> with <code>11</code>, so <code>ABRACADABRA!</code> would be encoded as <code>0 1 00 0 01 0 10 0 1 00 0 11</code>. This representation uses only 17 bits compared to the 77 for 7-bit ASCII, but it is not really a code because it depends on the blanks to delimit the characters. Without the blanks, the bitstring would be</p>
<p class="center"><code>01000010100100011</code></p>
<p>and could be decoded as <code>CRRDDCRCB</code> or as several other strings. Still, the count of 17 bits plus 10 delimiters is rather more compact than the standard code, primarily because no bits are used to encode letters not appearing in the message. The next step is to take advantage of the fact that <em>delimiters are not needed if no character code is the prefix of another</em>. A code with this property is known as a <em>prefix-free code</em>. The code just given is not prefix-free because <code>0</code>, the code for <code>A</code>, is a prefix of <code>00</code>, the code for <code>R</code>. For example, if we encode <code>A</code> with <code>0</code>, <code>B</code> with <code>1111</code>, <code>C</code> with <code>110</code>, <code>D</code> with <code>100</code>, <code>R</code> with <code>1110</code>, and <code>!</code> with <code>101</code>, there is only one way to decode the 30-bit string</p>
<p class="center"><code>011111110011001000111111100101</code></p>
<p><code>ABRACADABRA</code> ! All prefix-free codes are <em>uniquely decodable</em> (without needing any delimiters) in this way, so prefix-free codes are widely used in practice. Note that fixed-length codes such as 7-bit ASCII are prefix-free.</p>
<p><a id="ch05sec3lev85"/></p>
<h5><a id="page_827"/><em>Trie representation for prefix-free codes</em></h5>
<p>One convenient way to represent a prefix-free code is with a trie (see <a href="#ch05sec1lev8"><small>SECTION 5.2</small></a>). In fact, any trie with <code>M</code> null links defines a prefix-free code for <code>M</code> characters: we replace the null links by links to <em>leaves</em> (nodes with two null links), each containing a character to be encoded, and define the code for each character with the bitstring defined by the path from the root to the character, in the standard manner for tries where we associate <code>0</code> with moving left and <code>1</code> with moving right. For example, the figure at right shows two prefix-free codes for the characters in <code>ABRACADABRA!</code>. On top is the variable-length code just considered; below is a code that produces the string</p>
<p class="center"><code>11000111101011100110001111101</code></p>
<p class="image"><img alt="image" src="graphics/05_65-huffabratwo.jpg"/></p>
<p>which is 29 bits, 1 bit shorter. Is there a trie that leads to even more compression? How do we find the trie that leads to the best prefix-free code? It turns out that there is an elegant answer to these questions in the form of an algorithm that computes a trie which leads to a bitstream of minimal length for any given string. To make a fair comparison with other codes, we also need to count the bits in the code itself, since the string cannot be decoded without it, and, as you will see, the code depends on the string. The general method for finding the optimal prefix-free code was discovered by D. Huffman (while a student!) in 1952 and is called <em>Huffman encoding</em>.</p>
<p><a id="ch05sec3lev86"/></p>
<h5><em>Overview</em></h5>
<p>Using a prefix-free code for data compression involves five major steps. We view the bitstream to be encoded as a bytestream and use a prefix-free code for the characters as follows:</p>
<p class="indenthangingB">• Build an encoding trie.</p>
<p class="indenthangingB">• Write the trie (encoded as a bitstream) for use in expansion.</p>
<p class="indenthangingB">• Use the trie to encode the bytestream as a bitstream.</p>
<p>Then expansion requires that we</p>
<p class="indenthangingB">• Read the trie (encoded at the beginning of the bitstream)</p>
<p class="indenthangingB">• Use the trie to decode the bitstream</p>
<p>To help you best understand and appreciate the process, we consider these steps in order of difficulty.</p>
<p><a id="ch05sec3lev87"/></p>
<h5><a id="page_828"/><em>Trie nodes</em></h5>
<p>We begin with the <code>Node</code> class at left. It is similar to the nested classes that we have used before to construct binary trees and tries: each <code>Node</code> has <code>left</code> and <code>right</code> references to <code>Node</code>s, which define the trie structure. Each <code>Node</code> also has an instance variable <code>freq</code> that is used in construction, and an instance variable <code>ch</code>, which is used in leaves to represent characters to be encoded.</p>
<p class="image"><img alt="image" src="graphics/p0828-01.jpg"/></p>
<p><a id="ch05sec3lev88"/></p>
<h5><em>Expansion for prefix-free codes</em></h5>
<p>Expanding a bitstream that was encoded with a prefix-free code is simple, given the trie that defines the code. The <code>expand()</code> method at left is an implementation of this process. After reading the trie from standard input using the <code>readTrie()</code> method to be described later, we use it to expand the rest of the bitstream as follows: Starting at the root, proceed down the trie as directed by the bitstream (read in input bit, move left if it is <code>0</code>, and move right if it is <code>1</code>). When you encounter a leaf, output the character at that node and restart at the root. If you study the operation of this method on the small prefix code example on the next page, you will understand and appreciate this process: For example, to decode the bitstring <code>011111001011...</code> we start at the root, move left because the first bit is 0, output <code>A</code>; go back to the root, move right three times, then output <code>B</code>; go back to the root, move right twice, then left, then output <code>R</code>; and so forth. The simplicity of expansion is one reason for the popularity of prefix-free codes in general and Huffman compression in particular.</p>
<p class="image"><img alt="image" src="graphics/p0828-02.jpg"/></p>
<p class="image"><a id="page_829"/><img alt="image" src="graphics/p0829-01.jpg"/></p>
<p><a id="ch05sec3lev89"/></p>
<h5><em>Compression for prefix-free codes</em></h5>
<p>For compression, we use the trie that defines the code to build the code table, as shown in the <code>buildCode()</code> method at the top of this page. This method is compact and elegant, but a bit tricky, so it deserves careful study. For any trie, it produces a table giving the bitstring associated with each character in the trie (represented as a <code>String</code> of <code>0</code>s and <code>1</code>s). The coding table is a symbol table that associates a <code>String</code> with each character: we use a character-indexed array <code>st[]</code> instead of a general symbol table for efficiency, because the number of characters is not large. To create it, <code>buildCode()</code> recursively walks the tree, maintaining a binary string that corresponds to the path from the root to each node (<code>0</code> for left links and <code>1</code> for right links), and setting the codeword corresponding to each character when the character is found in a leaf. Once the coding table is built, compression is a simple matter: just look up the code for each character in the input. To use the encoding at right to compress <code>ABRACADABRA!</code> we write <code>0</code> (the codeword associated with <code>A</code>), then <code>111</code> (the codeword associated with <code>B</code>), then <code>110</code> (the codeword associated with <code>R</code>), and so forth. The code snippet at right accomplishes this task: we look up the <code>String</code> associated with each character in the input, convert it to <code>0</code>/<code>1</code> values in a <code>char</code> array, and write the corresponding bitstring to the output.</p>
<p class="image"><img alt="image" src="graphics/05_66-huffabraopt.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0829-02.jpg"/></p>
<p><a id="ch05sec3lev90"/></p>
<h5><a id="page_830"/><em>Trie construction</em></h5>
<p>For reference as we describe the process, the figure on the facing page illustrates the process of constructing a Huffman trie for the input</p>
<p class="programlisting4"><code>it was the best of times it was the worst of times</code></p>
<p>We keep the characters to be encoded in leaves and maintain the <code>freq</code> instance variable in each node that represents the frequency of occurrence of all characters in the subtree rooted at that node. The first step is to create a forest of 1-node trees (leaves), one for each character in the input stream, each assigned a <code>freq</code> value equal to its frequency of occurrence in the input. In the example, the input has 8 <code>t</code>s, 5 <code>e</code>s, 11 spaces, and so forth. (<em>Important note</em>: To obtain these frequencies, we need to read the whole input stream—Huffman encoding is a <em>two-pass</em> algorithm because we will need to read the input stream a second time to compress it.) Next, we build the coding trie from the bottom up according to the frequencies. When building the trie, we view it as a binary trie with frequencies stored in the nodes; after it has been built, we view it as a trie for coding, as just described. The process works as follows: we find the two nodes with the smallest frequencies and then create a new node with those two nodes as children (and with frequency value set to the sum of the values of the children). This operation reduces the number of tries in the forest by one. Then we iterate the process: find the two nodes with smallest frequency in that forest and a create a new node created in the same way. Implementing the process is straightforward with a priority queue, as shown in the <code>buildTrie()</code> method on page <a href="#page_830">830</a>. (For clarity, the tries in the figure are kept in sorted order.) Continuing, we build up larger and larger tries and at the same time reduce the number of tries in the forest by one at each step (remove two, add one). Ultimately, all the <a id="page_832"/>nodes are combined together into a single trie. The leaves in this trie have the characters to be encoded and their frequencies in the input; each non-leaf node is the sum of the frequencies of its two children. Nodes with low frequencies end up far down in the trie, and nodes with high frequencies end up near the root of the trie. The frequency in the root equals the number of characters in the input. Since it is a binary trie with characters only in its leaves, it defines a prefix-free code for the characters. Using the codeword table created by <code>buildCode()</code> for this example (shown at right in the diagram at the top of this page), we get the output bitstring which is 176 bits, a savings of 57 percent over the 408 bits needed to encode the 51 characters in standard 8-bit ASCII (not counting the cost of including the code, which we will soon consider). Moreover, since it is a <em>Huffman</em> code, no other prefix-free code can encode the input with fewer bits.</p>
<p class="center"><code>10111110100101101110001111110010000110101100-</code><br/><code>01001110100111100001111101111010000100011011-</code><br/><code>11101001011011100011111100100001001000111010-</code><br/><code>01001110100111100001111101111010000100101010.</code></p>
<p class="image"><img alt="image" src="graphics/p0830-01.jpg"/></p>
<p class="image"><a id="page_831"/><img alt="image" src="graphics/05_67-hufftinytinytrace.jpg"/></p>
<p class="image"><img alt="image" src="graphics/05_68-hufftinytiny.jpg"/></p>
<p><a id="ch05sec3lev91"/></p>
<h5><em>Optimality</em></h5>
<p>We have observed that high-frequency characters are nearer the root of the tree than lower-frequency characters and are therefore encoded with fewer bits, so this is a good code, but why is it an <em>optimal</em> prefix-free code? To answer this question, we begin by defining the <em>weighted external path length</em> of a tree to be the sum of the weight (associated frequency count) times depth (see page <a href="ch01a.html#page_226">226</a>) of all of the leaves.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb34"/></p>
<p><a id="page_833"/><strong>Proposition T.</strong> For any prefix-free code, the length of the encoded bitstring is equal to the weighted external path length of the corresponding trie.</p>
<p><strong>Proof:</strong> The depth of each leaf is the number of bits used to encode the character in the leaf. Thus, the weighted external path length is the length of encoded bitstring: it is equivalent to the sum over all letters of the number of occurrences times the number of bits per occurrence.</p>
<hr/>
</div>
<p>For our example, there is one leaf at distance 2 (<code>SP</code>, with frequency 11), three leaves at distance 3 (<code>e</code>, <code>s</code>, and <code>t</code>, with total frequency 19), three leaves at distance 4 (<code>w</code>, <code>o</code>, and <code>i</code>, with total frequency 10), five leaves at distance 5 (<code>r</code>, <code>f</code>, <code>h</code>, <code>m</code>, and <code>a</code>, with total frequency 9) and two leaves at distance 6 (<code>LF</code> and <code>b</code>, with total frequency 2), so the sum total is 2·11 + 3·19 + 4·10 + 5·9 + 6·2 = 176, the length of the output bitstring, as expected.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch05sb35"/></p>
<p><strong>Proposition U.</strong> Given a set of <em>r</em> symbols and frequencies, the Huffman algorithm builds an optimal prefix-free code.</p>
<p><strong>Proof:</strong> By induction on <em>r</em>. Assume that the Huffman code is optimal for any set of fewer than <em>r</em> symbols. Let <em>T<sub>H</sub></em> be the code computed by Huffman for the set of symbols and associated frequencies (<em>s<sub>1</sub></em>, <em>r<sub>1</sub></em>), . . ., (<em>s<sub>r</sub></em>, <em>f<sub>r</sub></em>) and denote the length of the code (weighted external path length of the trie) by <em>W</em>(<em>T<sub>H</sub></em>). Suppose that (<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>) and (<em>s<sub>j</sub></em>, <em>f<sub>j</sub></em>) are the first two symbols chosen. The algorithm then computes the code <em>T<sub>H</sub>*</em> for the set of <em>n</em>−1 symbols with (<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>) and (<em>s<sub>i</sub></em>, <em>f<sub>j</sub></em>) replaced by (<em>s</em>*, <em>f<sub>i</sub> + f<sub>j</sub></em>) where <em>s</em>* is a new symbol in a leaf at some depth <em>d</em>. Note that</p>
<p class="center"><em>W</em>(<em>T<sub>H</sub></em>) <em>= W</em>(<em>T<sub>H</sub>*</em>) − <em>d</em>(<em>f<sub>i</sub> + f<sub>j</sub></em>) <em>+</em> (<em>d +</em> 1)(<em>f<sub>i</sub> + f<sub>j</sub></em>) <em>= W</em>(<em>T<sub>H</sub>*</em>) <em>+</em> (<em>f<sub>i</sub> + f<sub>j</sub></em>)</p>
<p>Now consider an optimal trie <em>T</em> for (<em>s<sub>1</sub></em>, <em>r<sub>1</sub></em>), . . ., (<em>s<sub>r</sub></em>, <em>f<sub>r</sub></em>), of height <em>h</em>. Note that that (<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>) and (<em>s<sub>j</sub></em>, <em>f<sub>j</sub></em>) must be at depth <em>h</em> (else we could make a trie with lower external path length by swapping them with nodes at depth <em>h</em>). Also, assume (<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>) and (<em>s<sub>j</sub></em>, <em>f<sub>j</sub></em>) are siblings by swapping (<em>s<sub>j</sub></em>, <em>f<sub>j</sub></em>) with (<em>s<sub>i</sub></em>, <em>f<sub>i</sub></em>)’s sibling. Now consider the tree <em>T*</em> obtained by replacing their parent with (<em>s</em>*, <em>f<sub>i</sub> + f<sub>j</sub></em>). Note that (by the same argument as above) <em>W</em>(<em>T</em>) = <em>W</em>(<em>T*</em>) + (<em>f<sub>i</sub> + f<sub>j</sub></em>).</p>
<p>By the inductive hypothesis <em>T<sub>H</sub>*</em> is optimal: <em>W</em>(<em>T<sub>H</sub>*</em>) ≤ <em>W</em>(<em>T*</em>). Therefore,</p>
<p class="center"><em>W</em>(<em>T<sub>H</sub></em>) <em>= W</em>(<em>T<sub>H</sub>*</em>) <em>+</em> (<em>f<sub>i</sub> + f<sub>j</sub></em>) ≤ <em>W</em>(<em>T*</em>) <em>+</em> (<em>f<sub>i</sub> + f<sub>j</sub></em>) <em>= W</em>(<em>T</em>)</p>
<p>Since <em>T</em> is optimal, equality must hold, and <em>T<sub>H</sub></em> is optimal.</p>
<hr/>
</div>
<p><a id="page_834"/>Whenever a node is to be picked, it can be the case that there are several nodes with the same weight. Huffman’s method does not specify how such ties are to be broken. It also does not specify the left/right positions of the children. Different choices lead to different Huffman codes, but all such codes will encode the message with the optimal number of bits among prefix-free codes.</p>
<p class="image"><img alt="image" src="graphics/05_69-hufftrieencode.jpg"/></p>
<p><a id="ch05sec3lev92"/></p>
<h5><em>Writing and reading the trie</em></h5>
<p>As we have emphasized, the savings figure quoted above is not entirely accurate, because the compressed bitstream cannot be encoded without the trie, so we must account for the cost of including the trie in the compressed output, along with the bitstring. For long inputs, this cost is relatively small, but in order for us to have a full data-compression scheme, we must write the trie onto a bitstream when compressing and read it back when expanding. How can we encode a trie as a bitstream, and then expand it? Remarkably, both tasks can be achieved with simple recursive procedures, based on a <em>preorder traversal</em> of the trie. The procedure <code>writeTrie()</code> below traverses a trie in preorder: when it visits an internal node, it writes a single <code>0</code> bit; when it visits a leaf, it writes a <code>1</code> bit, followed by the 8-bit ASCII code of the character in the leaf. The bitstring encoding of the Huffman trie for our <code>ABRACADABRA!</code> example is shown above. The first bit is 0, corresponding to the root; since the leaf containing <code>A</code> is encountered next, the next bit is 1, followed by <code>0100001</code>, the 8-bit ASCII code for <code>A</code>; the next two bits are 0 because two internal nodes are encountered next, and so forth. The corresponding method <code>readTrie()</code> on page <a href="#ch05sec3lev93">835</a> reconstructs the trie from the bitstring: it reads a single bit to learn which type of node comes next: if a leaf (the bit is <code>1</code>) it reads the next character and creates a leaf; if an internal node (the bit is <code>0</code>) it creates an internal node and <a id="page_835"/>then (recursively) builds its left and right subtrees. <em>Be sure that you understand these methods</em>: their simplicity is somewhat deceiving.</p>
<p class="image"><img alt="image" src="graphics/p0834-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0835-01.jpg"/></p>
<p><a id="ch05sec3lev93"/></p>
<h5><em>Huffman compression implementation</em></h5>
<p>Along with the methods <code>buildCode()</code>, <code>buildTrie()</code>, <code>readTrie()</code> and <code>writeTrie()</code> that we have just considered (and the <code>expand()</code> method that we considered first), <a href="#ch05sb36"><small>ALGORITHM 5.10</small></a> is a complete implementation of Huffman compression. To expand the overview that we considered several pages earlier, we view the bitstream to be encoded as a stream of 8-bit <code>char</code> values and compress it as follows:</p>
<p class="indenthangingB">• Read the input.</p>
<p class="indenthangingB">• Tabulate the frequency of occurrence of each <code>char</code> value in the input.</p>
<p class="indenthangingB">• Build the Huffman encoding trie corresponding to those frequencies.</p>
<p class="indenthangingB">• Build the corresponding codeword table, to associate a bitstring with each <code>char</code> value in the input.</p>
<p class="indenthangingB">• Write the trie, encoded as a bitstring.</p>
<p class="indenthangingB">• Write the count of characters in the input, encoded as a bitstring.</p>
<p class="indenthangingB">• Use the codeword table to write the codeword for each input character.</p>
<p>To expand a bitstream encoded in this way, we</p>
<p class="indenthangingB">• Read the trie (encoded at the beginning of the bitstream)</p>
<p class="indenthangingB">• Read the count of characters to be decoded</p>
<p class="indenthangingB">• Use the trie to decode the bitstream</p>
<p>With four recursive trie-processing methods and a seven-step compression process, Huffman compression is one of the more involved algorithms that we have considered, but it is also one of the most widel-used, because of its effectiveness.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb36"/></p>
<h3><a id="page_836"/>Algorithm 5.10 Huffman compression</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0836-01.jpg"/></p>
<p>This implementation of Huffman encoding builds an explicit coding trie, using various helper methods that are presented and explained in the last several pages of text.</p>
<hr/>
</div>
<p class="image"><a id="page_837"/><img alt="image" src="graphics/05_70-medtale.huf.jpg"/></p>
<p><a id="page_838"/><small>ONE REASON FOR THE POPULARITY</small> of Huffman compression is that it is effective for various types of files, not just natural language text. We have been careful to code the method so that it can work properly for any 8-bit value in each 8-bit character. In other words, we can apply it to any bytestream whatsoever. Several examples, for file types that we have considered earlier in this section, are shown in the figure at the bottom of this page. These examples show that Huffman compression is competitive with both fixed-length encoding and run-length encoding, even though those methods are designed to perform well for certain types of files. Understanding the reason Huffman encoding performs well in these domains is instructive. In the case of genomic data, Huffman compression essentially discovers a 2-bit code, as the four letters appear with approximately equal frequency so that the Huffman trie is balanced, with each character assigned a 2-bit code. In the case of run-length encoding, <code>00000000</code> and <code>11111111</code> are likely to be the most frequently occurring characters, so they are likely to be encoded with 2 or 3 bits, leading to substantial compression.</p>
<p class="image"><img alt="image" src="graphics/05_71-huffexamplestwo.jpg"/></p>
<p><a id="page_839"/>A remarkable alternative to Huffman compression that was developed in the late 1970s and the early 1980s by A. Lempel, J. Ziv, and T. Welch has emerged as one of the most widely used compression methods because it is easy to implement and works well for a variety of file types.</p>
<p>The basic plan complements the basic plan for Huffman coding. Rather than maintain a table of <em>variable</em>-length codewords for <em>fixed</em>-length patterns in the input, we maintain a table of <em>fixed</em>-length codewords for <em>variable</em>-length patterns in the input. A surprising added feature of the method is that, by contrast with Huffman encoding, <em>we do not have to encode the table</em>.</p>
<p><a id="ch05sec3lev94"/></p>
<h5><strong>LZW compression</strong></h5>
<p>To fix ideas, we will consider a compression example where we read the input as a stream of 7-bit ASCII characters and write the output as a stream of 8-bit bytes. (In practice, we typically use larger values for these parameters—our implementations use 8-bit inputs and 12-bit outputs.) We refer to input bytes as <em>characters</em>, sequences of input bytes as <em>strings</em>, and output bytes as <em>codewords</em>, even though these terms have slightly different meanings in other contexts. The LZW compression algorithm is based on maintaining a symbol table that associates string keys with (fixed-length) codeword values. We initialize the symbol table with the 128 possible single-character string keys and associate them with 8-bit codewords obtained by prepending 0 to the 7-bit value defining each character. For economy and clarity, we use hexadecimal to refer to codeword values, so <code>41</code> is the codeword for ASCII <code>A</code>, <code>52</code> for <code>R</code>, and so forth. We reserve the codeword <code>80</code> to signify end of file. We will assign the rest of the codeword values (<code>81</code> through <code>FF</code>) to various substrings of the input that we encounter, by starting at <code>81</code> and incrementing the value for each new key added. To compress, we perform the following steps as long as there are unscanned input characters:</p>
<p class="indenthangingB">• Find the longest string <code>s</code> in the symbol table that is a prefix of the unscanned input.</p>
<p class="indenthangingB">• Write the 8-bit value (codeword) associated with <code>s</code>.</p>
<p class="indenthangingB">• Scan one character past <code>s</code> in the input.</p>
<p class="indenthangingB">• Associate the next codeword value with <code>s + c</code> (<code>c</code> appended to <code>s</code>) in the symbol table, where <code>c</code> is the next character in the input.</p>
<p>In the last of these steps, we look ahead to see the next character in the input to build the next dictionary entry, so we refer to that character <code>c</code> as the <em>lookahead</em> character. For the moment, we simply stop adding entries to the symbol table when we run out of codeword values (after assigning the value <code>FF</code> to some string)—we will later discuss alternate strategies.</p>
<p><a id="ch05sec3lev95"/></p>
<h5><a id="page_840"/><em>LZW compression example</em></h5>
<p>The figure below gives details of the operation of LZW compression for the example input <code>ABRACADABRABRABRA</code>. For the first seven characters, the longest prefix match is just one character, so we output the codeword associated with the character and associate the codewords from <code>81</code> through <code>87</code> to two-character strings. Then we find prefix matches with <code>AB</code> (so we output <code>81</code> and add <code>ABR</code> to the table), <code>RA</code> (so we output <code>83</code> and add <code>RAB</code> to the table), <code>BR</code> (so we output <code>82</code> and add <code>BRA</code> to the table), and <code>ABR</code> (so we output <code>88</code> and add <code>ABRA</code> to the table), leaving the last <code>A</code> (so we output its codeword, <code>41</code>).</p>
<p class="image"><img alt="image" src="graphics/05_72-lzwabracompress.jpg"/></p>
<p>The input is 17 ASCII characters of 7 bits each for a total of 119 bits; the output is 12 codewords of 8 bits each for a total of 96 bits—a compression ratio of 82 percent even for this tiny example.</p>
<p><a id="ch05sec3lev96"/></p>
<h5><em>LZW trie representation</em></h5>
<p>LZW compression involves two symbol-table operations:</p>
<p class="indenthangingB">• Find a longest-prefix match of the input with a symbol-table key.</p>
<p class="indenthangingB">• Add an entry associating the next codeword with the key formed by appending the lookahead character to that key.</p>
<p class="image"><img alt="image" src="graphics/05_73-lzwabracode.jpg"/></p>
<p>Our trie data structures of <a href="#ch05sec1lev8"><small>SECTION 5.2</small></a> are tailor-made for these operations. The trie representation for our example is shown at right. To find a longest prefix match, we traverse the trie from the root, matching node labels with input characters; to add a new codeword, we connect a new node labeled with the next codeword and the lookahead character to the node where the search terminated. In practice, we use a TST for <a id="page_841"/>space efficiency, as described in <a href="#ch05sec1lev8"><small>SECTION 5.2</small></a>. The contrast with the use of tries in Huffman encoding is worth noting: for Huffman encoding, tries are useful because no prefix of a codeword is also a codeword; for LZW tries are useful because <em>every</em> prefix of an input-substring key is also a key.</p>
<p><a id="ch05sec3lev97"/></p>
<h5><em>LZW expansion</em></h5>
<p>The input for LZW expansion in our example is a sequence of 8-bit codewords; the output is a string of 7-bit ASCII characters. To implement expansion, we maintain a symbol table that associates strings of characters with codeword values (the inverse of the table used for compression). We fill the table entries from <code>00</code> to <code>7F</code> with one-character strings, one for each ASCII character, set the first unassigned codeword value to <code>81</code> (reserving <code>80</code> for end of file), set the current string <code>val</code> to the one-character string consisting of the first character, and perform the following steps until reading codeword <code>80</code> (end of file):</p>
<p class="indenthangingB">• Write the current string <code>val</code>.</p>
<p class="indenthangingB">• Read a codeword <code>x</code> from the input.</p>
<p class="indenthangingB">• Set <code>s</code> to the value associated with <code>x</code> in the symbol table.</p>
<p class="indenthangingB">• Associate the next unasssigned codeword value to <code>val + c</code> in the symbol table, where <code>c</code> is the first character of <code>s</code>.</p>
<p class="indenthangingB">• Set the current string <code>val</code> to <code>s</code>.</p>
<p>This process is more complicated than compression because of the lookahead character: we need to read the next codeword to get the first character in the string associated with it, which puts the process one step out of synch. For the first seven codewords, we just look up and write the appropriate character, then look ahead one character and add a two-character entry to the symbol table, as before. Then we read <code>81</code> (so we write <code>AB</code> and add <code>ABR</code> to the table), <code>83</code> (so we write <code>RA</code> and add <code>RAB</code> to the table), <code>82</code> (so we write <code>BR</code> and add <code>BRA</code> to the table), and <code>88</code> (so we write <code>ABR</code> and add <code>ABRA</code> to the table), leaving <code>41</code>. Finally we read the end-of-file character <code>80</code> (so we write <code>A</code>). At <a id="page_843"/>the end of the process, we have written the original input, as expected, and also built the same code table as for compression, but with the key-value roles inverted. Note that we can use a simple array-of-strings representation for the table, indexed by codeword.</p>
<p class="image"><img alt="image" src="graphics/05_74-lzwabraexpand.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb37"/></p>
<h3><a id="page_842"/>Algorithm 5.11 LZW compression</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0842-01.jpg"/></p>
<p>This implementation of Lempel-Ziv-Welch data compression uses 8-bit input bytes and 12-bit codewords and is appropriate for arbitrary large files. Its codewords for the small example are similar to those discussed in the text: the single-character codewords have a leading <code>0</code>; the others start at <code>100</code>.</p>
<p class="image"><img alt="image" src="graphics/p0842-02.jpg"/></p>
<hr/>
</div>
<p><a id="ch05sec3lev98"/></p>
<h5><em>Tricky situation</em></h5>
<p>There is a subtle bug in the process just described, one that is often discovered by students (and experienced programmers!) only after developing an implementation based on the description above. The problem, illustrated in the example at right, is that it is possible for the lookahead process to get one character ahead of itself. In the example, the input string</p>
<p class="center"><code>ABABABA</code></p>
<p>is compressed to five output codewords</p>
<p class="center"><code>41 42 81 83 80</code></p>
<p>as shown in the top part of the figure. To expand, we read the codeword <code>41</code>, output <code>A</code>, read the codeword <code>42</code> to get the lookahead character, add <code>AB</code> as table entry <code>81</code>, output the <code>B</code> associated with <code>42</code>, read the codeword <code>81</code> to get the lookahead character, add <code>BA</code> as table entry <code>82</code>, and output the <code>AB</code> associated with <code>81</code>. So far, so good. But when we read the codeword <code>83</code> to get the lookahead character, we are stuck, because the reason that we are reading that codeword is to complete table entry <code>83</code>! Fortunately, it is easy to test for that condition (it happens precisely when the codeword is the same as the table entry to be completed) and to correct it (the lookahead character must be the first character in that table entry, since that will be the next character to be output). In this example, this logic tells us that the lookahead character must be <code>A</code> (the first character in <code>ABA</code>). Thus, both the next output string and table entry <code>83</code> should be <code>ABA</code>.</p>
<p class="image"><img alt="image" src="graphics/05_75-lzwabratricky.jpg"/></p>
<p><a id="ch05sec3lev99"/></p>
<h5><em>Implementation</em></h5>
<p>With these descriptions, implementing LZW encoding is straightforward, given in <a href="#ch05sb37"><small>ALGORITHM 5.11</small></a> on the facing page (the implementation of <code>expand()</code> is on the next page). These implementations take 8-bit bytes as input (so we can compress any file, not just strings) and produce 12-bit codewords as output (so that we can get better compression by having a much larger dictionary). These values are specified in the (final) instance variables <code>R</code>, <code>L</code>, and <code>W</code> in the code. We use a TST (see <a href="#ch05sec1lev8"><small>SECTION 5.2</small></a>) for the code table in <code>compress()</code> (taking advantage of the ability of trie data structures to support efficient implementations of <code>longestPrefixOf()</code>) and an array of strings <a id="page_845"/>for the inverse code table in <code>expand()</code>. With these choices, the code for <code>compress()</code> and <code>expand()</code> is little more than a line-by-line translation of the descriptions in the text. These methods are very effective as they stand. For certain files, they can be further improved by emptying the codeword table and starting over each time all the codeword values are used. These improvements, along with experiments to evaluate their effectiveness, are addressed in the exercises at the end of this section.</p>
<div class="sidebar">
<hr/>
<p><a id="ch05sb38"/></p>
<h3><a id="page_844"/>Algorithm 5.11 (continued) LZW expansion</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0844-01.jpg"/></p>
<p>This implementation of expansion for the Lempel-Ziv-Welch algorithm is a bit more complicated than compression because of the need to extract the lookahead character from the next codeword and because of a tricky situation where lookahead is invalid (see text).</p>
<p class="image"><img alt="image" src="graphics/p0844-02.jpg"/></p>
<hr/>
</div>
<p><small>AS USUAL</small>, it is worth your while to study carefully the examples given with the programs and at the bottom of this page of LZW compression in action. Over the several decades since its invention, it has proven to be a versatile and effective data-compression method.</p>
<p class="image"><img alt="image" src="graphics/05_76-lzwexamples.jpg"/></p>
<p><a id="ch05sec2lev53"/></p>
<h4><a id="page_846"/>Q&amp;A</h4>
<p><strong>Q.</strong> Why <code>BinaryStdIn</code> and <code>BinaryStdOut</code>?</p>
<p><strong>A.</strong> It’s a tradeoff between efficiency and convenience. <code>StdIn</code> can handle 8 bits at a time; <code>BinaryStdIn</code> has to handle each bit. Most applications are bytestream-oriented; data compression is a special case.</p>
<p><strong>Q.</strong> Why <code>close()</code> ?</p>
<p><strong>A.</strong> This requirement stems from the fact that standard output is actually a bytestream, so <code>BinaryStdOut</code> needs to know when to write the last byte.</p>
<p><strong>Q.</strong> Can we mix <code>StdIn</code> and <code>BinaryStdIn</code> ?</p>
<p><strong>A.</strong> That is not a good idea. Because of system and implementation dependencies, there is no guarantee of what might happen. Our implementations will raise an exception. On the other hand, there is no problem with mixing <code>StdOut</code> and <code>BinaryStdOut</code> (we do it in our code).</p>
<p><strong>Q.</strong> Why is the <code>Node</code> class <code>static</code> in <code>Huffman</code>?</p>
<p><strong>A.</strong> Our data-compression algorithms are organized as collections of static methods, not data-type implementations.</p>
<p><strong>Q.</strong> Can I at least guarantee that my compression algorithm will not increase the length of a bitstream?</p>
<p><strong>A.</strong> You can just copy it from input to output, but you still need to signify not to use a standard compression scheme. Commercial implementations sometimes make this guarantee, but it is quite weak and far from universal compression. Indeed, typical compression algorithms do not even make it past the second step of our first proof of <a href="#ch05sb33"><small>PROPOSITION S</small></a>: few algorithms will further compress a bitstring produced by that same algorithm.</p>
<p><a id="ch05sec2lev54"/></p>
<h4><a id="page_847"/>Exercises</h4>
<p><a id="ch05qa5q1"/><strong>5.5.1</strong> Consider the four variable-length codes shown in the table at right. Which of the codes are prefix-free? Uniquely decodable? For those that are uniquely decodable, give the encoding of <code>1000000000000</code>.</p>
<p class="image"><img alt="image" src="graphics/t0847-01.jpg"/></p>
<p><a id="ch05qa5q2"/><strong>5.5.2</strong> Given a example of a uniquely decodable code that is not prefix-free.</p>
<p><em>Answer</em>: Any <em>suffix-free</em> code is uniquely decodable.</p>
<p><a id="ch05qa5q3"/><strong>5.5.3</strong> Give an example of a uniquely decodable code that is not prefix free or suffix free.</p>
<p><em>Answer</em>: <code>{0011, 011, 11, 1110}</code> or <code>{01, 10, 011, 110}</code></p>
<p><a id="ch05qa5q4"/><strong>5.5.4</strong> Are <code>{01, 1001, 1011, 111, 1110}</code> and <code>{01, 1001, 1011, 111, 1110}</code> uniquely decodable? If not, find a string with two encodings.</p>
<p><a id="ch05qa5q5"/><strong>5.5.5</strong> Use <code>RunLength</code> on the file <code>q128x192.bin</code> from the booksite. How many bits are there in the compressed file?</p>
<p><a id="ch05qa5q6"/><strong>5.5.6</strong> How many bits are needed to encode <em>N</em> copies of the symbol <code>a</code> (as a function of <em>N</em>)? <em>N</em> copies of the sequence <code>abc</code>?</p>
<p><a id="ch05qa5q7"/><strong>5.5.7</strong> Give the result of encoding the strings <code>a</code>, <code>aa</code>, <code>aaa</code>, <code>aaaa</code>, ... (strings consisting of <em>N</em> <code>a</code>’s) with run-length, Huffman, and LZW encoding. What is the compression ratio as a function of <em>N</em>?</p>
<p><a id="ch05qa5q8"/><strong>5.5.8</strong> Give the result of encoding the strings <code>ab</code>, <code>abab</code>, <code>ababab</code>, <code>abababab</code>, ... (strings consisting of <em>N</em> repetitions of <code>ab</code>) with run-length, Huffman, and LZW encoding. What is the compression ratio as a function of <em>N</em>?</p>
<p><a id="ch05qa5q9"/><strong>5.5.9</strong> Estimate the compression ratio achieved by run-length, Huffman, and LZW encoding for a <em>random</em> ASCII string of length <em>N</em> (all characters equally likely at each position, independently).</p>
<p><a id="ch05qa5q10"/><strong>5.5.10</strong> In the style of the figure in the text, show the Huffman coding tree construction process when you use <code>Huffman</code> for the string <code>"it was the age of foolishness"</code>. How many bits does the compressed bitstream require?</p>
<p><a id="page_848"/><a id="ch05qa5q11"/><strong>5.5.11</strong> What is the Huffman code for a string whose characters are all from a two-character alphabet? Give an example showing the maximum number of bits that could be used in a Huffman code for an <em>N</em>-character string whose characters are all from a two-character alphabet.</p>
<p><a id="ch05qa5q12"/><strong>5.5.12</strong> Suppose that all of the symbol probabilities are negative powers of 2. Describe the Huffman code.</p>
<p><a id="ch05qa5q13"/><strong>5.5.13</strong> Suppose that all of the symbol frequencies are equal. Describe the Huffman code.</p>
<p><a id="ch05qa5q14"/><strong>5.5.14</strong> Suppose that the frequencies of the occurrence of all the characters to be encoded are different. Is the Huffman encoding tree unique?</p>
<p><a id="ch05qa5q15"/><strong>5.5.15</strong> Huffman coding could be extended in a straightforward way to encode in 2-bit characters (using 4-way trees). What would be the main advantage and the main disadvantage of doing so?</p>
<p><a id="ch05qa5q16"/><strong>5.5.16</strong> What is the LZW encoding of the following inputs?</p>
<p class="indenthangingN"><em>a.</em> <code>T O B E O R N O T T O B E</code></p>
<p class="indenthangingN"><em>b.</em> <code>Y A B B A D A B B A D A B B A D O O</code></p>
<p class="indenthangingN"><em>c.</em> <code>A A A A A A A A A A A A A A A A A A A A A</code></p>
<p><a id="ch05qa5q17"/><strong>5.5.17</strong> Characterize the tricky situation in LZW coding.</p>
<p><em>Solution</em>: Whenever it encounters <code>cScSc</code>, where <code>c</code> is a symbol and <code>S</code> is a string, <code>cS</code> is in the dictionary already but <code>cSc</code> is not.</p>
<p><a id="ch05qa5q18"/><strong>5.5.18</strong> Let <em>F<sub>k</sub></em> be the <em>k</em>th Fibonacci number. Consider <em>N</em> symbols, where the <em>k</em>th symbol has frequency <em>F<sub>k</sub></em>. Note that <em>F</em><sub>1</sub> + <em>F</em><sub>1</sub> + ... + <em>F<sub>N</sub></em> = <em>F<sub>N</sub></em><sub>+2</sub> − 1. Describe the Huffman code. <em>Hint</em>: The longest codeword has length <em>N</em> − 1.</p>
<p><a id="ch05qa5q19"/><strong>5.5.19</strong> Show that there are at least 2<em><sup>N</sup></em><sup>−1</sup> different Huffman codes corresponding to a given set of <em>N</em> symbols.</p>
<p><a id="ch05qa5q20"/><strong>5.5.20</strong> Give a Huffman code where the frequency of 0s in the output is much, much higher than the frequency of 1s.</p>
<p><em>Answer</em>: If the character <code>A</code> occurs 1 million times and the character <code>B</code> occurs once, the codeword for <code>A</code> will be <code>0</code> and the codeword for <code>B</code> will be <code>1</code>.</p>
<p><a id="page_849"/><a id="ch05qa5q21"/><strong>5.5.21</strong> Prove that the two longest codewords in a Huffman code have the same length.</p>
<p><a id="ch05qa5q22"/><strong>5.5.22</strong> Prove the following fact about Huffman codes: If the frequency of symbol <em>i</em> is strictly larger than the frequency of symbol <em>j</em>, then the length of the codeword for symbol <em>i</em> is less than or equal to the length of the codeword for symbol <em>j</em>.</p>
<p><a id="ch05qa5q23"/><strong>5.5.23</strong> What would be the result of breaking up a Huffman-encoded string into five-bit characters and Huffman-encoding that string?</p>
<p><a id="ch05qa5q24"/><strong>5.5.24</strong> In the style of the figures in the text, show the encoding trie and the compression and expansion processes when <code>LZW</code> is used for the string</p>
<p class="programlisting">it was the best of times it was the worst of times</p>
<p><a id="ch05sec2lev55"/></p>
<h4><a id="page_850"/>Creative Problems</h4>
<p><a id="ch05qa5q25"/><strong>5.5.25</strong> <em>Fixed length width code.</em> Implement a class <code>RLE</code> that uses fixed-length encoding, to compress ASCII bytestreams using relatively few different characters, including the code as part of the encoded bitstream. Add code to <code>compress()</code> to make a string <code>alpha</code> with all the distinct characters in the message and use it to make an <code>Alphabet</code> for use in <code>compress()</code>, prepend <code>alpha</code> (8-bit encoding plus its length) to the compressed bitstream, then add code to <code>expand()</code> to read the alphabet before expansion.</p>
<p><a id="ch05qa5q26"/><strong>5.5.26</strong> <em>Rebuilding the LZW dictionary.</em> Modify <code>LZW</code> to empty the dictionary and start over when it is full. This approach is recommended in some applications because it better adapts to changes in the general character of the input.</p>
<p><a id="ch05qa5q27"/><strong>5.5.27</strong> <em>Long repeats.</em> Estimate the compression ratio achieved by run-length, Huffman, and LZW encoding for a string of length 2<em>N</em> formed by concatenating <em>two copies</em> of a random ASCII string of length <em>N</em> (see <a href="#ch05qa5q9"><small>EXERCISE 5.5.9</small></a>), under any assumptions that you think are reasonable.</p>
</body>
</html>