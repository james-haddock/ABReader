<?xml version="1.0" encoding="UTF-8" standalone="no"?><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Three. Searching</title>
<link href="9780132762564.css" rel="stylesheet" type="text/css"/>
<link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml"/>
<meta name="Adept.resource" value="urn:uuid:7baf5dbb-ffe1-4201-87bd-b993ed04f947"/>
</head>
<body>
<p><a id="ch03"/></p>
<h2><a id="page_360"/>Three. Searching</h2>
<div class="sidebar">
<hr/>
<p><a id="ch03sb01"/></p>
<p class="indenthangingN"><strong><a href="#ch03sec1lev1">3.1</a></strong> <a href="#ch03sec1lev1">Symbol Tables</a> <a href="#ch03sec1lev1">362</a></p>
<p class="indenthangingN"><strong><a href="#ch03sec1lev2">3.2</a></strong> <a href="#ch03sec1lev2">Binary Search Trees</a> <a href="#ch03sec1lev2">396</a></p>
<p class="indenthangingN"><strong><a href="#ch03sec1lev3">3.3</a></strong> <a href="#ch03sec1lev3">Balanced Search Trees</a> <a href="#ch03sec1lev3">424</a></p>
<p class="indenthangingN"><strong><a href="#ch03sec1lev4">3.4</a></strong> <a href="#ch03sec1lev4">Hash Tables</a> <a href="#ch03sec1lev4">458</a></p>
<p class="indenthangingN"><strong><a href="ch03a.html#ch03sec1lev5">3.5</a></strong> <a href="ch03a.html#ch03sec1lev5">Applications</a> <a href="ch03a.html#ch03sec1lev5">486</a></p>
<hr/>
</div>
<p><a id="page_361"/>Modern computing and the internet have made accessible a vast amount of information. The ability to efficiently search through this information is fundamental to processing it. This chapter describes classical <em>searching</em> algorithms that have proven to be effective in numerous diverse applications for decades. Without algorithms like these, the development of the computational infrastructure that we enjoy in the modern world would not have been possible.</p>
<p>We use the term <em>symbol table</em> to describe an abstract mechanism where we save information (a <em>value</em>) that we can later search for and retrieve by specifying a <em>key</em>. The nature of the keys and the values depends upon the application. There can be a huge number of keys and a huge amount of information, so implementing an efficient symbol table is a significant computational challenge.</p>
<p>Symbol tables are sometimes called <em>dictionaries</em>, by analogy with the time-honored system of providing definitions for words by listing them alphabetically in a reference book. In an English-language dictionary, a key is a word and its values is the entry associated with the word that contains the definition, pronunciation, and etymology. Symbol tables are also sometimes called <em>indices</em>, by analogy with another time-honored system of providing access to terms by listing them alphabetically at the end of a book such as a textbook. In a book index, a key is a term of interest and its value is the list of page numbers that tell readers where to find that term in the book.</p>
<p>After describing the basic APIs and two fundamental implementations, we consider three classic data structures that can support efficient symbol-table implementations: binary search trees, red-black trees, and hash tables. We conclude with several extensions and applications, many of which would not be feasible without the efficient algorithms that you will learn about in this chapter.</p>
<p><a id="ch03sec1lev1"/></p>
<h3><a id="page_362"/>3.1 Symbol Tables</h3>
<p>The primary purpose of a symbol table is to associate a <em>value</em> with a <em>key</em>. The client can <em>insert</em> key-value pairs into the symbol table with the expectation of later being able to <em>search</em> for the value associated with a given key, from among all of the key-value pairs that have been put into the table. This chapter describes several ways to structure this data so as to make efficient not just the <em>insert</em> and <em>search</em> operations, but several other convenient operations as well. To implement a symbol table, we need to define an underlying data structure and then specify algorithms for insert, search, and other operations that create and manipulate the data structure.</p>
<p>Search is so important to so many computer applications that symbol tables are available as high-level abstractions in many programming environments, including Java—we shall discuss Java’s symbol-table implementations in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>. The table below gives some examples of keys and values that you might use in typical applications. We consider some illustrative reference clients soon, and <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a> is devoted to showing you how to use symbol tables effectively in your own clients. We also use symbol tables in developing other algorithms throughout the book.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb02"/></p>
<p><strong>Definition.</strong> A <em>symbol table</em> is a data structure for key-value pairs that supports two operations: <em>insert</em> (put) a new pair into the table and <em>search</em> for (get) the value associated with a given key.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/t0362-01.jpg"/></p>
<p><a id="ch03sec2lev1"/></p>
<h4><a id="page_363"/>API</h4>
<p>The symbol table is a prototypical <em>abstract data type</em> (see <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>): it represents a well-defined set of values and operations on those values, enabling us to develop clients and implementations separately. As usual, we precisely define the operations by specifying an applications programming interface (API) that provides the contract between client and implementation:</p>
<p class="image"><img alt="image" src="graphics/t0363-01.jpg"/></p>
<p>Before examining client code, we consider several design choices for our implementations to make our code consistent, compact, and useful.</p>
<p><a id="ch03sec3lev1"/></p>
<h5><em>Generics</em></h5>
<p>As we did with sorting, we will consider the methods without specifying the types of the items being processed, using generics. For symbol tables, we emphasize the separate roles played by keys and values in search by specifying the key and value types explicitly instead of viewing keys as implicit in items as we did for priority queues in <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>. After we have considered some of the characteristics of this basic API (for example, note that there is no mention of order among the keys), we will consider an extension for the typical case when keys are <code>Comparable</code>, which enables numerous additional methods.</p>
<p><a id="ch03sec3lev2"/></p>
<h5><em>Duplicate keys</em></h5>
<p>We adopt the following conventions in all of our implementations:</p>
<p class="indenthangingB">• Only one value is associated with each key (no duplicate keys in a table).</p>
<p class="indenthangingB">• When a client puts a key-value pair into a table already containing that key (and an associated value), the new value replaces the old one.</p>
<p>These conventions define the <em>associative array abstraction</em>, where you can think of a symbol table as being just like an array, where keys are indices and values are array <a id="page_364"/>entries. In a conventional array, keys are integer indices that we use to quickly access array values; in an associative array (symbol table), keys are of arbitrary type, but we can still use them to quickly access values. Some programming languages (not Java) provide special support that allows programmers to use code such as <code>st[key]</code> for <code>st.get(key)</code> and <code>st[key] = val</code> for <code>st.put(key, val)</code> where <code>key</code> and <code>val</code> are objects of arbitrary type.</p>
<p><a id="ch03sec3lev3"/></p>
<h5><em>Null keys</em></h5>
<p>Keys must not be <code>null</code>. As with many mechanisms in Java, use of a null key results in an exception at runtime (see the third <a href="#ch03sec2lev8">Q&amp;A</a> on page <a href="#ch03sec2lev8">387</a>).</p>
<p><a id="ch03sec3lev4"/></p>
<h5><em>Null values</em></h5>
<p>We also adopt the convention that no key can be associated with the value <code>null</code>. This convention is directly tied to our specification in the API that <code>get()</code> should return <code>null</code> for keys not in the table, effectively associating the value <code>null</code> with every key not in the table. This convention has two (intended) consequences: First, we can test whether or not the symbol table defines a value associated with a given key by testing whether <code>get()</code> returns <code>null</code>. Second, we can use the operation of calling <code>put()</code> with <code>null</code> as its second (value) argument to implement deletion, as described in the next paragraph.</p>
<p><a id="ch03sec3lev5"/></p>
<h5><em>Deletion</em></h5>
<p>Deletion in symbol tables generally involves one of two strategies: <em>lazy</em> deletion, where we associate keys in the table with <code>null</code>, then perhaps remove all such keys at some later time; and <em>eager</em> deletion, where we remove the key from the table immediately. As just discussed, the code <code>put(key, null)</code> is an easy (lazy) implementation of <code>delete(key)</code>. When we give an (eager) implementation of <code>delete()</code>, it is intended to replace this default. In our symbol-table implementations that do not use the default <code>delete()</code>, the <code>put()</code> implementations on the booksite begin with the defensive code</p>
<p class="programlisting">if (val == null) {  delete(key); return; }</p>
<p>to ensure that no key in the table is associated with <code>null</code>. For economy, we do not include this code in the book (and we do not call <code>put()</code> with a <code>null</code> value in client code).</p>
<p><a id="ch03sec3lev6"/></p>
<h5><em>Shorthand methods</em></h5>
<p>For clarity in client code, we include the methods <code>contains()</code> and <code>isEmpty()</code> in the API, with the default one-line implementations shown here. For economy, we do not repeat this code, but we assume it to be present in all implementations of the symbol-table API and use these methods freely in client code.</p>
<p class="image"><img alt="image" src="graphics/t0364-01.jpg"/></p>
<p><a id="ch03sec3lev7"/></p>
<h5><a id="page_365"/><em>Iteration</em></h5>
<p>To enable clients to process all the keys and values in the table, we might add the phrase <code>implements Iterable&lt;Key&gt;</code> to the first line of the API to specify that every implementation must implement an <code>iterator()</code> method that returns an iterator having appropriate implementations of <code>hasNext()</code> and <code>next()</code>, as described for stacks and queues in <a href="ch01a.html#ch01sec1lev5"><small>SECTION 1.3</small></a>. For symbol tables, we adopt a simpler alternative approach, where we specify a <code>keys()</code> method that returns an <code>Iterable&lt;Key&gt;</code> object for clients to use to iterate through the keys. Our reason for doing so is to maintain consistency with methods that we will define for ordered symbol tables that allow clients to iterate through a specified subset of keys in the table.</p>
<p><a id="ch03sec3lev8"/></p>
<h5><em>Key equality</em></h5>
<p>Determining whether or not a given key is in a symbol table is based on the concept of <em>object equality</em>, which we discussed at length in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a> (see page <a href="ch01.html#page_102">102</a>). Java’s convention that all objects inherit an <code>equals()</code> method and its implementation of <code>equals()</code> both for standard types such as <code>Integer</code>, <code>Double</code>, and <code>String</code> and for more complicated types such as <code>File</code> and <code>URL</code> is a head start—when using these types of data, you can just use the built-in implementation. For example, if <code>x</code> and <code>y</code> are <code>String</code> values, then <code>x.equals(y)</code> is <code>true</code> if and only if <code>x</code> and <code>y</code> have the same length and are identical in each character position. For such client-defined keys, you need to override <code>equals()</code>, as discussed in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a>. You can use our implementation of <code>equals()</code> for <code>Date</code> (page <a href="ch01.html#page_103">103</a>) as a template to develop <code>equals()</code> for a type of your own. As discussed for priority queues on page <a href="ch02.html#page_320">320</a>, a best practice is to make <code>Key</code> types immutable, because consistency cannot otherwise be guaranteed.</p>
<p><a id="ch03sec2lev2"/></p>
<h4><a id="page_366"/>Ordered symbol tables</h4>
<p>In typical applications, keys are <code>Comparable</code> objects, so the option exists of using the code <code>a.compareTo(b)</code> to compare two keys <code>a</code> and <code>b.</code> Several symbol-table implementations take advantage of order among the keys that is implied by <code>Comparable</code> to provide efficient implementations of the <code>put()</code> and <code>get()</code> operations. More important, in such implementations, we can think of the symbol table as <em>keeping the keys in order</em> and consider a significantly expanded API that defines numerous natural and useful operations involving relative key order. For example, suppose that your keys are times of the day. You might be interested in knowing the earliest or the latest time, the set of keys that fall between two given times, and so forth. In most cases, such operations are not difficult to implement with the same data structures and methods underlying the <code>put()</code> and <code>get()</code> implementations. Specifically, for applications where keys are <code>Comparable</code>, we implement in this chapter the following API:</p>
<p class="image"><img alt="image" src="graphics/t0366-01.jpg"/></p>
<p><a id="page_367"/>Your signal that one of our programs is implementing this API is the presence of the <code>Key extends Comparable&lt;Key&gt;</code> generic type variable in the class declaration, which specifies that the code depends upon the keys being <code>Comparable</code> and implements the richer set of operations. Together, these operations define for client programs an <em>ordered symbol table.</em></p>
<p><a id="ch03sec3lev9"/></p>
<h5><em>Minimum and maximum</em></h5>
<p>Perhaps the most natural queries for a set of ordered keys are to ask for the smallest and largest keys. We have already encountered these operations, in our discussion of priority queues in <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>. In ordered symbol tables, we also have methods to delete the maximum and minimum keys (and their associated values). With this capability, the symbol table can operate like the <code>IndexMinPQ()</code> class that we discussed in <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>. The primary differences are that equal keys are allowed in priority queues but not in symbol tables and that ordered symbol tables support a much larger set of operations.</p>
<p class="image"><img alt="image" src="graphics/03_01-orderedex.jpg"/></p>
<p><a id="ch03sec3lev10"/></p>
<h5><em>Floor and ceiling</em></h5>
<p>Given a key, it is often useful to be able to perform the <em>floor</em> operation (find the largest key that is less than or equal to the given key) and the <em>ceiling</em> operation (find the smallest key that is greater than or equal to the given key). The nomenclature comes from functions defined on real numbers (the floor of a real number <em>x</em> is the largest integer that is smaller than or equal to <em>x</em> and the ceiling of a real number <em>x</em> is the smallest integer that is greater than or equal to <em>x</em>).</p>
<p><a id="ch03sec3lev11"/></p>
<h5><em>Rank and selection</em></h5>
<p>The basic operations for determining where a new key fits in the order are the <em>rank</em> operation (find the number of keys less than a given key) and the <em>select</em> operation (find the key with a given rank). To test your understanding of their meaning, confirm for yourself that both <code>i == rank(select(i))</code> for all <code>i</code> between <code>0</code> and <code>size()-1</code> and all keys in the table satisfy <code>key == select(rank(key))</code>. We have already encountered the need for these operations, in our discussion of sort applications in <a href="ch02.html#ch02sec1lev5"><small>SECTION 2.5</small></a>. For symbol tables, our challenge is to perform these operations quickly, intermixed with insertions, deletions, and searches.</p>
<p><a id="ch03sec3lev12"/></p>
<h5><a id="page_368"/><em>Range queries</em></h5>
<p>How many keys fall within a given range (between two given keys)? Which keys fall in a given range? The two-argument <code>size()</code> and <code>keys()</code> methods that answer these questions are useful in many applications, particularly in large databases. The capability to handle such queries is one prime reason that ordered symbol tables are so widely used in practice.</p>
<p><a id="ch03sec3lev13"/></p>
<h5><em>Exceptional cases</em></h5>
<p>When a method is to return a key and there is no key fitting the description in the table, our convention is to throw an exception (an alternate approach, which is also reasonable, would be to return <code>null</code> in such cases). For example, <code>min()</code>, <code>max()</code>, <code>deleteMin()</code>, <code>deleteMax()</code>, <code>floor()</code>, and <code>ceiling()</code> all throw exceptions if the table is empty, as does <code>select(k)</code> if <code>k</code> is less than <code>0</code> or not less than <code>size()</code>.</p>
<p><a id="ch03sec3lev14"/></p>
<h5><em>Shorthand methods</em></h5>
<p>As we have already seen with <code>isEmpty()</code> and <code>contains()</code> in our basic API, we keep some redundant methods in the API for clarity in client code. For economy in the text, we assume that the following default implementations are included in any implementation of the ordered symbol-table API unless otherwise specified:</p>
<p class="image"><img alt="image" src="graphics/t0368-01.jpg"/></p>
<p><a id="ch03sec3lev15"/></p>
<h5><em>Key equality (revisited)</em></h5>
<p>The best practice in Java is to make <code>compareTo()</code> consistent with <code>equals()</code> in all <code>Comparable</code> types. That is, for every pair of values <code>a</code> and <code>b</code> in any given <code>Comparable</code> type, it should be the case that <code>(a.compareTo(b) == 0)</code> and <code>a.equals(b)</code> have the same value. To avoid any potential ambiguities, we avoid the use of <code>equals()</code> in ordered symbol-table implementations. Instead, we use <code>compareTo()</code> exclusively to compare keys: we take the boolean expression <code>a.compareTo(b) == 0</code> to <a id="page_369"/>mean “Are <code>a</code> and <code>b</code> equal?” Typically, such a test marks the successful end of a search for <code>a</code> in the symbol table (by finding <code>b</code>). As you saw with sorting algorithms, Java provides standard implementations of <code>compareTo()</code> for many commonly used types of keys, and it is not difficult to develop a <code>compareTo()</code> implementation for your own data types (see <a href="ch02.html#ch02sec1lev5"><small>SECTION 2.5</small></a>).</p>
<p><a id="ch03sec3lev16"/></p>
<h5><em>Cost model</em></h5>
<p>Whether we use <code>equals()</code> (for symbol tables where keys are not <code>Comparable</code>) or <code>compareTo()</code> (for ordered symbol tables with <code>Comparable</code> keys), we use the term <em>compare</em> to refer to the operation of comparing a symbol-table entry against a search key. In most symbol-table implementations, this operation is in the inner loop. In the few cases where that is not the case, we also count array accesses.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb03"/></p>
<p><strong>Searching cost model.</strong> When studying symbol-table implementations, we count <em>compares</em> (equality tests or key comparisons). In (rare) cases where compares are not in the inner loop, we count <em>array accesses</em>.</p>
<hr/>
</div>
<p><small>SYMBOL-TABLE IMPLEMENTATIONS</small> are generally characterized by their underlying data structures and their implementations of <code>get()</code> and <code>put()</code>. We do not always provide implementations of all of the other methods in the text because many of them make good exercises to test your understanding of the underlying data structures. To distinguish implementations, we add a descriptive prefix to <code>ST</code> that refers to the implementation in the class name of symbol-table implementations. In clients, we use <code>ST</code> to call on a reference implementation unless we wish to refer to a specific implementation. You will gradually develop a better feeling for the rationale behind the methods in the APIs in the context of the numerous clients and symbol-table implementations that we present and discuss throughout this chapter and throughout the rest of this book. We also discuss alternatives to the various design choices that we have described here in the Q&amp;A and exercises.</p>
<p><a id="ch03sec2lev3"/></p>
<h4><a id="page_370"/>Sample clients</h4>
<p>While we defer detailed consideration of applications to <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>, it is worthwhile to consider some client code before considering implementations. Accordingly, we now consider two clients: a test client that we use to trace algorithm behavior on small inputs and a performance client that we use to motivate the development of efficient implementations.</p>
<p><a id="ch03sec3lev17"/></p>
<h5><em>Test client</em></h5>
<p>For tracing our algorithms on small inputs we assume that all of our implementations use the test client below, which takes a sequence of strings from standard input, builds a symbol table that associates the value <code>i</code> with the <code>i</code>th string in the input, and then prints the table. In the traces in the text, we assume that the input is a sequence of single-character strings. Most often, we use the string <code>"S E A R C H E X A M P L E"</code>. By our conventions, this client associates the key <code>S</code> with the value <code>0</code>, the key <code>R</code> with the value <code>3</code>, and so forth. But <code>E</code> is associated with the value <code>12</code> (not <code>1</code> or <code>6</code>) and <code>A</code> with the value <code>8</code> (not <code>2</code>) because our associative-array convention implies that each key is associated with the value used in the most recent call to <code>put()</code>. For basic (unordered) implementations, the order of the keys in the output of this test client is not specified (it depends on characteristics of the implementation); for an ordered symbol table the test client prints the keys in sorted order. This client is an example of an <em>indexing</em> client, a special case of a fundamental symbol-table application that we discuss in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>.</p>
<p class="image"><img alt="image" src="graphics/p0370-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0370-01.jpg"/></p>
<p><a id="ch03sec3lev18"/></p>
<h5><a id="page_371"/><em>Performance client</em></h5>
<p><code>FrequencyCounter</code> (on the next page) is a symbol-table client that finds the number of occurrences of each string (having at least as many characters as a given threshold length) in a sequence of strings from standard input, then iterates through the keys to find the one that occurs the most frequently. This client is an example of a <em>dictionary</em> client, an application that we discuss in more detail in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>. This client answers a simple question: Which word (no shorter than a given length) occurs most frequently in a given text? Throughout this chapter, we measure performance of this client with three reference inputs: the first five lines of C. Dickens’s <em>Tale of Two Cities</em> (<code>tinyTale.txt</code>), the full text of the book (<code>tale.txt</code>), and a popular database of 1 million sentences taken at random from the web that is known as the <em>Leipzig Corpora Collection</em> (<code>leipzig1M.txt</code>). For example, here is the content of <code>tinyTale.txt</code>:</p>
<p class="image"><img alt="image" src="graphics/p0371-01.jpg"/></p>
<p>This text has 60 words taken from 20 distinct words, four of which occur ten times (the highest frequency). Given this input, <code>FrequencyCounter</code> will print out any of <code>it</code>, <code>was</code>, <code>the</code>, or <code>of</code> (the one chosen may vary, depending upon characteristics of the symbol-table implementation), followed by the frequency, <code>10</code>.</p>
<p>To study performance for the larger inputs, it is clear that two measures are of interest: Each word in the input is used as a search key once, so the total number of words in the text is certainly relevant. Second, each <em>distinct</em> word in the input is put into the <a id="page_373"/>symbol table (and the total number of distinct words in the input gives the size of the table after all keys have been inserted), so the total number of distinct words in the input stream is certainly relevant. We need to know both of these quantities in order to estimate the running time of <code>FrequencyCounter</code> (for a start, see <a href="#ch03qa1q6"><small>EXERCISE 3.1.6</small></a>). We defer details until we consider some algorithms, but you should have in mind a general idea of the needs of typical applications like this one. For example, running <code>FrequencyCounter</code> on <code>leipzig1M.txt</code> for words of length 8 or more involves millions of searches in a table with hundreds of thousands of keys and values. A server on the web might need to handle billions of transactions on tables with millions of keys and values.</p>
<p class="image"><img alt="image" src="graphics/t0371-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb04"/></p>
<h3><a id="page_372"/>A symbol-table client</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0372-01.jpg"/></p>
<p>This <code>ST</code> client counts the frequency of occurrence of the strings in standard input, then prints out one that occurs with highest frequency. The command-line argument specifies a lower bound on the length of keys considered.</p>
<p class="image"><img alt="image" src="graphics/p0372-02.jpg"/></p>
<hr/>
</div>
<p><small>THE BASIC QUESTION THAT THIS CLIENT</small> and these examples raise is the following: Can we develop a symbol-table implementation that can handle a huge number of <code>get()</code> operations on a large table, which itself was built with a large number of intermixed <code>get()</code> and <code>put()</code> operations? If you are only doing a few searches, any implementation will do, but you cannot make use of a client like <code>FrequencyCounter</code> for large problems without a good symbol-table implementation. <code>FrequencyCounter</code> is surrogate for a very common situation. Specifically, it has the following characteristics, which are shared by many other symbol-table clients:</p>
<p class="indenthangingB">• Search and insert operations are intermixed.</p>
<p class="indenthangingB">• The number of distinct keys is not small.</p>
<p class="indenthangingB">• Substantially more searches than inserts are likely.</p>
<p class="indenthangingB">• Search and insert patterns, though unpredictable, are not random.</p>
<p>Our goal is to develop symbol-table implementations that make it feasible to use such clients to solve typical practical problems.</p>
<p>Next, we consider two elementary implementations and their performance for <code>FrequencyCounter</code>. Then, in the next several sections, you will learn classic implementations that can achieve excellent performance for such clients, even for huge input streams and tables.</p>
<p><a id="ch03sec2lev4"/></p>
<h4><a id="page_374"/>Sequential search in an unordered linked list</h4>
<p>One straightforward option for the underlying data structure for a symbol table is a linked list of nodes that contain keys and values, as in the code on the facing page. To implement <code>get()</code>, we scan through the list, using <code>equals()</code> to compare the search key with the key in each node in the list. If we find the match, we return the associated value; if not, we return <code>null</code>. To implement <code>put()</code>, we also scan through the list, using <code>equals()</code> to compare the client key with the key in each node in the list. If we find the match, we update the value associated with that key to be the value given in the second argument; if not, we create a new node with the given key and value and insert it at the beginning of the list. This method is known as <em>sequential search</em>: we search by considering the keys in the table one after another, using <code>equals()</code> to test for a match with our search key.</p>
<p><a href="#ch03sb05"><small>ALGORITHM 3.1</small></a> (<code>SequentialSearchST</code>) is an implementation of our basic symbol-table API that uses standard list-processing mechanisms, which we have used for elementary data structures in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>. We have left the implementations of <code>size()</code>, <code>keys()</code>, and eager <code>delete()</code> for exercises. You are encouraged to work these exercises to cement your understanding of the linked-list data structure and the basic symbol-table API.</p>
<p>Can this linked-list-based implementation handle applications that need large tables, such as our sample clients? As we have noted, analyzing symbol-table algorithms is more complicated than analyzing sorting algorithms because of the difficulty of <a id="page_376"/>characterizing the sequence of operations that might be invoked by a given client. As noted for <code>FrequencyCounter</code>, the most common situation is that, while search and insert patterns are unpredictable, they certainly are not random. For this reason, we pay careful attention to worst-case performance. For economy, we use the term <em>search hit</em> to refer to a successful search and <em>search miss</em> to refer to an unsuccessful search.</p>
<p class="image"><img alt="image" src="graphics/03_03-linkedlist.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb05"/></p>
<h3><a id="page_375"/>Algorithm 3.1 Sequential search (in an unordered linked list)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0375-01.jpg"/></p>
<p>This <code>ST</code> implementation uses a private <code>Node</code> inner class to keep the keys and values in an unordered linked list. The <code>get()</code> implementation searches the list sequentially to find whether the key is in the table (and returns the associated value if so). The <code>put()</code> implementation also searches the list sequentially to check whether the key is in the table. If so, it updates the associated value; if not, it creates a new node with the given key and value and inserts it at the beginning of the list. Implementations of <code>size()</code>, <code>keys()</code>, and eager <code>delete()</code> are left for exercises.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb06"/></p>
<p><strong>Proposition A.</strong> Search misses and insertions in an (unordered) linked-list symbol table having <em>N</em> key-value pairs both require <em>N</em> compares, and search hits <em>N</em> compares in the worst case. In particular, inserting <em>N</em> distinct keys into an initially empty linked-list symbol table uses ~<em>N</em><sup>2</sup>/2 compares.</p>
<p><strong>Proof:</strong> When searching for a key that is not in the list, we test every key in the table against the search key. Because of our policy of disallowing duplicate keys, we need to do such a search before each insertion.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb07"/></p>
<p><strong>Corollary.</strong> Inserting <em>N</em> distinct keys into an initially empty linked-list symbol table uses ~<em>N</em><sup>2</sup>/2 compares.</p>
<hr/>
</div>
<p>It is true that searches for keys that are <em>in</em> the table need not take linear time. One useful measure is to compute the total cost of searching for all of the keys in the table, divided by <em>N</em>. This quantity is precisely the expected number of compares required for a search under the condition that searches for each key in the table are equally likely. We refer to such a search as a <em>random search hit</em>. Though client search patterns are not likely to be random, they often are well-described by this model. It is easy to show that the average number of compares for a random search hit is ~ <em>N</em>/2: the <code>get()</code> method in <a href="#ch03sb05"><small>ALGORITHM 3.1</small></a> uses 1 compare to find the first key, 2 compares to find the second key, and so forth, for an average cost of (1 + 2 + ... + <em>N</em>)/<em>N</em> = (<em>N</em> + 1)/2 ~ <em>N</em>/2.</p>
<p>This analysis strongly indicates that a linked-list implementation with sequential search is too slow for it to be used to solve huge problems such as our reference inputs with clients like <code>FrequencyCounter</code>. The total number of compares is proportional to the product of the number of searches and the number of inserts, which is more than 10<sup>9</sup> for <em>Tale of Two Cities</em> and more than 10<sup>14</sup> for the Leipzig Corpora.</p>
<p>As usual, to validate analytic results, we need to run experiments. As an example, we study the operation of <code>FrequencyCounter</code> with command-line argument <code>8</code> for <code>tale.txt</code>, which involves 14,350 <code>put()</code> operations (recall that every word in the input leads to a <code>put()</code>, to update its frequency, and we ignore the cost of easily avoided calls <a id="page_377"/>to <code>contains()</code>). The symbol table grows to 5,737 keys, so about one-third of the operations increase the size of the table; the rest are searches. To visualize the performance, we use <code>VisualAccumulator</code> (see page <a href="ch01.html#page_95">95</a>) to plot two points corresponding to each <code>put()</code> operation as follows: for the <em>i</em>th <code>put()</code> operation we plot a gray point with <em>x</em> coordinate <em>i</em> and <em>y</em> coordinate the number of key compares it uses and a red point with <em>x</em> coordinate <em>i</em> and <em>y</em> coordinate the cumulative average number of key compares used for the first <em>i</em> <code>put()</code> operations. As with any scientific data, there is a great deal of information in this data for us to investigate (this plot has 14,350 gray points and 14,350 red points). In this context, our primary interest is that the plot validates our hypothesis that about half the list is accessed for the average <code>put()</code> operation. The actual total is slightly lower than half, but this fact (and the precise shape of the curves) is perhaps best explained by characteristics of the application, not our algorithms (see <a href="#ch03qa1q36"><small>EXERCISE 3.1.36</small></a>).</p>
<p>While detailed characterization of performance for particular clients can be complicated, specific hypotheses are easy to formulate and to test for <code>FrequencyCount</code> with our reference inputs or with randomly ordered inputs, using a client like the <code>DoublingTest</code> client that we introduced in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>. We will reserve such testing for exercises and for the more sophisticated implementations that follow. If you are not already convinced that we need faster implementations, be sure to work these exercises (or just run <code>FrequencyCounter</code> with <code>SequentialSearchST</code> on <code>leipzig1M.txt</code>!).</p>
<p class="image"><img alt="image" src="graphics/03_04-freqll.jpg"/></p>
<p><a id="ch03sec2lev5"/></p>
<h4><a id="page_378"/>Binary search in an ordered array</h4>
<p>Next, we consider a full implementation of our ordered symbol-table API. The underlying data structure is a pair of parallel arrays, one for the keys and one for the values. <a href="#ch03sb08"><small>ALGORITHM 3.2</small></a> (<code>BinarySearchST</code>) on the facing page keeps <code>Comparable</code> keys in order in the array, then uses array indexing to enable fast implementation of <code>get()</code> and other operations.</p>
<p>The heart of the implementation is the <code>rank()</code> method, which returns the number of keys smaller than a given key. For <code>get()</code>, the rank tells us precisely where the key is to be found if it is in the table (and, if it is not there, that it is <em>not</em> in the table).</p>
<p>For <code>put()</code>, the rank tells us precisely where to update the value when the key is in the table, and precisely where to put the key when the key is not in the table. We move all larger keys over one position to make room (working from back to front) and insert the given key and value into the proper positions in their respective arrays. Again, studying <code>BinarySearchST</code> in conjunction with a trace of our test client is an instructive introduction to this data structure.</p>
<p>This code maintains parallel arrays of keys and values (see <a href="#ch03qa1q12"><small>EXERCISE 3.1.12</small></a> for an alternative). As with our implementations of generic stacks and queues in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, this code carries the inconvenience of having to create a <code>Key</code> array of type <code>Comparable</code> and a <code>Value</code> array of type <code>Object</code>, and to cast them back to <code>Key[]</code> and <code>Value[]</code> in the constructor. As usual, we can use array resizing so that clients do not have to be concerned with the size of the array (noting, as you shall see, that this method is too slow to use with large arrays).</p>
<p class="image"><img alt="image" src="graphics/03_05-array.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb08"/></p>
<h3><a id="page_379"/>Algorithm 3.2 Binary search (in an ordered array)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0379-01.jpg"/></p>
<p>This <code>ST</code> implementation keeps the keys and values in parallel arrays. The <code>put()</code> implementation moves larger keys one position to the right before growing the table as in the array-based stack implementation in <a href="ch01a.html#ch01sec1lev5"><small>SECTION 1.3</small></a>. Array-resizing code is omitted here.</p>
<hr/>
</div>
<p><a id="ch03sec3lev19"/></p>
<h5><a id="page_380"/><em>Binary search</em></h5>
<p>The reason that we keep keys in an ordered array is so that we can use array indexing to dramatically reduce the number of compares required for each search, using the classic and venerable <em>binary search</em> algorithm that we used as an example in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>. We maintain indices into the sorted key array that delimit the subarray that might contain the search key. To search, we compare the search key against the key in the middle of the subarray. If the search key is less than the key in the middle, we search in the left half of the subarray; if the search key is greater than the key in the middle, we search in the right half of the subarray; otherwise the key in the middle is equal to the search key. The <code>rank()</code> code on the facing page uses binary search to complete the symbol-table implementation just discussed. This implementation is worthy of careful study. To study it, we start with the equivalent recursive code at left. A call to <code>rank(key, 0, N-1)</code> does the same sequence of compares as a call to the nonrecursive implementation in <a href="#ch03sb08"><small>ALGORITHM 3.2</small></a>, but this alternate version better exposes the structure of the algorithm, as discussed in <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a>. This recursive <code>rank()</code> preserves the following properties:</p>
<p class="indenthangingB">• If <code>key</code> is in the table, <code>rank()</code> returns its index in the table, which is the same as the number of keys in the table that are smaller than <code>key</code>.</p>
<p class="indenthangingB">• If <code>key</code> is not in the table, <code>rank()</code> <em>also</em> returns the number of keys in the table that are smaller than <code>key</code>.</p>
<p class="image"><img alt="image" src="graphics/p0380-01.jpg"/></p>
<p>Taking the time to convince yourself that the nonrecursive <code>rank()</code> in <a href="#ch03sb08"><small>ALGORITHM 3.2</small></a> operates as expected (either by proving that it is equivalent to the recursive version or by proving directly that the loop always terminates with the value of <code>lo</code> precisely equal to the number of keys in the table that are smaller than <code>key</code>) is a worthwhile exercise for any programmer. (<em>Hint</em>: Note that <code>lo</code> starts at <code>0</code> and never decreases.)</p>
<p><a id="ch03sec3lev20"/></p>
<h5><em>Other operations</em></h5>
<p>Since the keys are kept in an ordered array, most of the order-based operations are compact and straightforward, as you can see from the code on page <a href="#ch03sb10">382</a>. For example, a call to <code>select(k)</code> just returns <code>keys[k].</code> We have left <code>delete()</code> and <code>floor()</code> as exercises. You are encouraged to study the implementation of <code>ceiling()</code> and the two-argument <code>keys()</code> and to work these exercises to cement your understanding of the ordered symbol-table API and this implementation.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb09"/></p>
<h3><a id="page_381"/>Algorithm 3.2 (continued) Binary search in an ordered array (iterative)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0381-01.jpg"/></p>
<p>This method uses the classic method described in the text to compute the number of keys in the table that are smaller than <code>key</code>. Compare <code>key</code> with the key in the middle: if it is equal, return its index; if it is less, look in the left half; if it is greater, look in the right half.</p>
<p class="image"><img alt="image" src="graphics/03_06-binarysearch.jpg"/></p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch03sb10"/></p>
<h3><a id="page_382"/>Algorithm 3.2 (continued) Ordered symbol-table operations for binary search</h3>
<p class="image"><img alt="image" src="graphics/p0382-01.jpg"/></p>
<p>These methods, along with the methods of <a href="#ch03qa1q16"><small>EXERCISE 3.1.16</small></a> and <a href="#ch03qa1q17"><small>EXERCISE 3.1.17</small></a>, complete the implementation of our (ordered) symbol-table API using binary search in an ordered array. The <code>min()</code>, <code>max()</code>, and <code>select()</code> methods are trivial, just amounting to returning the appropriate key from its known position in the array. The <code>rank()</code> method, which is the basis of binary search, plays a central role in the others. The <code>floor()</code> and <code>delete()</code> implementations, left for exercises, are more complicated, but still straightforward.</p>
<hr/>
</div>
<p><a id="ch03sec2lev6"/></p>
<h4><a id="page_383"/>Analysis of binary search</h4>
<p>The recursive implementation of <code>rank()</code> also leads to an immediate argument that binary search guarantees fast search, because it corresponds to a recurrence relation that describes an upper bound on the number of compares.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb11"/></p>
<p><strong>Proposition B.</strong> Binary search in an ordered array with <em>N</em> keys uses no more than lg <em>N</em> + 1 compares for a search (successful or unsuccessful).</p>
<p><strong>Proof:</strong> This analysis is similar to (but simpler than) the analysis of mergesort (<a href="ch02.html#ch02sb14"><small>PROPOSITION F</small></a> in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>). Let <em>C</em>(<em>N</em>) be the number of compares needed to search for a key in a symbol table of size <em>N</em>. We have <em>C</em>(0) = 0, <em>C</em>(1) = 1, and for <em>N &gt;</em> 0 we can write a recurrence relationship that directly mirrors the recursive method:</p>
<p class="center"><em>C</em>(<em>N</em>) ≤ <em>C</em>(<img alt="image" src="graphics/lfloor.jpg"/><em>N/</em>2<img alt="image" src="graphics/rfloor.jpg"/>) + 1</p>
<p>Whether the search goes to the left or to the right, the size of the subarray is no more than <img alt="image" src="graphics/lfloor.jpg"/><em>N/</em>2<img alt="image" src="graphics/rfloor.jpg"/>, and we use one compare to check for equality and to choose whether to go left or right. When <em>N</em> is one less than a power of 2 (say <em>N</em> = 2<em><sup>n</sup></em>−1), this recurrence is not difficult to solve. First, since <img alt="image" src="graphics/lfloor.jpg"/><em>N/</em>2<img alt="image" src="graphics/rfloor.jpg"/> = 2<em><sup>n</sup></em><sup>−1</sup>−1, we have</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>−1) ≤ <em>C</em>(2<em><sup>n</sup></em><sup>−1</sup>−1) + 1</p>
<p>Applying the same equation to the first term on the right, we have</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>−1) ≤ <em>C</em>(2<em><sup>n</sup></em><sup>−2</sup>−1) + 1 + 1</p>
<p>Repeating the previous step <em>n</em> − 2 additional times gives</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>−1) ≤ <em>C</em>(2<sup>0</sup>) + <em>n</em></p>
<p>which leaves us with the solution</p>
<p class="center"><em>C</em>(<em>N</em>) = <em>C</em>(2<em><sup>n</sup></em>) ≤ <em>n</em> + 1 &lt; lg <em>N</em> + 1</p>
<p>Exact solutions for general <em>N</em> are more complicated, but it is not difficult to extend this argument to establish the stated property for all values of <em>N</em> (see <a href="#ch03qa1q20"><small>EXERCISE 3.1.20</small></a>). With binary search, we achieve a logarithmic-time search guarantee.</p>
<hr/>
</div>
<p>The implementation just given for <code>ceiling()</code> is based on a single call to <code>rank()</code>, and the default two-argument <code>size()</code> implementation calls <code>rank()</code> twice, so this proof also establishes that these operations (and <code>floor()</code>) are supported in guaranteed logarithmic time (<code>min()</code>, <code>max()</code>, and <code>select()</code> are constant-time operations).</p>
<p><a id="page_384"/>Despite its guaranteed logarithmic search, <code>BinarySearchST</code> still does not enable us to use clients like <code>FrequencyCounter</code> to solve huge problems, because the <code>put()</code> method is too slow. Binary search reduces the number of compares, but not the running time, because its use does not change the fact that the number of array accesses required to build a symbol table in an ordered array is quadratic in the size of the array when keys are randomly ordered (and in typical practical situations where the keys, while not random, are well-described by this model).</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb12"/></p>
<p><strong>Proposition B (continued).</strong> Inserting a new key into an ordered array of size <em>N</em> uses ~ 2<em>N</em> array accesses in the worst case, so inserting <em>N</em> keys into an initially empty table uses ~ <em>N</em><sup>2</sup> array accesses in the worst case.</p>
<p><strong>Proof:</strong> Same as for <a href="#ch03sb06"><small>PROPOSITION A</small></a>.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/t0384-01.jpg"/></p>
<p>For <em>Tale of Two Cities</em>, with over 10<sup>4</sup> distinct keys, the cost to build the table is nearly 10<sup>8</sup> array accesses, and for the Leipzig project, with over 10<sup>6</sup> distinct keys, the cost to build the table is over 10<sup>11</sup> array accesses. While not quite prohibitive on modern computers, these costs are extremely (and unnecessarily) high.</p>
<p>Returning to the cost of the <code>put()</code> operations for <code>FrequencyCounter</code> for words of length 8 or more, we see a reduction in the average cost from 2,246 compares (plus array accesses) per operation for <code>SequentialSearchST</code> to 484 for <code>BinarySearchST</code>. As before, this cost is even better than would be predicted by analysis, and the extra improvement is likely again explained by properties of the application (see <a href="#ch03qa1q36"><small>EXERCISE 3.1.36</small></a>). This improvement is impressive, but we can do much better, as you shall see.</p>
<p class="image"><img alt="image" src="graphics/03_07-freqa.jpg"/></p>
<p><a id="ch03sec2lev7"/></p>
<h4><a id="page_385"/>Preview</h4>
<p>Binary search is typically far better than sequential search and is the method of choice in numerous practical applications. For a static table (with no insert operations allowed), it is worthwhile to initialize and sort the table, as in the version of binary search that we considered in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a> (see page <a href="ch01.html#page_99">99</a>). Even when the bulk of the key-value pairs is known before the bulk of the searches (a common situation in applications), it is worthwhile to add to <code>BinarySearchST</code> a constructor that initializes and sorts the table (see <a href="#ch03qa1q12"><small>EXERCISE 3.1.12</small></a>). Still, binary search is infeasible for use in many other applications. For example, it fails for our Leipzig Corpora application because searches and inserts are intermixed and the table size is too large. As we have emphasized, typical modern search clients require symbol tables that can support fast implementations of <em>both</em> search and insert. That is, we need to be able to build huge tables where we can insert (and perhaps remove) key-value pairs in unpredictable patterns, intermixed with searches.</p>
<p>The table below summarizes performance characteristics for the elementary symbol-table implementations considered in this section. The table entries give the leading term of the cost (number of array accesses for binary search, number of compares for the others), which implies the order of growth of the running time.</p>
<p class="image"><img alt="image" src="graphics/t0385-01.jpg"/></p>
<p>The central question is whether we can devise algorithms and data structures that achieve logarithmic performance for <em>both</em> search and insert. The answer is a resounding <em>yes</em>! Providing that answer is the main thrust of this chapter. Along with the fast sort capability discussed in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>, fast symbol-table search/insert is one of the most important contributions of algorithmics and one of the most important steps toward the development of the rich computational infrastructure that we now enjoy.</p>
<p>How can we achieve this goal? To support efficient insertion, it seems that we need a linked structure. But a singly linked list forecloses the use of binary search, because the efficiency of binary search depends on our ability to get to the middle of any subarray <a id="page_386"/>quickly via indexing (and the only way to get to the middle of a singly linked list is to follow links). To combine the efficiency of binary search with the flexibility of linked structures, we need more complicated data structures. That combination is provided both by <em>binary search trees</em>, the subject of the next two sections, and by <em>hash tables,</em> the subject of <a href="#ch03sec1lev4"><small>SECTION 3.4</small></a>.</p>
<p>We consider six symbol-table implementations in this chapter, so a brief preview is in order. The table below is a list of the data structures, along with the primary reasons in favor of and against using each for an application. They appear in the order in which we consider them.</p>
<p>We will get into more detail on properties of the algorithms and implementations as we discuss them, but the brief characterizations in this table will help you keep them in a broader context as you learn them. The bottom line is that we have several fast symbol-table implementations that can be and are used to great effect in countless applications.</p>
<p class="image"><img alt="image" src="graphics/t0386-01.jpg"/></p>
<p><a id="ch03sec2lev8"/></p>
<h4><a id="page_387"/>Q&amp;A</h4>
<p><strong>Q.</strong> Why not use an <code>Item</code> type that implements <code>Comparable</code> for symbol tables, in the same way as we did for priority queues in <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>, instead of having separate keys and values?</p>
<p><strong>A.</strong> That is also a reasonable option. These two approaches illustrate two different ways to associate information with keys—we can do so <em>implicitly</em> by building a data type that includes the key or <em>explicitly</em> by separating keys from values. For symbol tables, we have chosen to highlight the associative array abstraction. Note also that a client specifies just a key in search, not a key-value aggregation.</p>
<p><strong>Q.</strong> Why bother with <code>equals()</code>? Why not just use <code>compareTo()</code> throughout?</p>
<p><strong>A.</strong> Not all data types lead to key values that are easy to compare, even though having a symbol table still might make sense. To take an extreme example, you may wish to use pictures or songs as keys. There is no natural way to compare them, but we can certainly test equality (with some work).</p>
<p><strong>Q.</strong> Why not allow keys to take the value <code>null</code>?</p>
<p><strong>A.</strong> We assume that <code>Key</code> is an <code>Object</code> because we use it to invoke <code>compareTo()</code> or <code>equals()</code>. But a call like <code>a.compareTo(b)</code> would cause a null pointer exception if <code>a</code> is <code>null</code>. By ruling out this possibility, we allow for simpler client code.</p>
<p><strong>Q.</strong> Why not use a method like the <code>less()</code> method that we used for sorting?</p>
<p><strong>A.</strong> Equality plays a special role in symbol tables, so we also would need a method for testing equality. To avoid proliferation of methods that have essentially the same function, we adopt the built-in Java methods <code>equals()</code> and <code>compareTo()</code>.</p>
<p><strong>Q.</strong> Why not declare <code>key[]</code> as <code>Object[]</code> (instead of <code>Comparable[]</code>) in <code>BinarySearchST</code> before casting, in the same way that <code>val[]</code> is declared as <code>Object</code>?</p>
<p><strong>A.</strong> Good question. If you do so, you will get a <code>ClassCastException</code> because keys need to be <code>Comparable</code> (to ensure that entries in <code>key[]</code> have a <code>compareTo()</code> method). Thus, declaring <code>key[]</code> as <code>Comparable[]</code> is required. Delving into the details of programming-language design to explain the reasons would take us somewhat off topic. We use precisely this idiom (and nothing more complicated) in any code that uses <code>Comparable</code> generics and arrays throughout this book.</p>
<p><a id="page_388"/><strong>Q.</strong> What if we need to associate multiple values with the same key? For example, if we use <code>Date</code> as a key in an application, wouldn’t we have to process equal keys?</p>
<p><strong>A.</strong> Maybe, maybe not. For example, you can’t have two trains arrive at the station on the same track at the same time (but they could arrive on different tracks at the same time). There are two ways to handle the situation: use some other information to disambiguate or make the value a <code>Queue</code> of values having the same key. We consider applications in detail in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>.</p>
<p><strong>Q.</strong> Presorting the table as discussed on page <a href="#page_385">385</a> seems like a good idea. Why relegate that to an exercise (see <a href="#ch03qa1q12"><small>EXERCISE 3.1.12</small></a>)?</p>
<p><strong>A.</strong> Indeed, this may be the method of choice in some applications. But adding a slow insert method to a data structure designed for fast search “for convenience” is a performance trap, because an unsuspecting client might intermix searches and inserts in a huge table and experience quadratic performance. Such traps are all too common, so that “buyer beware” is certainly appropriate when using software developed by others, particularly when interfaces are too wide. This problem becomes acute when a large number of methods are included “for convenience” leaving performance traps throughout, while a client might expect efficient implementations of all methods. Java’s <code>ArrayList</code> class is an example (see <a href="ch03a.html#ch03qa5q27"><small>EXERCISE 3.5.27</small></a>).</p>
<p><a id="ch03sec2lev9"/></p>
<h4><a id="page_389"/>Exercises</h4>
<p><a id="ch03qa1q1"/><strong>3.1.1</strong> Write a client that creates a symbol table mapping letter grades to numerical scores, as in the table below, then reads from standard input a list of letter grades and computes and prints the GPA (the average of the numbers corresponding to the grades).</p>
<p class="image"><img alt="image" src="graphics/t0389-01.jpg"/></p>
<p><a id="ch03qa1q2"/><strong>3.1.2</strong> Develop a symbol-table implementation <code>ArrayST</code> that uses an (unordered) array as the underlying data structure to implement our basic symbol-table API.</p>
<p><a id="ch03qa1q3"/><strong>3.1.3</strong> Develop a symbol-table implementation <code>OrderedSequentialSearchST</code> that uses an ordered linked list as the underlying data structure to implement our ordered symbol-table API.</p>
<p><a id="ch03qa1q4"/><strong>3.1.4</strong> Develop <code>Time</code> and <code>Event</code> ADTs that allow processing of data as in the example illustrated on page <a href="#page_367">367</a>.</p>
<p><a id="ch03qa1q5"/><strong>3.1.5</strong> Implement <code>size()</code>, <code>delete()</code>, and <code>keys()</code> for <code>SequentialSearchST</code>.</p>
<p><a id="ch03qa1q6"/><strong>3.1.6</strong> Give the number of calls to <code>put()</code> and <code>get()</code> issued by <code>FrequencyCounter</code>, as a function of the number <em>W</em> of words and the number <em>D</em> of distinct words in the input.</p>
<p><a id="ch03qa1q7"/><strong>3.1.7</strong> What is the average number of distinct keys that <code>FrequencyCounter</code> will find among <em>N</em> random nonnegative integers less than 1,000, for <em>N</em>=10, 10<sup>2</sup>, 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>?</p>
<p><a id="ch03qa1q8"/><strong>3.1.8</strong> What is the most frequently used word of ten letters or more in <em>Tale of Two Cities</em>?</p>
<p><a id="ch03qa1q9"/><strong>3.1.9</strong> Add code to <code>FrequencyCounter</code> to keep track of the <em>last</em> call to <code>put()</code>. Print the last word inserted and the number of words that were processed in the input stream prior to this insertion. Run your program for <code>tale.txt</code> with length cutoffs 1, 8, and 10.</p>
<p><a id="ch03qa1q10"/><strong>3.1.10</strong> Give a trace of the process of inserting the keys <code>E A S Y Q U E S T I O N</code> into an initially empty table using <code>SequentialSearchST</code>. How many compares are involved?</p>
<p><a id="ch03qa1q11"/><strong>3.1.11</strong> Give a trace of the process of inserting the keys <code>E A S Y Q U E S T I O N</code> into an initially empty table using <code>BinarySearchST</code>. How many compares are involved?</p>
<p><a id="ch03qa1q12"/><strong>3.1.12</strong> Modify <code>BinarySearchST</code> to maintain one array of <code>Item</code> objects that contain keys and values, rather than two parallel arrays. Add a constructor that takes an array of <a id="page_390"/><code>Item</code> values as argument and uses mergesort to sort the array.</p>
<p><a id="ch03qa1q13"/><strong>3.1.13</strong> Which of the symbol-table implementations in this section would you use for an application that does 10<sup>3</sup> <code>put()</code> operations and 10<sup>6</sup> <code>get()</code> operations, randomly intermixed? Justify your answer.</p>
<p><a id="ch03qa1q14"/><strong>3.1.14</strong> Which of the symbol-table implementations in this section would you use for an application that does 10<sup>6</sup> <code>put()</code> operations and 10<sup>3</sup> <code>get()</code> operations, randomly intermixed? Justify your answer.</p>
<p><a id="ch03qa1q15"/><strong>3.1.15</strong> Assume that searches are 1,000 times more frequent than insertions for a <code>BinarySearchST</code> client. Estimate the percentage of the total time that is devoted to insertions, when the number of searches is 10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>.</p>
<p><a id="ch03qa1q16"/><strong>3.1.16</strong> Implement the <code>delete()</code> method for <code>BinarySearchST</code>.</p>
<p><a id="ch03qa1q17"/><strong>3.1.17</strong> Implement the <code>floor()</code> method for <code>BinarySearchST</code>.</p>
<p><a id="ch03qa1q18"/><strong>3.1.18</strong> Prove that the <code>rank()</code> method in <code>BinarySearchST</code> is correct.</p>
<p><a id="ch03qa1q19"/><strong>3.1.19</strong> Modify <code>FrequencyCounter</code> to print all of the values having the highest frequency of occurrence, not just one of them. <em>Hint</em>: Use a <code>Queue</code>.</p>
<p><a id="ch03qa1q20"/><strong>3.1.20</strong> Complete the proof of <a href="#ch03sb11"><small>PROPOSITION B</small></a> (show that it holds for all values of <em>N</em>). <em>Hint</em>: Start by showing that <em>C</em>(<em>N</em>) is monotonic: <em>C</em>(<em>N</em>) ≤ <em>C</em>(<em>N</em>+1) for all <em>N</em> &gt; 0.</p>
<p><a id="ch03sec2lev10"/></p>
<h4><a id="page_391"/>Creative Problems</h4>
<p><a id="ch03qa1q21"/><strong>3.1.21</strong> <em>Memory usage.</em> Compare the memory usage of <code>BinarySearchST</code> with that of <code>SequentialSearchST</code> for <em>N</em> key-value pairs, under the assumptions described in <a href="ch01a.html#ch01sec1lev6"><small>SECTION 1.4</small></a>. Do not count the memory for the keys and values themselves, but do count references to them. For <code>BinarySearchST</code>, assume that array resizing is used, so that the array is between 25 percent and 100 percent full.</p>
<p><a id="ch03qa1q22"/><strong>3.1.22</strong> <em>Self-organizing search.</em> A self-organizing search algorithm is one that rearranges items to make those that are accessed frequently likely to be found early in the search. Modify your search implementation for <a href="#ch03qa1q2"><small>EXERCISE 3.1.2</small></a> to perform the following action on every search hit: move the key-value pair found to the beginning of the list, moving all pairs between the beginning of the list and the vacated position to the right one position. This procedure is called the <em>move-to-front</em> heuristic.</p>
<p><a id="ch03qa1q23"/><strong>3.1.23</strong> <em>Analysis of binary search.</em> Prove that the maximum number of compares used for a binary search in a table of size <em>N</em> is precisely the number of bits in the binary representation of <em>N</em>, because the operation of shifting 1 bit to the right converts the binary representation of <em>N</em> into the binary representation of <img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/>.</p>
<p><a id="ch03qa1q24"/><strong>3.1.24</strong> <em>Interpolation search.</em> Suppose that arithmetic operations are allowed on keys (for example, they may be <code>Double</code> or <code>Integer</code> values). Write a version of binary search that mimics the process of looking near the beginning of a dictionary when the word begins with a letter near the beginning of the alphabet. Specifically, if <em>k<sub>x</sub></em> is the key value sought, <em>k<sub>lo</sub></em> is the key value of the first key in the table, and <em>k<sub>hi</sub></em> is the key value of the last key in the table, look first <img alt="image" src="graphics/lfloor.jpg"/>(<em>k<sub>x</sub></em> − <em>k<sub>lo</sub></em>)/(<em>k<sub>hi</sub></em> − <em>k<sub>lo</sub></em>)<img alt="image" src="graphics/rfloor.jpg"/>-way through the table, not half-way. Test your implementation against <code>BinarySearchST</code> for <code>FrequencyCounter</code> using <code>SearchCompare</code>.</p>
<p><a id="ch03qa1q25"/><strong>3.1.25</strong> <em>Software caching.</em> Since the default implementation of <code>contains()</code> calls <code>get()</code>, the inner loop of <code>FrequencyCounter</code></p>
<p class="programlisting"><img alt="image" src="graphics/p0391-01.jpg"/></p>
<p>leads to two or three searches for the same key. To enable clear client code like this without sacrificing efficiency, we can use a technique known as <em>software caching</em>, where we save the location of the most recently accessed key in an instance variable. Modify <code>SequentialSearchST</code> and <code>BinarySearchST</code> to take advantage of this idea.</p>
<p><a id="page_392"/><a id="ch03qa1q26"/><strong>3.1.26</strong> <em>Frequency count from a dictionary.</em> Modify <code>FrequencyCounter</code> to take the name of a dictionary file as its argument, count frequencies of the words from standard input that are also in that file, and print two tables of the words with their frequencies, one sorted by frequency, the other sorted in the order found in the dictionary file.</p>
<p><a id="ch03qa1q27"/><strong>3.1.27</strong> <em>Small tables.</em> Suppose that a <code>BinarySearchST</code> client has <em>S</em> search operations and <em>N</em> distinct keys. Give the order of growth of <em>S</em> such that the cost of building the table is the same as the cost of all the searches.</p>
<p><a id="ch03qa1q28"/><strong>3.1.28</strong> <em>Ordered insertions.</em> Modify <code>BinarySearchST</code> so that inserting a key that is larger than all keys in the table takes constant time (so that building a table by calling <code>put()</code> for keys that are in order takes linear time).</p>
<p><a id="ch03qa1q29"/><strong>3.1.29</strong> <em>Test client.</em> Write a test client for <code>BinarySearchST</code> that tests the implementations of <code>min()</code>, <code>max()</code>, <code>floor()</code>, <code>ceiling()</code>, <code>select()</code>, <code>rank()</code>, <code>deleteMin()</code>, <code>deleteMax()</code>, and <code>keys()</code> that are given in the text. Start with the standard indexing client given on page <a href="#ch03sec3lev17">370</a>. Add code to take additional command-line arguments, as appropriate.</p>
<p><a id="ch03qa1q30"/><strong>3.1.30</strong> <em>Certification.</em> Add assert statements to <code>BinarySearchST</code> to check algorithm invariants and data structure integrity after every insertion and deletion. For example, every index <code>i</code> should always be equal to <code>rank(select(i))</code> and the array should always be in order.</p>
<p><a id="ch03sec2lev11"/></p>
<h4><a id="page_393"/>Experiments</h4>
<p><a id="ch03qa1q31"/><strong>3.1.31</strong> <em>Performance driver.</em> Write a performance driver program that uses <code>put()</code> to fill a symbol table, then uses <code>get()</code> such that each key in the table is hit an average of ten times and there is about the same number of misses, doing so multiple times on random sequences of string keys of various lengths ranging from 2 to 50 characters; measures the time taken for each run; and prints out or plots the average running times.</p>
<p><a id="ch03qa1q32"/><strong>3.1.32</strong> <em>Exercise driver.</em> Write an exercise driver program that uses the methods in our ordered symbol-table API on difficult or pathological cases that might turn up in practical applications. Simple examples include key sequences that are already in order, key sequences in reverse order, key sequences where all keys are the same, and keys consisting of only two distinct values.</p>
<p><a id="ch03qa1q33"/><strong>3.1.33</strong> <em>Driver for self-organizing search.</em> Write a driver program for self-organizing search implementations (see <a href="#ch03qa1q22"><small>EXERCISE 3.1.22</small></a>) that uses <code>get()</code> to fill a symbol table with <em>N</em> keys, then does 10 <em>N</em> successful searches according to a predefined probability distribution. Use this driver to compare the running time of your implementation from <a href="#ch03qa1q22"><small>EXERCISE 3.1.22</small></a> with <code>BinarySearchST</code> for <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup> using the probability distribution where search hits the <em>i</em>th smallest key with probability 1/2<em><sup>i</sup></em>.</p>
<p><a id="ch03qa1q34"/><strong>3.1.34</strong> <em>Zipf’s law.</em> Do the previous exercise for the probability distribution where search hits the <em>i</em>th smallest key with probability 1/(<em>i</em>H<em><sub>N</sub></em>) where H<em><sub>N</sub></em> is a Harmonic number (see page <a href="ch01a.html#page_185">185</a>). This distribution is called <em>Zipf’s law</em>. Compare the move-to-front heuristic with the optimal arrangement for the distributions in the previous exercise, which is to keep the keys in increasing order (decreasing order of their expected frequency).</p>
<p><a id="ch03qa1q35"/><strong>3.1.35</strong> <em>Performance validation I.</em> Run doubling tests that use the first <code>N</code> words of <em>Tale of Two Cities</em> for various values of <code>N</code> to test the hypothesis that the running time of <code>FrequencyCounter</code> is quadratic when it uses <code>SequentialSearchST</code> for its symbol table.</p>
<p><a id="ch03qa1q36"/><strong>3.1.36</strong> <em>Performance validation II.</em> Explain why the performance of <code>BinarySearchST</code> and <code>SequentialSearchST</code> for <code>FrequencyCounter</code> is even better than predicted by analysis.</p>
<p><a id="ch03qa1q37"/><strong>3.1.37</strong> <em>Put/get ratio.</em> Determine empirically the ratio of the amount of time that <code>BinarySearchST</code> spends on <code>put()</code> operations to the time that it spends on <code>get()</code> operations when <code>FrequencyCounter</code> is used to find the frequency of occurrence of values <a id="page_394"/>in 1 million random <em>M</em>-bit <code>int</code> values, for <em>M</em> = 10, 20, and 30. Answer the same question for <code>tale.txt</code> and compare the results.</p>
<p><a id="ch03qa1q38"/><strong>3.1.38</strong> <em>Amortized cost plots.</em> Develop instrumentation for <code>FrequencyCounter</code>, <code>SequentialSearchST</code>, and <code>BinarySearchST</code> so that you can produce plots like the ones in this section showing the cost of each <code>put()</code> operation during the computation.</p>
<p><a id="ch03qa1q39"/><strong>3.1.39</strong> <em>Actual timings.</em> Instrument <code>FrequencyCounter</code> to use <code>Stopwatch</code> and <code>StdDraw</code> to make a plot where the <em>x</em>-axis is the number of calls on <code>get()</code> or <code>put()</code> and the <em>y</em>-axis is the total running time, with a point plotted of the cumulative time after each call. Run your program for <em>Tale of Two Cities</em> using <code>SequentialSearchST</code> and again using <code>BinarySearchST</code> and discuss the results. <em>Note</em>: Sharp jumps in the curve may be explained by <em>caching</em>, which is beyond the scope of this question.</p>
<p><a id="ch03qa1q40"/><strong>3.1.40</strong> <em>Crossover to binary search.</em> Find the values of <em>N</em> for which binary search in a symbol table of size <em>N</em> becomes 10, 100, and 1,000 times faster than sequential search. Predict the values with analysis and verify them experimentally.</p>
<p><a id="ch03qa1q41"/><strong>3.1.41</strong> <em>Crossover to interpolation search.</em> Find the values of <em>N</em> for which interpolation search in a symbol table of size <em>N</em> becomes 1, 2, and 10 times faster than binary search, assuming the keys to be random 32-bit integers (see <a href="#ch03qa1q24"><small>EXERCISE 3.1.24</small></a>). Predict the values with analysis, and verify them experimentally.</p>
<p><a id="ch03sec1lev2"/></p>
<h3><a id="page_396"/>3.2 Binary Search Trees</h3>
<p><small>IN THIS SECTION</small>, we will examine a symbol-table implementation that combines the flexibility of insertion in a linked list with the efficiency of search in an ordered array. Specifically, using <em>two</em> links per node (instead of the one link per node found in linked lists) leads to an efficient symbol-table implementation based on the binary search tree data structure, which qualifies as one of the most fundamental algorithms in computer science.</p>
<p class="image"><img alt="image" src="graphics/03_08-btanatomy.jpg"/></p>
<p>To begin, we define basic terminology. We are working with data structures made up of <em>nodes</em> that contain <em>links</em> that are either <em>null</em> or references to other nodes. In a <em>binary tree</em>, we have the restriction that every node is pointed to by just one other node, which is called its <em>parent</em> (except for one node, the <em>root</em>, which has no nodes pointing to it), and that each node has exactly two links, which are called its <em>left</em> and <em>right</em> links, that point to nodes called its <em>left child</em> and <em>right child</em>, respectively. Although links point to nodes, we can view each link as pointing to a binary tree, the tree whose root is the referenced node. Thus, we can define a binary tree as a either a null link or a node with a left link and a right link, each references to (disjoint) <em>subtrees</em> that are themselves binary trees. In a <em>binary search tree</em>, each node also has a key and a value, with an ordering restriction to support efficient search.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb13"/></p>
<p><strong>Definition.</strong> A <em>binary search tree</em> (BST) is a binary tree where each node has a <code>Comparable</code> key (and an associated value) and satisfies the restriction that the key in any node is larger than the keys in all nodes in that node’s left subtree and smaller than the keys in all nodes in that node’s right subtree.</p>
<hr/>
</div>
<p>We draw BSTs with keys in the nodes and use terminology such as “<code>A</code> is the left child of <code>E</code>” that associates nodes with keys. Lines connecting the nodes represent links, and we give the value associated with a key in black, beside the nodes (suppressing the value as dictated by context). Each node’s links connect it to nodes below it on the page, except for null links, which are short segments at the bottom. As usual, our examples use the single-letter keys that are generated by our indexing test client.</p>
<p class="image"><img alt="image" src="graphics/03_09-bstanatomy.jpg"/></p>
<p><a id="ch03sec2lev12"/></p>
<h4><a id="page_397"/>Basic implementation</h4>
<p><a href="#ch03sb14"><small>ALGORITHM 3.3</small></a> defines the BST data structure that we use throughout this section to implement the ordered symbol-table API. We begin by considering this classic data structure definition and the characteristic associated implementations of the <code>get()</code> (search) and <code>put()</code> (insert) methods.</p>
<p><a id="ch03sec3lev21"/></p>
<h5><em>Representation</em></h5>
<p>We define a private nested class to define nodes in BSTs, just as we did for linked lists. Each node contains a key, a value, a left link, a right link, and a node count (when relevant, we include node counts in red above the node in our drawings). The left link points to a BST for items with smaller keys, and the right link points to a BST for items with larger keys. The instance variable <code>N</code> gives the node count in the subtree rooted at the node. This field facilitates the implementation of various ordered symbol-table operations, as you will see. The private <code>size()</code> method in <a href="#ch03sb14"><small>ALGORITHM 3.3</small></a> is implemented to assign the value 0 to null links, so that we can maintain this field by making sure that the invariant</p>
<p class="programlisting">size(x) = size(x.left) + size(x.right) + 1</p>
<p class="image"><img alt="image" src="graphics/03_10-bsttwosame.jpg"/></p>
<p>holds for every node <code>x</code> in the tree.</p>
<p>A BST represents a <em>set</em> of keys (and associated values), and there are many different BSTs that represent the same set. If we project the keys in a BST such that all keys in each node’s left subtree appear to the left of the key in the node and all keys in each node’s right subtree appear to the right of the key in the node, then we always get the keys in sorted order. We take advantage of the flexibility inherent in having many BSTs represent this sorted order to develop efficient algorithms for building and using BSTs.</p>
<p><a id="ch03sec3lev22"/></p>
<h5><em>Search</em></h5>
<p>As usual, when we search for a key in a symbol table, we have one of two possible outcomes. If a node containing the key is in the table, we have a <em>search hit</em>, so we return the associated value. Otherwise, we have a <em>search miss</em> (and return <code>null</code>). A recursive algorithm to search for a key in a BST follows immediately from the recursive structure: if the tree is empty, we have a search miss; if the search key is equal to the key at the root, we have a search hit. Otherwise, we search (recursively) in the appropriate subtree, moving left if the search key is smaller, right if it is larger. The recursive <code>get()</code> method on page <a href="#ch03sb15">399</a> implements this algorithm directly. It takes a node (root of a subtree) as first argument and a key as second argument, starting with the root of the tree and the search key. The code maintains the invariant that no parts of the tree other than the subtree rooted at the current node can have a node whose key is equal to the search key. Just as the size of the interval in binary search shrinks by about half on each iteration, <a id="page_400"/>the size of the subtree rooted at the current node when searching in a BST shrinks when we go down the tree (by about half, ideally, but at least by one). The procedure stops either when a node containing the search key is found (search hit) or when the current subtree becomes empty (search miss). Starting at the top, the search procedure at each node involves a recursive invocation for one of that node’s children, so the search defines a path through the tree. For a search hit, the path terminates at the node containing the key. For a search miss, the path terminates at a null link.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb14"/></p>
<h3><a id="page_398"/>Algorithm 3.3 Binary search tree symbol table</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0398-01.jpg"/></p>
<p>This implementation of the ordered symbol-table API uses a binary search tree built from <code>Node</code> objects that each contain a key, associated value, two links, and a node count <code>N</code>. Each <code>Node</code> is the root of a subtree containing <code>N</code> nodes, with its left link pointing to a <code>Node</code> that is the root of a subtree with smaller keys and its right link pointing to a <code>Node</code> that is the root of a subtree with larger keys. The instance variable <code>root</code> points to the <code>Node</code> at the root of the BST (which has all the keys and associated values in the symbol table). Implementations of other methods appear throughout this section.</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch03sb15"/></p>
<h3><a id="page_399"/>Algorithm 3.3 (continued) Search and insert for BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0399-01.jpg"/></p>
<p>These implementations of <code>get()</code> and <code>put()</code> for the symbol-table API are characteristic recursive BST methods that also serve as models for several other implementations that we consider later in the chapter. Each method can be understood as both working code and a proof by induction of the inductive hypothesis in the opening comment.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/03_11-bstsearch.jpg"/></p>
<p><a id="ch03sec3lev23"/></p>
<h5><em>Insert</em></h5>
<p>The search code in <a href="#ch03sb14"><small>ALGORITHM 3.3</small></a> is almost as simple as binary search; that simplicity is an essential feature of BSTs. A more important essential feature of BSTs is that <em>insert</em> is not much more difficult to implement than <em>search</em>. Indeed, a search for a key not in the tree ends at a null link, and all that we need to do is replace that link with a new node containing the key (see the diagram on the next page). The recursive <code>put()</code> method in <a href="#ch03sb14"><small>ALGORITHM 3.3</small></a> accomplishes this task using logic similar to that we used for the recursive search: if the tree is empty, we return a new node containing the key and value; if the search key is less than the key at the root, we set the left link to the result of inserting the key into the left subtree; otherwise, we set the right link to the result of inserting the key into the right subtree.</p>
<p><a id="ch03sec3lev24"/></p>
<h5><a id="page_401"/><em>Recursion</em></h5>
<p>It is worthwhile to take the time to understand the dynamics of these recursive implementations. You can think of the code <em>before</em> the recursive calls as happening on the way <em>down</em> the tree: it compares the given key against the key at each node and moves right or left accordingly. Then, think of the code <em>after</em> the recursive calls as happening on the way <em>up</em> the tree. For <code>get()</code> this amounts to a series of return statements, but for <code>put()</code>, it corresponds to resetting the link of each parent to its child on the search path and to incrementing the counts on the way up the path. In simple BSTs, the only new link is the one at the bottom, but resetting the links higher up on the path is as easy as the test to avoid setting them. Also, we just need to increment the node count on each node on the path, but we use more general code that sets each to one plus the sum of the counts in its subtrees. Later in this section and in the next section, we shall study more advanced algorithms that are naturally expressed with this same recursive scheme but that can change more links on the search paths and need the more general node-count-update code. Elementary BSTs are often implemented with nonrecursive code (see <a href="#ch03qa2q12"><small>EXERCISE 3.2.12</small></a>)—we use recursion in our implementations both to make it easy for you to convince yourself that the code is operating as described and to prepare the groundwork for more sophisticated algorithms.</p>
<p class="image"><img alt="image" src="graphics/03_12-bstinsert.jpg"/></p>
<p><small>A CAREFUL STUDY</small> of the trace for our standard indexing client that is shown on the next page will give you a feeling for the way in which binary search trees grow. New nodes are attached to null links at the bottom of the tree; the tree structure is not otherwise changed. For example, the root has the first key inserted, one of the children of the root has the second key inserted, and so forth. Because each node has two links, the tree tends to grow out, rather than just down. Moreover, only the keys on the path from the root to the sought or inserted key are examined, so the number of keys examined becomes a smaller and smaller fraction of the number of keys in the tree as the tree size increases.</p>
<p class="image"><a id="page_402"/><img alt="image" src="graphics/03_13-bsttrace.jpg"/></p>
<p><a id="ch03sec2lev13"/></p>
<h4><a id="page_403"/>Analysis</h4>
<p>The running times of algorithms on binary search trees depend on the shapes of the trees, which, in turn, depend on the order in which keys are inserted. In the best case, a tree with <em>N</em> nodes could be perfectly balanced, with <em>~</em> lg <em>N</em> nodes between the root and each null link. In the worst case there could be <em>N</em> nodes on the search path. The balance in typical trees turns out to be much closer to the best case than the worst case.</p>
<p class="image"><img alt="image" src="graphics/03_17-bstseven.jpg"/></p>
<p>For many applications, the following simple model is reasonable: We assume that the keys are (uniformly) <em>random</em>, or, equivalently, that they are inserted in random order. Analysis of this model stems from the observation that BSTs are dual to quicksort. The node at the root of the tree corresponds to the first partitioning item in quicksort (no keys to the left are larger, and no keys to the right are smaller) and the subtrees are built recursively, corresponding to quicksort’s recursive subarray sorts. This observation leads us to the analysis of properties of the trees.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb16"/></p>
<p><strong>Proposition C.</strong> Search hits in a BST built from <em>N</em> random keys require <em>~</em> 2 ln <em>N</em> (about 1.39 lg <em>N)</em> compares, on the average.</p>
<p><strong>Proof:</strong> The number of compares used for a search hit ending at a given node is 1 plus the depth. Adding the depths of all nodes, we get a quantity known as the <em>internal path length</em> of the tree. Thus, the desired quantity is 1 plus the average internal path length of the BST, which we can analyze with the same argument that we used for <a href="ch02.html#ch02sb23"><small>PROPOSITION K</small></a> in <a href="ch02.html#ch02sec1lev3"><small>SECTION 2.3</small></a>: Let <em>C<sub>N</sub></em> be the total internal path length of a BST built from inserting <em>N</em> randomly ordered distinct keys, so that the average cost of a search hit is 1 +<em>C<sub>N</sub> / N</em>. We have <em>C</em><sub>0</sub> = <em>C</em><sub>1</sub> = 0 and for <em>N &gt;</em> 1 we can write a recurrence relationship that directly mirrors the recursive BST structure:</p>
<p class="center"><em>C<sub>N</sub></em> = <em>N</em> − 1 + (<em>C</em><sub>0</sub> + <em>C<sub>N</sub></em><sub>−1</sub>) / <em>N</em> + (<em>C</em><sub>1</sub> + <em>C<sub>N</sub></em><sub>−2</sub>)/<em>N</em> + . . . (<em>C<sub>N</sub></em><sub>−1</sub> + <em>C</em><sub>0</sub>)/<em>N</em></p>
<p>The <em>N</em> − 1 term takes into account that the root contributes 1 to the path length of each of the other <em>N</em> − 1 nodes in the tree; the rest of the expression accounts for the subtrees, which are equally likely to be any of the <em>N</em> sizes. After rearranging terms, this recurrence is nearly identical to the one that we solved in <a href="ch02.html#ch02sec1lev3"><small>SECTION 2.3</small></a> for quicksort, and we can derive the approximation <em>C<sub>N</sub></em> ~ 2<em>N</em> ln <em>N</em>.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb17"/></p>
<p><a id="page_404"/><strong>Proposition D.</strong> Insertions and search misses in a BST built from <em>N</em> random keys require <em>~</em> 2 ln <em>N</em> (about 1.39 lg <em>N)</em> compares, on the average.</p>
<p><strong>Proof:</strong> Insertions and search misses take one more compare, on the average, than search hits. This fact is not difficult to establish by induction (see <a href="#ch03qa2q16"><small>EXERCISE 3.2.16</small></a>).</p>
<hr/>
</div>
<p><a href="#ch03sb16"><small>PROPOSITION C</small></a> says that we should expect the BST search cost for random keys to be about 39 percent higher than that for binary search. <a href="#ch03sb17"><small>PROPOSITION D</small></a> says that the extra cost is well worthwhile, because the cost of inserting a new key is also expected to be logarithmic—flexibility not available with binary search in an ordered array, where the number of array accesses required for an insertion is typically linear. As with quicksort, the standard deviation of the number of compares is known to be low, so that these formulas become increasingly accurate as <em>N</em> increases.</p>
<p><a id="ch03sec3lev25"/></p>
<h5><em>Experiments</em></h5>
<p>How well does our random-key model match what is found in typical symbol-table clients? As usual, this question has to be studied carefully for particular practical applications, because of the large potential variation in performance. Fortunately, for many clients, the model is quite good for BSTs.</p>
<p>For our example study of the cost of the <code>put()</code> operations for <code>FrequencyCounter</code> for words of length 8 or more, we see a reduction in the average cost from 484 array accesses or compares per operation for <code>BinarySearchST</code> to 13 for <code>BST</code>, again providing a quick validation of the logarithmic performance predicted by the theoretical model. More extensive experiments for larger inputs are illustrated in the table on the next page. On the basis of <a href="#ch03sb16"><small>PROPOSITIONS C</small></a> and <a href="#ch03sb17"><small>D</small></a>, it is reasonable to predict that this number should be about twice the natural logarithm of the table size, because the preponderance of operations are searches in a nearly full table. This prediction has at least the following inherent inaccuracies:</p>
<p class="indenthangingB">• Many operations are for smaller tables.</p>
<p class="indenthangingB">• The keys are not random.</p>
<p class="indenthangingB">• The table size may be too small for the approximation 2 ln <em>N</em> to be accurate.</p>
<p>Nevertheless, as you can see in the table, this prediction is accurate for our <code>FrequencyCounter</code> test cases to within a few compares. Actually, most of the difference can be explained by refining the mathematics in the approximation (see <a href="#ch03qa2q35"><small>EXERCISE 3.2.35</small></a>).</p>
<p class="image"><a id="page_405"/><img alt="image" src="graphics/03_18-bst150.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_19-freqb.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0405-01.jpg"/></p>
<p><a id="ch03sec2lev14"/></p>
<h4><a id="page_406"/>Order-based methods and deletion</h4>
<p>An important reason that BSTs are widely used is that they allow us to <em>keep the keys in order</em>. As such, they can serve as the basis for implementing the numerous methods in our ordered symbol-table API (see page <a href="#ch03sec2lev2">366</a>) that allow clients to access key-value pairs not just by providing the key, but also by relative key order. Next, we consider implementations of the various methods in our ordered symbol-table API.</p>
<p><a id="ch03sec3lev26"/></p>
<h5><em>Minimum and maximum</em></h5>
<p>If the left link of the root is null, the smallest key in a BST is the key at the root; if the left link is not null, the smallest key in the BST is the smallest key in the subtree rooted at the node referenced by the left link. This statement is both a description of the recursive <code>min()</code> method on page <a href="#ch03sb18">407</a> and an inductive proof that it finds the smallest key in the BST. The computation is equivalent to a simple iteration (move left until finding a null link), but we use recursion for consistency. We might have the recursive method return a <code>Key</code> instead of a <code>Node</code>, but we will later have a need to use this method to access the <code>Node</code> containing the minimum key. Finding the maximum key is similar, moving to the right instead of to the left.</p>
<p><a id="ch03sec3lev27"/></p>
<h5><em>Floor and ceiling</em></h5>
<p>If a given key <code>key</code> is <em>less than</em> the key at the root of a BST, then the floor of <code>key</code> (the largest key in the BST less than or equal to <code>key</code>) <em>must</em> be in the left subtree. If <code>key</code> is <em>greater than</em> the key at the root, then the floor of <code>key</code> <em>could</em> be in the right subtree, but only if there is a key smaller than or equal to <code>key</code> in the right subtree; if not (or if <code>key</code> is equal to the key at the root), then the key at the root is the floor of <code>key</code>. Again, this description serves both as the basis for the recursive <code>floor()</code> method and for an inductive proof that it computes the desired result. Interchanging right and left (and <em>less</em> and <em>greater</em>) gives <code>ceiling()</code>.</p>
<p class="image"><img alt="image" src="graphics/03_20-bstfloor.jpg"/></p>
<p><a id="ch03sec3lev28"/></p>
<h5><em>Selection</em></h5>
<p>Selection in a BST works in a manner analogous to the partition-based method of selection in an array that we studied in <a href="ch02.html#ch02sec1lev5"><small>SECTION 2.5</small></a>. We maintain in BST nodes the variable <code>N</code> that counts the number of keys in the subtree rooted at that node precisely to support this operation. <a id="page_408"/>Suppose that we seek the key of rank <em>k</em> (the key such that precisely <em>k</em> other keys in the BST are smaller). If the number of keys <em>t</em> in the left subtree is larger than <em>k</em>, we look (recursively) for the key of rank <em>k</em> in the left subtree; if <em>t</em> is equal to <em>k</em>, we return the key at the root; and if <em>t</em> is smaller than <em>k</em>, we look (recursively) for the key of rank <em>k</em> − <em>t</em> − 1 in the right subtree. As usual, this description serves both as the basis for the recursive <code>select()</code> method on the facing page and for a proof by induction that it works as expected.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb18"/></p>
<h3><a id="page_407"/>Algorithm 3.3 (continued) Min, max, floor, and ceiling in BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0407-01.jpg"/></p>
<p>Each client method calls a corresponding private method that takes an additional link (to a <code>Node</code>) as argument and returns <code>null</code> or a <code>Node</code> containing the desired <code>Key</code> via the recursive procedure described in the text. The <code>max()</code> and <code>ceiling()</code> methods are the same as <code>min()</code> and <code>floor()</code> (respectively) with right and left (and <code>&lt;</code> and <code>&gt;</code>) interchanged.</p>
<hr/>
</div>
<p><a id="ch03sec3lev29"/></p>
<h5><em>Rank</em></h5>
<p>The inverse method <code>rank()</code> that returns the rank of a given key is similar: if the given key is equal to the key at the root, we return the number of keys <em>t</em> in the left subtree; if the given key is less than the key at the root, we return the rank of the key in the left subtree (recursively computed); and if the given key is larger than the key at the root, we return <em>t</em> plus one (to count the key at the root) plus the rank of the key in the right subtree (recursively computed).</p>
<p class="image"><img alt="image" src="graphics/03_22-bstselect.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_21-bstdeletemin.jpg"/></p>
<p><a id="ch03sec3lev30"/></p>
<h5><em>Delete the minimum/maximum</em></h5>
<p>The most difficult BST operation to implement is the <code>delete()</code> method that removes a key-value pair from the symbol table. As a warmup, consider <code>deleteMin()</code> (remove the key-value pair with the smallest key). As with <code>put()</code> we write a recursive method that takes a link to a <code>Node</code> as argument and returns a link to a <code>Node</code>, so that we can reflect changes to the tree by assigning the result to the link used as argument. For <code>deleteMin()</code> we go left until finding a <code>Node</code> that has a null left link and then replace the link to that node by its right link (simply by returning the right link in the recursive method). The deleted node, with no link now pointing to it, is <a id="page_410"/>available for garbage collection. Our standard recursive setup accomplishes, after the deletion, the task of setting the appropriate link in the parent and updating the counts in all nodes in the path to the root. The symmetric method works for <code>deleteMax()</code>.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb19"/></p>
<h3><a id="page_409"/>Algorithm 3.3 (continued) Selection and rank in BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0409-01.jpg"/></p>
<p>This code uses the same recursive scheme that we have been using throughout this chapter to implement the <code>select()</code> and <code>rank()</code> methods. It depends on using the private <code>size()</code> method given at the beginning of this section that gives the number of subtrees rooted at each node.</p>
<hr/>
</div>
<p><a id="ch03sec3lev31"/></p>
<h5><em>Delete</em></h5>
<p>We can proceed in a similar manner to delete any node that has one child (or no children), but what can we do to delete a node that has two children? We are left with two links, but have a place in the parent node for only one of them. An answer to this dilemma, first proposed by T. Hibbard in 1962, is to delete a node <code>x</code> by replacing it with its <em>successor</em>. Because <code>x</code> has a right child, its successor is the node with the smallest key in its right subtree. The replacement preserves order in the tree because there are no keys between <code>x.key</code> and the successor’s key. We can accomplish the task of replacing <code>x</code> by its successor in four (!) easy steps:</p>
<p class="indenthangingB">• Save a link to the node to be deleted in <code>t</code>.</p>
<p class="indenthangingB">• Set <code>x</code> to point to its successor <code>min(t.right)</code>.</p>
<p class="indenthangingB">• Set the right link of <code>x</code> (which is supposed to point to the BST containing all the keys larger than <code>x.key</code>) to <code>deleteMin(t.right)</code>, the link to the BST containing all the keys that are larger than <code>x.key</code> after the deletion.</p>
<p class="indenthangingB">• Set the left link of <code>x</code> (which was null) to <code>t.left</code> (all the keys that are less than both the deleted key and its successor).</p>
<p class="image"><img alt="image" src="graphics/03_23-bstdelete.jpg"/></p>
<p>Our standard recursive setup accomplishes, after the recursive calls, the task of setting the appropriate link in the parent and decrementing the node counts in the nodes on the path to the root (again, we accomplish the task of updating the counts by setting the counts in each node on the search path to be one plus the sum of the counts in its children). While this method does the job, it has a flaw that might cause performance problems in some practical situations. The problem is that the choice of using the successor is arbitrary and not symmetric. Why not use the predecessor? In practice, it is worthwhile to choose at random between the predecessor and the successor. See <a href="#ch03qa2q42"><small>EXERCISE 3.2.42</small></a> for details.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb20"/></p>
<h3><a id="page_411"/>Algorithm 3.3 (continued) Deletion in BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0411-01.jpg"/></p>
<p>These methods implement eager Hibbard deletion in BSTs, as described in the text on the facing page. The <code>delete()</code> code is compact, but tricky. Perhaps the best way to understand it is to read the description at left, try to write the code yourself on the basis of the description, then compare your code with this code. This method is typically effective, but performance in large-scale applications can become a bit problematic (see <a href="#ch03qa2q42"><small>EXERCISE 3.2.42</small></a>). The <code>deleteMax()</code> method is the same as <code>deleteMin()</code> with right and left interchanged.</p>
<hr/>
</div>
<p><a id="ch03sec3lev32"/></p>
<h5><a id="page_412"/><em>Range queries</em></h5>
<p>To implement the <code>keys()</code> method that returns the keys in a given range, we begin with a basic recursive BST traversal method, known as <em>inorder traversal</em>. To illustrate the method, we consider the task of printing all the keys in a BST in order. To do so, print all the keys in the left subtree (which are less than the key at the root by definition of BSTs), then print the key at the root, then print all the keys in the right subtree (which are greater than the key at the root by definition of BSTs), as in the code at left. As usual, the description serves as an argument by induction that this code prints the keys in order. To implement the two-argument <code>keys()</code> method that returns to a client all the keys in a specified range, we modify this code to add each key that is in the range to a <code>Queue</code>, and to skip the recursive calls for subtrees that cannot contain keys in the range. As with <code>BinarySearchST</code>, the fact that we gather the keys in a <code>Queue</code> is hidden from the client. The intention is that clients should process all the keys in the range of interest using Java’s <em>foreach</em> construct rather than needing to know what data structure we use to implement <code>Iterable&lt;Key&gt;</code>.</p>
<p class="image"><img alt="image" src="graphics/p0412-01.jpg"/></p>
<p><a id="ch03sec3lev33"/></p>
<h5><em>Analysis</em></h5>
<p>How efficient are the order-based operations in BSTs? To study this question, we consider the <em>tree height</em> (the maximum depth of any node in the tree). Given a tree, its height determines the worst-case cost of all BST operations (except for range search which incurs additional cost proportional to the number of keys returned).</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb21"/></p>
<p><strong>Proposition E.</strong> In a BST, all operations take time proportional to the height of the tree, in the worst case.</p>
<p><strong>Proof:</strong> All of these methods go down one or two paths in the tree. The length of any path is no more than the height, by definition.</p>
<hr/>
</div>
<p>We expect the tree height (the worst-case cost) to be higher than the average internal path length that we defined on page <a href="#page_403">403</a> (which averages in the short paths as well), but how much higher? This question may seem to you to be similar to the questions answered by <a href="#ch03sb16"><small>PROPOSITION C</small></a> and <a href="#ch03sb17"><small>PROPOSITION D</small></a>, but it is far more difficult to answer, certainly beyond the scope of this book. The average height of a BST built from random keys was shown to be logarithmic by J. Robson in 1979, and L. Devroye later showed that the value approaches 2.99 lg <em>N</em> for large <em>N.</em> Thus, if the insertions in our application are well-described by the random-key model, we are well on the way toward our goal of developing a symbol-table implementation that supports all of these operations <a id="page_414"/>in logarithmic time. We can expect that no path in a tree built from random keys is longer than 3 lg <em>N,</em> but what can we expect if the keys are not random? In the next section, you will see why this question is moot in practice because of <em>balanced BSTs</em>, which guarantee that the BST height will be logarithmic regardless of the order in which keys are inserted.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb22"/></p>
<h3><a id="page_413"/>Algorithm 3.3 (continued) Range searching in BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0413-01.jpg"/></p>
<p>To enqueue all the keys from the tree rooted at a given node that fall in a given range onto a queue, we (recursively) enqueue all the keys from the left subtree (if any of them could fall in the range), then enqueue the node at the root (if it falls in the range), then (recursively) enqueue all the keys from the right subtree (if any of them could fall in the range).</p>
<p class="image"><img alt="image" src="graphics/03_24-bstrange.jpg"/></p>
<hr/>
</div>
<p><small>IN SUMMARY</small>, BSTs are not difficult to implement and can provide fast search and insert for practical applications of all kinds <em>if</em> the key insertions are well-approximated by the random-key model. For our examples (and for many practical applications) BSTs make the difference between being able to accomplish the task at hand and not being able to do so. Moreover, many programmers choose BSTs for symbol-table implementations because they also support fast rank, select, delete, and range query operations. Still, as we have emphasized, the bad worst-case performance of BSTs may not be tolerable in some situations. Good performance of the basic BST implementation is dependent on the keys being sufficiently similar to random keys that the tree is not likely to contain many long paths. With quicksort, we were able to randomize; with a symbol-table API, we do not have that freedom, because the client controls the mix of operations. Indeed, the worst-case behavior is not unlikely in practice—it arises when a client inserts keys in order or in reverse order, a sequence of operations that some client certainly might attempt in the absence of any explicit warnings to avoid doing so. This possibility is a primary reason to seek better algorithms and data structures, which we consider next.</p>
<p class="image"><img alt="image" src="graphics/t0414-01.jpg"/></p>
<p><a id="ch03sec2lev15"/></p>
<h4><a id="page_415"/>Q&amp;A</h4>
<p><strong>Q.</strong> I’ve seen BSTs before, but not using recursion. What are the tradeoffs?</p>
<p><strong>A.</strong> Generally, recursive implementations are a bit easier to verify for correctness; nonrecursive implementations are a bit more efficient. See <a href="#ch03qa2q13"><small>EXERCISE 3.2.13</small></a> for an implementation of <code>get()</code>, the one case where you might notice the improved efficiency. If trees are unbalanced, the depth of the function-call stack could be a problem in a recursive implementation. Our primary reason for using recursion is to ease the transition to the balanced BST implementations of the next section, which definitely are easier to implement and debug with recursion.</p>
<p><strong>Q.</strong> Maintaining the node count field in <code>Node</code> seems to require a lot of code. Is it really necessary? Why not maintain a single instance variable containing the number of nodes in the tree to implement the <code>size()</code> client method?</p>
<p><strong>A.</strong> The <code>rank()</code> and <code>select()</code> methods need to have the size of the subtree rooted at each node. If you are not using these ordered operations, you can streamline the code by eliminating this field (see <a href="#ch03qa2q12"><small>EXERCISE 3.2.12</small></a>). Keeping the node count correct for all nodes is admittedly error-prone, but also a good check for debugging. You might also use a recursive method to implement <code>size()</code> for clients, but that would take <em>linear</em> time to count all the nodes and is a dangerous choice because you might experience poor performance in a client program, not realizing that such a simple operation is so expensive.</p>
<p><a id="ch03sec2lev16"/></p>
<h4><a id="page_416"/>Exercises</h4>
<p><a id="ch03qa2q1"/><strong>3.2.1</strong> Draw the BST that results when you insert the keys <code>E A S Y Q U E S T I O N</code>, in that order (associating the value <code>i</code> with the <code>i</code>th key, as per the convention in the text) into an initially empty tree. How many compares are needed to build the tree?</p>
<p><a id="ch03qa2q2"/><strong>3.2.2</strong> Inserting the keys in the order <code>A X C S E R H</code> into an initially empty BST gives a worst-case tree where every node has one null link, except one at the bottom, which has two null links. Give five other orderings of these keys that produce worst-case trees.</p>
<p><a id="ch03qa2q3"/><strong>3.2.3</strong> Give five orderings of the keys <code>A X C S E R H</code> that, when inserted into an initially empty BST, produce the <em>best-case</em> tree.</p>
<p><a id="ch03qa2q4"/><strong>3.2.4</strong> Suppose that a certain BST has keys that are integers between <code>1</code> and <code>10</code>, and we search for <code>5</code>. Which sequence below <em>cannot</em> be the sequence of keys examined?</p>
<p class="indenthangingN"><em>a.</em> <code>10, 9, 8, 7, 6, 5</code></p>
<p class="indenthangingN"><em>b.</em> <code>4, 10, 8, 7, 53</code></p>
<p class="indenthangingN"><em>c.</em> <code>1, 10, 2, 9, 3, 8, 4, 7, 6, 5</code></p>
<p class="indenthangingN"><em>d.</em> <code>2, 7, 3, 8, 4, 5</code></p>
<p class="indenthangingN"><em>e.</em> <code>1, 2, 10, 4, 8, 5</code></p>
<p><a id="ch03qa2q5"/><strong>3.2.5</strong> Suppose that we have an estimate ahead of time of how often search keys are to be accessed in a BST, and the freedom to insert them in any order that we desire. Should the keys be inserted into the tree in increasing order, decreasing order of likely frequency of access, or some other order? Explain your answer.</p>
<p><a id="ch03qa2q6"/><strong>3.2.6</strong> Add to <code>BST</code> a method <code>height()</code> that computes the height of the tree. Develop two implementations: a recursive method (which takes linear time and space proportional to the height), and a method like <code>size()</code> that adds a field to each node in the tree (and takes linear space and constant time per query).</p>
<p><a id="ch03qa2q7"/><strong>3.2.7</strong> Add to <code>BST</code> a recursive method <code>avgCompares()</code> that computes the average number of compares required by a random search hit in a given BST (the internal path length of the tree divided by its size, plus one). Develop two implementations: a recursive method (which takes linear time and space proportional to the height), and a method like <code>size()</code> that adds a field to each node in the tree (and takes linear space and constant time per query).</p>
<p><a id="ch03qa2q8"/><strong>3.2.8</strong> Write a static method <code>optCompares()</code> that takes an integer argument <code>N</code> and computes the number of compares required by a random search hit in an optimal (perfectly <a id="page_417"/>balanced) BST, where all the null links are on the same level if the number of links is a power of 2 or on one of two levels otherwise.</p>
<p><a id="ch03qa2q9"/><strong>3.2.9</strong> Draw all the different BST shapes that can result when <code>N</code> keys are inserted into an initially empty tree, for <code>N</code> = <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>, and <code>6</code>.</p>
<p><a id="ch03qa2q10"/><strong>3.2.10</strong> Write a test client for <code>BST</code> that tests the implementations of <code>min()</code>, <code>max()</code>, <code>floor()</code>, <code>ceiling()</code>, <code>select()</code>, <code>rank()</code>, <code>delete()</code>, <code>deleteMin()</code>, <code>deleteMax()</code>, and <code>keys()</code> that are given in the text. Start with the standard indexing client given on page <a href="#ch03sec3lev17">370</a>. Add code to take additional command-line arguments, as appropriate.</p>
<p><a id="ch03qa2q11"/><strong>3.2.11</strong> How many binary tree shapes of <em>N</em> nodes are there with height <em>N</em>? How many different ways are there to insert <em>N</em> distinct keys into an initially empty BST that result in a tree of height <em>N</em>? (See <a href="#ch03qa2q2"><small>EXERCISE 3.2.2</small></a>.)</p>
<p><a id="ch03qa2q12"/><strong>3.2.12</strong> Develop a <code>BST</code> implementation that omits <code>rank()</code> and <code>select()</code> and does not use a count field in <code>Node</code>.</p>
<p><a id="ch03qa2q13"/><strong>3.2.13</strong> Give nonrecursive implementations of <code>get()</code> and <code>put()</code> for <code>BST</code>.</p>
<p><em>Partial solution</em>: Here is an implementation of <code>get()</code>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0417-01.jpg"/></p>
<p>The implementation of <code>put()</code> is more complicated because of the need to save a pointer to the parent node to link in the new node at the bottom. Also, you need a separate pass to check whether the key is already in the table because of the need to update the counts. Since there are many more searches than inserts in performance-critical implementations, using this code for <code>get()</code> is justified; the corresponding change for <code>put()</code> might not be noticed.</p>
<p><a id="page_418"/><a id="ch03qa2q14"/><strong>3.2.14</strong> Give nonrecursive implementations of <code>min()</code>, <code>max()</code>, <code>floor()</code>, <code>ceiling()</code>, <code>rank()</code>, and <code>select()</code>.</p>
<p><a id="ch03qa2q15"/><strong>3.2.15</strong> Give the sequences of nodes examined when the methods in <code>BST</code> are used to compute each of the following quantities for the tree drawn at right.</p>
<p class="indenthangingN"><em>a.</em> <code>floor("Q")</code></p>
<p class="indenthangingN"><em>b.</em> <code>select(5)</code></p>
<p class="indenthangingN"><em>c.</em> <code>ceiling("Q")</code></p>
<p class="indenthangingN"><em>d.</em> <code>rank("J")</code></p>
<p class="indenthangingN"><em>e.</em> <code>size("D", "T")</code></p>
<p class="indenthangingN"><em>f.</em> <code>keys("D", "T")</code></p>
<p class="image"><img alt="image" src="graphics/03_25-bstex15.jpg"/></p>
<p><a id="ch03qa2q16"/><strong>3.2.16</strong> Define the <em>external path length</em> of a tree to be the sum of the number of nodes on the paths from the root to all null links. Prove that the difference between the external and internal path lengths in any binary tree with <em>N</em> nodes is 2<em>N</em> (see <a href="#ch03sb16"><small>PROPOSITION C</small></a>).</p>
<p><a id="ch03qa2q17"/><strong>3.2.17</strong> Draw the sequence of BSTs that results when you delete the keys from the tree of <a href="#ch03qa2q1"><small>EXERCISE 3.2.1</small></a>, one by one, in the order they were inserted.</p>
<p><a id="ch03qa2q18"/><strong>3.2.18</strong> Draw the sequence of BSTs that results when you delete the keys from the tree of <a href="#ch03qa2q1"><small>EXERCISE 3.2.1</small></a>, one by one, in alphabetical order.</p>
<p><a id="ch03qa2q19"/><strong>3.2.19</strong> Draw the sequence of BSTs that results when you delete the keys from the tree of <a href="#ch03qa2q1"><small>EXERCISE 3.2.1</small></a>, one by one, by successively deleting the key at the root.</p>
<p><a id="ch03qa2q20"/><strong>3.2.20</strong> Prove that the running time of the two-argument <code>keys()</code> in a BST with <em>N</em> nodes is at most proportional to the tree height plus the number of keys in the range.</p>
<p><a id="ch03qa2q21"/><strong>3.2.21</strong> Add a BST method <code>randomKey()</code> that returns a random key from the symbol table in time proportional to the tree height, in the worst case.</p>
<p><a id="ch03qa2q22"/><strong>3.2.22</strong> Prove that if a node in a BST has two children, its successor has no left child and its predecessor has no right child.</p>
<p><a id="ch03qa2q23"/><strong>3.2.23</strong> Is <code>delete()</code> commutative? (Does deleting <code>x</code>, then <code>y</code> give the same result as deleting <code>y</code>, then <code>x</code>?)</p>
<p><a id="ch03qa2q24"/><strong>3.2.24</strong> Prove that no compare-based algorithm can build a BST using fewer than lg(<em>N</em>!) ~<em>N</em> lg <em>N</em> compares.</p>
<p><a id="ch03sec2lev17"/></p>
<h4><a id="page_419"/>Creative Problems</h4>
<p><a id="ch03qa2q25"/><strong>3.2.25</strong> <em>Perfect balance.</em> Write a program that inserts a set of keys into an initially empty BST such that the tree produced is equivalent to binary search, in the sense that the sequence of compares done in the search for any key in the BST is the same as the sequence of compares used by binary search for the same set of keys.</p>
<p><a id="ch03qa2q26"/><strong>3.2.26</strong> <em>Exact probabilities.</em> Find the probability that each of the trees in <a href="#ch03qa2q9"><small>EXERCISE 3.2.9</small></a> is the result of inserting <code>N</code> random distinct elements into an initially empty tree.</p>
<p><a id="ch03qa2q27"/><strong>3.2.27</strong> <em>Memory usage.</em> Compare the memory usage of <code>BST</code> with the memory usage of <code>BinarySearchST</code> and <code>SequentialSearchST</code> for <em>N</em> key-value pairs, under the assumptions described in <a href="ch01a.html#ch01sec1lev6"><small>SECTION 1.4</small></a> (see <a href="#ch03qa1q21"><small>EXERCISE 3.1.21</small></a>). Do not count the memory for the keys and values themselves, but do count references to them. Then draw a diagram that depicts the precise memory usage of a BST with <code>String</code> keys and <code>Integer</code> values (such as the ones built by <code>FrequencyCounter</code>), and then estimate the memory usage (in bytes) for the BST built when <code>FrequencyCounter</code> uses <code>BST</code> for <em>Tale of Two Cities</em>.</p>
<p><a id="ch03qa2q28"/><strong>3.2.28</strong> <em>Sofware caching.</em> Modify <code>BST</code> to keep the most recently accessed <code>Node</code> in an instance variable so that it can be accessed in constant time if the next <code>put()</code> or <code>get()</code> uses the same key (see <a href="#ch03qa1q25"><small>EXERCISE 3.1.25</small></a>).</p>
<p><a id="ch03qa2q29"/><strong>3.2.29</strong> <em>Tree traversal with constant extra memory.</em> Design an algorithm that performs an inorder tree traversal of a BST using only a constant amount of extra memory. <em>Hint:</em> On the way down the tree, make the child point to the parent and reverse it on the way back up the tree.</p>
<p><a id="ch03qa2q30"/><strong>3.2.30</strong> <em>BST reconstruction.</em> Given the preorder (or postorder) traversal of a BST (not including null nodes), design an algorithm to reconstruct the BST.</p>
<p><a id="ch03qa2q31"/><strong>3.2.31</strong> <em>Equal key check.</em> Write a method <code>hasNoDuplicates()</code> that takes a <code>Node</code> as argument and returns <code>true</code> if there are no equal keys in the binary tree rooted at the argument node, <code>false</code> otherwise. Assume that the test of the previous exercise has passed.</p>
<p><a id="ch03qa2q32"/><strong>3.2.32</strong> <em>Certification.</em> Write a method <code>isBST()</code> that takes a <code>Node</code> as argument and returns <code>true</code> if the argument node is the root of a binary search tree, <code>false</code> otherwise. <em>Hint</em>: This task is also more difficult than it might seem, because the order in which you call the methods in the previous three exercises is important.</p>
<p><a id="page_420"/><em>Solution</em>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0420-01.jpg"/></p>
<p><a id="ch03qa2q33"/><strong>3.2.33</strong> <em>Select/rank check.</em> Write a method that checks, for all <code>i</code> from <code>0</code> to <code>size()-1</code>, whether <code>i</code> is equal to <code>rank(select(i))</code> and, for all keys in the BST, whether <code>key</code> is equal to <code>select(rank(key))</code>.</p>
<p><a id="ch03qa2q34"/><strong>3.2.34</strong> <em>Threading.</em> Your goal is to support an extended API <code>ThreadedST</code> that supports the following additional operations in constant time:</p>
<p class="image"><img alt="image" src="graphics/p0420-02.jpg"/></p>
<p>To do so, add fields <code>pred</code> and <code>succ</code> to <code>Node</code> that contain links to the predecessor and successor nodes, and modify <code>put()</code>, <code>deleteMin()</code>, <code>deleteMax()</code>, and <code>delete()</code> to maintain these fields.</p>
<p><a id="ch03qa2q35"/><strong>3.2.35</strong> <em>Refined analysis.</em> Refine the mathematical model to better explain the experimental results in the table given in the text. Specifically, show that the average number of compares for a successful search in a tree built from random keys approaches the limit 2 ln <em>N</em> + 2γ − 3 ≈ 1.39 lg <em>N</em> <strong>−</strong> 1.85 as <em>N</em> increases, where γ = .57721... is <em>Euler’s constant</em>. <em>Hint</em>: Referring to the quicksort analysis in <a href="ch02.html#ch02sec1lev3"><small>SECTION 2.3</small></a>, use the fact that the integral of 1/<em>x</em> approaches ln <em>N</em> + γ.</p>
<p><a id="ch03qa2q36"/><strong>3.2.36</strong> <em>Iterator.</em> Is it possible to write a nonrecursive version of <code>keys()</code> that uses space proportional to the tree height (independent of the number of keys in the range)?</p>
<p><a id="ch03qa2q37"/><strong>3.2.37</strong> <em>Level-order traversal.</em> Write a method <code>printLevel()</code> that takes a <code>Node</code> as argument and prints the keys in the subtree rooted at that node in level order (in order of their distance from the root, with nodes on each level in order from left to right). <em>Hint</em>: Use a <code>Queue</code>.</p>
<p><a id="page_421"/><a id="ch03qa2q38"/><strong>3.2.38</strong> <em>Tree drawing.</em> Add a method <code>draw()</code> to <code>BST</code> that draws BST figures in the style of the text. <em>Hint</em>: Use instance variables to hold node coordinates, and use a recursive method to set the values of these variables.</p>
<p><a id="ch03sec2lev18"/></p>
<h4><a id="page_422"/>Experiments</h4>
<p><a id="ch03qa2q39"/><strong>3.2.39</strong> <em>Average case.</em> Run empirical studies to estimate the average and standard deviation of the number of compares used for search hits and for search misses in a BST built by running 100 trials of the experiment of inserting <em>N</em> random keys into an initially empty tree, for <em>N</em> = 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>. Compare your results against the formula for the average given in <a href="#ch03qa2q35"><small>EXERCISE 3.2.35</small></a>.</p>
<p><a id="ch03qa2q40"/><strong>3.2.40</strong> <em>Height.</em> Run empirical studies to estimate average BST height by running 100 trials of the experiment of inserting <em>N</em> random keys into an initially empty tree, for <em>N</em> = 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>. Compare your results against the 2.99 lg <em>N</em> estimate that is described in the text.</p>
<p><a id="ch03qa2q41"/><strong>3.2.41</strong> <em>Array representation.</em> Develop a BST implementation that represents the BST with three arrays (preallocated to the maximum size given in the constructor): one with the keys, one with array indices corresponding to left links, and one with array indices corresponding to right links. Compare the performance of your program with that of the standard implementation.</p>
<p><a id="ch03qa2q42"/><strong>3.2.42</strong> <em>Hibbard deletion degradation.</em> Write a program that takes an integer <em>N</em> from the command line, builds a random BST of size <em>N</em>, then enters into a loop where it deletes a random key (using the code <code>delete(select(StdRandom.uniform(N)))</code>) and then inserts a random key, iterating the loop <em>N</em><sup>2</sup> times. After the loop, measure and print the average length of a path in the tree (the internal path length divided by <em>N,</em> plus 1). Run your program for <em>N</em> = 10<sup>2</sup>, 10<sup>3</sup>, and 10<sup>4</sup> to test the somewhat counterintuitive hypothesis that this process increases the average path length of the tree to be proportional to the square root of <em>N.</em> Run the same experiments for a <code>delete()</code> implementation that makes a random choice whether to use the predecessor or the successor node.</p>
<p><a id="ch03qa2q43"/><strong>3.2.43</strong> <em>Put/get ratio.</em> Determine empirically the ratio of the amount of time that <code>BST</code> spends on <code>put()</code> operations to the time that it spends on <code>get()</code> operations when <code>FrequencyCounter</code> is used to find the frequency of occurrence of values in 1 million randomly-generated integers.</p>
<p><a id="ch03qa2q44"/><strong>3.2.44</strong> <em>Cost plots.</em> Instrument <code>BST</code> so that you can produce plots like the ones in this section showing the cost of each <code>put()</code> operation during the computation (see <a href="#ch03qa1q38"><small>EXERCISE 3.1.38</small></a>).</p>
<p><a id="ch03qa2q45"/><strong>3.2.45</strong> <em>Actual timings.</em> Instrument <code>FrequencyCounter</code> to use <code>Stopwatch</code> and <code>StdDraw</code> to make a plot where the <em>x</em> axis is the number of calls on <code>get()</code> or <code>put()</code> and the <em>y</em> axis <a id="page_423"/>is the total running time, with a point plotted of the cumulative time after each call. Run your program for <em>Tale of Two Cities</em> using <code>SequentialSearchST</code> and again using <code>BinarySearchST</code> and again using <code>BST</code> and discuss the results. <em>Note</em>: Sharp jumps in the curve may be explained by <em>caching</em>, which is beyond the scope of this question (see <a href="#ch03qa1q39"><small>EXERCISE 3.1.39</small></a>).</p>
<p><a id="ch03qa2q46"/><strong>3.2.46</strong> <em>Crossover to binary search trees.</em> Find the values of <em>N</em> for which using a binary search tree to build a symbol table of <em>N</em> random <code>double</code> keys becomes 10, 100, and 1,000 times faster than binary search. Predict the values with analysis and verify them experimentally.</p>
<p><a id="ch03qa2q47"/><strong>3.2.47</strong> <em>Average search time.</em> Run empirical studies to compute the average and standard deviation of the average length of a path to a random node (internal path length divided by tree size, plus 1) in a BST built by insertion of <em>N</em> random keys into an initially empty tree, for <em>N</em> from 100 to 10,000. Do 1,000 trials for each tree size. Plot the results in a Tufte plot, like the one at the bottom of this page, fit with a curve plotting the function 1.39 lg <em>N <strong>−</strong></em> 1.85 (see <a href="#ch03qa2q35"><small>EXERCISE 3.2.35</small></a> and <a href="#ch03qa2q39"><small>EXERCISE 3.2.39</small></a>).</p>
<p class="image"><img alt="image" src="graphics/03_26-avgbst.jpg"/></p>
<p><a id="ch03sec1lev3"/></p>
<h3><a id="page_424"/>3.3 Balanced Search Trees</h3>
<p>The algorithms in the previous section work well for a wide variety of applications, but they have poor worst-case performance. We introduce in this section a type of binary search tree where costs are <em>guaranteed</em> to be logarithmic, no matter what sequence of keys is used to construct them. Ideally, we would like to keep our binary search trees perfectly balanced. In an <em>N</em>-node tree, we would like the height to be ~lg <em>N</em> so that we can guarantee that all searches can be completed in ~lg <em>N</em> compares, just as for binary search (see <a href="#ch03sb11"><small>PROPOSITION B</small></a>). Unfortunately, maintaining perfect balance for dynamic insertions is too expensive. In this section, we consider a data structure that slightly relaxes the perfect balance requirement to provide guaranteed logarithmic performance not just for the <em>insert</em> and <em>search</em> operations in our symbol-table API but also for all of the ordered operations (except range search).</p>
<p><a id="ch03sec2lev19"/></p>
<h4>2-3 search trees</h4>
<p>The primary step to get the flexibility that we need to guarantee balance in search trees is to allow the nodes in our trees to hold more than one key. Specifically, referring to the nodes in a standard BST as <em>2-nodes</em> (they hold two links and one key), we now also allow <em>3-nodes</em>, which hold three links and two keys. Both 2-nodes and 3-nodes have one link for each of the intervals subtended by its keys.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb23"/></p>
<p><strong>Definition.</strong> A <em>2-3 search tree</em> is a tree that is either empty or</p>
<p class="indenthangingB1">• A <em>2-node</em>, with one key (and associated value) and two links, a left link to a 2-3 search tree with smaller keys, and a right link to a 2-3 search tree with larger keys</p>
<p class="indenthangingB1">• A <em>3-node</em>, with two keys (and associated values) and <em>three</em> links, a left link to a 2-3 search tree with smaller keys, a middle link to a 2-3 search tree with keys between the node’s keys, and a right link to a 2-3 search tree with larger keys</p>
<p>As usual, we refer to a link to an empty tree as a <em>null link</em>.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/03_27-ttanatomy.jpg"/></p>
<p>A <em>perfectly balanced</em> 2-3 search tree is one whose null links are all the same distance from the root. To be concise, we use the term <em>2-3 tree</em> to refer to a perfectly balanced 2-3 search tree (the term denotes a more general structure in other contexts). Later, we shall see efficient ways to define and implement the basic operations on 2-nodes, 3-nodes, and 2-3 trees; for now, let us assume that we can manipulate them conveniently and see how we can use them as search trees.</p>
<p class="image"><a id="page_425"/><img alt="image" src="graphics/03_28-ttsearch.jpg"/></p>
<p><a id="ch03sec3lev34"/></p>
<h5><em>Search</em></h5>
<p>The search algorithm for keys in a 2-3 tree directly generalizes the search algorithm for BSTs. To determine whether a key is in the tree, we compare it against the keys at the root. If it is equal to any of them, we have a search hit; otherwise, we follow the link from the root to the subtree corresponding to the interval of key values that could contain the search key. If that link is null, we have a search miss; otherwise we recursively search in that subtree.</p>
<p><a id="ch03sec3lev35"/></p>
<h5><em>Insert into a 2-node</em></h5>
<p>To insert a new node in a 2-3 tree, we might do an unsuccessful search and then hook on the node at the bottom, as we did with BSTs, but the new tree would not remain perfectly balanced. The primary reason that 2-3 trees are useful is that we can do insertions and still maintain perfect balance. It is easy to accomplish this task if the node at which the search terminates is a 2-node: we just replace the node with a 3-node containing its key and the new key to be inserted. If the node where the search terminates is a 3-node, we have more work to do.</p>
<p class="image"><img alt="image" src="graphics/03_29-ttinserttwo.jpg"/></p>
<p><a id="ch03sec3lev36"/></p>
<h5><a id="page_426"/><em>Insert into a tree consisting of a single 3-node</em></h5>
<p>As a first warmup before considering the general case, suppose that we want to insert into a tiny 2-3 tree consisting of just a single 3-node. Such a tree has two keys, but no room for a new key in its one node. To be able to perform the insertion, we temporarily put the new key into a <em>4-node</em>, a natural extension of our node type that has three keys and four links. Creating the 4-node is convenient because it is easy to convert it into a 2-3 tree made up of three 2-nodes, one with the middle key (at the root), one with the smallest of the three keys (pointed to by the left link of the root), and one with the largest of the three keys (pointed to by the right link of the root). Such a tree is a 3-node BST and also a perfectly balanced 2-3 search tree, with all the null links at the same distance from the root. Before the insertion, the height of the tree is 0; after the insertion, the height of the tree is 1. This case is simple, but it is worth considering because it illustrates height growth in 2-3 trees.</p>
<p class="image"><img alt="image" src="graphics/03_30-ttinsertthree.jpg"/></p>
<p><a id="ch03sec3lev37"/></p>
<h5><em>Insert into a 3-node whose parent is a 2-node</em></h5>
<p>As a second warmup, suppose that the search ends at a 3-node at the bottom whose parent is a 2-node. In this case, we can still make room for the new key <em>while maintaining perfect balance in the tree</em>, by making a temporary 4-node as just described, then splitting the 4-node as just described, but then, instead of creating a new node to hold the middle key, moving the middle key to the node’s parent. You can think of the transformation as replacing the link to the old 3-node in the parent by the middle key with links on either side to the new 2-nodes. By our assumption, there is room for doing so in the parent: the parent was a 2-node (with one key and two links) and becomes a 3-node (with two keys and three links). Also, this transformation does not affect the defining properties of (perfectly balanced) 2-3 trees. The tree remains ordered because the middle key is moved to the parent, and it remains perfectly balanced: if all null links are the same distance from the root before the insertion, they are all the same distance from the root after the insertion. Be certain that you understand this transformation—it is the crux of 2-3 tree dynamics.</p>
<p class="image"><img alt="image" src="graphics/03_31-ttinserttwo.jpg"/></p>
<p><a id="ch03sec3lev38"/></p>
<h5><a id="page_427"/><em>Insert into a 3-node whose parent is a 3-node</em></h5>
<p>Now suppose that the search ends at a node whose parent is a 3-node. Again, we make a temporary 4-node as just described, then split it and insert its middle key into the parent. The parent was a 3-node, so we replace it with a temporary new 4-node containing the middle key from the 4-node split. Then, we perform <em>precisely the same transformation on that node</em>. That is, we split the new 4-node and insert its middle key into <em>its</em> parent. Extending to the general case is clear: we continue up the tree, splitting 4-nodes and inserting their middle keys in their parents until reaching a 2-node, which we replace with a 3-node that does not need to be further split, or until reaching a 3-node at the root.</p>
<p class="image"><img alt="image" src="graphics/03_33-ttinsertpthree.jpg"/></p>
<p><a id="ch03sec3lev39"/></p>
<h5><em>Splitting the root</em></h5>
<p>If we have 3-nodes along the whole path from the insertion point to the root, we end up with a temporary 4-node at the root. In this case we can proceed in precisely the same way as for insertion into a tree consisting of a single 3-node. We split the temporary 4-node into three 2-nodes, increasing the height of the tree by 1. Note that this last transformation preserves perfect balance because it is performed at the root.</p>
<p class="image"><img alt="image" src="graphics/03_32-ttsplitroot.jpg"/></p>
<p><a id="ch03sec3lev40"/></p>
<h5><em>Local transformations</em></h5>
<p>Splitting a temporary 4-node in a 2-3 tree involves one of six transformations, summarized at the bottom of the next page. The 4-node may be the root; it may be the left child or the right child of a 2-node; or it may be the left child, middle child, or right child of a 3-node. The basis of the 2-3 tree insertion algorithm is that all of these transformations are purely <em>local</em>: no part of the tree needs to be examined or modified other than the specified nodes and links. The number of <a id="page_428"/>links changed for each transformation is bounded by a small constant. In particular, the transformations are effective when we find the specified patterns <em>anywhere</em> in the tree, not just at the bottom. Each of the transformations passes up one of the keys from a 4-node to that node’s parent in the tree and then restructures links accordingly, without touching any other part of the tree.</p>
<p class="image"><img alt="image" src="graphics/03_34-ttflocal.jpg"/></p>
<p><a id="ch03sec3lev41"/></p>
<h5><em>Global properties</em></h5>
<p>Moreover, these <em>local</em> transformations preserve the <em>global</em> properties that the tree is ordered and perfectly balanced: the number of links on the path from the root to any null link is the same. For reference, a complete diagram illustrating this point for the case that the 4-node is the middle child of a 3-node is shown above. If the length of every path from a root to a null link is <em>h</em> before the transformation, then it is <em>h</em> after the transformation. <em>Each transformation preserves this property</em>, even while splitting the 4-node into two 2-nodes and while changing the parent from a 2-node to a 3-node or from a 3-node into a temporary 4-node. When the root splits into three 2-nodes, the length of every path from the root to a null link increases by 1. If you are not fully convinced, work <a href="#ch03qa3q7"><small>EXERCISE 3.3.7</small></a>, which asks you to <a id="page_429"/>extend the diagrams at the top of the previous page for the other five cases to illustrate the same point. Understanding that every local transformation preserves order and perfect balance in the whole tree is the key to understanding the algorithm.</p>
<p class="image"><img alt="image" src="graphics/03_35-ttfsplit.jpg"/></p>
<p><small>UNLIKE STANDARD BSTS</small>, which grow down from the top, 2-3 trees grow up from the bottom. If you take the time to carefully study the figure on the next page, which gives the sequence of 2-3 trees that is produced by our standard indexing test client and the sequence of 2-3 trees that is produced when the same keys are inserted in increasing order, you will have a good understanding of the way that 2-3 trees are built. Recall that in a BST, the increasing-order sequence for 10 keys results in a worst-case tree of height 9. In the 2-3 trees, the height is 2.</p>
<p>The preceding description is sufficient to define a symbol-table implementation with 2-3 trees as the underlying data structure. Analyzing 2-3 trees is different from analyzing BSTs because our primary interest is in <em>worst-case</em> performance, as opposed to average-case performance (where we analyze expected performance under the random-key model). In symbol-table implementations, we normally have no control over the order in which clients insert keys into the table and worst-case analysis is one way to provide performance guarantees.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb24"/></p>
<p><strong>Proposition F.</strong> Search and insert operations in a 2-3 tree with <em>N</em> keys are guaranteed to visit at most lg <em>N</em> nodes.</p>
<p><strong>Proof:</strong> The height of an <em>N</em>-node 2-3 tree is between <img alt="image" src="graphics/lfloor.jpg"/>log<sub>3</sub> <em>N</em><img alt="image" src="graphics/rfloor.jpg"/> = <img alt="image" src="graphics/lfloor.jpg"/>(lg <em>N</em>)/(lg 3)<img alt="image" src="graphics/rfloor.jpg"/> (if the tree is all 3-nodes) and <img alt="image" src="graphics/lfloor.jpg"/>lg <em>N</em><img alt="image" src="graphics/rfloor.jpg"/> (if the tree is all 2-nodes) (see <a href="#ch03qa3q4"><small>EXERCISE 3.3.4</small></a>).</p>
<hr/>
</div>
<p>Thus, we can guarantee good worst-case performance with 2-3 trees. The amount of time required at each node by each of the operations is bounded by a constant, and both operations examine nodes on just one path, so the total cost of any search or insert is guaranteed to be logarithmic. As you can see from comparing the 2-3 tree depicted at the bottom of page <a href="#page_431">431</a> with the BST formed from the same keys on page <a href="#page_405">405</a>, a perfectly balanced 2-3 tree strikes a remarkably flat posture. For example, the height of a 2-3 tree that contains 1 billion keys is between 19 and 30. It is quite remarkable that we can guarantee to perform arbitrary search and insertion operations among 1 billion keys by examining at most 30 nodes.</p>
<p>However, we are only part of the way to an implementation. Although it is possible to write code that performs transformations on distinct data types representing 2- and 3-nodes, most of the tasks that we have described are inconvenient to implement in <a id="page_431"/>this direct representation because there are numerous different cases to be handled. We would need to maintain two different types of nodes, compare search keys against each of the keys in the nodes, copy links and other information from one type of node to another, convert nodes from one type to another, and so forth. Not only is there a substantial amount of code involved, but the overhead incurred could make the algorithms slower than standard BST search and insert. The primary purpose of balancing is to provide insurance against a bad worst case, but we would prefer the overhead cost for that insurance to be low. Fortunately, as you will see, we can do the transformations in a uniform way using little overhead.</p>
<p class="image"><a id="page_430"/><img alt="image" src="graphics/03_36-tttrace.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_37-ttf100.jpg"/></p>
<p><a id="ch03sec2lev20"/></p>
<h4><a id="page_432"/>Red-black BSTs</h4>
<p>The insertion algorithm for 2-3 trees just described is not difficult to understand; now, we will see that it is also not difficult to implement. We will consider a simple representation known as a <em>red-black BST</em> that leads to a natural implementation. In the end, not much code is required, but understanding how and why the code gets the job done requires a careful look.</p>
<p><a id="ch03sec3lev42"/></p>
<h5><em>Encoding 3-nodes</em></h5>
<p>The basic idea behind red-black BSTs is to encode 2-3 trees by starting with standard BSTs (which are made up of 2-nodes) and adding extra information to encode 3-nodes. We think of the links as being of two different types: <em>red</em> links, which bind together two 2-nodes to represent 3-nodes, and <em>black</em> links, which bind together the 2-3 tree. Specifically, we represent 3-nodes as two 2-nodes connected by a single red link that <em>leans left</em> (one of the 2-nodes is the left child of the other). One advantage of using such a representation is that it allows us to use our <code>get()</code> code for standard BST search <em>without modification</em>. Given any 2-3 tree, we can immediately derive a corresponding BST, just by converting each node as specified. We refer to BSTs that represent 2-3 trees in this way as <em>red-black BSTs</em>.</p>
<p class="image"><img alt="image" src="graphics/03_38-rbnodestt.jpg"/></p>
<p><a id="ch03sec3lev43"/></p>
<h5><em>An equivalent definition</em></h5>
<p>Another way to proceed is to <em>define</em> red-black BSTs as BSTs having red and black links and satisfying the following three restrictions:</p>
<p class="indenthangingB">• Red links lean left.</p>
<p class="indenthangingB">• No node has two red links connected to it.</p>
<p class="indenthangingB">• The tree has <em>perfect black balance</em>: every path from the root to a null link has the same number of black links.</p>
<p>There is a 1-1 correspondence between red-black BSTs defined in this way and 2-3 trees.</p>
<p><a id="ch03sec3lev44"/></p>
<h5><em>A 1-1 correspondence</em></h5>
<p>If we draw the red links horizontally in a red-black BST, all of the null links are the same distance from the root, and if we then collapse together the nodes connected by red links, the result is a 2-3 tree. Conversely, if we draw 3-nodes in <a id="page_433"/>a 2-3 tree as two 2-nodes connected by a red link that leans left, then no node has two red links connected to it, and the tree has perfect black balance, since the black links correspond to the 2-3 tree links, which are perfectly balanced by definition. Whichever way we choose to define them, red-black BSTs are <em>both</em> BSTs and 2-3 trees. Thus, if we can implement the 2-3 tree insertion algorithm by maintaining the 1-1 correspondence, then we get the best of both worlds: the simple and efficient search method from standard BSTs and the efficient insertion-balancing method from 2-3 trees.</p>
<p class="image"><img alt="image" src="graphics/03_39-ttfllrb100.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_40-rbanatomytt.jpg"/></p>
<p><a id="ch03sec3lev45"/></p>
<h5><em>Color representation</em></h5>
<p>For convenience, since each node is pointed to by precisely one link (from its parent), we encode the color of links in <em>nodes</em>, by adding a <code>boolean</code> instance variable <code>color</code> to our <code>Node</code> data type, which is <code>true</code> if the link from the parent is red and <code>false</code> if it is black. By convention, null links are black. For clarity in our code, we define constants <code>RED</code> and <code>BLACK</code> for use in setting and testing this variable. We use a private method <code>isRed()</code> to test the color of a node’s link to its parent. When we refer to the color of a node, we are referring to the color of the link pointing to it, and vice versa.</p>
<p class="image"><img alt="image" src="graphics/03_41-rbnode.jpg"/></p>
<p><a id="ch03sec3lev46"/></p>
<h5><em>Rotations</em></h5>
<p>The implementation that we will consider might allow right-leaning red links or two red links in a row during an operation, but it always corrects these conditions before completion, through judicious use of an operation called <em>rotation</em> that switches the orientation of <a id="page_434"/>red links. First, suppose that we have a right-leaning red link that needs to be rotated to lean to the left (see the diagram at left). This operation is called a <em>left rotation</em>. We organize the computation as a method that takes a link to a red-black BST as argument and, assuming that link to be to a <code>Node h</code> whose right link is red, makes the necessary adjustments and returns a link to a node that is the root of a red-black BST for the same set of keys whose <em>left</em> link is red. If you check each of the lines of code against the before/after drawings in the diagram, you will find this operation is easy to understand: we are switching from having the smaller of the two keys at the root to having the larger of the two keys at the root. Implementing a <em>right rotation</em> that converts a left-leaning red link to a right-leaning one amounts to the same code, with left and right interchanged (see the diagram at right below).</p>
<p class="image"><img alt="image" src="graphics/03_42-rbrotatel.jpg"/></p>
<p><a id="ch03sec3lev47"/></p>
<h5><em>Resetting the link in the parent after a rotation</em></h5>
<p>Whether left or right, every rotation leaves us with a link. We always use the link returned by <code>rotateRight()</code> or <code>rotateLeft()</code> to reset the appropriate link in the parent (or the root of the tree). That may be a right or a left link, but we can always use it to reset the link in the parent. This link may be red or black—both <code>rotateLeft()</code> and <code>rotateRight()</code> preserve its color by setting <code>x.color</code> to <code>h.color</code>. This might allow two red links in a row to occur within the tree, but our algorithms will also use rotations to correct this condition when it arises. For example, the code</p>
<p class="programlisting">h = rotateLeft(h);</p>
<p class="image"><img alt="image" src="graphics/03_43-rbrotater.jpg"/></p>
<p>rotates left a right-leaning red link that is to the right of node <code>h</code>, setting <code>h</code> to point to the root of the resulting subtree (which contains all the same nodes as the subtree pointed to by <code>h</code> before the rotation, but a different root). The ease of writing this type of code is the primary reason we use recursive implementations of BST methods, as it makes doing rotations an easy supplement to normal insertion, as you will see.</p>
<p><a id="page_435"/><small>WE CAN USE ROTATIONS</small> to help maintain the 1-1 correspondence between 2-3 trees and red-black BSTs as new keys are inserted because they preserve the two defining properties of red-black BSTs: <em>order</em> and <em>perfect black balance</em>. That is, we can use rotations on a red-black BST without having to worry about losing its order or its perfect black balance. Next, we see how to use rotations to preserve the other two defining properties of red-black BSTs (no consecutive red links on any path and no right-leaning red links). We warm up with some easy cases.</p>
<p class="image"><img alt="image" src="graphics/03_44-rbinserttwo.jpg"/></p>
<p><a id="ch03sec3lev48"/></p>
<h5><em>Insert into a single 2-node</em></h5>
<p>A red-black BST with 1 key is just a single 2-node. Inserting the second key immediately shows the need for having a rotation operation. If the new key is smaller than the key in the tree, we just make a new (red) node with the new key and we are done: we have a red-black BST that is equivalent to a single 3-node. But if the new key is larger than the key in the tree, then attaching a new (red) node gives a right-leaning red link, and the code <code>root = rotateLeft(root);</code> completes the insertion by rotating the red link to the left and updating the tree root link. The result in both cases is the red-black representation of a single 3-node, with two keys, one left-leaning red link, and black height 1.</p>
<p><a id="ch03sec3lev49"/></p>
<h5><em>Insert into a 2-node at the bottom</em></h5>
<p>We insert keys into a red-black BST as usual into a BST, adding a new node at the bottom (respecting the order), but always connected to its parent with a red link. If the parent is a 2-node, then the same two cases just discussed are effective. If the new node is attached to the left link, the parent simply becomes a 3-node; if it is attached to a right link, we have a 3-node leaning the wrong way, but a left rotation finishes the job.</p>
<p><a id="ch03sec3lev50"/></p>
<h5><em>Insert into a tree with two keys (in a 3-node)</em></h5>
<p>This case reduces to three subcases: the new key is either less than both keys in the tree, between them, or greater than both of them. Each of the cases introduces a node with two red links connected to it; our goal is to correct this condition.</p>
<p class="indenthangingB">• The simplest of the three cases is when the new key is <em>larger</em> than the two in the tree and is therefore attached on the rightmost link of the 3-node, making a balanced tree with the middle key at the root, connected with red links to nodes containing a smaller and a larger key. If we flip the colors of those two links from red to black, then we have a balanced tree of height 2 with three nodes, exactly what we need to maintain our 1-1 correspondence to 2-3 trees. The other two cases eventually reduce to this case.</p>
<p class="image"><img alt="image" src="graphics/03_45-rbtracettslow5.jpg"/></p>
<p class="indenthangingB"><a id="page_436"/>• If the new key is <em>smaller</em> than the two keys in the tree and goes on the left link, then we have two red links in a row, both leaning to the left, which we can reduce to the previous case (middle key at the root, connected to the others by two red links) by rotating the top link to the right.</p>
<p class="image"><img alt="image" src="graphics/03_46-rbinsertthree.jpg"/></p>
<p class="indenthangingB">• If the new key goes <em>between</em> the two keys in the tree, we again have two red links in a row, a right-leaning one below a left-leaning one, which we can reduce to the previous case (two red links in a row, to the left) by rotating left the bottom link.</p>
<p class="image"><img alt="image" src="graphics/03_47-rbflip.jpg"/></p>
<p>In summary, we achieve the desired result by doing zero, one, or two rotations followed by flipping the colors of the two children of the root. As with 2-3 trees, <em>be certain that you understand these transformations</em>, as they are the key to red-black tree dynamics.</p>
<p><a id="ch03sec3lev51"/></p>
<h5><em>Flipping colors</em></h5>
<p>To flip the colors of the two red children of a node, we use a method <code>flipColors()</code>, shown at left. In addition to flipping the colors of the children from red to black, we also flip the color of the parent from black to red. A critically important characteristic of this operation is that, like rotations, it is a local transformation that <em>preserves perfect black balance</em> in the tree. Moreover, this convention immediately leads us to a full implementation, as we describe next.</p>
<p><a id="ch03sec3lev52"/></p>
<h5><a id="page_437"/><em>Keeping the root black</em></h5>
<p>In the case just considered (insert into a single 3-node), the color flip will color the root red. This can also happen in larger trees. Strictly speaking, a red root implies that the root is part of a 3-node, but that is not the case, so we color the root black after each insertion. Note that the black height of the tree increases by 1 whenever the color of the color of the root is flipped from black to red.</p>
<p><a id="ch03sec3lev53"/></p>
<h5><em>Insert into a 3-node at the bottom</em></h5>
<p>Now suppose that we add a new node at the bottom that is connected to a 3-node. The same three cases just discussed arise. Either the new link is connected to the right link of the 3-node (in which case we just flip colors) or to the left link of the 3-node (in which case we need to rotate the top link right and flip colors) or to the middle link of the 3-node (in which case we rotate left the bottom link, then rotate right the top link, then flip colors). Flipping the colors makes the link to the middle node red, which amounts to passing it up to its parent, putting us back in the same situation with respect to the parent, which we can fix by moving up the tree.</p>
<p class="image"><img alt="image" src="graphics/03_48-rbtracettslow.jpg"/></p>
<p><a id="ch03sec3lev54"/></p>
<h5><em>Passing a red link up the tree</em></h5>
<p>The 2-3 tree insertion algorithm calls for us to split the 3-node, passing the middle key up to be inserted into its parent, continuing until encountering a 2-node or the root. In every case we have considered, we precisely accomplish this objective: after doing any necessary rotations, we flip colors, which turns the middle node to red. From the point of view of the parent of that node, that link becoming red can be handled in precisely the same manner as if the red link came from attaching a new node: we pass up a red link to the middle node. The three cases summarized in the diagram on the next page precisely capture the operations necessary in a red-black tree to implement the key operation in 2-3 tree insertion: to insert into a 3-node, create a temporary 4-node, split it, and pass a red link to the middle key up to its parent. Continuing the same process, we pass a red link up the tree until reaching a 2-node or the root.</p>
<p><a id="page_438"/><small>IN SUMMARY, WE CAN MAINTAIN</small> our 1-1 correspondence between 2-3 trees and red-black BSTs during insertion by judicious use of three simple operations: left rotate, right rotate, and color flip. We can accomplish the insertion by performing the following operations, one after the other, on each node as we pass up the tree from the point of insertion:</p>
<p class="indenthangingB">• If the right child is red and the left child is black, rotate left.</p>
<p class="indenthangingB">• If both the left child and its left child are red, rotate right.</p>
<p class="indenthangingB">• If both children are red, flip colors.</p>
<p class="image"><img alt="image" src="graphics/03_49-rbpassup.jpg"/></p>
<p>It certainly is worth your while to check that this sequence handles each of the cases just described. Note that the first operation handles both the rotation necessary to lean the 3-node to the left when the parent is a 2-node and the rotation necessary to lean the bottom link to the left when the new red link is the middle link in a 3-node.</p>
<p><a id="ch03sec2lev21"/></p>
<h4>Implementation</h4>
<p>Since the balancing operations are to be performed on the way <em>up</em> the tree from the point of insertion, implementing them is easy in our standard recursive implementation: we just do them after the recursive calls, as shown in <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a>. The three operations listed in the previous paragraph each can be accomplished with a single <code>if</code> statement that tests the colors of two nodes in the tree. Even though it involves a small amount of code, this implementation would be quite difficult to understand without the two layers of abstraction that we have developed (2-3 trees and red-black BSTs) to implement it. At a cost of testing three to five node colors (and perhaps doing a rotation or two or flipping colors when a test succeeds), we get BSTs that have nearly perfect balance.</p>
<p>The traces for our standard indexing client and for the same keys inserted in increasing order are given on page <a href="#page_440">440</a>. Considering these examples simply in terms of our three operations on red-black trees, as we have been doing, is an instructive exercise. Another instructive exercise is to check the correspondence with 2-3 trees that the algorithm maintains (using the figure for the same keys given on page <a href="#page_430">430</a>). In both cases, you can test your understanding of the algorithm by considering the transformations (two color flips and two rotations) that are needed when <code>P</code> is inserted into the red-black BST (see <a href="#ch03qa3q12"><small>EXERCISE 3.3.12</small></a>).</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb25"/></p>
<h3><a id="page_439"/>Algorithm 3.4 Insert for red-black BSTs</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0439-01.jpg"/></p>
<p>The code for the recursive <code>put()</code> for red-black BSTs is identical to <code>put()</code> in elementary BSTs except for the three <code>if</code> statements after the recursive calls, which provide near-perfect balance in the tree by maintaining a 1-1 correspondence with 2-3 trees, on the way up the search path. The first rotates left any right-leaning 3-node (or a right-leaning red link at the bottom of a temporary 4-node); the second rotates right the top link in a temporary 4-node with two left-leaning red links; and the third flips colors to pass a red link up the tree (see text).</p>
<hr/>
</div>
<p class="image"><a id="page_440"/><img alt="image" src="graphics/03_50-rbtracett.jpg"/></p>
<p><a id="ch03sec2lev22"/></p>
<h4><a id="page_441"/>Deletion</h4>
<p>Since <code>put()</code> in <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a> is already one of the most intricate methods that we consider in this book, and the implementations of <code>deleteMin()</code>, <code>deleteMax()</code>, and <code>delete()</code> for red-black BSTs are a bit more complicated, we defer their full implementations to exercises. Still, the basic approach is worthy of study. To describe it, we begin by returning to 2-3 trees. As with insertion, we can define a sequence of local transformations that allow us to delete a node while still maintaining perfect balance. The process is somewhat more complicated than for insertion, because we do the transformations both on the way <em>down</em> the search path, when we introduce temporary 4-nodes (to allow for a node to be deleted), and also on the way <em>up</em> the search path, where we split any leftover 4-nodes (in the same manner as for insertion).</p>
<p class="image"><img alt="image" src="graphics/03_51-ttftopdown.jpg"/></p>
<p><a id="ch03sec3lev55"/></p>
<h5><em>Top-down 2-3-4 trees</em></h5>
<p>As a first warmup for deletion, we consider a simpler algorithm that does transformations both on the way down the path and on the way up the path: an insertion algorithm for <em>2-3-4 trees</em>, where the temporary 4-nodes that we saw in 2-3 trees can persist in the tree. The insertion algorithm is based on doing transformations on the way down the path to maintain the invariant that the current node is not a 4-node (so we are assured that there will be room to insert the new key at the bottom) and transformations on the way up the path to balance any 4-nodes that may have been created. The transformations on the way down are <em>precisely</em> the same transformations that we used for splitting 4-nodes in 2-3 trees. If the root is a 4-node, we split it into three 2-nodes, increasing the height of the tree by 1. On the way down the tree, if we encounter a 4-node with a 2-node as parent, we split the 4-node into two 2-nodes and pass the middle key to the parent, making it a 3-node; if we encounter a 4-node with a 3-node as parent, we split the 4-node into two 2-nodes and pass the middle key to the parent, making it a 4-node. We do not need to worry about encountering a 4-node with a 4-node as parent by virtue of the invariant. At the bottom, we have, again by virtue of the invariant, a 2-node or a 3-node, so we have room to insert the new key. To implement this algorithm with red-black BSTs, we</p>
<p class="indenthangingB">• Represent 4-nodes as a balanced subtree of three 2-nodes, with both the left and right child connected to the parent with a red link</p>
<p class="indenthangingB">• Split 4-nodes on the way <em>down</em> the tree with color flips</p>
<p class="indenthangingB">• Balance 4-nodes on the way <em>up</em> the tree with rotations, as for insertion</p>
<p><a id="page_442"/>Remarkably, you can implement top-down 2-3-4 trees by moving one line of code in <code>put()</code> in <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a>: move the <code>colorFlip()</code> call (and accompanying test) to before the recursive calls (between the test for null and the comparison). This algorithm has some advantages over 2-3 trees in applications where multiple processes have access to the same tree, because it always is operating within a link or two of the current node. The deletion algorithms that we describe next are based on a similar scheme and are effective for these trees as well as for 2-3 trees.</p>
<p><a id="ch03sec3lev56"/></p>
<h5><em>Delete the minimum</em></h5>
<p>As a second warmup for deletion, we consider the operation of deleting the minimum from a 2-3 tree. The basic idea is based on the observation that we can easily delete a key from a 3-node at the bottom of the tree, but not from a 2-node. Deleting the key from a 2-node leaves a node with no keys; the natural thing to do would be to replace the node with a null link, but that operation would violate the perfect balance condition. So, we adopt the following approach: to ensure that we do not end up on a 2-node, we perform appropriate transformations on the way down the tree to preserve the invariant that the current node is not a 2-node (it might be a 3-node or a temporary 4-node). First, at the root, there are two possibilities: if the root is a 2-node and both children are 2-nodes, we can just convert the three nodes to a 4-node; otherwise we can borrow from the right sibling if necessary to ensure that the left child of the root is not a 2-node. Then, on the way down the tree, one of the following cases must hold:</p>
<p class="indenthangingB">• If the left child of the current node is not a 2-node, there is nothing to do.</p>
<p class="indenthangingB">• If the left child is a 2-node and its immediate sibling is not a 2-node, move a key from the sibling to the left child.</p>
<p class="indenthangingB">• If the left child and its immediate sibling are 2-nodes, then combine them with the smallest key in the parent to make a 4-node, changing the parent from a 3-node to a 2-node or from a 4-node to a 3-node.</p>
<p class="image"><img alt="image" src="graphics/03_52-ttfdeletemin.jpg"/></p>
<p>Following this process as we traverse left links to the bottom, we wind up on a 3-node or a 4-node with the smallest key, so we can just remove it, converting the 3-node to a <a id="page_443"/>2-node or the 4-node to a 3-node. Then, on the way up the tree, we split any unused temporary 4-nodes.</p>
<p><a id="ch03sec3lev57"/></p>
<h5><em>Delete</em></h5>
<p>The same transformations along the search path just described for deleting the minimum are effective to ensure that the current node is not a 2-node during a search for any key. If the search key is at the bottom, we can just remove it. If the key is not at the bottom, then we have to exchange it with its successor as in regular BSTs. Then, since the current node is not a 2-node, we have reduced the problem to deleting the minimum in a subtree whose root is not a 2-node, and we can use the procedure just described for that subtree. After the deletion, as usual, we split any remaining 4-nodes on the search path on the way up the tree.</p>
<p><small>SEVERAL OF THE EXERCISES</small> at the end of this section are devoted to examples and implementations related to these deletion algorithms. People with an interest in developing or understanding implementations need to master the details covered in these exercises. People with a general interest in the study of algorithms need to recognize that these methods are important because they represent the first symbol-table implementation that we have seen where <em>search</em>, <em>insert</em>, and <em>delete</em> are all guaranteed to be efficient, as we will establish next.</p>
<p><a id="ch03sec2lev23"/></p>
<h4><a id="page_444"/>Properties of red-black BSTs</h4>
<p>Studying the properties of red-black BSTs is a matter of checking the correspondence with 2-3 trees and then applying the analysis of 2-3 trees. The end result is that <em>all symbol-table operations in red-black BSTs are guaranteed to be logarithmic in the size of the tree</em> (except for range search, which additionally costs time proportional to the number of keys returned). We repeat and emphasize this point because of its importance.</p>
<p><a id="ch03sec3lev58"/></p>
<h5><em>Analysis</em></h5>
<p>First, we establish that red-black BSTs, while not perfectly balanced, are always nearly so, regardless of the order in which the keys are inserted. This fact immediately follows from the 1-1 correspondence with 2-3 trees and the defining property of 2-3 trees (perfect balance).</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb26"/></p>
<p><strong>Proposition G.</strong> The height of a red-black BST with <em>N</em> nodes is no more than 2 lg <em>N</em>.</p>
<p><strong>Proof sketch:</strong> The worst case is a 2-3 tree that is all 2-nodes except that the leftmost path is made up of 3-nodes. The path taking left links from the root is twice as long as the paths of length ~ lg <em>N</em> that involve just 2-nodes. It is possible, but not easy, to develop key sequences that cause the construction of red-black BSTs whose average path length is the worst-case 2 lg <em>N.</em> If you are mathematically inclined, you might enjoy exploring this issue by working <a href="#ch03qa3q24"><small>EXERCISE 3.3.24</small></a>.</p>
<hr/>
</div>
<p>This upper bound is conservative: experiments involving both random insertions and insertion sequences found in typical applications support the hypothesis that each search in a red-black BST of <em>N</em> nodes uses about 1.00 lg <em>N</em> − .5 compares, on the average. Moreover, you are not likely to encounter a substantially higher average number of compares in practice.</p>
<p class="image"><img alt="image" src="graphics/03_53-llrb256.jpg"/></p>
<p class="image"><a id="page_445"/><img alt="image" src="graphics/t0445-01.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb27"/></p>
<p><strong>Property H.</strong> The average length of a path from the root to a node in a red-black BST with <em>N</em> nodes is ~1.00 lg <em>N</em>.</p>
<p><strong>Evidence:</strong> Typical trees, such as the one at the bottom of the previous page (and even the one built by inserting keys in increasing order at the bottom of this page) are quite well-balanced, by comparison with typical BSTs (such as the tree depicted on page <a href="#page_405">405</a>). The table at the top of this page shows that path lengths (search costs) for our <code>FrequencyCounter</code> application are about 40 percent lower than from elementary BSTs, as expected. This performance has been observed in countless applications and experiments since the invention of red-black BSTs.</p>
<hr/>
</div>
<p>For our example study of the cost of the <code>put()</code> operations for <code>FrequencyCounter</code> for words of length 8 or more, we see a further reduction in the average cost, again providing a quick validation of the logarithmic performance predicted by the theoretical model, though this validation is less surprising than for BSTs because of the guarantee provided by <a href="#ch03sb26">PROPERTY G</a>. The total savings is less than the 40 per cent savings in the search cost because we count rotations and color flips as well as compares.</p>
<p class="image"><img alt="image" src="graphics/03_54-llrb130inorder.jpg"/></p>
<p class="image"><a id="page_446"/><img alt="image" src="graphics/03_55-freqc.jpg"/></p>
<p>The <code>get()</code> method in red-black BSTs does not examine the node color, so the balancing mechanism adds no overhead; search is faster than in elementary BSTs because the tree is balanced. Each key is inserted just once, but may be involved in many, many search operations, so the end result is that we get search times that are close to optimal (because the trees are nearly balanced and no work for balancing is done during the searches) at relatively little cost (unlike binary search, insertions are guaranteed to be logarithmic). The inner loop of the search is a compare followed by updating a link, which is quite short, like the inner loop of binary search (compare and index arithmetic). This implementation is the first we have seen with logarithmic guarantees for both search and insert, and it has a tight inner loop, so its use is justified in a broad variety of applications, including library implementations.</p>
<p><a id="ch03sec3lev59"/></p>
<h5><em>Ordered symbol-table API</em></h5>
<p>One of the most appealing features of red-black BSTs is that the complicated code is limited to the <code>put()</code> (and deletion) methods. Our code for the minimum/maximum, select, rank, floor, ceiling and range queries in standard BSTs can be used <em>without any change</em>, since it operates on BSTs and has no need to refer to the node color. <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a>, together with these methods (and the deletion methods), leads to a complete implementation of our ordered symbol-table API. Moreover, all of the methods benefit from the near-perfect balance in the tree because they all require time proportional to the tree height, at most. Thus <a href="#ch03sb26"><small>PROPOSITION G</small></a>, in combination with <a href="#ch03sb21"><small>PROPOSITION E</small></a>, suffices to establish a logarithmic performance guarantee for <em>all</em> of them.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb28"/></p>
<p><a id="page_447"/><strong>Proposition I.</strong> In a red-black BST, the following operations take logarithmic time in the worst case: search, insertion, finding the minimum, finding the maximum, floor, ceiling, rank, select, delete the minimum, delete the maximum, delete, and range count.</p>
<p><strong>Proof:</strong> We have just discussed <code>get()</code>, <code>put()</code>, and the deletion operations. For the others, the code from <a href="#ch03sec1lev2"><small>SECTION 3.2</small></a> can be used <em>verbatim</em> (it just ignores the node color). Guaranteed logarithmic performance follows from <a href="#ch03sb21"><small>PROPOSITIONS E</small></a> and <a href="#ch03sb26"><small>G</small></a>, and the fact that each algorithm performs a constant number of operations on each node examined.</p>
<hr/>
</div>
<p>On reflection, it is quite remarkable that we are able to achieve such guarantees. In a world awash with information, where people maintain tables with trillions or quadrillions of entries, the fact is that we can guarantee to complete any one of these operations in such tables with just a few dozen compares.</p>
<p class="image"><img alt="image" src="graphics/t0447-01.jpg"/></p>
<p><a id="ch03sec2lev24"/></p>
<h4><a id="page_448"/>Q&amp;A</h4>
<p><strong>Q.</strong> Why not let the 3-nodes lean either way and also allow 4-nodes in the trees?</p>
<p><strong>A.</strong> Those are fine alternatives, used by many for decades. You can learn about several of these alternatives in the exercises. The left-leaning convention reduces the number of cases and therefore requires substantially less code.</p>
<p><strong>Q.</strong> Why not use an array of <code>Key</code> values to represent 2-, 3-, and 4-nodes with a single <code>Node</code> type?</p>
<p><strong>A.</strong> Good question. That is precisely what we do for B-trees (see <a href="ch06.html#ch06"><small>CHAPTER 6</small></a>), where we allow many more keys per node. For the small nodes in 2-3 trees, the overhead for the array is too high a price to pay.</p>
<p><strong>Q.</strong> When we split a 4-node, we sometimes set the color of the right node to <code>RED</code> in <code>rotateRight()</code> and then immediately set it to <code>BLACK</code> in <code>flipColors()</code>. Isn’t that wasteful?</p>
<p><strong>A.</strong> Yes, and we also sometimes unnecessarily recolor the middle node. In the grand scheme of things, resetting a few extra bits is not in the same league with the improvement from linear to logarithmic that we get for all operations, but in performance-critical applications, you can put the code for <code>rotateRight()</code> and <code>flipColors()</code> inline and eliminate those extra tests. We use those methods for deletion, as well, and find them slightly easier to use, understand, and maintain by making sure that they preserve perfect black balance.</p>
<p><a id="ch03sec2lev25"/></p>
<h4><a id="page_449"/>Exercises</h4>
<p><a id="ch03qa3q1"/><strong>3.3.1</strong> Draw the 2-3 tree that results when you insert the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty tree.</p>
<p><a id="ch03qa3q2"/><strong>3.3.2</strong> Draw the 2-3 tree that results when you insert the keys <code>Y L P M X H C R A E S</code> in that order into an initially empty tree.</p>
<p><a id="ch03qa3q3"/><strong>3.3.3</strong> Find an insertion order for the keys <code>S E A R C H X M</code> that leads to a 2-3 tree of height 1.</p>
<p><a id="ch03qa3q4"/><strong>3.3.4</strong> Prove that the height of a 2-3 tree with <em>N</em> keys is between ~ <img alt="image" src="graphics/lfloor.jpg"/>log<sub>3</sub> <em>N</em><img alt="image" src="graphics/rfloor.jpg"/> ≈ .63 lg <em>N</em> (for a tree that is all 3-nodes) and ~<img alt="image" src="graphics/lfloor.jpg"/>lg <em>N</em><img alt="image" src="graphics/rfloor.jpg"/> (for a tree that is all 2-nodes).</p>
<p class="image"><img alt="image" src="graphics/03_56-ex5.jpg"/></p>
<p><a id="ch03qa3q5"/><strong>3.3.5</strong> The figure at right shows all the <em>structurally different</em> 2-3 trees with <em>N</em> keys, for <em>N</em> from 1 up to 6 (ignore the order of the subtrees). Draw all the structurally different trees for <em>N</em> = 7, 8, 9, and 10.</p>
<p><a id="ch03qa3q6"/><strong>3.3.6</strong> Find the probability that each of the 2-3 trees in <a href="#ch03qa3q5"><small>EXERCISE 3.3.5</small></a> is the result of the insertion of <em>N</em> random distinct keys into an initially empty tree.</p>
<p><a id="ch03qa3q7"/><strong>3.3.7</strong> Draw diagrams like the one at the top of page <a href="#page_428">428</a> for the other five cases in the diagram at the bottom of that page.</p>
<p><a id="ch03qa3q8"/><strong>3.3.8</strong> Show all possible ways that one might represent a 4-node with three 2-nodes bound together with red links (not necessarily left-leaning).</p>
<p><a id="ch03qa3q9"/><strong>3.3.9</strong> Which of the following are red-black BSTs?</p>
<p class="image"><img alt="image" src="graphics/03_57-ex8.jpg"/></p>
<p><a id="ch03qa3q10"/><strong>3.3.10</strong> Draw the red-black BST that results when you insert items with the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty tree.</p>
<p><a id="ch03qa3q11"/><strong>3.3.11</strong> Draw the red-black BST that results when you insert items with the keys <code>Y L P M X H C R A E S</code> in that order into an initially empty tree.</p>
<p><a id="page_450"/><a id="ch03qa3q12"/><strong>3.3.12</strong> Draw the red-black BST that results after each transformation (color flip or rotation) during the insertion of <code>P</code> for our standard indexing client.</p>
<p><a id="ch03qa3q13"/><strong>3.3.13</strong> True or false: If you insert keys in increasing order into a red-black BST, the tree height is monotonically increasing.</p>
<p><a id="ch03qa3q14"/><strong>3.3.14</strong> Draw the red-black BST that results when you insert letters <code>A</code> through <code>K</code> in order into an initially empty tree, then describe what happens in general when trees are built by insertion of keys in ascending order (see also the figure in the text).</p>
<p><a id="ch03qa3q15"/><strong>3.3.15</strong> Answer the previous two questions for the case when the keys are inserted in <em>descending</em> order.</p>
<p class="image"><img alt="image" src="graphics/03_58-rbinsertgeneral.jpg"/></p>
<p><a id="ch03qa3q16"/><strong>3.3.16</strong> Show the result of inserting <code>n</code> into the red-black BST drawn at right (only the search path is shown, and you need to include only these nodes in your answer).</p>
<p><a id="ch03qa3q17"/><strong>3.3.17</strong> Generate two random 16-node red-black BSTs. Draw them (either by hand or with a program). Compare them with the (unbalanced) BSTs built with the same keys.</p>
<p><a id="ch03qa3q18"/><strong>3.3.18</strong> Draw all the structurally different red-black BSTs with <em>N</em> keys, for <em>N</em> from 2 up to 10 (see <a href="#ch03qa3q5"><small>EXERCISE 3.3.5</small></a>).</p>
<p><a id="ch03qa3q19"/><strong>3.3.19</strong> With 1 bit per node for color, we can represent 2-, 3-, and 4-nodes. How many bits per node would we need to represent 5-, 6-, 7-, and 8-nodes with a binary tree?</p>
<p><a id="ch03qa3q20"/><strong>3.3.20</strong> Compute the internal path length in a perfectly balanced BST of <em>N</em> nodes, when <em>N</em> is a power of 2 minus 1.</p>
<p><a id="ch03qa3q21"/><strong>3.3.21</strong> Create a test client for <code>RedBlackBST</code>, based on your solution to <a href="#ch03qa2q10"><small>EXERCISE 3.2.10</small></a>.</p>
<p><a id="ch03qa3q22"/><strong>3.3.22</strong> Find a sequence of keys to insert into a BST and into a red-black BST such that the height of the BST is less than the height of the red-black BST, or prove that no such sequence is possible.</p>
<p><a id="ch03sec2lev26"/></p>
<h4><a id="page_451"/>Creative Problems</h4>
<p><a id="ch03qa3q23"/><strong>3.3.23</strong> <em>2-3 trees without balance restriction.</em> Develop an implementation of the basic symbol-table API that uses 2-3 trees that are not necessarily balanced as the underlying data structure. Allow 3-nodes to lean either way. Hook the new node onto the bottom with a <em>black</em> link when inserting into a 3-node at the bottom. Run experiments to develop a hypothesis estimating the average path length in a tree built from <em>N</em> random insertions.</p>
<p><a id="ch03qa3q24"/><strong>3.3.24</strong> <em>Worst case for red-black BSTs.</em> Show how to construct a red-black BST demonstrating that, in the worst case, almost all the paths from the root to a null link in a red-black BST of <em>N</em> nodes are of length 2 lg <em>N</em>.</p>
<p><a id="ch03qa3q25"/><strong>3.3.25</strong> <em>Top-down 2-3-4 trees.</em> Develop an implementation of the basic symbol-table API that uses balanced 2-3-4 trees as the underlying data structure, using the red-black representation and the insertion method described in the text, where 4-nodes are split by flipping colors on the way down the search path and balancing on the way up.</p>
<p><a id="ch03qa3q26"/><strong>3.3.26</strong> <em>Single top-down pass.</em> Develop a modified version of your solution to <a href="#ch03qa3q25"><small>EXERCISE 3.3.25</small></a> that does <em>not</em> use recursion. Complete all the work splitting and balancing 4-nodes (and balancing 3-nodes) on the way down the tree, finishing with an insertion at the bottom.</p>
<p><a id="ch03qa3q27"/><strong>3.3.27</strong> <em>Allow right-leaning red links.</em> Develop a modified version of your solution to <a href="#ch03qa3q25"><small>EXERCISE 3.3.25</small></a> that allows right-leaning red links in the tree.</p>
<p><a id="ch03qa3q28"/><strong>3.3.28</strong> <em>Bottom-up 2-3-4 trees.</em> Develop an implementation of the basic symbol-table API that uses balanced 2-3-4 trees as the underlying data structure, using the red-black representation and a <em>bottom-up</em> insertion method based on the same recursive approach as <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a>. Your insertion method should split only the sequence of 4-nodes (if any) on the bottom of the search path.</p>
<p><a id="ch03qa3q29"/><strong>3.3.29</strong> <em>Optimal storage.</em> Modify <code>RedBlackBST</code> so that it does not use any extra storage for the color bit, based on the following trick: To color a node red, swap its two links. Then, to test whether a node is red, test whether its left child is larger than its right child. You have to modify the compares to accommodate the possible link swap, and this trick replaces bit compares with key compares that are presumably more expensive, but it shows that the bit in the nodes can be eliminated, if necessary.</p>
<p><a id="ch03qa3q30"/><strong>3.3.30</strong> <em>Sofware caching.</em> Modify <code>RedBlackBST</code> to keep the most recently accessed <code>Node</code> in an instance variable so that it can be accessed in constant time if the next <code>put()</code> or <a id="page_452"/><code>get()</code> uses the same key (see <a href="#ch03qa1q25"><small>EXERCISE 3.1.25</small></a>).</p>
<p><a id="ch03qa3q31"/><strong>3.3.31</strong> <em>Tree drawing.</em> Add a method <code>draw()</code> to <code>RedBlackBST</code> that draws red-black BST figures in the style of the text (see <a href="#ch03qa2q38"><small>EXERCISE 3.2.38</small></a>)</p>
<p><a id="ch03qa3q32"/><strong>3.3.32</strong> <em>AVL trees.</em> An <em>AVL tree</em> is a BST where the height of every node and that of its sibling differ by at most 1. (The oldest balanced tree algorithms are based on using rotations to maintain height balance in AVL trees.) Show that coloring red links that go from nodes of even height to nodes of odd height in an AVL tree gives a (perfectly balanced) 2-3-4 tree, where red links are not necessarily left-leaning. <em>Extra credit</em>: Develop an implementation of the symbol-table API that uses this as the underlying data structure. One approach is to keep a height field in each node, using rotations after the recursive calls to adjust the height as necessary; another is to use the red-black representation and use methods like <code>moveRedLeft()</code> and <code>moveRedRight()</code> in <a href="#ch03qa3q39"><small>EXERCISE 3.3.39</small></a> and <a href="#ch03qa3q40"><small>EXERCISE 3.3.40</small></a>.</p>
<p><a id="ch03qa3q33"/><strong>3.3.33</strong> <em>Certification.</em> Add to <code>RedBlackBST</code> a method <code>is23()</code> to check that no node is connected to two red links and that there are no right-leaing red links and a method <code>isBalanced()</code> to check that all paths from the root to a null link have the same number of black links. Combine these methods with code from <code>isBST()</code> in <a href="#ch03qa2q31"><small>EXERCISE 3.2.31</small></a> to create a method <code>isRedBlackBST()</code> that checks that the tree is a red-black BST.</p>
<p><a id="ch03qa3q34"/><strong>3.3.34</strong> <em>All 2-3 trees.</em> Write code to generate all structurally different 2-3 trees of height 2, 3, and 4. There are 2, 7, and 122 such trees, respectively. (<em>Hint</em>: Use a symbol table.)</p>
<p><a id="ch03qa3q35"/><strong>3.3.35</strong> <em>2-3 trees.</em> Write a program <code>TwoThreeST.java</code> that uses two node types to implement 2-3 search trees directly.</p>
<p><a id="ch03qa3q36"/><strong>3.3.36</strong> <em>2-3-4-5-6-7-8 trees.</em> Describe algorithms for search and insertion in balanced 2-3-4-5-6-7-8 search trees.</p>
<p><a id="ch03qa3q37"/><strong>3.3.37</strong> <em>Memoryless.</em> Show that red-black BSTs are <em>not memoryless</em>: for example, if you insert a key that is smaller than all the keys in the tree and then immediately delete the minimum, you may get a different tree.</p>
<p><a id="ch03qa3q38"/><strong>3.3.38</strong> <em>Fundamental theorem of rotations.</em> Show that any BST can be transformed into any other BST on the same set of keys by a sequence of left and right rotations.</p>
<p><a id="page_453"/><a id="ch03qa3q39"/><strong>3.3.39</strong> <em>Delete the minimum.</em> Implement the <code>deleteMin()</code> operation for red-black BSTs by maintaining the correspondence with the transformations given in the text for moving down the left spine of the tree while maintaining the invariant that the current node is not a 2-node.</p>
<p><em>Solution</em>:</p>
<p class="programlisting3"><img alt="image" src="graphics/p0453-01.jpg"/></p>
<p>This code assumes a <code>balance()</code> method that consists of the line of code</p>
<p class="programlisting">if (isRed(h.right)) h = rotateLeft(h);</p>
<p><a id="page_454"/>followed by the last five lines of the recursive <code>put()</code> in <a href="#ch03sb25"><small>ALGORITHM 3.4</small></a> and a <code>flipColors()</code> implementation that complements the three colors, instead of the method given in the text for insertion. For deletion, we set the parent to <code>BLACK</code> and the two children to <code>RED</code>.</p>
<p><a id="ch03qa3q40"/><strong>3.3.40</strong> <em>Delete the maximum.</em> Implement the <code>deleteMax()</code> operation for red-black BSTs. Note that the transformations involved differ slightly from those in the previous exercise because red links are left-leaning.</p>
<p><em>Solution</em>:</p>
<p class="programlisting3"><img alt="image" src="graphics/p0454-01.jpg"/></p>
<p><a id="page_455"/><a id="ch03qa3q41"/><strong>3.3.41</strong> <em>Delete.</em> Implement the <code>delete()</code> operation for red-black BSTs, combining the methods of the previous two exercises with the <code>delete()</code> operation for BSTs.</p>
<p><em>Solution</em>:</p>
<p class="programlisting3"><img alt="image" src="graphics/p0455-01.jpg"/></p>
<p><a id="ch03sec2lev27"/></p>
<h4><a id="page_456"/>Experiments</h4>
<p><a id="ch03qa3q42"/><strong>3.3.42</strong> <em>Count red nodes.</em> Write a program that computes the percentage of red nodes in a given red-black BST. Test your program by running at least 100 trials of the experiment of inserting <em>N</em> random keys into an initially empty tree, for N = 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>, and formulate an hypothesis.</p>
<p><a id="ch03qa3q43"/><strong>3.3.43</strong> <em>Cost plots.</em> Instrument <code>RedBlackBST</code> so that you can produce plots like the ones in this section showing the cost of each <code>put()</code> operation during the computation (see <a href="#ch03qa1q38"><small>EXERCISE 3.1.38</small></a>).</p>
<p><a id="ch03qa3q44"/><strong>3.3.44</strong> <em>Average search time.</em> Run empirical studies to compute the average and standard deviation of the average length of a path to a random node (internal path length divided by tree size) in a red-black BST built by insertion of <em>N</em> random keys into an initially empty tree, for <em>N</em> from 1 to 10,000. Do at least 1,000 trials for each tree size. Plot the results in a Tufte plot, like the one at the bottom of this page, fit with a curve plotting the function lg <em>N <strong>−</strong></em> .5.</p>
<p><a id="ch03qa3q45"/><strong>3.3.45</strong> <em>Count rotations.</em> Instrument your program for <a href="#ch03qa3q43"><small>EXERCISE 3.3.43</small></a> to plot the number of rotations and node splits that are used to build the trees. Discuss the results.</p>
<p><a id="ch03qa3q46"/><strong>3.3.46</strong> <em>Height.</em> Instrument your program for <a href="#ch03qa3q43"><small>EXERCISE 3.3.43</small></a> to plot the height of red-black BSTs. Discuss the results.</p>
<p class="image"><img alt="image" src="graphics/03_59-avgrb.jpg"/></p>
<p><a id="ch03sec1lev4"/></p>
<h3><a id="page_458"/>3.4 Hash Tables</h3>
<p>If keys are small integers, we can use an array to implement an unordered symbol table, by interpreting the key as an array index so that we can store the value associated with key <code>i</code> in array entry <code>i</code>, ready for immediate access. In this section, we consider <em>hashing</em>, an extension of this simple method that handles more complicated types of keys. We reference key-value pairs using arrays by doing arithmetic operations to transform keys into array indices.</p>
<p>Search algorithms that use hashing consist of two separate parts. The first part is to compute a <em>hash function</em> that transforms the search key into an array index. Ideally, different keys would map to different indices. This ideal is generally beyond our reach, so we have to face the possibility that two or more different keys may hash to the same array index. Thus, the second part of a hashing search is a <em>collision-resolution</em> process that deals with this situation. After describing ways to compute hash functions, we shall consider two different approaches to collision resolution: <em>separate chaining</em> and <em>linear probing</em>.</p>
<p class="image"><img alt="image" src="graphics/03_60-hashcrux.jpg"/></p>
<p>Hashing is a classic example of a <em>time-space tradeoff</em>. If there were no memory limitation, then we could do any search with only one memory access by simply using the key as an index in a (potentially huge) array. This ideal often cannot be achieved, however, because the amount of memory required is prohibitive when the number of possible key values is huge. On the other hand, if there were no time limitation, then we can get by with only a minimum amount of memory by using sequential search in an unordered array. Hashing provides a way to use a reasonable amount of both memory and time to strike a balance between these two extremes. Indeed, it turns out that we can trade off time and memory in hashing algorithms by adjusting parameters, not by rewriting code. To help choose values of the parameters, we use classical results from probability theory.</p>
<p>Probability theory is a triumph of mathematical analysis that is beyond the scope of this book, but the hashing algorithms we consider that take advantage of the knowledge gained from that theory are quite simple, and widely used. With hashing, you can implement search and insert for symbol tables that require <em>constant</em> (amortized) time per operation in typical applications, making it the method of choice for implementing basic symbol tables in many situations.</p>
<p><a id="ch03sec2lev28"/></p>
<h4><a id="page_459"/>Hash functions</h4>
<p>The first problem that we face is the computation of the hash function, which transforms keys into array indices. If we have an array that can hold <em>M</em> key-value pairs, then we need a <em>hash function</em> that can transform any given key into an index into that array: an integer in the range [0, <em>M <strong>−</strong></em> 1]. We seek a hash function that both is easy to compute and uniformly distributes the keys: for each key, every integer between 0 and <em>M <strong>−</strong></em> 1 should be equally likely (independently for every key). This ideal is somewhat mysterious; to understand hashing, we to begin by thinking carefully about how to implement such a function.</p>
<p class="image"><img alt="image" src="graphics/t0459-01.jpg"/></p>
<p>In principle, any key can be represented as a sequence of bits, so we might design a generic hash function that maps sequences of bits to integers in the desired range. In practice, programmers implement hash functions based on higher-level representations. For example, if the key involves a number, such as a social security number, we could start with that number; if the key involves a string, such as a person’s name, we need to convert the string into a number; and if the key has multiple parts, such as a mailing address, we need to combine the parts somehow. For many common types of keys, we can make use of default implementations provided by Java. We briefl y discuss potential implementations for various types of keys so that you can see what is involved because you do need to provide implementations for key types that you create.</p>
<p><a id="ch03sec3lev60"/></p>
<h5><em>Typical example</em></h5>
<p>Suppose that we have an application where the keys are U.S. social security numbers. A social security number such as <code>123-45-6789</code> is a nine-digit number divided into three fields. The first field identifies the geographical area where the number was issued (for example, social security numbers whose first field is <code>035</code> are from Rhode Island and numbers whose first field is <code>214</code> are from Maryland) and the other two fields identify the individual. There are a billion (10<sup>9</sup>) different social security numbers, but suppose that our application will need to process just a few hundred keys, so that we could use a hash table of size <em>M =</em> 1,000. One possible approach to implementing a hash function is to use three digits from the key. Using three digits from the third field is likely to be preferable to using the three digits in the first field (since customers may not be uniformly dispersed over geographic areas), but a better approach is to use all nine digits to make an <code>int</code> value, then consider hash functions for integers, described next.</p>
<p><a id="ch03sec3lev61"/></p>
<h5><em>Positive integers</em></h5>
<p>The most commonly used method for hashing integers is called <em>modular hashing</em>: we choose the array size <em>M</em> to be prime and, for any positive integer key <em>k</em>, compute the remainder when dividing <em>k</em> by <em>M</em>. This function is very easy to compute (<code>k % M</code>, in Java) and is effective in dispersing the keys evenly between 0 and <a id="page_460"/><em>M <strong>−</strong></em> 1. If <em>M</em> is not prime, it may be the case that not all of the bits of the key play a role, which amounts to missing an opportunity to disperse the values evenly. For example, if the keys are base-10 numbers and <em>M</em> is 10<em><sup>k</sup></em>, then only the <em>k</em> least significant digits are used. As a simple example where such a choice might be problematic, suppose that the keys are telephone area codes and <em>M</em> = 100. For historical reasons, most area codes in the United States have middle digit 0 or 1, so this choice strongly favors the values less than 20, where the use of the prime value 97 better disperses them (a prime value not close to 100 would do even better). Similarly, IP addresses that are used in the internet are binary numbers that are not random for similar historical reasons as for telephone area codes, so we need to use a table size that is a prime (in particular, <em>not</em> a power of 2) if we want to use modular hashing to disperse them.</p>
<p><a id="ch03sec3lev62"/></p>
<h5><em>Floating-point numbers</em></h5>
<p>If the keys are real numbers between 0 and 1, we might just multiply by <em>M</em> and round off to the nearest integer to get an index between 0 and <em>M <strong>−</strong></em> 1. Although this approach is intuitive, it is defective because it gives more weight to the most significant bits of the keys; the least significant bits play no role. One way to address this situation is to use modular hashing on the binary representation of the key (this is what Java does).</p>
<p><a id="ch03sec3lev63"/></p>
<h5><em>Strings</em></h5>
<p>Modular hashing works for long keys such as strings, too: we simply treat them as huge integers. For example, the code at left computes a modular hash function for a <code>String s</code>: recall that <code>charAt()</code> returns a <code>char</code> value in Java, which is a 16-bit nonnegative integer. If <code>R</code> is greater than any character value, this computation would be equivalent to treating the <code>String</code> as an <code>N</code>-digit base-<code>R</code> integer, computing the remainder that results when dividing that number by <code>M</code>. A classic algorithm known as <em>Horner’s method</em> gets the job done with <code>N</code> multiplications, additions, and remainder operations. If the value of <code>R</code> is sufficiently small that no overflow occurs, the result is an integer between <code>0</code> and <code>M − 1</code>, as desired. The use of a small prime integer such as 31 ensures that the bits of all the characters play a role. Java’s default implementation for <code>String</code> uses a method like this.</p>
<p class="image"><img alt="image" src="graphics/p0460-01.jpg"/></p>
<p><a id="ch03sec3lev64"/></p>
<h5><em>Compound keys</em></h5>
<p>If the key type has multiple integer fields, we can typically mix them together in the way just described for <code>String</code> values. For example, suppose that search keys are of type <code>Date</code>, which has three integer fields: <code>day</code> (two-digit day), <code>month</code> (two-digit month), and <code>year</code> (four-digit year). We compute the number</p>
<p class="programlisting">int hash = (((day * R + month) % M ) * R + year) % M;</p>
<p><a id="page_461"/>which, if the value of <code>R</code> is sufficiently small that no overflow occurs, is an integer between <code>0</code> and <code>M − 1</code>, as desired. In this case, we could save the cost of the inner <code>% M</code> operation by choosing a moderate prime value such as <code>31</code> for <code>R</code>. As with strings, this method generalizes to handle any number of fields.</p>
<p><a id="ch03sec3lev65"/></p>
<h5><em>Java conventions</em></h5>
<p>Java helps us address the basic problem that every type of data needs a hash function by ensuring that every data type inherits a method called <code>hashCode()</code> that returns a 32-bit integer. The implementation of <code>hashCode()</code> for a data type must be <em>consistent with equals</em>. That is, if <code>a.equals(b)</code> is true, then <code>a.hashCode()</code> must have the same numerical value as <code>b.hashCode()</code>. Conversely, if the <code>hashCode()</code> values are different, then we know that the objects are not equal. If the <code>hashCode()</code> values are the same, the objects may or may not be equal, and we must use <code>equals()</code> to decide which condition holds. This convention is a basic requirement for clients to be able to use <code>hashCode()</code> for symbol tables. Note that it implies that you must override both <code>hashCode()</code> and <code>equals()</code> if you need to hash with a user-defined type. The default implementation returns the machine address of the key object, which is seldom what you want. Java provides <code>hashCode()</code> implementations that override the defaults for many common types (including <code>String</code>, <code>Integer</code>, <code>Double</code>, <code>File</code>, and <code>URL</code>).</p>
<p><a id="ch03sec3lev66"/></p>
<h5><em>Converting a</em> <code>hashCode()</code> <em>to an array index</em></h5>
<p>Since our goal is an array index, not a 32-bit integer, we combine <code>hashCode()</code> with modular hashing in our implementations to produce integers between <code>0</code> and <code>M − 1</code>, as follows:</p>
<p class="programlisting"><img alt="image" src="graphics/p0461-01.jpg"/></p>
<p>This code masks off the sign bit (to turn the 32-bit number into a 31-bit nonnegative integer) and then computes the remainder when dividing by <code>M</code>, as in modular hashing. Programmers commonly use a <em>prime</em> number for the hash table size <code>M</code> when using code like this, to attempt to make use of all the bits of the hash code. <em>Note</em>: To avoid confusion, we omit all of these calculations in our hashing examples and use instead the hash values in the table at right.</p>
<p class="image"><img alt="image" src="graphics/t0461-01.jpg"/></p>
<p><a id="ch03sec3lev67"/></p>
<h5><em>User-defined</em> <code>hashCode()</code></h5>
<p>Client code expects that <code>hashCode()</code> disperses the keys uniformly among the possible 32-bit result values. That is, for any object <code>x</code>, you can write <code>x.hashCode()</code> and, in principle, expect to get any one of the 2<sup>32</sup> possible 32-bit values with equal likelihood. Java’s <code>hashCode()</code> implementations for <code>String</code>, <code>Integer</code>, <code>Double</code>, <code>File</code>, and <code>URL</code> aspire to this functionality; for your own type, you have to try to do it on your own. The <code>Date</code> example that we considered on page <a href="#page_460">460</a> illustrates <a id="page_462"/>one way to proceed: make integers from the instance variables and use modular hashing. In Java, the convention that all data types inherit a <code>hashCode()</code> method enables an even simpler approach: use the <code>hashCode()</code> method for the instance variables to convert each to a 32-bit <code>int</code> value and then do the arithmetic, as illustrated at left for <code>Transaction</code>. For primitive-type instance variables, note that a cast to a wrapper type is necessary to access the <code>hashCode()</code> method. Again, the precise values of the multiplier (<code>31</code> in our example) is not particularly important.</p>
<p class="image"><img alt="image" src="graphics/p0462-01.jpg"/></p>
<p><a id="ch03sec3lev68"/></p>
<h5><em>Software caching</em></h5>
<p>If computing the hash code is expensive, it may be worthwhile to <em>cache the hash</em> for each key. That is, we maintain an instance variable <code>hash</code> in the key type that contains the value of <code>hashCode()</code> for each key object (see <a href="#ch03qa4q25"><small>EXERCISE 3.4.25</small></a>). On the first call to <code>hashCode()</code>, we have to compute the full hash code (and set the value of <code>hash</code>), but subsequent calls on <code>hashCode()</code> simply return the value of <code>hash</code>. Java uses this technique to reduce the cost of computing <code>hashCode()</code> for <code>String</code> objects.</p>
<p><small>IN SUMMARY, WE HAVE THREE PRIMARY REQUIREMENTS</small> in implementing a good hash function for a given data type:</p>
<p class="indenthangingB">• It should be <em>consistent</em>—equal keys must produce the same hash value.</p>
<p class="indenthangingB">• It should be <em>efficient to compute</em>.</p>
<p class="indenthangingB">• It should <em>uniformly distribute the set of keys</em>.</p>
<p>Satisfying these requirements simultaneously in Java is a job for experts. As with many built-in capabilities, Java programmers who use hashing assume that <code>hashCode()</code> does the job, absent any evidence to the contrary.</p>
<p>Still, you should be vigilant whenever using hashing in situations where good performance is critical, because a bad hash function is a classic example of a <em>performance bug</em>: everything will work properly, but much more slowly than expected. Perhaps the easiest way to ensure uniformity is to make sure that all the bits of the key play an equal role in computing every hash value; perhaps the most common mistake in implementing hash functions is to ignore significant numbers of the key bits. Whatever the implementation, it is wise to test any hash function that you use, when performance is important. Which takes more time: computing a hash function or comparing two keys? Does your <a id="page_463"/>hash function spread a typical set of keys uniformly among the values between 0 and <em>M <strong>−</strong></em> 1? Doing simple experiments that answer these questions can protect future clients from unfortunate surprises. For example, the histogram above shows that our <code>hash()</code> implementation using the <code>hashCode()</code> from Java’s <code>String</code> data type produces a reasonable dispersion of the words for our <em>Tale of Two Cities</em> example.</p>
<p class="image"><img alt="image" src="graphics/03_61-hashfreqs.jpg"/></p>
<p>Underlying this discussion is a fundamental assumption that we make when using hashing; it is an idealized model that we do not actually expect to achieve, but it guides our thinking when implementing hashing algorithms and facilitates their analyses:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb29"/></p>
<p><strong>Assumption J</strong> (<em>uniform hashing assumption</em>). The hash functions that we use uniformly and independently distribute keys among the integer values between 0 and <em>M <strong>−</strong></em> 1.</p>
<p><strong>Discussion:</strong> With all of the arbitrary choices we have made, the Java hash functions that we have considered do not satisfy these conditions; nor can any <em>deterministic</em> hash function. The idea of constructing hash functions that uniformly and independently distribute keys leads to deep issues in theoretical computer science. In 1977, L. Carter and M. Wegman described how to construct a <em>universal</em> family of hash functions. If a hash function is chosen at random from a universal family, the hash function uniformly distributes the keys, but only with partial independence. Although weaker than full independence, the partial independence is sufficient to establish performance guarantees similar to those stated in <code>Propositions K</code> and <code>M</code>.</p>
<hr/>
</div>
<p><a href="#ch03sb29"><small>ASSUMPTION J</small></a> is a useful way to think about hashing for two primary reasons. First, it is a worthy goal when designing hash functions that guides us away from making arbitrary decisions that might lead to an excessive number of collisions. Second, we will use it to develop hypotheses about the performance of hashing —even when hash functions are not known to satisfy <small>ASSUMPTION J</small>, we can perform computational experiments and validate that they achieve the predicted performance.</p>
<p><a id="ch03sec2lev29"/></p>
<h4><a id="page_464"/>Hashing with separate chaining</h4>
<p>A hash function converts keys into array indices. The second component of a hashing algorithm is <em>collision resolution</em>: a strategy for handling the case when two or more keys to be inserted hash to the same index. A straightforward and general approach to collision resolution is to build, for each of the <em>M</em> array indices, a linked list of the key-value pairs whose keys hash to that index. This method is known as <em>separate chaining</em> because items that collide are chained together in separate linked lists. The basic idea is to choose <em>M</em> to be sufficiently large that the lists are sufficiently short to enable efficient search through a two-step process: hash to find the list that could contain the key, then sequentially search through that list for the key.</p>
<p>One way to proceed is to expand <code>SequentialSearchST</code> (<a href="#ch03sb05"><small>ALGORITHM 3.1</small></a>) to implement separate chaining using linked-list primitives (see <a href="#ch03qa4q2"><small>EXERCISE 3.4.2</small></a>). A simpler (though slightly less efficient) way to proceed is to adopt a more general approach: we build, for each of the <em>M</em> array indices, a <em>symbol table</em> of the keys that hash to that index, thus reusing code that we have already developed. The implementation <code>SeparateChainingHashST</code> in <a href="#ch03sb30"><small>ALGORITHM 3.5</small></a> maintains an array of <code>SequentialSearchST</code> objects and implements <code>get()</code> and <code>put()</code> by computing a hash function to choose which <code>SequentialSearchST</code> object can contain the key and then using <code>get()</code> and <code>put()</code> (respectively) from <code>SequentialSearchST</code> to complete the job.</p>
<p class="image"><img alt="image" src="graphics/03_62-hashsctrace.jpg"/></p>
<p>Since we have <em>M</em> lists and <em>N</em> keys, the average length of the lists is <em>always N</em> / <em>M</em>, no matter how the keys are distributed among the lists. For example, suppose that all the items fall onto the first list—the average length of the lists is (<em>N</em> + 0 + 0 + 0 + . . . + 0)/<em>M</em> = <em>N</em> / <em>M</em>. <em>However</em> the keys are distributed on the lists, the sum of the list lengths is <em>N</em> and the average is <em>N</em> / <em>M</em>. Separate chaining is useful in practice because <em>each</em> list is <em>extremely likely</em> to have about <em>N</em> / <em>M</em> key-value pairs. In typical situations, we can verify this consequence of <a href="#ch03sb29"><small>ASSUMPTION J</small></a> and count on fast search and insert.</p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb30"/></p>
<h3><a id="page_465"/>Algorithm 3.5 Hashing with separate chaining</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0465-01.jpg"/></p>
<p>This basic symbol-table implementation maintains an array of linked lists, using a hash function to choose a list for each key. For simplicity, we use <code>SequentialSearchST</code> methods. We need a cast when creating <code>st[]</code> because Java prohibits arrays with generics. The default constructor specifies 997 lists, so that for large tables, this code is about a factor of 1,000 faster than <code>SequentialSearchST</code>. This quick solution is an easy way to get good performance when you have some idea of the number of key-value pairs to be <code>put()</code> by a client. A more robust solution is to use array resizing to make sure that the lists are short no matter how many key-value pairs are in the table (see page <a href="#page_474">474</a> and <a href="#ch03qa4q18"><small>EXERCISE 3.4.18</small></a>).</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb31"/></p>
<p><a id="page_466"/><strong>Proposition K.</strong> In a separate-chaining hash table with <em>M</em> lists and <em>N</em> keys, the probability (under <a href="#ch03sb29"><small>ASSUMPTION J</small></a>) that the number of keys in a list is within a small constant factor of <em>N</em>/<em>M</em> is extremely close to 1.</p>
<p><strong>Proof sketch:</strong> <a href="#ch03sb29"><small>ASSUMPTION J</small></a> makes this an application of classical probability theory. We sketch the proof, for readers who are familiar with basic probabilistic analysis. The probability that a given list will contain exactly <em>k</em> keys is given by the <em>binomial distribution</em></p>
<p class="image"><img alt="image" src="graphics/03_63-math1.jpg"/></p>
<p>by the following argument: Choose <em>k</em> out of the <em>N</em> keys. Those <em>k</em> keys hash to the given list with probability 1 / <em>M</em>, and the other <em>N <strong>−</strong> k</em> keys do not hash to the given list with probability 1 − (1 / <em>M</em>). In terms of α = <em>N</em> / <em>M</em>, we can rewrite this expression as</p>
<p class="image"><img alt="image" src="graphics/03_64-math2.jpg"/></p>
<p>which (for small α) is closely approximated by the classical <em>Poisson distribution</em></p>
<p class="image"><img alt="image" src="graphics/03_65-math3.jpg"/></p>
<p>It follows that the probability that a list has more than <em>t</em> α keys on it is bounded by the quantity (α <em>e</em>/<em>t</em>)<em><sup>t</sup></em> e<strong><em><sup>−</sup></em></strong><sup>α</sup>. This probability is extremely small for practical ranges of the parameters. For example, if the average length of the lists is 10, the probability that we will hash to some list with more than 20 keys on it is less than (10 <em>e</em>/2)<sup>2</sup> e<strong><em><sup>−</sup></em></strong><sup>10</sup> ≈ 0.0084, and if the average length of the lists is 20, the probability that we will hash to some list with more than 40 keys on it is less than (20 <em>e</em>/2)<sup>2</sup> e<strong><em><sup>−</sup></em></strong><sup>20</sup> ≈ 0.0000016. This concentration result does not guarantee that <em>every</em> list will be short. Indeed it is known that, if α is a constant, the average length of the longest list grows with log <em>N</em> / log log <em>N</em>.</p>
<hr/>
</div>
<p><a id="page_467"/>This classical mathematical analysis is compelling, but it is important to note that it <em>completely depends</em> on <a href="#ch03sb29"><small>ASSUMPTION J</small></a>. If the hash function is not uniform and independent, the search and insert cost could be proportional to <em>N</em>, no better than with sequential search. <a href="#ch03sb29"><small>ASSUMPTION J</small></a> is much stronger than the corresponding assumption for other probabilistic algorithms that we have seen, and much more difficult to verify. With hashing, we are assuming that each and every key, no matter how complex, is equally likely to be hashed to one of <em>M</em> indices. We cannot afford to run experiments to test every possible key, so we would have to do more sophisticated experiments involving random sampling from the set of possible keys used in an application, followed by statistical analysis. Better still, we can use the algorithm itself as part of the test, to validate both <a href="#ch03sb29"><small>ASSUMPTION J</small></a> and the mathematical results that we derive from it.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb32"/></p>
<p><strong>Property L.</strong> In a separate-chaining hash table with <em>M</em> lists and <em>N</em> keys, the number of compares (equality tests) for search miss and insert is ~<em>N</em>/<em>M</em>.</p>
<p><strong>Evidence:</strong> Good performance of the algorithms in practice does not require the hash function to be fully uniform in the technical sense of <a href="#ch03sb29"><small>ASSUMPTION J</small></a>. Countless programmers since the 1950s have seen the speedups predicted by <a href="#ch03sb31"><small>PROPOSITION K</small></a>, even for hash functions that are certainly not uniform. For example, the diagram on page <a href="#page_468">468</a> shows that list length distribution for our <code>FrequencyCounter</code> example (using our <code>hash()</code> implementation based on the <code>hashCode()</code> from Java’s <code>String</code> data type) precisely matches the theoretical model. One exception that has been documented on numerous occasions is poor performance due to hash functions not taking all of the bits of the keys into account. Otherwise, the preponderance of the evidence from the experience of practical programmers puts us on solid ground in stating that hashing with separate chaining using an array of size <em>M</em> speeds up search and insert in a symbol table by a factor of <em>M</em>.</p>
<hr/>
</div>
<p><a id="ch03sec3lev69"/></p>
<h5><em>Table size</em></h5>
<p>In a separate-chaining implementation, our goal is to choose the table size <em>M</em> to be sufficiently small that we do not waste a huge area of contiguous memory with empty chains but sufficiently large that we do not waste time searching through long chains. One of the virtues of separate chaining is that this decision is not critical: if more keys arrive than expected, then searches will take a little longer than if we had chosen a bigger table size ahead of time; if fewer keys are in the table, then we have extra-fast search with some wasted space. When space is not a critical resource, <em>M</em> can be chosen sufficiently large that search time is constant; when space is a critical resource, we still can get a factor of <em>M</em> improvement in performance by choosing <em>M</em> to be as <a id="page_468"/>large as we can afford. For our example <code>FrequencyCounter</code> study, we see in the figure below a reduction in the average cost from thousands of compares per operation for <code>SequentialSearchST</code> to a small constant for <code>SeparateChainingHashST</code>, as expected. Another option is to use array resizing to keep the lists short (see <a href="#ch03qa4q18"><small>EXERCISE 3.4.18</small></a>).</p>
<p class="image"><img alt="image" src="graphics/03_66-hashscexperi.jpg"/></p>
<p><a id="ch03sec3lev70"/></p>
<h5><em>Deletion</em></h5>
<p>To delete a key-value pair, simply hash to find the <code>SequentialSearchST</code> containing the key, then invoke the <code>delete()</code> method for that table (see <a href="#ch03qa1q5"><small>EXERCISE 3.1.5</small></a>). Reusing code in this way is preferable to reimplementing this basic operation on linked lists.</p>
<p><a id="ch03sec3lev71"/></p>
<h5><em>Ordered operations</em></h5>
<p>The whole point of hashing is to uniformly disperse the keys, so any order in the keys is lost when hashing. If you need to quickly find the maximum or minimum key, find keys in a given range, or implement any of the other operations in the ordered symbol-table API on page <a href="#ch03sec2lev2">366</a>, then hashing is <em>not</em> appropriate, since these operations will all take linear time.</p>
<p><small>HASHING WITH SEPARATE CHAINING</small> is easy to implement and probably the fastest (and most widely used) symbol-table implementation for applications where key order is not important. When your keys are built-in Java types or your own type with well-tested implementations of <code>hashCode()</code>, <a href="#ch03sb30"><small>ALGORITHM 3.5</small></a> provides a quick and easy path to fast search and insert. Next, we consider an alternative scheme for collision resolution that is also effective.</p>
<p class="image"><img alt="image" src="graphics/03_67-freqscx.jpg"/></p>
<p><a id="ch03sec2lev30"/></p>
<h4><a id="page_469"/>Hashing with linear probing</h4>
<p>Another approach to implementing hashing is to store <em>N</em> key-value pairs in a hash table of size <em>M</em> &gt; <em>N</em>, relying on empty entries in the table to help with collision resolution. Such methods are called <em>open-addressing</em> hashing methods.</p>
<p>The simplest open-addressing method is called <em>linear probing</em>: when there is a collision (when we hash to a table index that is already occupied with a key different from the search key), then we just check the next entry in the table (by incrementing the index). Linear probing is characterized by identifying three possible outcomes:</p>
<p class="indenthangingB">• Key equal to search key: search hit</p>
<p class="indenthangingB">• Empty position (null key at indexed position): search miss</p>
<p class="indenthangingB">• Key not equal to search key: try next entry</p>
<p>We hash the key to a table index, check whether the search key matches the key there, and continue (incrementing the index, wrapping back to the beginning of the table if we reach the end) until finding either the search key or an empty table entry. It is customary to refer to the operation of determining whether or not a given table entry <a id="page_471"/>holds an item whose key is equal to the search key as a <em>probe</em>. We use the term interchangeably with the term <em>compare</em> that we have been using, even though some probes are tests for <code>null</code>.</p>
<p class="image"><img alt="image" src="graphics/03_68-hashlptrace.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch03sb33"/></p>
<h3><a id="page_470"/>Algorithm 3.6 Hashing with linear probing</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0470-01.jpg"/></p>
<p>This symbol-table implementation keeps keys and values in parallel arrays (as in <code>BinarySearchST</code>) but uses empty spaces (marked by <code>null</code>) to terminate clusters of keys. If a new key hashes to an empty entry, it is stored there; if not, we scan sequentially to find an empty position. To search for a key, we scan sequentially starting at its hash index until finding <code>null</code> (search miss) or the key (search hit). Implementation of <code>keys()</code> is left as <a href="#ch03qa4q19"><small>EXERCISE 3.4.19</small></a>.</p>
<hr/>
</div>
<p>The essential idea behind hashing with open addressing is this: rather than using memory space for references in linked lists, we use it for the empty entries in the hash table, which mark the ends of probe sequences. As you can see from <code>LinearProbingHashST</code> (<a href="#ch03sb33"><small>ALGORITHM 3.6</small></a>), applying this idea to implement the symbol-table API is quite straightforward. We implement the table with parallel arrays, one for the keys and one for the values, and use the hash function as an index to access the data as just discussed.</p>
<p><a id="ch03sec3lev72"/></p>
<h5><em>Deletion</em></h5>
<p>How do we delete a key-value pair from a linear-probing table? If you think about the situation for a moment, you will see that setting the key’s table position to <code>null</code> will not work, because that might prematurely terminate the search for a key that was inserted into the table later. As an example, suppose that we try to delete <code>C</code> in this way in our trace example, then search for <code>H</code>. The hash value for <code>H</code> is 4, but it sits at the end of the cluster, in position 7. If we set position 5 to <code>null</code>, then <code>get()</code> will not find <code>H</code>. As a consequence, we need to reinsert into the table all of the keys in the cluster to the right of the deleted key. This process is trickier than it might seem, so you are encouraged to trace through the code at right for an example that exercises it (see <a href="#ch03qa4q17"><small>EXERCISE 3.4.17</small></a>).</p>
<p class="image"><img alt="image" src="graphics/p0471-01.jpg"/></p>
<p><small>AS WITH SEPARATE CHAINING</small>, the performance of hashing with open addressing depends on the ratio α = <em>N</em> / <em>M</em>, but we interpret it differently. We refer to α as the <em>load factor</em> of a hash table. For separate chaining, α is the average number of keys per list and is generally larger than 1; for linear probing, α is the percentage of table entries that are occupied; it cannot be greater than 1. In fact, we cannot let the load factor reach 1 (completely full table) in <code>LinearProbingHashST</code> because a search miss would go into an infinite loop in a full table. Indeed, for the sake of good performance, we use array resizing to guarantee that the load factor is between one-eighth and one-half. This strategy is validated by mathematical analysis, which we consider before we discuss implementation details.</p>
<p><a id="ch03sec3lev73"/></p>
<h5><a id="page_472"/><em>Clustering</em></h5>
<p>The average cost of linear probing depends on the way in which the entries clump together into contiguous groups of occupied table entries, called <em>clusters</em>, when they are inserted. For example, when the key <code>C</code> is inserted in our example, the result is a cluster (<code>A C S</code>) of length 3, which means that four probes are needed to insert <code>H</code> because <code>H</code> hashes to the first position in the cluster. Short clusters are certainly a requirement for efficient performance. This requirement can be problematic as the table fills, because long clusters are common. Moreover, since all table positions are equally likely to be the hash value of the next key to be inserted (under the uniform hashing assumption), long clusters are <em>more likely</em> to increase in length than short ones, because a new key hashing to any entry in the cluster will cause the cluster to increase in length by 1 (and possibly much more, if there is just one table entry separating the cluster from the next one). Next, we turn to the challenge of quantifying the effect of clustering to predict performance in linear probing, and using that knowledge to set design parameters in our implementations.</p>
<p class="image"><img alt="image" src="graphics/03_69-lpcluster.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_70-lptables.jpg"/></p>
<p><a id="ch03sec3lev74"/></p>
<h5><a id="page_473"/><em>Analysis of linear probing</em></h5>
<p>Despite the relatively simple form of the results, precise analysis of linear probing is a very challenging task. Knuth’s derivation of the following formulas in 1962 was a landmark in the analysis of algorithms:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb34"/></p>
<p><strong>Proposition M.</strong> In a linear-probing hash table with <em>M</em> lists and <em>N</em> = α <em>M</em> keys, the average number of probes (under <a href="#ch03sb29"><small>ASSUMPTION J</small></a>) required is</p>
<p class="image"><img alt="image" src="graphics/03_71-math5.jpg"/></p>
<p>for search hits and search misses (or inserts), respectively. In particular, when α is about 1/2, the average number of probes for a search hit is about 3/2 and for a search miss is about 5/2. These estimates lose a bit of precision as α approaches 1, but we do not need them for that case, because we will only use linear probing for α less than one-half.</p>
<p><strong>Discussion:</strong> We compute the average by computing the cost of a search miss starting at each position in the table, then dividing the total by <em>M</em>. All search misses take at least 1 probe, so we count the number of probes after the first. Consider the following two extremes in a linear-probing table that is half full (<em>M</em> = 2<em>N</em>): In the best case, table positions with even indices could be empty, and table positions with odd indices could be occupied. In the worst case, the first half of the table positions could be empty, and the second half occupied. The average length of the clusters in both cases is <em>N</em>/(2<em>N</em>) = 1/2, but the average number of probes for a search miss is 1 (all searches take at least 1 probe) plus (0 + 1 + 0 + 1 + . . .)/(2<em>N</em>) = 1/2 in the best case, and is 1 plus (<em>N</em> + (<em>N</em> <strong>−</strong> 1) + . . .) / (2<em>N</em>) ~ <em>N</em>/4 in the worst case. This argument generalizes to show that the average number of probes for a search miss is proportional to the <em>squares</em> of the lengths of the clusters: If a cluster is of length <em>t</em>, then the expression (<em>t</em> + (<em>t</em> <strong>−</strong> 1) + . . . + 2 + 1) / <em>M</em> = <em>t</em>(<em>t</em> + 1)/(2<em>M</em>) counts the contribution of that cluster to the grand total. The sum of the cluster lengths is <em>N</em>, so, adding this cost for all entries in the table, we find that the total average cost for a search miss is 1 + <em>N</em> / (2<em>M</em>) plus the sum of the squares of the lengths of the clusters, divided by 2<em>M</em>. Thus, given a table, we can quickly compute the average cost of a search miss in that table (see <a href="#ch03qa4q21"><small>EXERCISE 3.4.21</small></a>). In general, the clusters are formed by a complicated dynamic process (the linear-probing algorithm) that is difficult to characterize analytically, and quite beyond the scope of this book.</p>
<hr/>
</div>
<p><a id="page_474"/><a href="#ch03sb34"><small>PROPOSITION M</small></a> tells us (under our usual <a href="#ch03sb29"><small>ASSUMPTION J</small></a>) that we can expect a search to require a huge number of probes in a nearly full table (as α approaches 1 the values of the formulas describing the number of probes grow very large) but that the expected number of probes is between 1.5 and 2.5 if we can ensure that the load factor α is less than 1/2. Next, we consider the use of array resizing for this purpose.</p>
<p><a id="ch03sec2lev31"/></p>
<h4>Array resizing</h4>
<p>We can use our standard array-resizing technique from <a href="ch01.html#ch01"><small>CHAPTER 1</small></a> to ensure that the load factor never exceeds one-half. First, we need a new constructor for <code>LinearProbingHashST</code> that takes a fixed capacity as argument (add a line to the constructor in <a href="#ch03sb33"><small>ALGORITHM 3.6</small></a> that sets <code>M</code> to the given value before creating the arrays). Next, we need the <code>resize()</code> method given at left, which creates a new <code>LinearProbingHashST</code> of the given size, puts all the keys and values in the table in the new one, then rehashes all the keys into the new table. These additions allow us to implement array doubling. The call to <code>resize()</code> in the first statement in <code>put()</code> ensures that the table is at most one-half full. This code builds a hash table twice the size with the same keys, thus halving the value of α. As in other applications of array resizing, we also need to add</p>
<p class="programlisting">if (N &gt; 0 &amp;&amp; N &lt;= M/8) resize(M/2);</p>
<p class="image"><img alt="image" src="graphics/p0474-01.jpg"/></p>
<p>as the last statement in <code>delete()</code> to ensure that the table is at least one-eighth full. This ensures that the amount of memory used is always within a constant factor of the number of key-value pairs in the table. With array resizing, we are assured that α ≤ 1/2.</p>
<p><a id="ch03sec3lev75"/></p>
<h5><em>Separate chaining</em></h5>
<p>The same method works to keep lists short (of average length between 2 and 8) in separate chaining: replace <code>LinearProbingHashST</code> by <code>SeparateChainingHashST</code> in <code>resize()</code>, call <code>resize(2*M)</code> when <code>(N &gt;= M/2)</code> in <code>put()</code>, and call <code>resize(M/2)</code> when <code>(N &gt; 0 &amp;&amp; N &lt;= M/8)</code> in <code>delete()</code>. For separate chaining, array resizing is <em>optional</em> and not worth your trouble if you have a decent estimate of the client’s <em>N</em>: just pick a table size <em>M</em> based on the knowledge that search times are proportional to 1+ <em>N</em>/<em>M</em>. For linear probing, array resizing is <em>necessary</em>. A client that inserts more key-value pairs than you expect will encounter not just excessively long search times, but an infinite loop when the table fills.</p>
<p><a id="ch03sec3lev76"/></p>
<h5><a id="page_475"/><em>Amortized analysis</em></h5>
<p>From a theoretical standpoint, when we use array resizing, we must settle for an amortized bound, since we know that those insertions that cause the table to double will require a large number of probes.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch03sb35"/></p>
<p><strong>Proposition N.</strong> Suppose a hash table is built with array resizing, starting with an empty table. Under <a href="#ch03sb29"><small>ASSUMPTION J</small></a>, any sequence of <em>t search</em>, <em>insert</em>, and <em>delete</em> symbol-table operations is executed in expected time proportional to <em>t</em> and with memory usage always within a constant factor of the number of keys in the table.</p>
<p><strong>Proof.:</strong> For both separate chaining and linear probing, this fact follows from a simple restatement of the amortized analysis for array growth that we first discussed in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, coupled with <a href="#ch03sb31"><small>PROPOSITION K</small></a> and <a href="#ch03sb34"><small>PROPOSITION M</small></a>.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/03_72-freqsc.jpg"/></p>
<p class="image"><img alt="image" src="graphics/03_73-freqlp.jpg"/></p>
<p><a id="page_476"/>The plots of the cumulative averages for our <code>FrequencyCounter</code> example (shown at the bottom of the previous page) nicely illustrate the dynamic behavior of array resizing in hashing. Each time the array doubles, the cumulative average increases by about 1, because each key in the table needs to be rehashed; then it decreases because about half as many keys hash to each table position, with the rate of decrease slowing as the table fills again.</p>
<p><a id="ch03sec2lev32"/></p>
<h4>Memory</h4>
<p>As we have indicated, understanding memory usage is an important factor if we want to tune hashing algorithms for optimum performance. While such tuning is for experts, it is a worthwhile exercise to calculate a rough estimate of the amount of memory required, by estimating the number of references used, as follows: Not counting the memory for keys and values, our implementation <code>SeparateChainingHashST</code> uses memory for <em>M</em> references to <code>SequentialSearchST</code> objects plus <em>M</em> <code>SequentialSearchST</code> objects. Each <code>SequentialSearchST</code> object has the usual 16 bytes of object overhead plus one 8-byte reference (<code>first</code>), and there are a total of <em>N</em> <code>Node</code> objects, each with 24 bytes of object overhead plus 3 references (<code>key</code>, <code>value</code>, and <code>next</code>). This compares with an extra reference per node for binary search trees. With array resizing to ensure that the table is between one-eighth and one-half full, linear probing uses between 4<em>N</em> and 16<em>N</em> references. Thus, choosing hashing on the basis of memory usage is not normally justified. The calculation is a bit different for primitive types (see <a href="#ch03qa4q24"><small>EXERCISE 3.4.24</small></a>)</p>
<p class="image"><img alt="image" src="graphics/t0476-01.jpg"/></p>
<p><a id="page_477"/><small>SINCE THE EARLIEST DAYS OF COMPUTING</small>, researchers have studied (and are studying) hashing and have found many ways to improve the basic algorithms that we have discussed. You can find a huge literature on the subject. Most of the improvements push down the space-time curve: you can get the same running time for searches using less space or get faster searches using the same amount of space. Other improvements involve better guarantees, on the expected worst-case cost of a search. Others involve improved hash-function designs. Some of these methods are addressed in the exercises.</p>
<p>Detailed comparison of separate chaining and linear probing depends on myriad implementation details and on client space and time requirements. It is not normally justified to choose separate chaining over linear probing on the basis of performance (see <a href="ch03a.html#ch03qa5q31"><small>EXERCISE 3.5.31</small></a>). In practice, the primary performance difference between the two methods has to do with the fact that separate chaining uses a small block of memory for each key-value pair, while linear probing uses two large arrays for the whole table. For huge tables, these needs place quite different burdens on the memory management system. In modern systems, this sort of tradeoff is best addressed by experts in extreme performance-critical situations.</p>
<p>With hashing, under generous assumptions, it is not unreasonable to expect to support the search and insert symbol-table operations in constant time, independent of the size of the table. This expectation is the theoretical optimum performance for any symbol-table implementation. Still, hashing is not a panacea, for several reasons, including:</p>
<p class="indenthangingB">• A good hash function for each type of key is required.</p>
<p class="indenthangingB">• The performance guarantee depends on the quality of the hash function.</p>
<p class="indenthangingB">• Hash functions can be difficult and expensive to compute.</p>
<p class="indenthangingB">• Ordered symbol-table operations are not easily supported.</p>
<p>Beyond these basic considerations, we defer the comparison of hashing with the other symbol-table methods that we have studied to the beginning of <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a>.</p>
<p><a id="ch03sec2lev33"/></p>
<h4><a id="page_478"/>Q&amp;A</h4>
<p><strong>Q.</strong> How does Java implement <code>hashCode()</code> for <code>Integer</code>, <code>Double</code>, and <code>Long</code>?</p>
<p><strong>A.</strong> For <code>Integer</code> it just returns the 32-bit value. For <code>Double</code> and <code>Long</code> it returns the <em>exclusive or</em> of the first 32 bits with the second 32 bits of the standard machine representation of the number. These choices may not seem to be very random, but they do serve the purpose of spreading out the values.</p>
<p class="image"><img alt="image" src="graphics/t0478-01.jpg"/></p>
<p><strong>Q.</strong> When using array resizing, the size of the table is always a power of 2. Isn’t that a potential problem, because it only uses the least significant bits of <code>hashCode()</code>?</p>
<p><strong>A.</strong> Yes, particularly with the default implementations. One way to address this problem is to first distribute the key values using a prime larger than <code>M</code>, as in the following example:</p>
<p class="programlisting"><img alt="image" src="graphics/p0478-01.jpg"/></p>
<p>This code assumes that we maintain an instance variable <code>lgM</code> that is equal to lg <em>M</em> (by initializing to the appropriate value, incrementing when doubling, and decrementing when halving) and an array <code>primes[]</code> of the smallest prime greater than each power of 2 (see the table at right). The constant 5 is an arbitrary choice—we expect the first <code>%</code> to distribute the values equally among the values less than the prime and the second to map about five of those values to each value less than <code>M</code>. Note that the point is moot for large <code>M</code>.</p>
<p><strong>Q.</strong> I’ve forgotten. Why don’t we implement <code>hash(x)</code> by returning <code>x.hashCode() % M</code>?</p>
<p><strong>A.</strong> We need a result between <code>0</code> and <code>M-1</code>, but in Java, the <code>%</code> function may be negative.</p>
<p><strong>Q.</strong> So, why not implement <code>hash(x)</code> by returning <code>Math.abs(x.hashcode()) % M</code>?</p>
<p><a id="page_479"/><strong>A.</strong> Nice try. Unfortunately, <code>Math.abs()</code> returns a negative result for the largest negative number. For many typical calculations, this overflow presents no real problem, but for hashing it would leave you with a program that is likely to crash after a few billion inserts, an unsettling possibility. For example, <code>s.hashCode()</code> is −2<sup>31</sup> for the Java <code>String</code> value <code>"polygenelubricants"</code>. Finding other strings that hash to this value (and to 0) has turned into an amusing algorithm-puzzle pastime.</p>
<p><strong>Q.</strong> Do Java library hash function satisfy <small>ASSUMPTION J?</small></p>
<p><strong>A.</strong> No. For example, the <code>hashCode()</code> implementation in the <code>String</code> data type is not only deterministic but it is specifi ed in the API.</p>
<p><strong>Q.</strong> Why not use <code>BinarySearchST</code> or <code>RedBlackBST</code> instead of <code>SequentialSearchST</code> in <a href="#ch03sb30"><small>ALGORITHM 3.5</small></a>?</p>
<p><strong>A.</strong> Generally, we set parameters so as to make the number of keys hashing to each value small, and elementary symbol tables are generally better for the small tables. In certain situations, slight performance gains may be achieved with such hybrid methods, but such tuning is best left for experts.</p>
<p><strong>Q.</strong> Is hashing faster than searching in red-black BSTs?</p>
<p><strong>A.</strong> It depends on the type of the key, which determines the cost of computing <code>hashCode()</code> versus the cost of <code>compareTo()</code>. For typical key types and for Java default implementations, these costs are similar, so hashing will be significantly faster, since it uses only a constant number of operations. But it is important to remember that this question is moot if you need ordered operations, which are not efficiently supported in hash tables. See <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a> for further discussion.</p>
<p><strong>Q.</strong> Why not let the linear probing table get, say, three-quarters full?</p>
<p><strong>A.</strong> No particular reason. You can choose any value of α, using <a href="#ch03sb34"><small>PROPOSITION M</small></a> to estimate search costs. For α = 3/4, the average cost of search hits is 2.5 and search misses is 8.5, but if you let α grow to 7/8, the average cost of a search miss is 32.5, perhaps more than you want to pay. As α gets close to 1, the estimate in <a href="#ch03sb34"><small>PROPOSITION M</small></a> becomes invalid, but you don’t want your table to get that close to being full.</p>
<p><a id="ch03sec2lev34"/></p>
<h4><a id="page_480"/>Exercises</h4>
<p><a id="ch03qa4q1"/><strong>3.4.1</strong> Insert the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty table of <em>M</em> = 5 lists, using separate chaining. Use the hash function <code>11 k % M</code> to transform the <em>k</em>th letter of the alphabet into a table index.</p>
<p><a id="ch03qa4q2"/><strong>3.4.2</strong> Develop an alternate implementation of <code>SeparateChainingHashST</code> that directly uses the linked-list code from <code>SequentialSearchST</code>.</p>
<p><a id="ch03qa4q3"/><strong>3.4.3</strong> Modify your implementation of the previous exercise to include an integer field for each key-value pair that is set to the number of entries in the table at the time that pair is inserted. Then implement a method that deletes all keys (and associated values) for which the field is greater than a given integer <code>k</code>. <em>Note</em>: This extra functionality is useful in implementing the symbol table for a compiler.</p>
<p><a id="ch03qa4q4"/><strong>3.4.4</strong> Write a program to find values of <code>a</code> and <code>M</code>, with <code>M</code> as small as possible, such that the hash function <code>(a * k) % M</code> for transforming the <em>k</em>th letter of the alphabet into a table index produces distinct values (no collisions) for the keys <code>S E A R C H X M P L</code>. The result is known as a <em>perfect hash function</em>.</p>
<p><a id="ch03qa4q5"/><strong>3.4.5</strong> Is the following implementation of <code>hashCode()</code> legal?</p>
<p class="programlisting">public int hashCode()<br/>
{  return 17;  }</p>
<p>If so, describe the effect of using it. If not, explain why.</p>
<p><a id="ch03qa4q6"/><strong>3.4.6</strong> Suppose that keys are <em>t</em>-bit integers. For a modular hash function with prime <em>M</em>, prove that each key bit has the property that there exist two keys differing only in that bit that have different hash values.</p>
<p><a id="ch03qa4q7"/><strong>3.4.7</strong> Consider the idea of implementing modular hashing for integer keys with the code <code>(a * k) % M</code>, where <code>a</code> is an arbitrary fixed prime. Does this change mix up the bits sufficiently well that you can use nonprime <code>M</code>?</p>
<p><a id="ch03qa4q8"/><strong>3.4.8</strong> How many empty lists do you expect to see when you insert <em>N</em> keys into a hash table with <code>SeparateChainingHashST</code>, for <em>N</em>=10, 10<sup>2</sup>, 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>? <em>Hint</em>: See <a href="ch02.html#ch02qa5q31"><small>EXERCISE 2.5.31</small></a>.</p>
<p><a id="ch03qa4q9"/><strong>3.4.9</strong> Implement an eager <code>delete()</code> method for <code>SeparateChainingHashST.</code></p>
<p><a id="ch03qa4q10"/><strong>3.4.10</strong> Insert the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty table <a id="page_481"/>of size <em>M</em> =16 using linear probing. Use the hash function <code>11 k % M</code> to transform the <em>k</em>th letter of the alphabet into a table index. Redo this exercise for <em>M</em> = 10.</p>
<p><a id="ch03qa4q11"/><strong>3.4.11</strong> Give the contents of a linear-probing hash table that results when you insert the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty table of initial size <em>M</em> = 4 that is expanded with doubling whenever half full. Use the hash function <code>11 k % M</code> to transform the <em>k</em>th letter of the alphabet into a table index.</p>
<p><a id="ch03qa4q12"/><strong>3.4.12</strong> Suppose that the keys <code>A</code> through <code>G</code>, with the hash values given below, are inserted in some order into an initially empty table of size 7 using a linear-probing table (with no resizing for this problem).</p>
<p class="indenthangingN">key               A B C D E F G</p>
<p class="indenthangingN">hash (M = 7)  2 0  0 4  4  4 2</p>
<p>Which of the following could not possibly result from inserting these keys?</p>
<p class="indenthangingN"><em>a.</em> <code>E  F  G  A  C  B  D</code></p>
<p class="indenthangingN"><em>b.</em> <code>C  E  B  G  F  D  A</code></p>
<p class="indenthangingN"><em>c.</em> <code>B  D  F  A  C  E  G</code></p>
<p class="indenthangingN"><em>d.</em> <code>C  G  B  A  D  E  F</code></p>
<p class="indenthangingN"><em>e.</em> <code>F  G  B  D  A  C  E</code></p>
<p class="indenthangingN"><em>f.</em> <code>G  E  C  A  D  B  F</code></p>
<p>Give the minimum and the maximum number of probes that could be required to build a table of size 7 with these keys, and an insertion order that justifies your answer.</p>
<p><a id="ch03qa4q13"/><strong>3.4.13</strong> Which of the following scenarios leads to expected <em>linear</em> running time for a random search hit in a linear-probing hash table?</p>
<p class="indenthangingN"><em>a.</em> All keys hash to the same index.</p>
<p class="indenthangingN"><em>b.</em> All keys hash to different indices.</p>
<p class="indenthangingN"><em>c.</em> All keys hash to an even-numbered index.</p>
<p class="indenthangingN"><em>d.</em> All keys hash to different even-numbered indices.</p>
<p><a id="ch03qa4q14"/><strong>3.4.14</strong> Answer the previous question for search <em>miss</em>, assuming the search key is equally likely to hash to each table position.</p>
<p><a id="ch03qa4q15"/><strong>3.4.15</strong> How many compares could it take, in the worst case, to insert <em>N</em> keys into an initially empty table, using linear probing with array resizing?</p>
<p><a id="ch03qa4q16"/><strong>3.4.16</strong> Suppose that a linear-probing table of size 10<sup>6</sup> is half full, with occupied positions chosen at random. Estimate the probability that all positions with indices divisible <a id="page_482"/>by 100 are occupied.</p>
<p><a id="ch03qa4q17"/><strong>3.4.17</strong> Show the result of using the <code>delete()</code> method on page <a href="#page_471">471</a> to delete <code>C</code> from the table resulting from using <code>LinearProbingHashST</code> with our standard indexing client (shown on page <a href="#page_469">469</a>).</p>
<p><a id="ch03qa4q18"/><strong>3.4.18</strong> Add a constructor to <code>SeparateChainingHashST</code> that gives the client the ability to specify the average number of probes to be tolerated for searches. Use array resizing to keep the average list size less than the specified value, and use the technique described on page <a href="#page_478">478</a> to ensure that the modulus for <code>hash()</code> is prime.</p>
<p><a id="ch03qa4q19"/><strong>3.4.19</strong> Implement <code>keys()</code> for <code>SeparateChainingHashST</code> and <code>LinearProbingHashST</code>.</p>
<p><a id="ch03qa4q20"/><strong>3.4.20</strong> Add a method to <code>LinearProbingHashST</code> that computes the average cost of a search hit in the table, assuming that each key in the table is equally likely to be sought.</p>
<p><a id="ch03qa4q21"/><strong>3.4.21</strong> Add a method to <code>LinearProbingHashST</code> that computes the average cost of a search <em>miss</em> in the table, assuming a random hash function. <em>Note</em>: You do not have to compute any hash functions to solve this problem.</p>
<p><a id="ch03qa4q22"/><strong>3.4.22</strong> Implement <code>hashCode()</code> for various types: <code>Point2D</code>, <code>Interval</code>, <code>Interval2D</code>, and <code>Date</code>.</p>
<p><a id="ch03qa4q23"/><strong>3.4.23</strong> Consider modular hashing for string keys with <code>R = 256</code> and <code>M = 255</code>. Show that this is a bad choice because any permutation of letters within a string hashes to the same value.</p>
<p><a id="ch03qa4q24"/><strong>3.4.24</strong> Analyze the space usage of separate chaining, linear probing, and BSTs for <code>double</code> keys. Present your results in a table like the one on page <a href="#page_476">476</a>.</p>
<p><a id="ch03sec2lev35"/></p>
<h4><a id="page_483"/>Creative Problems</h4>
<p><a id="ch03qa4q25"/><strong>3.4.25</strong> <em>Hash cache.</em> Modify <code>Transaction</code> on page <a href="#page_462">462</a> to maintain an instance variable <code>hash</code>, so that <code>hashCode()</code> can save the hash value the first time it is called for each object and does not have to recompute it on subsequent calls. <em>Note</em>: This idea works only for immutable types.</p>
<p><a id="ch03qa4q26"/><strong>3.4.26</strong> <em>Lazy delete for linear probing.</em> Add to <code>LinearProbingHashST</code> a <code>delete()</code> method that deletes a key-value pair by setting the value to <code>null</code> (but not removing the key) and later removing the pair from the table in <code>resize()</code>. Your primary challenge is to decide when to call <code>resize()</code>. <em>Note</em>: You should overwrite the <code>null</code> value if a subsequent <code>put()</code> operation associates a new value with the key. Make sure that your program takes into account the number of such <em>tombstone</em> items, as well as the number of empty positions, in making the decision whether to expand or contract the table.</p>
<p><a id="ch03qa4q27"/><strong>3.4.27</strong> <em>Double probing.</em> Modify <code>SeparateChainingHashST</code> to use a second hash function and pick the shorter of the two lists. Give a trace of the process of inserting the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty table of size <em>M</em> =3 using the function <code>11 k % M</code> (for the <em>k</em>th letter) as the first hash function and the function <code>17 k % M</code> (for the <em>k</em>th letter) as the second hash function. Give the average number of probes for random search hit and search miss in this table.</p>
<p><a id="ch03qa4q28"/><strong>3.4.28</strong> <em>Double hashing.</em> Modify <code>LinearProbingHashST</code> to use a second hash function to define the probe sequence. Specifically, replace <code>(i + 1) % M</code> (both occurrences) by <code>(i + k) % M</code> where <code>k</code> is a nonzero key-dependent integer that is relatively prime to <code>M</code>. <em>Note</em>: You may meet the last condition by assuming that <code>M</code> is prime. Give a trace of the process of inserting the keys <code>E A S Y Q U T I O N</code> in that order into an initially empty table of size <em>M</em> =11, using the hash functions described in the previous exercise. Give the average number of probes for random search hit and search miss in this table.</p>
<p><a id="ch03qa4q29"/><strong>3.4.29</strong> <em>Deletion.</em> Implement an eager <code>delete()</code> method for the methods described in each of the previous two exercises.</p>
<p><a id="ch03qa4q30"/><strong>3.4.30</strong> <em>Chi-square statistic.</em> Add a method to <code>SeparateChainingST</code> to compute the χ<sup>2</sup> statistic for the hash table. With <em>N</em> keys and table size <em>M</em>, this number is defined by the equation</p>
<p class="center">χ<sup>2</sup> = (<em>M/N</em>) ( (<em>f</em><sub>0</sub> − <em>N/M</em>)<sup>2</sup> + (<em>f</em><sub>1</sub> − <em>N/M</em>)<sup>2</sup> + . . . (<em>f<sub>M</sub></em> <sub>−1</sub> − <em>N/M</em>)<sup>2</sup> )</p>
<p><a id="page_484"/>where <em>f<sub>i</sub></em> is the number of keys with hash value <em>i</em>. This statistic is one way of checking our assumption that the hash function produces random values. If so, this statistic, for <em>N &gt; cM</em>, should be between <img alt="image" src="graphics/484fig01.jpg"/> and <img alt="image" src="graphics/484fig02.jpg"/> with probability 1 − 1/<em>c</em>.</p>
<p><a id="ch03qa4q31"/><strong>3.4.31</strong> <em>Cuckoo hashing.</em> Develop a symbol-table implementation that maintains two hash tables and two hash functions. Any given key is in one of the tables, but not both. When inserting a new key, hash to one of the tables; if the table position is occupied, replace that key with the new key and hash the old key into the other table (again kicking out a key that might reside there). If this process cycles, restart. Keep the tables less than half full. This method uses a constant number of equality tests in the worst case for search (trivial) and amortized constant time for insert.</p>
<p><a id="ch03qa4q32"/><strong>3.4.32</strong> <em>Hash attack.</em> Find 2<em><sup>N</sup></em> strings, each of length 2<em><sup>N</sup></em>, that have the same <code>hashCode()</code> value, supposing that the <code>hashCode()</code> implementation for <code>String</code> is the following:</p>
<p class="programlisting"><img alt="image" src="graphics/p0484-01.jpg"/></p>
<p><em>Strong hint</em>: <code>Aa</code> and <code>BB</code> have the same value.</p>
<p><a id="ch03qa4q33"/><strong>3.4.33</strong> <em>Bad hash function.</em> Consider the following <code>hashCode()</code> implementation for <code>String</code>, which was used in early versions of Java:</p>
<p class="programlisting"><img alt="image" src="graphics/p0484-02.jpg"/></p>
<p>Explain why you think the designers chose this implementation and then why you think it was abandoned in favor of the one in the previous exercise.</p>
<p><a id="ch03sec2lev36"/></p>
<h4><a id="page_485"/>Experiments</h4>
<p><a id="ch03qa4q34"/><strong>3.4.34</strong> <em>Hash cost.</em> Determine empirically the ratio of the time required for <code>hash()</code> to the time required for <code>compareTo()</code>, for as many commonly-used types of keys for which you can get meaningful results.</p>
<p><a id="ch03qa4q35"/><strong>3.4.35</strong> <em>Chi-square test.</em> Use your solution from <a href="#ch03qa4q30"><small>EXERCISE 3.4.30</small></a> to check the assumption that the hash functions for commonly-used key types produce random values.</p>
<p><a id="ch03qa4q36"/><strong>3.4.36</strong> <em>List length range.</em> Write a program that inserts <em>N</em> random <code>int</code> keys into a table of size <em>N</em> / 100 using separate chaining, then finds the length of the shortest and longest lists, for <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, 10<sup>6</sup>.</p>
<p><a id="ch03qa4q37"/><strong>3.4.37</strong> <em>Hybrid.</em> Run experimental studies to determine the effect of using <code>RedBlackBST</code> instead of <code>SequentialSearchST</code> to handle collisions in <code>SeparateChainingHashST</code>. This solution carries the advantage of guaranteeing logarithmic performance even for a bad hash function and the disadvantage of necessitating maintenance of two different symbol-table implementations. What are the practical effects?</p>
<p><a id="ch03qa4q38"/><strong>3.4.38</strong> <em>Separate-chaining distribution.</em> Write a program that inserts 10<sup>5</sup> random nonnegative integers less than 10<sup>6</sup> into a table of size 10<sup>5</sup> using linear probing, and that plots the total number of probes used for each 10<sup>3</sup> consecutive insertions. Discuss the extent to which your results validate <a href="#ch03sb31"><small>PROPOSITION K</small></a>.</p>
<p><a id="ch03qa4q39"/><strong>3.4.39</strong> <em>Linear-probing distribution.</em> Write a program that inserts <em>N</em>/2 random <code>int</code> keys into a table of size <em>N</em> using linear probing, then computes the average cost of a search miss in the resulting table from the cluster lengths, for <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, 10<sup>6</sup>. Discuss the extent to which your results validate <a href="#ch03sb34"><small>PROPOSITION M</small></a>.</p>
<p><a id="ch03qa4q40"/><strong>3.4.40</strong> <em>Plots.</em> Instrument <code>LinearProbingHashST</code> and <code>SeparateChainingHashST</code> to produce plots like the ones shown in the text.</p>
<p><a id="ch03qa4q41"/><strong>3.4.41</strong> <em>Double probing.</em> Run experimental studies to evaluate the effectiveness of double probing (see <a href="#ch03qa4q27"><small>EXERCISE 3.4.27</small></a>).</p>
<p><a id="ch03qa4q42"/><strong>3.4.42</strong> <em>Double hashing.</em> Run experimental studies to evaluate the effectiveness of double hashing (see <a href="#ch03qa4q28"><small>EXERCISE 3.4.28</small></a>).</p>
<p><a id="ch03qa4q43"/><strong>3.4.43</strong> <em>Parking problem.</em> (D. Knuth) Run experimental studies to validate the hypothesis that the number of compares needed to insert <em>M</em> random keys into a linear-probing table of size <em>M</em> is ~<em>cM</em><sup>3/2</sup>, where <img alt="image" src="graphics/485fig01.jpg"/>.</p>
</body>
</html>