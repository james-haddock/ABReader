<?xml version="1.0" encoding="UTF-8" standalone="no"?><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>One. Fundamentals</title>
<link href="9780132762564.css" rel="stylesheet" type="text/css"/>
<link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml"/>
<meta name="Adept.resource" value="urn:uuid:7baf5dbb-ffe1-4201-87bd-b993ed04f947"/>
</head>
<body>
<p><a id="ch01a"/></p>
<p><a id="ch01sec1lev5"/></p>
<h3><a id="page_120"/>1.3 Bags, Queues, and Stacks</h3>
<p><small>SEVERAL FUNDAMENTAL DATA TYPES</small> involve <em>collections</em> of objects. Specifically, the set of values is a collection of objects, and the operations revolve around adding, removing, or examining objects in the collection. In this section, we consider three such data types, known as the <em>bag,</em> the <em>queue,</em> and the <em>stack</em>. They differ in the specification of which object is to be removed or examined next.</p>
<p>Bags, queues, and stacks are fundamental and broadly useful. We use them in implementations throughout the book. Beyond this direct applicability, the client and implementation code in this section serves as an introduction to our general approach to the development of data structures and algorithms.</p>
<p>One goal of this section is to emphasize the idea that the way in which we represent the objects in the collection directly impacts the efficiency of the various operations. For collections, we design data structures for representing the collection of objects that can support efficient implementation of the requisite operations.</p>
<p>A second goal of this section is to introduce <em>generics</em> and <em>iteration</em>, basic Java constructs that substantially simplify client code. These are advanced programming-language mechanisms that are not necessarily essential to the understanding of algorithms, but their use allows us to develop client code (and implementations of algorithms) that is more clear, compact, and elegant than would otherwise be possible.</p>
<p>A third goal of this section is to introduce and show the importance of <em>linked</em> data structures. In particular, a classic data structure known as the <em>linked list</em> enables implementation of bags, queues, and stacks that achieve efficiencies not otherwise possible. Understanding linked lists is a key first step to the study of algorithms and data structures.</p>
<p>For each of the three types, we consider APIs and sample client programs, then look at possible representations of the data type values and implementations of the data-type operations. This scenario repeats (with more complicated data structures) throughout this book. The implementations here are models of implementations later in the book and worthy of careful study.</p>
<p><a id="ch01sec2lev24"/></p>
<h4><a id="page_121"/>APIs</h4>
<p>As usual, we begin our discussion of abstract data types for collections by defining their APIs, shown below. Each contains a no-argument constructor, a method to add an item to the collection, a method to test whether the collection is empty, and a method that returns the size of the collection. <code>Stack</code> and <code>Queue</code> each have a method to remove a particular item from the collection. Beyond these basics, these APIs reflect two Java features that we will describe on the next few pages: <em>generics</em> and <em>iterable collections</em>.</p>
<p class="image"><img alt="image" src="graphics/t0121-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0121-02.jpg"/></p>
<p><a id="ch01sec3lev86"/></p>
<h5><a id="page_122"/><em>Generics</em></h5>
<p>An essential characteristic of collection ADTs is that we should be able to use them for any type of data. A specific Java mechanism known as <em>generics</em>, also known as <em>parameterized types</em>, enables this capability. The impact of generics on the programming language is sufficiently deep that they are not found in many languages (including early versions of Java), but our use of them in the present context involves just a small bit of extra Java syntax and is easy to understand. The notation <code>&lt;Item&gt;</code> after the class name in each of our APIs defines the name <code>Item</code> as a <em>type parameter</em>, a symbolic placeholder for some concrete type to be used by the client. You can read <code>Stack&lt;Item&gt;</code> as “stack of items.” When implementing <code>Stack</code>, we do not know the concrete type of <code>Item</code>, but a client can use our stack for any type of data, including one defined long after we develop our implementation. The client code provides a concrete type when the stack is created: we can replace <code>Item</code> with the name of <em>any</em> reference data type (consistently, everywhere it appears). This provides exactly the capability that we need. For example, you can write code such as</p>
<p class="programlisting"><img alt="image" src="graphics/p0122-01.jpg"/></p>
<p>to use a stack for <code>String</code> objects and code such as</p>
<p class="programlisting"><img alt="image" src="graphics/p0122-02.jpg"/></p>
<p>to use a queue for <code>Date</code> objects. If you try to add a <code>Date</code> (or data of any other type than <code>String</code>) to <code>stack</code> or a <code>String</code> (or data of any other type than <code>Date</code>) to <code>queue</code>, you will get a compile-time error. Without generics, we would have to define (and implement) different APIs for each type of data we might need to collect; with generics, we can use one API (and one implementation) for all types of data, even types that are implemented in the future. As you will soon see, generic types lead to clear client code that is easy to understand and debug, so we use them throughout this book.</p>
<p><a id="ch01sec3lev87"/></p>
<h5><em>Autoboxing</em></h5>
<p>Type parameters have to be instantiated as <em>reference</em> types, so Java has special mechanisms to allow generic code to be used with primitive types. Recall that Java’s wrapper types are reference types that correspond to primitive types: <code>Boolean</code>, <code>Byte</code>, <code>Character</code>, <code>Double</code>, <code>Float</code>, <code>Integer</code>, <code>Long</code>, and <code>Short</code> correspond to <code>boolean</code>, <code>byte</code>, <code>char</code>, <code>double</code>, <code>float</code>, <code>int</code>, <code>long</code>, and <code>short</code>, respectively. Java automatically converts between these reference types and the corresponding primitive types—in assignments, method arguments, and arithmetic/logic expressions. In the present context, <a id="page_123"/>this conversion is helpful because it enables us to use generics with primitive types, as in the following code:</p>
<p class="programlisting"><img alt="image" src="graphics/p0123-01.jpg"/></p>
<p>Automatically casting a primitive type to a wrapper type is known as <em>autoboxing</em>, and automatically casting a wrapper type to a primitive type is known as <em>auto-unboxing</em>. In this example, Java automatically casts (autoboxes) the primitive value 17 to be of type <code>Integer</code> when we pass it to the <code>push()</code> method. The <code>pop()</code> method returns an <code>Integer</code>, which Java casts (auto-unboxes) to an <code>int</code> before assigning it to the variable <code>i</code>.</p>
<p><a id="ch01sec3lev88"/></p>
<h5><em>Iterable collections</em></h5>
<p>For many applications, the client’s requirement is just to process each of the items in some way, or to <em>iterate</em> through the items in the collection. This paradigm is so important that it has achieved first-class status in Java and many other modern languages (the programming language itself has specific mechanisms to support it, not just the libraries). With it, we can write clear and compact code that is free from dependence on the details of a collection’s implementation. For example, suppose that a client maintains a collection of transactions in a <code>Queue</code>, as follows:</p>
<p class="programlisting">Queue&lt;Transaction&gt; collection = new Queue&lt;Transaction&gt;();</p>
<p>If the collection is iterable, the client can print a transaction list with a single statement:</p>
<p class="programlisting">for (Transaction t : collection)<br/>
{  StdOut.println(t);  }</p>
<p>This construct is known as the <em>foreach</em> statement: you can read the <code>for</code> statement as <em>for each transaction t in the collection, execute the following block of code</em>. This client code does not need to know anything about the representation or the implementation of the collection; it just wants to process each of the items in the collection. The same <code>for</code> loop would work with a <code>Bag</code> of transactions or any other iterable collection. We could hardly imagine client code that is more clear and compact. As you will see, supporting this capability requires extra effort in the implementation, but this effort is well worthwhile.</p>
<p><small>IT IS INTERESTING TO NOTE</small> that the only differences between the APIs for <code>Stack</code> and <code>Queue</code> are their names and the names of the methods. This observation highlights the idea that we cannot easily specify all of the characteristics of a data type in a list of method signatures. In this case, the true specification has to do with the English-language descriptions that specify the rules by which an item is chosen to be removed (or to be processed next in the <em>foreach</em> statement). Differences in these rules are profound, <em>part of the API</em>, and certainly of critical importance in developing client code.</p>
<p><a id="ch01sec3lev89"/></p>
<h5><a id="page_124"/><em>Bags</em></h5>
<p>A <em>bag</em> is a collection where removing items is not supported—its purpose is to provide clients with the ability to collect items and then to iterate through the collected items (the client can also test if a bag is empty and find its number of items). The order of iteration is unspecified and should be immaterial to the client. To appreciate the concept, consider the idea of an avid marble collector, who might put marbles in a bag, one at a time, and periodically process all the marbles to look for one having some particular characteristic. With our <code>Bag</code> API, a client can add items to a bag and process them all with a <em>foreach</em> statement whenever needed. Such a client could use a stack or a queue, but one way to emphasize that the order in which items are processed is immaterial is to use a <code>Bag</code>. The class <code>Stats</code> at right illustrates a typical <code>Bag</code> client. The task is simply to compute the average and the sample standard deviation of the <code>double</code> values on standard input. If there are <em>N</em> numbers on standard input, their average is computed by adding the numbers and dividing by <em>N</em>; their sample standard deviation is computed by adding the squares of the difference between each number and the average, dividing by <em>N</em>–1, and taking the square root. The order in which the numbers are considered is not relevant for either of these calculations, so we save them in a <code>Bag</code> and use the <em>foreach</em> construct to compute each sum. <em>Note</em>: It is possible to compute the standard deviation without saving all the numbers (as we did for the average in <code>Accumulator</code>—see <a href="ch01.html#ch01qa2q18"><small>EXERCISE 1.2.18</small></a>). Keeping the all numbers in a <code>Bag</code> <em>is</em> required for more complicated statistics.</p>
<p class="image"><img alt="image" src="graphics/01_28-basicbag.jpg"/></p>
<p class="image"><a id="page_125"/><img alt="image" src="graphics/p0125-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0125-02.jpg"/></p>
<p><a id="ch01sec3lev90"/></p>
<h5><a id="page_126"/><em>FIFO queues</em></h5>
<p>A <em>FIFO queue</em> (or just a <em>queue</em>) is a collection that is based on the <em>first-in-first-out</em> (FIFO) policy. The policy of doing tasks in the same order that they arrive is one that we encounter frequently in everyday life: from people waiting in line at a theater, to cars waiting in line at a toll booth, to tasks waiting to be serviced by an application on your computer. One bedrock principle of any service policy is the perception of fairness. The first idea that comes to mind when most people think about fairness is that whoever has been waiting the longest should be served first. That is precisely the FIFO discipline. Queues are a natural model for many everyday phenomena, and they play a central role in numerous applications. When a client iterates through the items in a queue with the <em>foreach</em> construct, the items are processed in the order they were added to the queue. A typical reason to use a queue in an application is to save items in a collection while at the same time <em>preserving their relative order</em>: they come out in the same order in which they were put in. For example, the client below is a possible implementation of the <code>readInts()</code> static method from our <code>In</code> class. The problem that this method solves for the client is that the client can get numbers from a file into an array <em>without knowing the file size ahead of time.</em> We <em>enqueue</em> the numbers from the file, use the <code>size()</code> method from <code>Queue</code> to find the size needed for the array, create the array, and then <em>dequeue</em> the numbers to move them to the array. A queue is appropriate because it puts the numbers into the array in the order in which they appear in the file (we might use a <code>Bag</code> if that order is immaterial). This code uses autoboxing and auto-unboxing to convert between the client’s <code>int</code> primitive type and the queue’s <code>Integer</code> wrapper type.</p>
<p class="image"><img alt="image" src="graphics/01_29-basicqueue.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0126-01.jpg"/></p>
<p><a id="ch01sec3lev91"/></p>
<h5><a id="page_127"/><em>Pushdown stacks</em></h5>
<p>A <em>pushdown stack</em> (or just a <em>stack</em>) is a collection that is based on the <em>last-in-first-out</em> (LIFO) policy. When you keep your mail in a pile on your desk, you are using a stack. You pile pieces of new mail on the top when they arrive and take each piece of mail from the top when you are ready to read it. People do not process as many papers as they did in the past, but the same organizing principle underlies several of the applications that you use regularly on your computer. For example, many people organize their email as a stack—they <em>push</em> messages on the top when they are received and <em>pop</em> them from the top when they read them, with most recently received first (last in, first out). The advantage of this strategy is that we see interesting email as soon as possible; the disadvantage is that some old email might never get read if we never empty the stack. You have likely encountered another common example of a stack when surfing the web. When you click a hyperlink, your browser displays the new page (and pushes onto a stack). You can keep clicking on hyperlinks to visit new pages, but you can always revisit the previous page by clicking the back button (popping it from the stack). The LIFO policy offered by a stack provides just the behavior that you expect. When a client iterates through the items in a stack with the <em>foreach</em> construct, the items are processed in the <em>reverse</em> of the order in which they were added. A typical reason to use a stack iterator in an application is to save items in a collection while at the same time <em>reversing</em> their relative order. For example, the client <code>Reverse</code> at right reverses the order of the integers on standard input, again without having to know ahead of time how many there are. The importance of stacks in computing is fundamental and profound, as indicated in the detailed example that we consider next.</p>
<p class="image"><img alt="image" src="graphics/01_30-basicstack.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0127-01.jpg"/></p>
<p><a id="ch01sec3lev92"/></p>
<h5><a id="page_128"/><em>Arithmetic expression evaluation</em></h5>
<p>As another example of a stack client, we consider a classic example that also demonstrates the utility of generics. Some of the first programs that we considered in <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a> involved computing the value of arithmetic expressions like this one:</p>
<p class="programlisting">( 1 + ( ( 2 + 3 ) * ( 4 * 5 ) ) )</p>
<p>If you multiply <code>4</code> by <code>5</code>, add <code>3</code> to <code>2</code>, multiply the result, and then add <code>1</code>, you get the value <code>101</code>. But how does the Java system do this calculation? Without going into the details of how the Java system is built, we can address the essential ideas by writing a Java program that can take a string as input (the expression) and produce the number represented by the expression as output. For simplicity, we begin with the following explicit recursive definition: an <em>arithmetic expression</em> is either a number, or a left parenthesis followed by an arithmetic expression followed by an operator followed by another arithmetic expression followed by a right parenthesis. For simplicity, this definition is for <em>fully parenthesized</em> arithmetic expressions, which specify precisely which operators apply to which operands—you are a bit more familiar with expressions such as <code>1 + 2 * 3</code>, where we often rely on precedence rules instead of parentheses. The same basic mechanisms that we consider can handle precedence rules, but we avoid that complication. For specificity, we support the familiar binary operators <code>*</code>, <code>+</code>, <code>-</code>, and <code>/</code>, as well as a square-root operator <code>sqrt</code> that takes just one argument. We could easily allow more operators and more kinds of operators to embrace a large class of familiar mathematical expressions, involving trigonometric, exponential, and logarithmic functions. Our focus is on understanding how to interpret the string of parentheses, operators, and numbers to enable performing in the proper order the low-level arithmetic operations that are available on any computer. Precisely how can we convert an arithmetic expression—a string of characters—to the value that it represents? A remarkably simple algorithm that was developed by E. W. Dijkstra in the 1960s uses two stacks (one for operands and one for operators) to do this job. An expression consists of parentheses, operators, and operands (numbers). Proceeding from left to right and taking these entities one at a time, we manipulate the stacks according to four possible cases, as follows:</p>
<p class="indenthangingB">• Push <em>operands</em> onto the operand stack.</p>
<p class="indenthangingB">• Push <em>operators</em> onto the operator stack.</p>
<p class="indenthangingB">• Ignore <em>left</em> parentheses.</p>
<p class="indenthangingB">• On encountering a <em>right</em> parenthesis, pop an operator, pop the requisite number of operands, and push onto the operand stack the result of applying that operator to those operands.</p>
<p>After the final right parenthesis has been processed, there is one value on the stack, which is the value of the expression. This method may seem mysterious at first, but it <a id="page_130"/>is easy to convince yourself that it computes the proper value: any time the algorithm encounters a subexpression consisting of two operands separated by an operator, all surrounded by parentheses, it leaves the result of performing that operation on those operands on the operand stack. The result is the same as if that value had appeared in the input instead of the subexpression, so we can think of replacing the subexpression by the value to get an expression that would yield the same result. We can apply this argument again and again until we get a single value. For example, the algorithm computes the same value for all of these expressions:</p>
<p class="programlisting"><img alt="image" src="graphics/p0130-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb03"/></p>
<h3><a id="page_129"/>Dijkstra’s Two-Stack Algorithm for Expression Evaluation</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0129-01.jpg"/></p>
<p>This <code>Stack</code> client uses two stacks to evaluate arithmetic expressions, illustrating an essential computational process: interpreting a string as a program and executing that program to compute the desired result. With generics, we can use the code in a single <code>Stack</code> implementation to implement one stack of <code>String</code> values and another stack of <code>Double</code> values. For simplicity, this code assumes that the expression is fully parenthesized, with numbers and characters separated by whitespace.</p>
<p class="image"><img alt="image" src="graphics/p0129-02.jpg"/></p>
<hr/>
</div>
<p><code>Evaluate</code> on the previous page is an implementation of this algorithm. This code is a simple example of an <em>interpreter</em>: a program that interprets the computation specified by a given string and performs the computation to arrive at the result.</p>
<p class="image"><a id="page_131"/><img alt="image" src="graphics/01_31-evaluatetrace.jpg"/></p>
<p><a id="ch01sec2lev25"/></p>
<h4><a id="page_132"/>Implementing collections</h4>
<p>To address the issue of implementing <code>Bag</code>, <code>Stack</code> and <code>Queue</code>, we begin with a simple classic implementation, then address improvements that lead us to implementations of the APIs articulated on page <a href="#ch01sec2lev24">121</a>.</p>
<p><a id="ch01sec3lev93"/></p>
<h5><em>Fixed-capacity stack</em></h5>
<p>As a strawman, we consider an abstract data type for a fixed-capacity stack of strings, shown on the opposite page. The API differs from our <code>Stack</code> API: it works only for <code>String</code> values, it requires the client to specify a capacity, and it does not support iteration. The primary choice in developing an API implementation is to <em>choose a representation for the data</em>. For <code>FixedCapacityStackOfStrings</code>, an obvious choice is to use an array of <code>String</code> values. Pursuing this choice leads to the implementation shown at the bottom on the opposite page, which could hardly be simpler (each method is a one-liner). The instance variables are an array <code>a[]</code> that holds the items in the stack and an integer <code>N</code> that counts the number of items in the stack. To remove an item, we decrement <code>N</code> and then return <code>a[N]</code>; to insert a new item, we set <code>a[N]</code> equal to the new item and then increment <code>N</code>. These operations preserve the following properties:</p>
<p class="indenthangingB">• The items in the array are in their insertion order.</p>
<p class="indenthangingB">• The stack is empty when <code>N</code> is <code>0</code>.</p>
<p class="indenthangingB">• The top of the stack (if it is nonempty) is at <code>a[N-1]</code>.</p>
<p>As usual, thinking in terms of invariants of this sort is the easiest way to verify that an implementation operates as intended. <em>Be sure that you fully understand this implementation.</em> The best way to do so is to examine a trace of the stack contents for a sequence of operations, as illustrated at left for the test client, which reads strings from standard input and pushes each string onto a stack, unless it is <code>"-"</code>, when it pops the stack and prints the result. The primary performance characteristic of this implementation is that the <em>push and pop operations take time independent of the stack size</em>. For many applications, it is the method of choice because of its simplicity. But it has several drawbacks that limit its potential applicability as a general-purpose tool, which we now address. With a moderate amount of effort (and some help from Java language mechanisms), we can develop an implementation that is broadly useful. This effort is worthwhile because the implementations that we develop serve as a model for implementations of other, more powerful, abstract data types throughout the book.</p>
<p class="image"><img alt="image" src="graphics/t0132-01.jpg"/></p>
<p class="image"><a id="page_133"/><img alt="image" src="graphics/p0133-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0133-02.jpg"/></p>
<p><a id="ch01sec3lev94"/></p>
<h5><a id="page_134"/><em>Generics</em></h5>
<p>The first drawback of <code>FixedCapacityStackOfStrings</code> is that it works only for <code>String</code> objects. If we want a stack of <code>double</code> values, we would need to develop another class with similar code, essentially replacing <code>String</code> with <code>double</code> everywhere. This is easy enough but becomes burdensome when we consider building a stack of <code>Transaction</code> values or a queue of <code>Date</code> values, and so forth. As discussed on page <a href="#ch01sec3lev86">122</a>, Java’s parameterized types (generics) are specifically designed to address this situation, and we saw several examples of client code (on pages <a href="#page_125">125</a>, <a href="#ch01sec3lev90">126</a>, <a href="#ch01sec3lev91">127</a>, and <a href="#ch01sb03">129</a>). But how do we <em>implement</em> a generic stack? The code on the facing page shows the details. It implements a class <code>FixedCapacityStack</code> that differs from <code>FixedCapacityStackOfStrings</code> only in the code highlighted in red—we replace every occurrence of <code>String</code> with <code>Item</code> (with one exception, discussed below) and declare the class with the following first line of code:</p>
<p class="programlisting">public class FixedCapacityStack&lt;Item&gt;</p>
<p>The name <code>Item</code> is a <em>type parameter</em>, a symbolic placeholder for some concrete type to be used by the client. You can read <code>FixedCapacityStack&lt;Item&gt;</code> as <em>stack of items</em>, which is precisely what we want. When implementing <code>FixedCapacityStack</code>, we do not know the actual type of <code>Item</code>, but a client can use our stack for any type of data by providing a concrete type when the stack is created. Concrete types must be reference types, but clients can depend on autoboxing to convert primitive types to their corresponding wrapper types. Java uses the type parameter <code>Item</code> to check for type mismatch errors—even though no concrete type is yet known, variables of type <code>Item</code> must be assigned values of type <code>Item</code>, and so forth. But there is one significant hitch in this story: We would like to implement the constructor in <code>FixedCapacityStack</code> with the code</p>
<p class="programlisting">a = new Item[cap];</p>
<p>which calls for creation of a generic array. For historical and technical reasons beyond our scope, <em>generic array creation is disallowed in Java</em>. Instead, we need to use a cast:</p>
<p class="programlisting">a = (Item[]) new Object[cap];</p>
<p>This code produces the desired effect (though the Java compiler gives a warning, which we can safely ignore), and we use this idiom throughout the book (the Java system library implementations of similar abstract data types use the same idiom).</p>
<p class="image"><a id="page_135"/><img alt="image" src="graphics/p0135-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0135-02.jpg"/></p>
<p><a id="ch01sec3lev95"/></p>
<h5><a id="page_136"/><em>Array resizing</em></h5>
<p>Choosing an array to represent the stack contents implies that clients must estimate the maximum size of the stack ahead of time. In Java, we cannot change the size of an array once created, so the stack always uses space proportional to that maximum. A client that chooses a large capacity risks wasting a large amount of memory at times when the collection is empty or nearly empty. For example, a transaction system might involve billions of items and thousands of collections of them. Such a client would have to allow for the possibility that each of those collections could hold all of those items, even though a typical constraint in such systems is that each item can appear in only one collection. Moreover, every client risks <em>overflow</em> if the collection grows larger than the array. For this reason, <code>push()</code> needs code to test for a full stack, and we should have an <code>isFull()</code> method in the API to allow clients to test for that condition. We omit that code, because our desire is to relieve the client from having to deal with the concept of a full stack, as articulated in our original <code>Stack</code> API. Instead, we modify the array implementation to dynamically adjust the size of the array <code>a[]</code> so that it is both sufficiently large to hold all of the items and not so large as to waste an excessive amount of space. Achieving these goals turns out to be remarkably easy. First, we implement a method that moves a stack into an array of a different size:</p>
<p class="programlisting"><img alt="image" src="graphics/p0136-01.jpg"/></p>
<p>Now, in <code>push()</code>, we check whether the array is too small. In particular, we check whether there is room for the new item in the array by checking whether the stack size <code>N</code> is equal to the array size <code>a.length</code>. If there is no room, we <em>double</em> the size of the array. Then we simply insert the new item with the code <code>a[N++] = item</code>, as before:</p>
<p class="programlisting"><img alt="image" src="graphics/p0136-02.jpg"/></p>
<p>Similarly, in <code>pop()</code>, we begin by deleting the item, then we <em>halve</em> the array size if it is too large. If you think a bit about the situation, you will see that the appropriate test is whether the stack size is less than <em>one-fourth</em> the array size. After the array is halved, it will be about half full and can accommodate a substantial number of <code>push()</code> and <code>pop()</code> operations before having to change the size of the array again.</p>
<p class="programlisting"><a id="page_137"/><img alt="image" src="graphics/p0137-01.jpg"/></p>
<p>With this implementation, the stack never overflows and never becomes less than one-quarter full (unless the stack is empty, when the array size is 1). We will address the performance analysis of this approach in more detail in <a href="#ch01sec1lev6"><small>SECTION 1.4</small></a>.</p>
<p><a id="ch01sec3lev96"/></p>
<h5><em>Loitering</em></h5>
<p>Java’s garbage collection policy is to reclaim the memory associated with any objects that can no longer be accessed. In our <code>pop()</code> implementations, the reference to the popped item remains in the array. The item is effectively an <em>orphan</em>—it will be never be accessed again—but the Java garbage collector has no way to know this until it is overwritten. Even when the client is done with the item, the reference in the array may keep it alive. This condition (holding a reference to an item that is no longer needed) is known as <em>loitering</em>. In this case, loitering is easy to avoid, by setting the array entry corresponding to the popped item to <code>null</code>, thus overwriting the unused reference and making it possible for the system to reclaim the memory associated with the popped item when the client is finished with it.</p>
<p class="image"><img alt="image" src="graphics/t0137-01.jpg"/></p>
<p><a id="ch01sec3lev97"/></p>
<h5><a id="page_138"/><em>Iteration</em></h5>
<p>As mentioned earlier in this section, one of the fundamental operations on collections is to process each item by <em>iterating</em> through the collection using Java’s <em>foreach</em> statement. This paradigm leads to clear and compact code that is free from dependence on the details of a collection’s implementation. To consider the task of implementing iteration, we start with a snippet of client code that prints all of the items in a collection of strings, one per line:</p>
<p class="programlisting"><img alt="image" src="graphics/p0138-01.jpg"/></p>
<p>Now, this <em>foreach</em> statement is shorthand for a <code>while</code> construct (just like the <code>for</code> statement itself). It is essentially equivalent to the following <code>while</code> statement:</p>
<p class="programlisting"><img alt="image" src="graphics/p0138-02.jpg"/></p>
<p>This code exposes the ingredients that we need to implement in any iterable collection:</p>
<p class="indenthangingB">• The collection must implement an <code>iterator()</code> method that returns an <code>Iterator</code> object.</p>
<p class="indenthangingB">• The <code>Iterator</code> class must include two methods: <code>hasNext()</code> (which returns a <code>boolean</code> value) and <code>next()</code> (which returns a generic item from the collection).</p>
<p>In Java, we use the <code>interface</code> mechanism to express the idea that a class implements a specific method (see page <a href="ch01.html#ch01sec3lev75">100</a>). For iterable collections, the necessary interfaces are already defined for us in Java. To make a class iterable, the first step is to add the phrase <code>implements Iterable&lt;Item&gt;</code> to its declaration, matching the interface</p>
<p class="programlisting"><img alt="image" src="graphics/p0138-03.jpg"/></p>
<p>(which is in <code>java.lang.Iterable</code>), and to add a method <code>iterator()</code> to the class that returns an <code>Iterator&lt;Item&gt;.</code> Iterators are generic, so we can use our parameterized type <code>Item</code> to allow clients to iterate through objects of whatever type is provided by our client. For the array representation that we have been using, we need to iterate through <a id="page_139"/>an array in reverse order, so we name the iterator <code>ReverseArrayIterator</code> and add this method:</p>
<p class="programlisting">public Iterator&lt;Item&gt; iterator()<br/>
{  return new ReverseArrayIterator();  }</p>
<p>What is an iterator? An object from a class that implements the methods <code>hasNext()</code> and <code>next()</code>, as defined in the following interface (which is in <code>java.util.Iterator</code>):</p>
<p class="programlisting"><img alt="image" src="graphics/p0139-02.jpg"/></p>
<p>Although the interface specifies a <code>remove()</code> method, we always use an empty method for <code>remove()</code> in this book, because interleaving iteration with operations that modify the data structure is best avoided. For <code>ReverseArrayIterator</code>, these methods are all one-liners, implemented in a nested class within our stack class:</p>
<p class="programlisting"><img alt="image" src="graphics/p0139-03.jpg"/></p>
<p>Note that this nested class can access the instance variables of the enclosing class, in this case <code>a[]</code> and <code>N</code> (this ability is the main reason we use nested classes for iterators). Technically, to conform to the <code>Iterator</code> specification, we should throw exceptions in two cases: an <code>UnsupportedOperationException</code> if a client calls <code>remove()</code> and a <code>NoSuchElementException</code> if a client calls <code>next()</code> when <code>i</code> is <code>0</code>. Since we only use iterators in the <em>foreach</em> construction where these conditions do not arise, we omit this code. One crucial detail remains: we have to include</p>
<p class="programlisting">import java.util.Iterator;</p>
<p>at the beginning of the program because (for historical reasons) <code>Iterator</code> is not part of <code>java.lang</code> (even though <code>Iterable</code> <em>is</em> part of <code>java.lang</code>). Now a client using the <em>foreach</em> statement for this class will get behavior equivalent to the common <code>for</code> loop for arrays, but does not need to be aware of the array representation (an implementation <a id="page_140"/>detail). This arrangement is of critical importance for implementations of fundamental data types like the collections that we consider in this book and those included in Java libraries. For example, it frees us to switch to a totally different representation <em>without having to change any client code</em>. More important, taking the client’s point of view, it allows clients to use iteration <em>without having to know any details of the class implementation</em>.</p>
<p><a href="#ch01sb04"><small>ALGORITHM 1.1</small></a> is an implementation of our <code>Stack</code> API that resizes the array, allows clients to make stacks for any type of data, and supports client use of <em>foreach</em> to iterate through the stack items in LIFO order. This implementation is based on Java language nuances involving <code>Iterator</code> and <code>Iterable</code>, but there is no need to study those nuances in detail, as the code itself is not complicated and can be used as a template for other collection implementations.</p>
<p>For example, we can implement the <code>Queue</code> API by maintaining two indices as instance variables, a variable <code>head</code> for the beginning of the queue and a variable <code>tail</code> for the end of the queue. To remove an item, use <code>head</code> to access it and then increment <code>head</code>; to insert an item, use <code>tail</code> to store it, and then increment <code>tail</code>. If incrementing an index brings it past the end of the array, reset it to 0. Developing the details of checking when the queue is empty and when the array is full and needs resizing is an interesting and worthwhile programming exercise (see <a href="#ch01qa3q14"><small>EXERCISE 1.3.14</small></a>).</p>
<p class="image"><img alt="image" src="graphics/t0140-01.jpg"/></p>
<p>In the context of the study of algorithms, <a href="#ch01sb04"><small>ALGORITHM 1.1</small></a> is significant because it almost (but not quite) achieves optimum performance goals for any collection implementation:</p>
<p class="indenthangingB">• Each operation should require time independent of the collection size.</p>
<p class="indenthangingB">• The space used should always be within a constant factor of the collection size.</p>
<p>The flaw in <code>ResizingArrayStack</code> is that some <em>push</em> and <em>pop</em> operations require resizing: this takes time proportional to the size of the stack. Next, we consider a way to correct this flaw, using a fundamentally different way to structure data.</p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb04"/></p>
<h3><a id="page_141"/>Algorithm 1.1 Pushdown (LIFO) stack (resizing array implementation)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0141-01.jpg"/></p>
<p class="programlisting2"><img alt="image" src="graphics/p0141-02.jpg"/></p>
<p>This generic, iterable implementation of our <code>Stack</code> API is a model for collection ADTs that keep items in an array. It resizes the array to keep the array size within a constant factor of the stack size.</p>
<hr/>
</div>
<p><a id="ch01sec2lev26"/></p>
<h4><a id="page_142"/>Linked lists</h4>
<p>Now we consider the use of a fundamental data structure that is an appropriate choice for representing the data in a collection ADT implementation. This is our first example of building a data structure that is not directly supported by the Java language. Our implementation serves as a model for the code that we use for building more complex data structures throughout the book, so you should read this section carefully, even if you have experience working with linked lists.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb05"/></p>
<p><strong>Definition.</strong> A <em>linked list</em> is a recursive data structure that is either empty (<em>null</em>) or a reference to a <em>node</em> having a generic item and a reference to a linked list.</p>
<hr/>
</div>
<p>The <em>node</em> in this definition is an abstract entity that might hold any kind of data, in addition to the node reference that characterizes its role in building linked lists. As with a recursive program, the concept of a recursive data structure can be a bit mindbending at first, but is of great value because of its simplicity.</p>
<p><a id="ch01sec3lev98"/></p>
<h5><em>Node record</em></h5>
<p>With object-oriented programming, implementing linked lists is not difficult. We start with a <em>nested class</em> that defines the node abstraction:</p>
<p class="programlisting"><img alt="image" src="graphics/p0142-01.jpg"/></p>
<p>A <code>Node</code> has two instance variables: an <code>Item</code> (a parameterized type) and a <code>Node</code>. We define <code>Node</code> within the class where we want to use it, and make it <code>private</code> because it is not for use by clients. As with any data type, we create an object of type <code>Node</code> by invoking the (no-argument) constructor with <code>new Node()</code>. The result is a reference to a <code>Node</code> object whose instance variables are both initialized to the value <code>null</code>. The <code>Item</code> is a placeholder for any data that we might want to structure with a linked list (we will use Java’s generic mechanism so that it can represent any reference type); the instance variable of type <code>Node</code> characterizes the linked nature of the data structure. To emphasize that we are just using the <code>Node</code> class to structure the data, we define no methods and we refer directly to the instance variables in code: if <code>first</code> is a variable associated with an object of type <code>Node</code>, we can refer to the instance variables with the code <code>first.item</code> and <code>first.next</code>. Classes of this kind are sometimes called <em>records.</em> They do not implement abstract data types because we refer directly to instance variables. However, <code>Node</code> and its client code are in the same class in all of our implementations and not accessible by clients of that class, so we still enjoy the benefits of data abstraction.</p>
<p><a id="ch01sec3lev99"/></p>
<h5><a id="page_143"/><em>Building a linked list</em></h5>
<p>Now, from the recursive definition, we can represent a linked list with a variable of type <code>Node</code> simply by ensuring that its value is either <em>null</em> or a reference to a <code>Node</code> whose <code>next</code> field is a reference to a linked list. For example, to build a linked list that contains the items <code>to</code>, <code>be</code>, and <code>or</code>, we create a <code>Node</code> for each item:</p>
<p class="programlisting"><img alt="image" src="graphics/p0143-01.jpg"/></p>
<p>and set the item field in each of the nodes to the desired value (for simplicity, these examples assume that <code>Item</code> is <code>String</code>):</p>
<p class="programlisting"><img alt="image" src="graphics/p0143-02.jpg"/></p>
<p class="image"><img alt="image" src="graphics/01_32-linkedlist.jpg"/></p>
<p>and set the <code>next</code> fields to build the linked list:</p>
<p class="programlisting">first.next = second;<br/>
second.next = third;</p>
<p>(Note that <code>third.next</code> remains <code>null</code>, the value it was initialized to at the time of creation.)As a result, <code>third</code> is a linked list (it is a reference to a node that has a reference to <code>null</code>, which is the null reference to an empty linked list), and <code>second</code> is a linked list (it is a reference to a node that has a reference to <code>third</code>, which is a linked list), and <code>first</code> is a linked list (it is a reference to a node that has a reference to <code>second</code>, which is a linked list). The code that we will examine does these assignment statements in a different order, depicted in the diagram on this page.</p>
<p>A LINKED LIST REPRESENTS A SEQUENCE of items. In the example just considered, <code>first</code> represents the sequence <code>to be or</code>. We can also use an array to represent a sequence of items. For example, we could use</p>
<p class="programlisting">String[] s = { "to", "be", "or" };</p>
<p>to represent the same sequence of strings. The difference is that it is easier to insert items into the sequence and to remove items from the sequence with linked lists. Next, we consider code to accomplish these tasks.</p>
<p><a id="page_144"/>When tracing code that uses linked lists and other linked structures, we use a visual representation where</p>
<p class="indenthangingB">• We draw a rectangle to represent each object</p>
<p class="indenthangingB">• We put the values of instance variables within the rectangle</p>
<p class="indenthangingB">• We use arrows that point to the referenced objects to depict references</p>
<p>This visual representation captures the essential characteristic of linked lists. For economy, we use the term <em>links</em> to refer to node references. For simplicity, when item values are strings (as in our examples), we put the string within the object rectangle rather than the more accurate rendition depicting the string object and the character array that we discussed in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a>. This visual representation allows us to focus on the links.</p>
<p><a id="ch01sec3lev100"/></p>
<h5><em>Insert at the beginning</em></h5>
<p>First, suppose that you want to insert a new node into a linked list. The easiest place to do so is at the beginning of the list. For example, to insert the string <code>not</code> at the beginning of a given linked list whose first node is <code>first</code>, we save <code>first</code> in <code>oldfirst</code>, assign to <code>first</code> a new <code>Node</code>, and assign its <code>item</code> field to <code>not</code> and its <code>next</code> field to <code>oldfirst</code>. This code for inserting a node at the beginning of a linked list involves just a few assignment statements, so the amount of time that it takes is independent of the length of the list.</p>
<p class="image"><img alt="image" src="graphics/01_33-linkedlistpush.jpg"/></p>
<p><a id="ch01sec3lev101"/></p>
<h5><a id="page_145"/><em>Remove from the beginning</em></h5>
<p>Next, suppose that you want to remove the first node from a list. This operation is even easier: simply assign to <code>first</code> the value <code>first.next</code>. Normally, you would retrieve the value of the item (by assigning it to some variable of type <code>Item</code>) before doing this assignment, because once you change the value of <code>first</code>, you may not have any access to the node to which it was referring. Typically, the node object becomes an orphan, and the Java memory management system eventually reclaims the memory it occupies. Again, this operation just involves one assignment statement, so its running time is independent of the length of the list.</p>
<p class="image"><img alt="image" src="graphics/01_34-linkedlistpop.jpg"/></p>
<p><a id="ch01sec3lev102"/></p>
<h5><em>Insert at the end</em></h5>
<p>How do we add a node to the <em>end</em> of a linked list? To do so, we need a link to the last node in the list, because that node’s link has to be changed to reference a new node containing the item to be inserted. Maintaining an extra link is not something that should be taken lightly in linked-list code, because every method that modifies the list needs code to check whether that variable needs to be modified (and to make the necessary modifications). For example, the code that we just examined for removing the first node in the list might involve changing the reference to the last node in the list, since when there is only one node in the list, it is both the first one and the last one! Also, this code does not work (it follows a null link) in the case that the list is empty. Details like these make linked-list code notoriously difficult to debug.</p>
<p class="image"><img alt="image" src="graphics/01_35-linkedqueue.jpg"/></p>
<p><a id="ch01sec3lev103"/></p>
<h5><em>Insert/remove at other positions</em></h5>
<p>In summary, we have shown that we can implement the following operations on linked lists with just a few instructions, provided that we have access to both a link <code>first</code> to the first element in the list and a link <code>last</code> to the last element in the list:</p>
<p class="indenthangingB">• Insert at the beginning.</p>
<p class="indenthangingB">• Remove from the beginning.</p>
<p class="indenthangingB">• Insert at the end.</p>
<p><a id="page_146"/>Other operations, such as the following, are not so easily handled:</p>
<p class="indenthangingB">• Remove a given node.</p>
<p class="indenthangingB">• Insert a new node before a given node.</p>
<p>For example, how can we remove the last node from a list? The link <code>last</code> is no help, because we need to set the link in the previous node in the list (the one with the same value as <code>last</code>) to <code>null</code>. In the absence of any other information, the only solution is to traverse the entire list looking for the node that links to <code>last</code> (see below and <a href="#ch01qa3q19"><small>EXERCISE 1.3.19</small></a>). Such a solution is undesirable because it takes time proportional to the length of the list. The standard solution to enable arbitrary insertions and deletions is to use a <em>doubly-linked list</em>, where each node has two links, one in each direction. We leave the code for these operations as an exercise (see <a href="#ch01qa3q31"><small>EXERCISE 1.3.31</small></a>). We do not need doubly linked lists for any of our implementations.</p>
<p><a id="ch01sec3lev104"/></p>
<h5><em>Traversal</em></h5>
<p>To examine every item in an array, we use familiar code like the following loop for processing the items in an array <code>a[]</code>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0146-01.jpg"/></p>
<p>There is a corresponding idiom for examining the items in a linked list: We initialize a loop index variable <code>x</code> to reference the first <code>Node</code> of the linked list. Then we find the item associated with <code>x</code> by accessing <code>x.item</code>, and then update <code>x</code> to refer to the next <code>Node</code> in the linked list, assigning to it the value of <code>x.next</code> and repeating this process until <code>x</code> is <code>null</code> (which indicates that we have reached the end of the linked list). This process is known as <em>traversing</em> the list and is succinctly expressed in code like the following loop for processing the items in a linked list whose first item is associated with the variable <code>first</code>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0146-02.jpg"/></p>
<p>This idiom is as natural as the standard idiom for iterating through the items in an array. In our implementations, we use it as the basis for iterators for providing client code the capability of iterating through the items, without having to know the details of the linked-list implementation.</p>
<p><a id="ch01sec3lev105"/></p>
<h5><a id="page_147"/><em>Stack implementation</em></h5>
<p>Given these preliminaries, developing an implementation for our <code>Stack</code> API is straightforward, as shown in <a href="#ch01sb06"><small>ALGORITHM 1.2</small></a> on page <a href="#ch01sb06">149</a>. It maintains the stack as a linked list, with the top of the stack at the beginning, referenced by an instance variable <code>first</code>. Thus, to <code>push()</code> an item, we add it to the beginning of the list, using the code discussed on page <a href="#page_144">144</a> and to <code>pop()</code> an item, we remove it from the beginning of the list, using the code discussed on page <a href="#ch01sec3lev101">145</a>. To implement <code>size()</code>, we keep track of the number of items in an instance variable <code>N</code>, incrementing <code>N</code> when we push and decrementing <code>N</code> when we pop. To implement <code>isEmpty()</code> we check whether <code>first</code> is <code>null</code> (alternatively, we could check whether <code>N</code> is <code>0</code>). The implementation uses the generic type <code>Item</code>—you can think of the code <code>&lt;Item&gt;</code> after the class name as meaning that any occurrence of <code>Item</code> in the implementation will be replaced by a client-supplied data-type name (see page <a href="#ch01sec3lev94">134</a>). For now, we omit the code to support iteration, which we consider on page <a href="#ch01sb08">155</a>. A trace for the test client that we have been using is shown on the next page. This use of linked lists achieves our optimum design goals:</p>
<p class="indenthangingB">• It can be used for any type of data.</p>
<p class="indenthangingB">• The space required is always proportional to the size of the collection.</p>
<p class="indenthangingB">• The time per operation is always independent of the size of the collection.</p>
<p>This implementation is a prototype for many <em>algorithm</em> implementations that we consider. It defines the linked-list <em>data structure</em> and implements the client methods <code>push()</code> and <code>pop()</code> that achieve the specified effect with just a few lines of code. The algorithms and data structure go hand in hand. In this case, the code for the algorithm implementations is quite simple, but the properties of the data structure are not at all elementary, requiring explanations on the past several pages. This interaction between data structure definition and algorithm implementation is typical and is our focus in ADT implementations throughout this book.</p>
<p class="image"><img alt="image" src="graphics/p0147-01.jpg"/></p>
<p class="image"><a id="page_148"/><img alt="image" src="graphics/01_36-linkedstack.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb06"/></p>
<h3><a id="page_149"/>Algorithm 1.2 Pushdown stack (linked-list implementation)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0149-01.jpg"/></p>
<p>This generic <code>Stack</code> implementation is based on a linked-list data structure. It can be used to create stacks containing any type of data. To support iteration, add the highlighted code described for <code>Bag</code> on page <a href="#ch01sb08">155</a>.</p>
<p class="image"><img alt="image" src="graphics/p0149-02.jpg"/></p>
<hr/>
</div>
<p><a id="ch01sec3lev106"/></p>
<h5><a id="page_150"/><em>Queue implementation</em></h5>
<p>An implementation of our <code>Queue</code> API based on the linked-list data structure is also straightforward, as shown in <a href="#ch01sb07"><small>ALGORITHM 1.3</small></a> on the facing page. It maintains the queue as a linked list in order from least recently to most recently added items, with the beginning of the queue referenced by an instance variable <code>first</code> and the end of the queue referenced by an instance variable <code>last</code>. Thus, to <code>enqueue()</code> an item, we add it to the end of the list (using the code discussed on page <a href="#ch01sec3lev101">145</a>, augmented to set both <code>first</code> and <code>last</code> to refer to the new node when the list is empty) and to <code>dequeue()</code> an item, we remove it from the beginning of the list (using the same code as for <code>pop()</code> in <code>Stack</code>, augmented to update <code>last</code> when the list becomes empty). The implementations of <code>size()</code> and <code>isEmpty()</code> are the same as for <code>Stack</code>. As with <code>Stack</code> the implementation uses the generic type parameter <code>Item</code>, and we omit the code to support iteration, which we consider in our <code>Bag</code> implementation on page <a href="#ch01sb08">155</a>. A development client similar to the one we used for <code>Stack</code> is shown below, and the trace for this client is shown on the following page. This implementation uses the same <em>data structure</em> as does <code>Stack</code>—a linked list—but it implements different <em>algorithms</em> for adding and removing items, which make the difference between LIFO and FIFO for the client. Again, the use of linked lists achieves our optimum design goals: it can be used for any type of data, the space required is proportional to the number of items in the collection, and the time required per operation is always independent of the size of the collection.</p>
<p class="image"><img alt="image" src="graphics/p0150-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0150-02.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb07"/></p>
<h3><a id="page_151"/>Algorithm 1.3 FIFO queue</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0151-01.jpg"/></p>
<p>This generic <code>Queue</code> implementation is based on a linked-list data structure. It can be used to create queues containing any type of data. To support iteration, add the highlighted code described for <code>Bag</code> on page <a href="#ch01sb08">155</a>.</p>
<hr/>
</div>
<p class="image"><a id="page_152"/><img alt="image" src="graphics/01_37-linkedqueue.jpg"/></p>
<p><a id="page_153"/><small>LINKED LISTS ARE A FUNDAMENTAL ALTERNATIVE</small> to arrays for structuring a collection of data. From a historical perspective, this alternative has been available to programmers for many decades. Indeed, a landmark in the history of programming languages was the development of LISP by John McCarthy in the 1950s, where linked lists are the primary structure for programs and data. Programming with linked lists presents all sorts of challenges and is notoriously difficult to debug, as you can see in the exercises. In modern code, the use of safe pointers, automatic garbage collection (see page <a href="ch01.html#page_111">111</a>), and ADTs allows us to encapsulate list-processing code in just a few classes such as the ones presented here.</p>
<p><a id="ch01sec3lev107"/></p>
<h5><a id="page_154"/><em>Bag implementation</em></h5>
<p>Implementing our <code>Bag</code> API using a linked-list data structure is simply a matter of changing the name of <code>push()</code> in <code>Stack</code> to <code>add()</code> and removing the implementation of <code>pop()</code>, as shown in <a href="#ch01sb08"><small>ALGORITHM 1.4</small></a> on the facing page (doing the same for <code>Queue</code> would also be effective but requires a bit more code). This implementation also highlights the code needed to make <code>Stack</code>, <code>Queue</code>, and <code>Bag</code> all iterable, by traversing the list. For <code>Stack</code> the list is in LIFO order; for <code>Queue</code> it is in FIFO order; and for <code>Bag</code> it happens to be in LIFO order, but the order is not relevant. As detailed in the highlighted code in <a href="#ch01sb08"><small>ALGORITHM 1.4</small></a>, to implement iteration in a collection, the first step is to include</p>
<p class="programlisting">import java.util.Iterator;</p>
<p>so that our code can refer to Java’s <code>Iterator</code> interface. The second step is to add</p>
<p class="programlisting">implements Iterable&lt;Item&gt;</p>
<p>to the class declaration, a promise to provide an <code>iterator()</code> method. The <code>iterator()</code> method itself simply returns an object from a class that implements the <code>Iterator</code> interface:</p>
<p class="programlisting">public Iterator&lt;Item&gt; iterator()<br/>
{  return new ListIterator();  }</p>
<p>This code is a promise to implement a class that implements the <code>hasNext()</code>, <code>next()</code>, and <code>remove()</code> methods that are called when a client uses the <em>foreach</em> construct. To implement these methods, the nested class <code>ListIterator</code> in <a href="#ch01sb08"><small>ALGORITHM 1.4</small></a> maintains an instance variable <code>current</code> that keeps track of the current node on the list. Then the <code>hasNext()</code> method tests if <code>current</code> is <code>null</code>, and the <code>next()</code> method saves a reference to the current item, updates <code>current</code> to refer to the next node on the list, and returns the saved reference.</p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb08"/></p>
<h3><a id="page_155"/>Algorithm 1.4 Bag</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0155-01.jpg"/></p>
<p>This <code>Bag</code> implementation maintains a linked list of the items provided in calls to <code>add()</code>. Code for <code>isEmpty()</code> and <code>size()</code> is the same as in <code>Stack</code> and is omitted. The iterator traverses the list, maintaining the current node in <code>current</code>. We can make <code>Stack</code> and <code>Queue</code> iterable by adding the code highlighted in red to <a href="#ch01sb04"><small>ALGORITHMS 1.1</small></a> and <a href="#ch01sb06">1.2</a>, because they use the same underlying data structure and <code>Stack</code> and <code>Queue</code> maintain the list in LIFO and FIFO order, respectively.</p>
<hr/>
</div>
<p><a id="ch01sec2lev27"/></p>
<h4><a id="page_156"/>Overview</h4>
<p>The implementations of bags, queues, and stacks that support generics and iteration that we have considered in this section provide a level of abstraction that allows us to write compact client programs that manipulate collections of objects. Detailed understanding of these ADTs is important as an introduction to the study of algorithms and data structures for three reasons. First, we use these data types as building blocks in higher-level data structures throughout this book. Second, they illustrate the interplay between data structures and algorithms and the challenge of simultaneously achieving natural performance goals that may conflict. Third, the focus of several of our implementations is on ADTs that support more powerful operations on collections of objects, and we use the implementations here as starting points.</p>
<p><a id="ch01sec3lev108"/></p>
<h5><em>Data structures</em></h5>
<p>We now have two ways to represent collections of objects, arrays and linked lists. Arrays are built in to Java; linked lists are easy to build with standard Java records. These two alternatives, often referred to as <em>sequential allocation</em> and <em>linked allocation</em>, are fundamental. Later in the book, we develop ADT implementations that combine and extend these basic structures in numerous ways. One important extension is to data structures with multiple links. For example, our focus in <a href="ch03.html#ch03sec1lev2"><small>SECTIONS 3.2</small></a> and <a href="ch03.html#ch03sec1lev3">3.3</a> is on data structures known as <em>binary trees</em> that are built from nodes that each have <em>two</em> links. Another important extension is to <em>compose</em> data structures: we can have a bag of stacks, a queue of arrays, and so forth. For example, our focus in <a href="ch04.html#ch04"><small>CHAPTER 4</small></a> is on graphs, which we represent as arrays of bags. It is very easy to define data structures of arbitrary complexity in this way: one important reason for our focus on abstract data types is an attempt to control such complexity.</p>
<p class="image"><img alt="image" src="graphics/t0156-01.jpg"/></p>
<p><a id="page_157"/><small>OUR TREATMENT OF BAGS, QUEUES, AND STACKS</small> in this section is a prototypical example of the approach that we use throughout this book to describe data structures and algorithms. In approaching a new applications domain, we identify computational challenges and use data abstraction to address them, proceeding as follows:</p>
<p class="indenthangingB">• Specify an API.</p>
<p class="indenthangingB">• Develop client code with reference to specific applications.</p>
<p class="indenthangingB">• Describe a data structure (representation of the set of values) that can serve as the basis for the <em>instance variables</em> in a class that will implement an ADT that meets the specification in the API.</p>
<p class="indenthangingB">• Describe algorithms (approaches to implementing the set of operations) that can serve as the basis for implementing the <em>instance methods</em> in the class.</p>
<p class="indenthangingB">• Analyze the performance characteristics of the algorithms.</p>
<p>In the next section, we consider this last step in detail, as it often dictates which algorithms and implementations can be most useful in addressing real-world applications.</p>
<p class="image"><img alt="image" src="graphics/t0159-01.jpg"/></p>
<p><a id="ch01sec2lev28"/></p>
<h4><a id="page_158"/>Q&amp;A</h4>
<p><strong>Q.</strong> Not all programming languages have generics, even early versions of Java. What are the alternatives?</p>
<p><strong>A.</strong> One alternative is to maintain a different implementation for each type of data, as mentioned in the text. Another is to build a stack of <code>Object</code> values, then cast to the desired type in client code for <code>pop()</code>. The problem with this approach is that type mismatch errors cannot be detected until run time. But with generics, if you write code to push an object of the wrong type on the stack, like this:</p>
<p class="programlisting"><img alt="image" src="graphics/p0158-01.jpg"/></p>
<p>you will get a compile-time error:</p>
<p class="programlisting">push(Apple) in Stack&lt;Apple&gt; cannot be applied to (Orange)</p>
<p>This ability to discover such errors at compile time is reason enough to use generics.</p>
<p><strong>Q.</strong> Why does Java disallow generic arrays?</p>
<p><strong>A.</strong> Experts still debate this point. You might need to become one to understand it! For starters, learn about <em>covariant arrays</em> and <em>type erasure</em>.</p>
<p><strong>Q.</strong> How do I create an array of stacks of strings?</p>
<p><strong>A.</strong> Use a cast, such as the following:</p>
<p class="programlisting"><img alt="image" src="graphics/p0158-03.jpg"/></p>
<p><em>Warning</em>: This cast, in client code, is different from the one described on page <a href="#ch01sec3lev94">134</a>. You might have expected to use <code>Object</code> instead of <code><span class="pd_red">Stack</span></code>. When using generics, Java checks for type safety at compile time, but throws away that information at run time, so it is left with <code>Stack&lt;Object&gt;[]</code> or just <code>Stack[]</code>, for short, which we must cast to <code>Stack&lt;String&gt;[]</code>.</p>
<p><strong>Q.</strong> What happens if my program calls <code>pop()</code> for an empty stack?</p>
<p><a id="page_159"/><strong>A.</strong> It depends on the implementation. For our implementation on page <a href="#ch01sb06">149</a>, you will get a <code>NullPointerException</code>. In our implementations on the booksite, we throw a runtime exception to help users pinpoint the error. Generally, including as many such checks as possible is wise in code that is likely to be used by many people.</p>
<p><strong>Q.</strong> Why do we care about resizing arrays, when we have linked lists?</p>
<p><strong>A.</strong> We will see several examples of ADT implementations that need to use arrays to perform other operations that are not easily supported with linked lists. <code>ResizingArrayStack</code> is a model for keeping their memory usage under control.</p>
<p><strong>Q.</strong> Why declare <code>Node</code> as a nested class? Why <code>private</code>?</p>
<p><strong>A.</strong> By declaring the nested class <code>Node</code> to be <code>private</code>, we restrict access to methods and instance variables within the enclosing class. One characteristic of a <code>private</code> nested class is that its instance variables can be directly accessed from within the enclosing class but nowhere else, so there is no need to declare the instance variables <code>public</code> or <code>private</code>. <em>Note for experts</em>: A nested class that is not static is known as an <em>inner</em> class, so technically our <code>Node</code> classes are inner classes, though the ones that are not generic could be static.</p>
<p><strong>Q.</strong> When I type <code>javac Stack.java</code> to run <a href="#ch01sb06"><small>ALGORITHM 1.2</small></a> and similar programs, I find <code>Stack.class</code> <em>and</em> a file <code>Stack$Node.class.</code> What is the purpose of that second one?</p>
<p><strong>A.</strong> That file is for the inner class <code>Node</code>. Java’s naming convention is to use <code>$</code> to separate the name of the outer class from the inner class.</p>
<p><strong>Q.</strong> Are there Java libraries for stacks and queues?</p>
<p><strong>A.</strong> Yes and no. Java has a built-in library called <code>java.util.Stack</code>, but you should avoid using it when you want a stack. It has several additional operations that are not normally associated with a stack, e.g., getting the <code>i</code>th element. It also allows adding an element to the bottom of the stack (instead of the top), so it can implement a queue! Although having such extra operations may appear to be a bonus, it is actually a curse. We use data types not just as libraries of all the operations we can imagine, but also as a mechanism to precisely specify the operations we need. The prime benefit of doing so is that the system can prevent us from performing operations that we do not actually <a id="page_160"/>want. The <code>java.util.Stack</code> API is an example of a <em>wide interface</em>, which we generally strive to avoid.</p>
<p><strong>Q.</strong> Should a client be allowed to insert <code>null</code> items onto a stack or queue?</p>
<p><strong>A.</strong> This question arises frequently when implementing collections in Java. Our implementation (and Java’s stack and queue libraries) do permit the insertion of <code>null</code> values.</p>
<p><strong>Q.</strong> What should the <code>Stack</code> iterator do if the client calls <code>push()</code> or <code>pop()</code> during iterator?</p>
<p><strong>A.</strong> Throw a <code>java.util.ConcurrentModificationException</code> to make it a <em>fail-fast iterator</em>. See <a href="#ch01qa3q50">1.3.50</a>.</p>
<p><strong>Q.</strong> Can I use a <em>foreach</em> loop with arrays?</p>
<p><strong>A.</strong> Yes (even though arrays do not implement the <code>Iterable</code> interface). The following one-liner prints out the command-line arguments:</p>
<p class="programlisting">public static void main(String[] args)<br/>
{  for (String s : args) StdOut.println(s);  }</p>
<p><strong>Q.</strong> Can I use a <em>foreach</em> loop with strings?</p>
<p><strong>A.</strong> No. <code>String</code> does not implement <code>Iterable</code>.</p>
<p><strong>Q.</strong> Why not have a single <code>Collection</code> data type that implements methods to add items, remove the most recently inserted, remove the least recently inserted, remove random, iterate, return the number of items in the collection, and whatever other operations we might desire? Then we could get them all implemented in a single class that could be used by many clients.</p>
<p><strong>A.</strong> Again, this is an example of a <em>wide interface</em>. Java has such implementations in its <code>java.util.ArrayList</code> and <code>java.util.LinkedList</code> classes. One reason to avoid them is that it there is no assurance that all operations are implemented efficiently. Throughout this book, we use APIs as starting points for designing efficient algorithms and data structures, which is certainly easier to do for interfaces with just a few operations as opposed to an interface with many operations. Another reason to insist on narrow interfaces is that they enforce a certain discipline on client programs, which makes client code much easier to understand. If one client uses <code>Stack&lt;String&gt;</code> and another uses <code>Queue&lt;Transaction&gt;</code>, we have a good idea that the LIFO discipline is important to the first and the FIFO discipline is important to the second.</p>
<p><a id="ch01sec2lev29"/></p>
<h4><a id="page_161"/>Exercises</h4>
<p><a id="ch01qa3q1"/><strong>1.3.1</strong> Add a method <code>isFull()</code> to <code>FixedCapacityStackOfStrings</code>.</p>
<p><a id="ch01qa3q2"/><strong>1.3.2</strong> Give the output printed by <code>java Stack</code> for the input</p>
<p class="programlisting">it was - the best - of times - - - it was - the - -</p>
<p><a id="ch01qa3q3"/><strong>1.3.3</strong> Suppose that a client performs an intermixed sequence of (stack) <em>push</em> and <em>pop</em> operations. The push operations put the integers 0 through 9 in order onto the stack; the pop operations print out the return values. Which of the following sequence(s) could <em>not</em> occur?</p>
<p class="indenthangingN"><em>a.</em> <code>4 3 2 1 0 9 8 7 6 5</code></p>
<p class="indenthangingN"><em>b.</em> <code>4 6 8 7 5 3 2 9 0 1</code></p>
<p class="indenthangingN"><em>c.</em> <code>2 5 6 7 4 8 9 3 1 0</code></p>
<p class="indenthangingN"><em>d.</em> <code>4 3 2 1 0 5 6 7 8 9</code></p>
<p class="indenthangingN"><em>e.</em> <code>1 2 3 4 5 6 9 8 7 0</code></p>
<p class="indenthangingN"><em>f.</em> <code>0 4 6 5 3 8 1 7 2 9</code></p>
<p class="indenthangingN"><em>g.</em> <code>1 4 7 9 8 6 5 3 0 2</code></p>
<p class="indenthangingN"><em>h.</em> <code>2 1 4 3 6 5 8 7 9 0</code></p>
<p><a id="ch01qa3q4"/><strong>1.3.4</strong> Write a stack client <code>Parentheses</code> that reads in a text stream from standard input and uses a stack to determine whether its parentheses are properly balanced. For example, your program should print <code>true</code> for <code>[()]{}{[()()]()}</code> and <code>false</code> for <code>[(])</code>.</p>
<p><a id="ch01qa3q5"/><strong>1.3.5</strong> What does the following code fragment print when <code>N</code> is <code>50</code>? Give a high-level description of what it does when presented with a positive integer <code>N</code>.</p>
<p class="programlisting"><img alt="image" src="graphics/p0161-02.jpg"/></p>
<p><em>Answer</em>: Prints the binary representation of <code>N</code> (<code>110010</code> when <code>N</code> is <code>50</code>).</p>
<p><a id="page_162"/><a id="ch01qa3q6"/><strong>1.3.6</strong> What does the following code fragment do to the queue <code>q</code>?</p>
<p class="programlisting"><img alt="image" src="graphics/p0162-01.jpg"/></p>
<p><a id="ch01qa3q7"/><strong>1.3.7</strong> Add a method <code>peek()</code> to <code>Stack</code> that returns the most recently inserted item on the stack (without popping it).</p>
<p><a id="ch01qa3q8"/><strong>1.3.8</strong> Give the contents and size of the array for <code>DoublingStackOfStrings</code> with the input</p>
<p class="programlisting">it was - the best - of times - - - it was - the - -</p>
<p><a id="ch01qa3q9"/><strong>1.3.9</strong> Write a program that takes from standard input an expression without left parentheses and prints the equivalent infix expression with the parentheses inserted. For example, given the input:</p>
<p class="programlisting">1 + 2 ) * 3 - 4 ) * 5 - 6 ) ) )</p>
<p>your program should print</p>
<p class="programlisting">( ( 1 + 2 ) * ( ( 3 - 4 ) * ( 5 - 6 ) ) )</p>
<p><a id="ch01qa3q10"/><strong>1.3.10</strong> Write a filter <code>InfixToPostfix</code> that converts an arithmetic expression from infix to postfix.</p>
<p><a id="ch01qa3q11"/><strong>1.3.11</strong> Write a program <code>EvaluatePostfix</code> that takes a postfix expression from standard input, evaluates it, and prints the value. (Piping the output of your program from the previous exercise to this program gives equivalent behavior to <code>Evaluate</code>.)</p>
<p><a id="ch01qa3q12"/><strong>1.3.12</strong> Write an iterable <code>Stack</code> <em>client</em> that has a static method <code>copy()</code> that takes a stack of strings as argument and returns a copy of the stack. <em>Note</em>: This ability is a prime example of the value of having an iterator, because it allows development of such functionality without changing the basic API.</p>
<p><a id="ch01qa3q13"/><strong>1.3.13</strong> Suppose that a client performs an intermixed sequence of (queue) <em>enqueue</em> and <em>dequeue</em> operations. The enqueue operations put the integers 0 through 9 in order onto <a id="page_163"/>the queue; the dequeue operations print out the return value. Which of the following sequence(s) could <em>not</em> occur?</p>
<p class="indenthangingN"><em>a.</em> <code>0 1 2 3 4 5 6 7 8 9</code></p>
<p class="indenthangingN"><em>b.</em> <code>4 6 8 7 5 3 2 9 0 1</code></p>
<p class="indenthangingN"><em>c.</em> <code>2 5 6 7 4 8 9 3 1 0</code></p>
<p class="indenthangingN"><em>d.</em> <code>4 3 2 1 0 5 6 7 8 9</code></p>
<p><a id="ch01qa3q14"/><strong>1.3.14</strong> Develop a class <code>ResizingArrayQueueOfStrings</code> that implements the queue abstraction with a fixed-size array, and then extend your implementation to use array resizing to remove the size restriction.</p>
<p><a id="ch01qa3q15"/><strong>1.3.15</strong> Write a <code>Queue</code> client that takes a command-line argument <code>k</code> and prints the <code>k</code>th from the last string found on standard input (assuming that standard input has <code>k</code> or more strings).</p>
<p><a id="ch01qa3q16"/><strong>1.3.16</strong> Using <code>readInts()</code> on page <a href="#ch01sec3lev90">126</a> as a model, write a static method <code>readDates()</code> for <code>Date</code> that reads dates from standard input in the format specified in the table on page <a href="ch01.html#page_119">119</a> and returns an array containing them.</p>
<p><a id="ch01qa3q17"/><strong>1.3.17</strong> Do <a href="#ch01qa3q16"><small>EXERCISE 1.3.16</small></a> for <code>Transaction</code>.</p>
<p><a id="ch01sec2lev30"/></p>
<h4><a id="page_164"/>Linked-List Exercises</h4>
<p><em>This list of exercises is intended to give you experience in working with linked lists. Suggestion: make drawings using the visual representation described in the text.</em></p>
<p><a id="ch01qa3q18"/><strong>1.3.18</strong> Suppose <code>x</code> is a linked-list node and not the last node on the list. What is the effect of the following code fragment?</p>
<p class="programlisting">x.next = x.next.next;</p>
<p><em>Answer</em>: Deletes from the list the node immediately following <code>x</code>.</p>
<p><a id="ch01qa3q19"/><strong>1.3.19</strong> Give a code fragment that removes the last node in a linked list whose first node is <code>first</code>.</p>
<p><a id="ch01qa3q20"/><strong>1.3.20</strong> Write a method <code>delete()</code> that takes an <code>int</code> argument <code>k</code> and deletes the <code>k</code>th element in a linked list, if it exists.</p>
<p><a id="ch01qa3q21"/><strong>1.3.21</strong> Write a method <code>find()</code> that takes a linked list and a string <code>key</code> as arguments and returns <code>true</code> if some node in the list has <code>key</code> as its item field, <code>false</code> otherwise.</p>
<p><a id="ch01qa3q22"/><strong>1.3.22</strong> Suppose that <code>x</code> is a linked list <code>Node</code>. What does the following code fragment do?</p>
<p class="programlisting">t.next = x.next;<br/>
x.next = t;</p>
<p><em>Answer</em>: Inserts node <code>t</code> immediately after node <code>x</code>.</p>
<p><a id="ch01qa3q23"/><strong>1.3.23</strong> Why does the following code fragment not do the same thing as in the previous question?</p>
<p class="programlisting">x.next = t;<br/>
t.next = x.next;</p>
<p><em>Answer</em>: When it comes time to update <code>t.next</code>, <code>x.next</code> is no longer the original node following <code>x</code>, but is instead <code>t</code> itself!</p>
<p><a id="ch01qa3q24"/><strong>1.3.24</strong> Write a method <code>removeAfter()</code> that takes a linked-list <code>Node</code> as argument and removes the node following the given one (and does nothing if the argument or the next field in the argument node is null).</p>
<p><a id="ch01qa3q25"/><strong>1.3.25</strong> Write a method <code>insertAfter()</code> that takes two linked-list <code>Node</code> arguments and inserts the second after the first on its list (and does nothing if either argument is null).</p>
<p><a id="page_165"/><a id="ch01qa3q26"/><strong>1.3.26</strong> Write a method <code>remove()</code> that takes a linked list and a string <code>key</code> as arguments and removes all of the nodes in the list that have <code>key</code> as its item field.</p>
<p><a id="ch01qa3q27"/><strong>1.3.27</strong> Write a method <code>max()</code> that takes a reference to the first node in a linked list as argument and returns the value of the maximum key in the list. Assume that all keys are positive integers, and return <code>0</code> if the list is empty.</p>
<p><a id="ch01qa3q28"/><strong>1.3.28</strong> Develop a recursive solution to the previous question.</p>
<p><a id="ch01qa3q29"/><strong>1.3.29</strong> Write a <code>Queue</code> implementation that uses a <em>circular</em> linked list, which is the same as a linked list except that no links are <em>null</em> and the value of <code>last.next</code> is <code>first</code> whenever the list is not empty. Keep only one <code>Node</code> instance variable (<code>last</code>).</p>
<p><a id="ch01qa3q30"/><strong>1.3.30</strong> Write a function that takes the first <code>Node</code> in a linked list as argument and (destructively) reverses the list, returning the first <code>Node</code> in the result.</p>
<p><em>Iterative solution</em>: To accomplish this task, we maintain references to three consecutive nodes in the linked list, <code>reverse</code>, <code>first</code>, and <code>second</code>. At each iteration, we extract the node <code>first</code> from the original linked list and insert it at the beginning of the reversed list. We maintain the invariant that <code>first</code> is the first node of what’s left of the original list, <code>second</code> is the second node of what’s left of the original list, and <code>reverse</code> is the first node of the resulting reversed list.</p>
<p class="programlisting"><img alt="image" src="graphics/p0165-01.jpg"/></p>
<p>When writing code involving linked lists, we must always be careful to properly handle the exceptional cases (when the linked list is empty, when the list has only one or two <a id="page_166"/>nodes) and the boundary cases (dealing with the first or last items). This is usually much trickier than handling the normal cases.</p>
<p><em>Recursive solution</em>: Assuming the linked list has <em>N</em> nodes, we recursively reverse the last <em>N</em>–1 nodes, and then carefully append the first node to the end.</p>
<p class="programlisting"><img alt="image" src="graphics/p0166-01.jpg"/></p>
<p><a id="ch01qa3q31"/><strong>1.3.31</strong> Implement a nested class <code>DoubleNode</code> for building doubly-linked lists, where each node contains a reference to the item preceding it and the item following it in the list (<code>null</code> if there is no such item). Then implement static methods for the following tasks: insert at the beginning, insert at the end, remove from the beginning, remove from the end, insert before a given node, insert after a given node, and remove a given node.</p>
<p><a id="ch01sec2lev31"/></p>
<h4><a id="page_167"/>Creative Problems</h4>
<p><a id="ch01qa3q32"/><strong>1.3.32</strong> <em>Steque.</em> A stack-ended queue or <em>steque</em> is a data type that supports <em>push</em>, <em>pop</em>, and <em>enqueue</em>. Articulate an API for this ADT. Develop a linked-list-based implementation.</p>
<p><a id="ch01qa3q33"/><strong>1.3.33</strong> <em>Deque.</em> A double-ended queue or <em>deque</em> (pronounced “deck”) is like a stack or a queue but supports adding and removing items at both ends. A deque stores a collection of items and supports the following API:</p>
<p class="image"><img alt="image" src="graphics/t0167-01.jpg"/></p>
<p>Write a class <code>Deque</code> that uses a doubly-linked list to implement this API and a class <code>ResizingArrayDeque</code> that uses a resizing array.</p>
<p><a id="ch01qa3q34"/><strong>1.3.34</strong> <em>Random bag.</em> A <em>random bag</em> stores a collection of items and supports the following API:</p>
<p class="image"><img alt="image" src="graphics/t0167-02.jpg"/></p>
<p>Write a class <code>RandomBag</code> that implements this API. Note that this API is the same as for <code>Bag</code>, except for the adjective <em>random</em>, which indicates that the iteration should provide <a id="page_168"/>the items in <em>random</em> order (all <em>N</em>! permutations equally likely, for each iterator). <em>Hint</em>: Put the items in an array and randomize their order in the iterator’s constructor.</p>
<p><a id="ch01qa3q35"/><strong>1.3.35</strong> <em>Random queue.</em> A <em>random queue</em> stores a collection of items and supports the following API:</p>
<p class="image"><img alt="image" src="graphics/t0168-01.jpg"/></p>
<p>Write a class <code>RandomQueue</code> that implements this API. <em>Hint</em>: Use an array representation (with resizing). To remove an item, swap one at a random position (indexed <code>0</code> through <code>N-1</code>) with the one at the last position (index <code>N-1</code>). Then delete and return the last object, as in <code>ResizingArrayStack</code>. Write a client that deals bridge hands (13 cards each) using <code>RandomQueue&lt;Card&gt;</code>.</p>
<p><a id="ch01qa3q36"/><strong>1.3.36</strong> <em>Random iterator.</em> Write an iterator for <code>RandomQueue&lt;Item&gt;</code> from the previous exercise that returns the items in random order.</p>
<p><a id="ch01qa3q37"/><strong>1.3.37</strong> <em>Josephus problem.</em> In the Josephus problem from antiquity, <em>N</em> people are in dire straits and agree to the following strategy to reduce the population. They arrange themselves in a circle (at positions numbered from 0 to <em>N</em>–1) and proceed around the circle, eliminating every <em>M</em>th person until only one person is left. Legend has it that Josephus figured out where to sit to avoid being eliminated. Write a <code>Queue</code> client <code>Josephus</code> that takes <em>M</em> and <em>N</em> from the command line and prints out the order in which people are eliminated (and thus would show Josephus where to sit in the circle).</p>
<p class="programlisting">% java Josephus 2 7<br/>
1 3 5 0 4 2 6</p>
<p><a id="page_169"/><a id="ch01qa3q38"/><strong>1.3.38</strong> <em>Delete</em> <code>k</code><em>th element.</em> Implement a class that supports the following API:</p>
<p class="image"><img alt="image" src="graphics/t0169-01.jpg"/></p>
<p>First, develop an implementation that uses an array implementation, and then develop one that uses a linked-list implementation. <em>Note</em>: the algorithms and data structures that we introduce in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> make it possible to develop an implementation that can guarantee that both <code>insert()</code> and <code>delete()</code> take time prortional to the logarithm of the number of items in the queue—see <a href="ch03a.html#ch03qa5q27"><small>EXERCISE 3.5.27</small></a>.</p>
<p><a id="ch01qa3q39"/><strong>1.3.39</strong> <em>Ring buffer.</em> A ring buffer, or circular queue, is a FIFO data structure of a fixed size <em>N</em>. It is useful for transferring data between asynchronous processes or for storing log files. When the buffer is empty, the consumer waits until data is deposited; when the buffer is full, the producer waits to deposit data. Develop an API for a <code>RingBuffer</code> and an implementation that uses an array representation (with circular wrap-around).</p>
<p><a id="ch01qa3q40"/><strong>1.3.40</strong> <em>Move-to-front.</em> Read in a sequence of characters from standard input and maintain the characters in a linked list with no duplicates. When you read in a previously unseen character, insert it at the front of the list. When you read in a duplicate character, delete it from the list and reinsert it at the beginning. Name your program <code>MoveToFront</code>: it implements the well-known <em>move-to-front</em> strategy, which is useful for caching, data compression, and many other applications where items that have been recently accessed are more likely to be reaccessed.</p>
<p><a id="ch01qa3q41"/><strong>1.3.41</strong> <em>Copy a queue.</em> Create a new constructor so that</p>
<p class="programlisting">Queue&lt;Item&gt; r = new Queue&lt;Item&gt;(q);</p>
<p>makes <code>r</code> a reference to a new and independent copy of the queue <code>q</code>. You should be able to push and pop from either <code>q</code> or <code>r</code> without influencing the other. <em>Hint</em>: Delete all of the elements from <code>q</code> and add these elements to both <code>q</code> and <code>r</code>.</p>
<p><a id="page_170"/><a id="ch01qa3q42"/><strong>1.3.42</strong> <em>Copy a stack.</em> Create a new constructor for the linked-list implementation of <code>Stack</code> so that</p>
<p class="programlisting">Stack&lt;Item&gt; t = new Stack&lt;Item&gt;(s);</p>
<p>makes <code>t</code> a reference to a new and independent copy of the stack <code>s</code>.</p>
<p><a id="ch01qa3q43"/><strong>1.3.43</strong> <em>Listing files.</em> A folder is a list of files and folders. Write a program that takes the name of a folder as a command-line argument and prints out all of the files contained in that folder, with the contents of each folder recursively listed (indented) under that folder’s name. <em>Hint</em>: Use a queue, and see <code>java.io.File</code>.</p>
<p><a id="ch01qa3q44"/><strong>1.3.44</strong> <em>Text editor buffer.</em> Develop a data type for a buffer in a text editor that implements the following API:</p>
<p class="image"><img alt="image" src="graphics/t0170-01.jpg"/></p>
<p><em>Hint</em>: Use two stacks.</p>
<p><a id="ch01qa3q45"/><strong>1.3.45</strong> <em>Stack generability.</em> Suppose that we have a sequence of intermixed <em>push</em> and <em>pop</em> operations as with our test stack client, where the integers <code>0</code>, <code>1</code>, ..., <code>N-1</code> in that order (<em>push</em> directives) are intermixed with <em>N</em> minus signs (<em>pop</em> directives). Devise an algorithm that determines whether the intermixed sequence causes the stack to underflow. (You may use only an amount of space independent of <em>N</em>—you cannot store the integers in a data structure.) Devise a linear-time algorithm that determines whether a given permutation can be generated as output by our test client (depending on where the <em>pop</em> directives occur).</p>
<p><a id="page_171"/><em>Solution</em>: The stack does not overflow unless there exists an integer <em>k</em> such that the first <em>k</em> pop operations occur before the first <em>k</em> push operations. If a given permutation can be generated, it is uniquely generated as follows: if the next integer in the output permutation is in the top of the stack, pop it; otherwise, push it onto the stack.</p>
<p><a id="ch01qa3q46"/><strong>1.3.46</strong> <em>Forbidden triple for stack generability.</em> Prove that a permutation can be generated by a stack (as in the previous question) if and only if it has no forbidden triple (<em>a</em>, <em>b</em>, <em>c</em>) such that <em>a</em> &lt; <em>b</em> &lt; <em>c</em> with <em>c</em> first, <em>a</em> second, and <em>b</em> third (possibly with other intervening integers between <em>c</em> and <em>a</em> and between <em>a</em> and <em>b</em>).</p>
<p><em>Partial solution</em>: Suppose that there is a forbidden triple (<em>a</em>, <em>b</em>, <em>c</em>). Item <em>c</em> is popped before <em>a</em> and <em>b</em>, but <em>a</em> and <em>b</em> are pushed before <em>c</em>. Thus, when <em>c</em> is pushed, both <em>a</em> and <em>b</em> are on the stack. Therefore, <em>a</em> cannot be popped before <em>b</em>.</p>
<p><a id="ch01qa3q47"/><strong>1.3.47</strong> <em>Catenable queues, stacks, or steques.</em> Add an extra operation <em>catenation</em> that (destructively) concatenates two queues, stacks, or steques (see <a href="#ch01qa3q32"><small>EXERCISE 1.3.32</small></a>). <em>Hint</em>: Use a circular linked list, maintaining a pointer to the last item.</p>
<p><a id="ch01qa3q48"/><strong>1.3.48</strong> <em>Two stacks with a deque.</em> Implement two stacks with a single deque so that each operation takes a constant number of deque operations (see <a href="#ch01qa3q33"><small>EXERCISE 1.3.33</small></a>).</p>
<p><a id="ch01qa3q49"/><strong>1.3.49</strong> <em>Queue with a constant number of stacks.</em> Implement a queue with a constant number of stacks so that each queue operation takes a constant (worst-case) number of stack operations. <em>Warning</em>: high degree of difficulty.</p>
<p><a id="ch01qa3q50"/><strong>1.3.50</strong> <em>Fail-fast iterator.</em> Modify the iterator code in <code>Stack</code> to immediately throw a <code>java.util.ConcurrentModificationException</code> if the client modifies the collection (via <code>push()</code> or <code>pop()</code>) during iteration?</p>
<p><em>Solution</em>: Maintain a counter that counts the number of <code>push()</code> and <code>pop()</code> operations. When creating an iterator, store this value as an <code>Iterator</code> instance variable. Before each call to <code>hasNext()</code> and <code>next()</code>, check that this value has not changed since construction of the iterator; if it has, throw the exception.</p>
<p><a id="ch01sec1lev6"/></p>
<h3><a id="page_172"/>1.4 Analysis of Algorithms</h3>
<p><small>AS PEOPLE GAIN EXPERIENCE USING COMPUTERS</small>, they use them to solve difficult problems or to process large amounts of data and are invariably led to questions like these:</p>
<p class="center"><em>How long will my program take?</em></p>
<p class="center"><em>Why does my program run out of memory?</em></p>
<p>You certainly have asked yourself these questions, perhaps when rebuilding a music or photo library, installing a new application, working with a large document, or working with a large amount of experimental data. The questions are much too vague to be answered precisely—the answers depend on many factors such as properties of the particular computer being used, the particular data being processed, and the particular program that is doing the job (which implements some algorithm). All of these factors leave us with a daunting amount of information to analyze.</p>
<p>Despite these challenges, the path to developing useful answers to these basic questions is often remarkably straightforward, as you will see in this section. This process is based on the <em>scientific method</em>, the commonly accepted body of techniques used by scientists to develop knowledge about the natural world. We apply <em>mathematical analysis</em> to develop concise models of costs and do <em>experimental studies</em> to validate these models.</p>
<p><a id="ch01sec2lev32"/></p>
<h4>Scientific method</h4>
<p>The very same approach that scientists use to understand the natural world is effective for studying the running time of programs:</p>
<p class="indenthangingB">• <em>Observe</em> some feature of the natural world, generally with precise measurements.</p>
<p class="indenthangingB">• <em>Hypothesize</em> a model that is consistent with the observations.</p>
<p class="indenthangingB">• <em>Predict</em> events using the hypothesis.</p>
<p class="indenthangingB">• <em>Verify</em> the predictions by making further observations.</p>
<p class="indenthangingB">• <em>Validate</em> by repeating until the hypothesis and observations agree.</p>
<p>One of the key tenets of the scientific method is that the experiments we design must be <em>reproducible</em>, so that others can convince themselves of the validity of the hypothesis. Hypotheses must also be <em>falsifiable</em>, so that we can know for sure when a given hypothesis is wrong (and thus needs revision). As Einstein famously is reported to have said (“<em>No amount of experimentation can ever prove me right; a single experiment can prove me wrong</em>”), we can never know for sure that any hypothesis is absolutely correct; we can only validate that it is consistent with our observations.</p>
<p><a id="ch01sec2lev33"/></p>
<h4><a id="page_173"/>Observations</h4>
<p>Our first challenge is to determine how to make quantitative measurements of the running time of our programs. This task is far easier than in the natural sciences. We do not have to send a rocket to Mars or kill laboratory animals or split an atom—we can simply run the program. Indeed, <em>every</em> time you run a program, you are performing a scientific experiment that relates the program to the natural world and answers one of our core questions: <em>How long will my program take?</em></p>
<p>Our first qualitative observation about most programs is that there is a <em>problem size</em> that characterizes the difficulty of the computational task. Normally, the problem size is either the size of the input or the value of a command-line argument. Intuitively, the running time should increase with problem size, but the question of <em>by how much</em> it increases naturally comes up every time we develop and run a program.</p>
<p>Another qualitative observation for many programs is that the running time is relatively insensitive to the input itself; it depends primarily on the problem size. If this relationship does not hold, we need to take steps to better understand and perhaps better control the running time’s sensitivity to the input. But it does often hold, so we now focus on the goal of better quantifying the relationship between problem size and running time.</p>
<p><a id="ch01sec3lev109"/></p>
<h5><em>Example</em></h5>
<p>As a running example, we will work with the program <code>ThreeSum</code> shown here, which counts the number of triples in a file of <code>N</code> integers that sum to 0 (assuming that overflow plays no role). This computation may seem contrived to you, but it is deeply related to numerous fundamental computational tasks (for example, see <a href="#ch01qa4q26"><small>EXERCISE 1.4.26</small></a>). As a test input, consider the file <code>1Mints.txt</code> from the booksite, which contains 1 million randomly generated <code>int</code> values. The second, eighth, and tenth entries in <code>1Mints.txt</code> sum to 0. How many more such triples are there in the file? <code>ThreeSum</code> can tell us, but can it do so in a reasonable amount of time? What is the relationship between the problem size <code>N</code> and running time for <code>ThreeSum</code>? As a first experiment, try running <code>ThreeSum</code> on your computer for the files <code>1Kints.tx</code>t, <code>2Kints.txt</code>, <code>4Kints.txt</code>, and <code>8Kints.txt</code> on the <a id="page_174"/>booksite that contain the first 1,000, 2,000, 4,000, and 8,000 integers from <code>1Mints.txt</code>, respectively. You can quickly determine that there are 70 triples that sum to 0 in <code>1Kints.txt</code> and that there are 528 triples that sum to 0 in <code>2Kints.txt</code>. The program takes substantially more time to determine that there are 4,039 triples that sum to 0 in <code>4Kints.txt</code>, and as you wait for the program to finish for <code>8Kints.txt</code>, you will find yourself asking the question <em>How long will my program take</em>? As you will see, answering this question for this program turns out to be easy. Indeed, you can often come up with a fairly accurate prediction while the program is running.</p>
<p class="image"><img alt="image" src="graphics/p0173-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0174-01.jpg"/></p>
<p><a id="ch01sec3lev110"/></p>
<h5><em>Stopwatch</em></h5>
<p>Reliably measuring the exact running time of a given program can be difficult. Fortunately, we are usually happy with estimates. We want to be able to distinguish programs that will finish in a few seconds or a few minutes from those that might require a few days or a few months or more, and we want to know when one program is twice as fast as another for the same task. Still, we need accurate measurements to generate experimental data that we can use to formulate and to check the validity of hypotheses about the relationship between running time and problem size. For this purpose, we use the <code>Stopwatch</code> data type shown on the facing page. Its <code>elapsedTime()</code> method returns the elapsed time since it was created, in seconds. The implementation is based on using the Java system’s <code>currentTimeMillis()</code> method, which gives the current time in milliseconds, to save the time when the constructor is invoked, then uses it again to compute the elapsed time when <code>elapsedTime()</code> is invoked.</p>
<p class="image"><img alt="image" src="graphics/01_38-time.jpg"/></p>
<p class="image"><a id="page_175"/><img alt="image" src="graphics/p0175-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0175-02.jpg"/></p>
<p><a id="ch01sec3lev111"/></p>
<h5><a id="page_176"/><em>Analysis of experimental data</em></h5>
<p>The program <code>DoublingTest</code> on the facing page is a more sophisticated <code>Stopwatch</code> client that produces experimental data for <code>ThreeSum.</code> It generates a sequence of random input arrays, doubling the array size at each step, and prints the running times of <code>ThreeSum.count()</code> for each input size. These experiments are certainly reproducible—you can also run them on your own computer, as many times as you like. When you run <code>DoublingTest</code>, you will find yourself in a prediction-verification cycle: it prints several lines very quickly, but then slows down considerably. Each time it prints a line, you find yourself wondering how long it will be until it prints the next line. Of course, since you have a different computer from ours, the actual running times that you get are likely to be different from those shown for our computer. Indeed, if your computer is twice as fast as ours, your running times will be about half ours, which leads immediately to the well-founded hypothesis that running times on different computers are likely to differ by a constant factor. Still, you will find yourself asking the more detailed question <em>How long will my program take, as a function of the input size?</em> To help answer this question, we plot the data. The diagrams at the bottom of the facing page show the result of plotting the data, both on a normal and on a log-log scale, with the problem size <em>N</em> on the <em>x</em>-axis and the running time <em>T</em>(<em>N</em>) on the <em>y</em>-axis. The log-log plot immediately leads to a hypothesis about the running time—the data fits a straight line of slope 3 on the log-log plot. The equation of such a line is</p>
<p class="center">lg(<em>T</em>(<em>N</em>)) = 3 lg <em>N</em> + lg <em>a</em></p>
<p>(where <em>a</em> is a constant) which is equivalent to</p>
<p class="center"><em>T</em>(<em>N</em>) = <em>aN</em><sup>3</sup></p>
<p>the running time, as a function of the input size, as desired. We can use one of our data points to solve for <em>a</em>—for example, <em>T</em>(8000) = 51.1 = <em>a</em>8000<sup>3</sup>, so <em>a</em> = 9.98×10<sup>−11</sup>—and then use the equation</p>
<p class="center"><em>T</em>(<em>N</em>) = 9.98×10<sup>−11</sup><em>N</em><sup>3</sup></p>
<p>to predict running times for large <em>N</em>. Informally, we are checking the hypothesis that the data points on the log-log plot fall close to this line. Statistical methods are available for doing a more careful analysis to find estimates of <em>a</em> and the exponent <em>b</em>, but our quick calculations suffice to estimate running time for most purposes. For example, we can estimate the running time on our computer for <em>N</em> = 16,000 to be about 9.98×10<sup>−11</sup>16000<sup>3</sup> = 408.8 seconds, or about 6.8 minutes (the actual time was 409.3 seconds). While waiting for your computer to print the line for <em>N</em> = 16,000 in <code>DoublingTest</code>, you might use this method to predict when it will finish, then check the result by waiting to see if your prediction is true.</p>
<p class="programlisting"><a id="page_177"/><img alt="image" src="graphics/p0177-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/01_39-dataplot.jpg"/></p>
<p><a id="page_178"/>So far, this process mirrors the process scientists use when trying to understand properties of the real world. A straight line in a log-log plot is equivalent to the hypothesis that the data fits the equation <em>T</em>(<em>N</em>) <em>= aN<sup>b</sup></em>. Such a fit is known as a <em>power law</em>. A great many natural and synthetic phenomena are described by power laws, and it is reasonable to hypothesize that the running time of a program does, as well. Indeed, for the analysis of algorithms, we have mathematical models that strongly support this and similar hypotheses, to which we now turn.</p>
<p><a id="ch01sec2lev34"/></p>
<h4>Mathematical models</h4>
<p>In the early days of computer science, D. E. Knuth postulated that, despite all of the complicating factors in understanding the running times of our programs, it is possible, in principle, to build a mathematical model to describe the running time of any program. Knuth’s basic insight is simple: the total running time of a program is determined by two primary factors:</p>
<p class="indenthangingB">• The cost of executing each statement</p>
<p class="indenthangingB">• The frequency of execution of each statement</p>
<p>The former is a property of the computer, the Java compiler and the operating system; the latter is a property of the program and the input. If we know both for all instructions in the program, we can multiply them together and sum for all instructions in the program to get the running time.</p>
<p>The primary challenge is to determine the frequency of execution of the statements. Some statements are easy to analyze: for example, the statement that sets <code>cnt</code> to 0 in <code>ThreeSum.count()</code> is executed exactly once. Others require higher-level reasoning: for example, the <code>if</code> statement in <code>ThreeSum.count()</code> is executed precisely</p>
<p class="center"><em>N(N</em>−1<em>)(N</em>−2<em>)/</em>6</p>
<p>times (the number of ways to pick three different numbers from the input array—see <a href="#ch01qa4q1"><small>EXERCISE 1.4.1</small></a>). Others depend on the input data: for example the number of times the instruction <code>cnt++</code> in <code>ThreeSum.count()</code> is executed is precisely the number of triples that sum to 0 in the input, which could range from 0 of them to all of them. In the case of <code>DoublingTest</code>, where we generate the numbers randomly, it is possible to do a probabilistic analysis to determine the expected value of this quantity (see <a href="#ch01qa4q40"><small>EXERCISE 1.4.40</small></a>).</p>
<p><a id="ch01sec3lev112"/></p>
<h5><em>Tilde approximations</em></h5>
<p>Frequency analyses of this sort can lead to complicated and lengthy mathematical expressions. For example, consider the count just considered of the number of times the <code>if</code> statement in <code>ThreeSum</code> is executed:</p>
<p class="center"><em>N(N</em>−1<em>)(N</em>−2<em>)/</em>6 = <em>N<sup>3</sup>/</em>6 − <em>N<sup>2</sup>/</em>2 + <em>N/</em>3</p>
<p><a id="page_179"/>As is typical in such expressions, the terms after the leading term are relatively small (for example, when <em>N</em> = 1,000 the value of − <em>N</em><sup>2</sup>/2 + <em>N</em>/3 ≈ −499,667 is certainly insignificant by comparison with <em>N</em><sup>3</sup>/6 ≈ 166,666,667). To allow us to ignore insignificant terms and therefore substantially simplify the mathematical formulas that we work with, we often use a mathematical device known as the <em>tilde notation</em> (~). This notation allows us to work with <em>tilde approximations</em>, where we throw away low-order terms that complicate formulas and represent a negligible contribution to values of interest:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb09"/></p>
<p><strong>Definition.</strong> We write ~<em>f</em>(<em>N</em>) to represent any function that, when divided by <em>f</em>(<em>N</em>), approaches 1 as <em>N</em> grows, and we write <em>g</em>(<em>N</em>) ~ <em>f</em>(<em>N</em>) to indicate that <em>g</em>(<em>N</em>)/<em>f</em>(<em>N</em>) approaches 1 as <em>N</em> grows.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/t0179-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/01_40-cubic.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0179-02.jpg"/></p>
<p>For example, we use the approximation <em>~N</em><sup>3</sup>/6 to describe the number of times the <code>if</code> statement in <code>ThreeSum</code> is executed, since <em>N</em><sup>3</sup>/6 − <em>N</em><sup>2</sup>/2 + <em>N</em>/3 divided by <em>N</em><sup>3</sup>/6 approaches 1 as <em>N</em> grows. Most often, we work with tilde approximations of the form <em>g</em>(<em>N</em>) ~<em>af</em>(<em>N</em>) where <em>f</em>(<em>N</em>) = <em>N<sup>b</sup></em>(log <em>N</em>)<em><sup>c</sup></em> with <em>a</em>, <em>b</em>, and <em>c</em> constants and refer to <em>f</em>(<em>N</em>) as the <em>order of growth</em> of <em>g</em>(<em>N</em>). When using the logarithm in the order of growth, we generally do not specify the base, since the constant <em>a</em> can absorb that detail. This usage covers the relatively few functions that are commonly encountered in studying the order of growth of a program’s running time shown in the table at left (with the exception of the exponential, which we defer to CONTEXT). We will describe these functions in more detail and briefly discuss why they appear in the analysis of algorithms after we complete our treatment of <code>ThreeSum</code>.</p>
<p><a id="ch01sec3lev113"/></p>
<h5><a id="page_180"/><em>Approximate running time</em></h5>
<p>To follow through on Knuth’s approach to develop a mathematical expression for the total running time of a Java program, we can (in principle) study our Java compiler to find the number of machine instructions corresponding to each Java instruction and study our machine specifications to find the time of execution of each of the machine instructions, to produce a grand total. This process, for <code>ThreeSum</code>, is briefly summarized on the facing page. We classify blocks of Java statements by their frequency of execution, develop leading-term approximations for the frequencies, determine the cost of each statement, and then compute a total. Note that some frequencies may depend on the input. In this case, the number of times <code>cnt++</code> is executed certainly depends on the input—it is the number of triples that sum to 0, and could range from 0 to ~<em>N</em><sup>3</sup>/6. We stop short of exhibiting the details (values of the constants) for any particular system, except to highlight that by using constant values <em>t</em><sub>0</sub>, <em>t</em><sub>1</sub>, <em>t</em><sub>2</sub>, ... for the time taken by the blocks of statements, we are assuming that each block of Java statements corresponds to machine instructions that require a specified fixed amount of time. A key observation from this exercise is to note that only the instructions that are executed the most frequently play a role in the final total—we refer to these instructions as the <em>inner loop</em> of the program. For <code>ThreeSum</code>, the inner loop is the statements that increment <code>k</code> and test that it is less than <code>N</code> and the statements that test whether the sum of three given numbers is 0 (and possibly the statement that implements the count, depending on the input). This behavior is typical: the running times of a great many programs depend only on a small subset of their instructions.</p>
<p><a id="ch01sec3lev114"/></p>
<h5><em>Order-of-growth hypothesis</em></h5>
<p>In summary, the experiments on page <a href="#page_177">177</a> and the mathematical model on page <a href="#page_181">181</a> both support the following hypothesis:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb10"/></p>
<p><strong>Property A.</strong> The order of growth of the running time of <code>ThreeSum</code> (to compute the number of triples that sum to 0 among <em>N</em> numbers) is <em>N</em><sup>3</sup>.</p>
<p><strong>Evidence:</strong> Let <em>T(N)</em> be the running time of <code>ThreeSum</code> for <em>N</em> numbers. The mathematical model just described suggests that <em>T(N) ~ aN</em><sup>3</sup> for some machine-dependent constant <em>a</em>; experiments on many computers (including yours and ours) validate that approximation.</p>
<hr/>
</div>
<p>Throughout this book, we use the term <em>property</em> to refer to a hypothesis that needs to be validated through experimentation. The end result of our mathematical analysis is precisely the same as the end result of our experimental analysis—the running time of <code>ThreeSum</code> is ~ <em>aN</em><sup>3</sup> for a machine-dependent constant <em>a</em>. This match validates both the experiments and the mathematical model and also exhibits more insight about the <a id="page_182"/>program because it does not require experimentation to determine the exponent. With some effort, we could validate the value of <em>a</em> on a particular system as well, though that activity is generally reserved for experts in situations where performance is critical.</p>
<p class="image"><a id="page_181"/><img alt="image" src="graphics/01_41-anatomythree.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0181-01.jpg"/></p>
<p><a id="ch01sec3lev115"/></p>
<h5><em>Analysis of algorithms</em></h5>
<p>Hypotheses such as <a href="#ch01sb10"><small>PROPERTY A</small></a> are significant because they relate the abstract world of a Java program to the real world of a computer running it. Working with the order of growth allows us to take one further step: to separate a program from the algorithm it implements. The idea that the order of growth of the running time of <code>ThreeSum</code> is <em>N</em><sup>3</sup> does not depend on the fact that it is implemented in Java or that it is running on your laptop or someone else’s cellphone or a supercomputer; it depends primarily on the fact that it examines all the different triples of numbers in the input. The <em>algorithm</em> that you are using (and sometimes the input model) determines the order of growth. Separating the algorithm from the implementation on a particular computer is a powerful concept because it allows us to develop knowledge about the performance of algorithms and then apply that knowledge to any computer. For example, we might say that <code>ThreeSum</code> is an implementation of the brute-force algorithm “<em>compute the sum of all different triples, counting those that sum to</em> 0”—we expect that an implementation of this algorithm in any programming language on any computer will lead to a running time that is proportional to <em>N</em><sup>3</sup>. In fact, much of the knowledge about the performance of classic algorithms was developed decades ago, but that knowledge is still relevant to today’s computers.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb11"/></p>
<p><strong>3-sum cost model.</strong> When studying algorithms to solve the 3-sum problem, we count <em>array accesses</em> (the number of times an array entry is accessed, for read or write).</p>
<hr/>
</div>
<p><a id="ch01sec3lev116"/></p>
<h5><em>Cost model</em></h5>
<p>We focus attention on properties of algorithms by articulating a <em>cost model</em> that defines the basic operations used by the algorithms we are studying to solve the problem at hand. For example, an appropriate cost model for the 3-sum problem, shown at right, is the number of times we access an array entry. With this cost model, we can make precise mathematical statements about properties of an algorithm, not just a particular implementation, as follows:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb12"/></p>
<p><strong>Proposition B.</strong> The brute-force 3-sum algorithm uses <em>~N</em><sup>3</sup>/2 array accesses to compute the number of triples that sum to 0 among <em>N</em> numbers.</p>
<p><strong>Proof:</strong> The algorithm accesses each of the 3 numbers for each of the <em>~N</em><sup>3</sup>/6 triples.</p>
<hr/>
</div>
<p>We use the term <em>proposition</em> to refer to mathematical truths about algorithms in terms of a cost model. Throughout this book, we study the algorithms that we consider within <a id="page_183"/>the framework of a specific cost model. Our intent is to articulate cost models such that the order of growth of the running time for a given implementation is the same as the order of growth of the cost of the underlying algorithm (in other words, the cost model should include operations that fall within the inner loop). We seek precise mathematical results about algorithms (propositions) and also hypotheses about performance of implementations (properties) that you can check through experimentation. In this case, <a href="#ch01sb12"><small>PROPOSITION B</small></a> is a mathematical truth that supports the hypothesis stated in <a href="#ch01sb10"><small>PROPERTY A</small></a>, which we have validated with experiments, in accordance with the scientific method.</p>
<p><a id="ch01sec3lev117"/></p>
<h5><a id="page_184"/><em>Summary</em></h5>
<p>For many programs, developing a mathematical model of running time reduces to the following steps:</p>
<p class="indenthangingB">• Develop an <em>input model</em>, including a definition of the problem size.</p>
<p class="indenthangingB">• Identify the <em>inner loop</em>.</p>
<p class="indenthangingB">• Define a <em>cost model</em> that includes operations in the inner loop.</p>
<p class="indenthangingB">• Determine the frequency of execution of those operations for the given input. Doing so might require mathematical <em>analysis</em>—we will consider some examples in the context of specific fundamental algorithms later in the book.</p>
<p>If a program is defined in terms of multiple methods, we normally consider the methods separately. As an example, consider our example program of <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a>, <code>BinarySearch</code>.</p>
<p class="indenthanging"><strong><em>Binary search.</em></strong> The <em>input model</em> is the array <code>a[]</code> of size <em>N</em>; the <em>inner loop</em> is the statements in the single <code>while</code> loop; the <em>cost model</em> is the compare operation (compare the values of two array entries); and the <em>analysis</em>, discussed in <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a> and given in full detail in <a href="#ch01sb12"><small>PROPOSITION B</small></a> in <a href="ch03.html#ch03sec1lev1"><small>SECTION 3.1</small></a>, shows that the number of compares is at most lg <em>N</em> + 1.</p>
<p class="indenthanging"><strong><em>Whitelist.</em></strong> The <em>input model</em> is the <em>N</em> numbers in the whitelist and the <em>M</em> numbers on standard input where we assume <em>M</em> &gt;&gt; <em>N</em>; the <em>inner loop</em> is the statements in the single <code>while</code> loop; the <em>cost model</em> is the compare operation (inherited from binary search); and the <em>analysis</em> is immediate given the analysis of binary search—the number of compares is at most <em>M</em> (lg <em>N</em> + 1).</p>
<p>Thus, we draw the conclusion that the order of growth of the running time of the whitelist computation is at most <em>M</em> lg <em>N,</em> subject to the following considerations:</p>
<p class="indenthangingB">• If <em>N</em> is small, the input-output cost might dominate.</p>
<p class="indenthangingB">• The number of compares depends on the input—it lies between ~<em>M</em> and ~<em>M</em> lg<em>N,</em> depending on how many of the numbers on standard input are in the whitelist and on how long the binary search takes to find the ones that are (typically it is ~<em>M</em> lg<em>N</em>).</p>
<p class="indenthangingB">• We are assuming that the cost of <code>Arrays.sort()</code> is small compared to <em>M</em> lg <em>N</em>. <code>Arrays.sort()</code> implements the <em>mergesort</em> algorithm, and in <a href="ch02.html#ch02sec1lev2"><small>SECTION 2.2</small></a>, we will see that the order of growth of the running time of mergesort is <em>N</em> log <em>N</em> (see <a href="ch02.html#ch02sb16"><small>PROPOSITION G</small></a> in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>), so this assumption is justified.</p>
<p>Thus, the model supports our hypothesis from <a href="ch01.html#ch01sec1lev3"><small>SECTION 1.1</small></a> that the <em>binary search algorithm</em> makes the computation feasible when <em>M</em> and <em>N</em> are large. If we double the length of the standard input stream, then we can expect the running time to double; if we double the size of the whitelist, then we can expect the running time to increase only slightly.</p>
<p><a id="page_185"/><small>DEVELOPING MATHEMATICAL MODELS</small> for the analysis of algorithms is a fruitful area of research that is somewhat beyond the scope of this book. Still, as you will see with binary search, mergesort, and many other algorithms, understanding certain mathematical models is critical to understanding the efficiency of fundamental algorithms, so we often present details and/or quote the results of classic studies. When doing so, we encounter various functions and approximations that are widely used in mathematical analysis. For reference, we summarize some of this information in the tables below.</p>
<p class="image"><img alt="image" src="graphics/t0185-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0185-02.jpg"/></p>
<p><a id="ch01sec2lev35"/></p>
<h4><a id="page_186"/>Order-of-growth classifications</h4>
<p>We use just a few structural primitives (statements, conditionals, loops, nesting, and method calls) to implement algorithms, so very often the order of growth of the cost is one of just a few functions of the problem size <em>N</em>. These functions are summarized in the table on the facing page, along with the names that we use to refer to them, typical code that leads to each function, and examples.</p>
<p><a id="ch01sec3lev118"/></p>
<h5><em>Constant</em></h5>
<p>A program whose running time’s order of growth is <em>constant</em> executes a fixed number of operations to finish its job; consequently its running time does not depend on <em>N</em>. Most Java operations take constant time.</p>
<p><a id="ch01sec3lev119"/></p>
<h5><em>Logarithmic</em></h5>
<p>A program whose running time’s order of growth is <em>logarithmic</em> is barely slower than a constant-time program. The classic example of a program whose running time is logarithmic in the problem size is <em>binary search</em> (see <code>BinarySearch</code> on page <a href="ch01.html#ch01sb02">47</a>). The base of the logarithm is not relevant with respect to the order of growth (since all logarithms with a constant base are related by a constant factor), so we use log <em>N</em> when referring to order of growth.</p>
<p><a id="ch01sec3lev120"/></p>
<h5><em>Linear</em></h5>
<p>Programs that spend a constant amount of time processing each piece of input data, or that are based on a single <code>for</code> loop, are quite common. The order of growth of such a program is said to be <em>linear</em>—its running time is proportional to <em>N</em>.</p>
<p><a id="ch01sec3lev121"/></p>
<h5><em>Linearithmic</em></h5>
<p>We use the term <em>linearithmic</em> to describe programs whose running time for a problem of size <em>N</em> has order of growth <em>N</em> log <em>N</em>. Again, the base of the logarithm is not relevant with respect to the order of growth. The prototypical examples of linearithmic algorithms are <code>Merge.sort()</code> (see <a href="ch02.html#ch02sb15"><small>ALGORITHM 2.4</small></a>) and <code>Quick.sort()</code> (see <a href="ch02.html#ch02sb21"><small>ALGORITHM 2.5</small></a>).</p>
<p><a id="ch01sec3lev122"/></p>
<h5><em>Quadratic</em></h5>
<p>A typical program whose running time has order of growth <em>N</em><sup>2</sup> has two nested <code>for</code> loops, used for some calculation involving all pairs of <em>N</em> elements. The elementary sorting algorithms <code>Selection.sort()</code> (see <a href="ch02.html#ch02sb05"><small>ALGORITHM 2.1</small></a>) and <code>Insertion.sort()</code> (see <a href="ch02.html#ch02sb07"><small>ALGORITHM 2.2</small></a>) are prototypes of the programs in this classification.</p>
<p><a id="ch01sec3lev123"/></p>
<h5><em>Cubic</em></h5>
<p>A typical program whose running time has order of growth <em>N</em><sup>3</sup> has three nested <code>for</code> loops, used for some calculation involving all triples of <em>N</em> elements. Our example for this section, <code>ThreeSum</code>, is a prototype.</p>
<p><a id="ch01sec3lev124"/></p>
<h5><em>Exponential</em></h5>
<p>In <a href="ch06.html#ch06"><small>CHAPTER 6</small></a> (but not until then!) we will consider programs whose running times are proportional to 2<em><sup>N</sup></em> or higher. Generally, we use the term <em>exponential</em> to refer to algorithms whose order of growth is <em>b<sup>N</sup></em> for any constant <em>b</em> &gt; 1, even though different values of <em>b</em> lead to vastly different running times. Exponential algorithms are extremely slow—you will never run one of them to completion for a large problem. Still, exponential algorithms play a critical role in the theory of algorithms because <a id="page_188"/>there exists a large class of problems for which it seems that an exponential algorithm is the best possible choice.</p>
<p class="image"><a id="page_187"/><img alt="image" src="graphics/t0187-01.jpg"/></p>
<p><small>THESE CLASSIFICATIONS ARE THE MOST COMMON,</small> but certainly not a complete set. The order of growth of an algorithm’s cost might be <em>N</em><sup>2</sup>log<em>N</em> or <em>N</em><sup>3/2</sup> or some similar function. Indeed, the detailed analysis of algorithms can require the full gamut of mathematical tools that have been developed over the centuries.</p>
<p>A great many of the algorithms that we consider have straightforward performance characteristics that can be accurately described by one of the orders of growth that we have considered. Accordingly, we can usually work with specific propositions with a cost model, such as <em>mergesort uses between ½N</em>lg <em>N and N</em>lg<em>N compares</em> that immediately imply hypotheses (properties) such as <em>the order of growth of mergesort’s running time is linearithmic</em>. For economy, we abbreviate such a statement to just say <em>mergesort is linearithmic</em>.</p>
<p class="image"><img alt="image" src="graphics/01_42-loglog.jpg"/></p>
<p>The plots at left indicate the importance of the order of growth in practice. The <em>x</em>-axis is the problem size; the <em>y</em>-axis is the running time. These charts make plain that quadratic and cubic algorithms are not feasible for use on large problems. As it turns out, several important problems have natural solutions that are quadratic but clever algorithms that are linearithmic. Such algorithms (including mergesort) are critically important in practice because they enable us to address problem sizes far larger than could be addressed with quadratic solutions. Naturally, we therefore focus in this book on developing logarithmic, linear, and linearithmic algorithms for fundamental problems.</p>
<p><a id="ch01sec2lev36"/></p>
<h4><a id="page_189"/>Designing faster algorithms</h4>
<p>One of the primary reasons to study the order of growth of a program is to help design a faster algorithm to solve the same problem. To illustrate this point, we consider next a faster algorithm for the 3-sum problem. How can we devise a faster algorithm, before even embarking on the study of algorithms? The answer to this question is that we <em>have</em> discussed and used two classic algorithms, <em>mergesort</em> and <em>binary search</em>, have introduced the facts that the mergesort is linearithmic and binary search is logarithmic. How can we take advantage of these algorithms to solve the 3-sum problem?</p>
<p><a id="ch01sec3lev125"/></p>
<h5><em>Warmup: 2-sum</em></h5>
<p>Consider the easier problem of determining the number of <em>pairs</em> of integers in an input file that sum to 0. To simplify the discussion, assume also that the integers are distinct. This problem is easily solved in quadratic time by deleting the <code>k</code> loop and <code>a[k]</code> from <code>ThreeSum.count()</code>, leaving a double loop that examines all pairs, as shown in the <em>quadratic</em> entry in the table on page <a href="#page_187">187</a> (we refer to such an implementation as <code>TwoSum</code>). The implementation below shows how mergesort and binary search (see page <a href="ch01.html#ch01sb02">47</a>) can serve as a basis for a <em>linearithmic</em> solution to the 2-sum problem. The improved algorithm is based on the fact that an entry <code>a[i]</code> is one of a pair that sums to 0 if and only if the value <code>-a[i]</code> is in the array (and <code>a[i]</code> is not zero). To solve the problem, we sort the array (to enable binary search) and then, for every entry <code>a[i]</code> in the array, do a binary search for <code>-a[i]</code> with <code>rank()</code> in <code>BinarySearch</code>. If the result is an index <code>j</code> with <code>j &gt; i</code>, we increment the count. This succinct test covers three cases:</p>
<p class="indenthangingB">• An unsuccessful binary search returns <code>-1</code>, so we do not increment the count.</p>
<p class="indenthangingB">• If the binary search returns <code>j &gt; i</code>, we have <code>a[i] + a[j] = 0</code>, so we increment the count.</p>
<p class="indenthangingB">• If the binary search returns <code>j</code> between <code>0</code> and <code>i</code>, we also have <code>a[i] + a[j] = 0</code> but do not increment the count, to avoid double counting.</p>
<p class="image"><img alt="image" src="graphics/p0189-01.jpg"/></p>
<p>The result of the computation is precisely the same as the result of the quadratic algorithm, but it takes much less time. The running time of the mergesort is <a id="page_190"/>proportional to <em>N</em> log <em>N</em>, and the <em>N</em> binary searches each take time proportional to log <em>N</em>, so the running time of the whole algorithm is proportional to <em>N</em> log <em>N</em>. Developing a faster algorithm like this is not merely an academic exercise—the faster algorithm enables us to address much larger problems. For example, you are likely to be able to solve the 2-sum problem for 1 million integers (<code>1Mints.txt</code>) in a reasonable amount of time on your computer, but you would have to wait quite a long time to do it with the quadratic algorithm (see <a href="#ch01qa4q41"><small>EXERCISE 1.4.41</small></a>).</p>
<p><a id="ch01sec3lev126"/></p>
<h5><em>Fast algorithm for 3-sum</em></h5>
<p>The very same idea is effective for the 3-sum problem. Again, assume also that the integers are distinct. A pair <code>a[i]</code> and <code>a[j]</code> is part of a triple that sums to 0 if and only if the value <code>-(a[i] + a[j])</code> is in the array (and not <code>a[i]</code> or <code>a[j]</code>). The code below sorts the array, then does <em>N</em>(<em>N</em>−1)<em>/</em>2 binary searches that each take time proportional to log <em>N</em>, for a total running time proportional to <em>N</em><sup>2</sup> log <em>N</em>. Note that in this case the cost of the sort is insignificant. Again, this solution enables us to address much larger problems (see <a href="#ch01qa4q42"><small>EXERCISE 1.4.42</small></a>). The plots in the figure at the bottom of the next page show the disparity in costs among these four algorithms for problem sizes in the range we have considered. Such differences certainly motivate the search for faster algorithms.</p>
<p><a id="ch01sec3lev127"/></p>
<h5><em>Lower bounds</em></h5>
<p>The table on page <a href="#page_191">191</a> summarizes the discussion of this section. An interesting question immediately arises: Can we find algorithms for the 2-sum and 3-sum problems that are substantially faster than <code>TwoSumFast</code> and <code>ThreeSumFast</code>? Is there a linear algorithm for 2-sum or a linearithmic algorithm for 3-sum? The answer to this question is <em>no</em> for 2-sum (under a model that counts and allows only comparisons of linear or quadratic functions of the numbers) and <em>no one knows</em> for 3-sum, though experts believe that the best possible algorithm for 3-sum is quadratic. The idea of a lower bound on the order of growth of the worst-case running time for all possible algorithms to solve a problem is a very powerful one, which we will <a id="page_191"/>revisit in detail in <a href="ch02.html#ch02sec1lev2"><small>SECTION 2.2</small></a> in the context of sorting. Nontrivial lower bounds are difficult to establish, but very helpful in guiding our search for efficient algorithms.</p>
<p class="image"><img alt="image" src="graphics/p0190-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0191-01.jpg"/></p>
<p><small>THE EXAMPLES IN THIS SECTION SET THE STAGE</small> for our treatment of algorithms in this book. Throughout the book, our strategy for addressing new problems is the following:</p>
<p class="indenthangingB">• Implement and analyze a straighforward solution to the problem. We usually refer to such solutions, like <code>ThreeSum</code> and <code>TwoSum</code>, as the <em>brute-force</em> solution.</p>
<p class="indenthangingB">• Examine algorithmic improvements, usually designed to reduce the order of growth of the running time, such as <code>TwoSumFast</code> and <code>ThreeSumFast</code>.</p>
<p class="indenthangingB">• Run experiments to validate the hypotheses that the new algorithms are faster.</p>
<p>In many cases, we examine <em>several</em> algorithms for the same problem, because running time is only one consideration when choosing an algorithm for a practical problem. We will develop this idea in detail in the context of fundamental problems throughout the book.</p>
<p class="image"><img alt="image" src="graphics/01_43-costplot.jpg"/></p>
<p><a id="ch01sec2lev37"/></p>
<h4><a id="page_192"/>Doubling ratio experiments</h4>
<p>The following is a simple and effective shortcut for predicting performance and for determining the approximate order of growth of the running time of any program:</p>
<p class="indenthangingB">• Develop an input generator that produces inputs that model the inputs expected in practice (such as the random integers in <code>timeTrial()</code> in <code>DoublingTest</code>.</p>
<p class="indenthangingB">• Run the program <code>DoublingRatio</code> given below, a modification of <code>DoublingTest</code> that calculates the ratio of each running time with the previous.</p>
<p class="indenthangingB">• Run until the ratios approach a limit 2<em><sup>b</sup></em>.</p>
<p>This test is not effective if the ratios do not approach a limiting value, but they do for many, many programs, implying the following conclusions:</p>
<p class="indenthangingB">• The order of growth of the running time is approximately <em>N<sup>b</sup></em>.</p>
<p class="indenthangingB">• To predict running times, multiply the last observed running time by 2<em><sup>b</sup></em> and double <em>N</em>, continuing as long as desired. If you want to predict for an input size that is not a power of 2 times <em>N</em>, you can adjust ratios accordingly (see <a href="#ch01qa4q9"><small>EXERCISE 1.4.9</small></a>).</p>
<p>As illustrated below, the ratio for <code>ThreeSum</code> is about 8 and we can predict the running times for <em>N</em> = 16,000, 32,000, 64,000 to be 408.8, 3270.4, 26163.2 seconds, respectively, just by successively multiplying the last time for 8,000 (51.1) by 8.</p>
<p class="image"><img alt="image" src="graphics/p0192-01.jpg"/></p>
<p><a id="page_193"/>This test is roughly equivalent to the process described on page <a href="#ch01sec3lev111">176</a> (run experiments, plot values on a log-log plot to develop the hypothesis that the running time is <em>aN<sup>b</sup></em>, determine the value of <em>b</em> from the slope of the line, then solve for <em>a</em>), but it is simpler to apply. Indeed, you can accurately predict preformance by hand when you run <code>DoublingRatio</code>. As the ratio approaches a limit, just multiply by that ratio to fill in later values in the table. Your approximate model of the order of growth is a power law with the binary logarithm of that ratio as the power.</p>
<p>Why does the ratio approach a constant? A simple mathematical calculation shows that to be the case for all of the common orders of growth just discussed (except exponential):</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb13"/></p>
<p><strong>Proposition C. (Doubling ratio)</strong> If T(<em>N</em>) ~ <em>aN<sup>b</sup></em>lg<em>N</em> then T(2<em>N</em>)/T(<em>N</em>) ~ 2<em><sup>b</sup></em>.</p>
<p><strong>Proof:</strong> Immediate from the following calculation:</p>
<p class="image"><img alt="image" src="graphics/t0193-01.jpg"/></p>
<hr/>
</div>
<p>Generally, the logarithmic factor cannot be ignored when developing a mathematical model, but it plays a less important role in predicting performance with a doubling hypothesis.</p>
<p><small>YOU SHOULD CONSIDER</small> running doubling ratio experiments for every program that you write where performance matters—doing so is a very simple way to estimate the order of growth of the running time, perhaps revealing a performance bug where a program may turn out to be not as efficient as you might think. More generally, we can use hypotheses about the order of growth of the running time of programs to predict performance in one of the following ways:</p>
<p><a id="ch01sec3lev128"/></p>
<h5><em>Estimating the feasibility of solving large problems</em></h5>
<p>You need to be able to answer this basic question for every program that you write: <em>Will the program be able to process this given input data in a reasonable amount of time?</em> To address such questions for a large amount of data, we extrapolate by a much larger factor than for doubling, say 10, as shown in the fourth column in the table at the bottom of the next page. Whether it is an investment banker running daily financial models or a scientist running a program to analyze experimental data or an engineer running simulations to test a design, it is not unusual for people to regularly run programs that take several hours to complete, <a id="page_194"/>so the table focuses on that situation. Knowing the order of growth of the running time of an algorithm provides precisely the information that you need to understand limitations on the size of the problems that you can solve. <em>Developing such understanding is the most important reason to study performance.</em> Without it, you are likely have no idea how much time a program will consume; with it, you can make a back-of-the-envelope calculation to estimate costs and proceed accordingly.</p>
<p><a id="ch01sec3lev129"/></p>
<h5><em>Estimating the value of using a faster computer</em></h5>
<p>You also may be faced with this basic question, periodically: <em>How much faster can I solve the problem if I get a faster computer?</em> Generally, if the new computer is <em>x</em> times faster than the old one, you can improve your running time by a factor of <em>x</em>. But it is usually the case that you can address larger problems with your new computer. How will that change affect the running time? Again, the order of growth is precisely the information needed to answer that question.</p>
<p>A FAMOUS RULE OF THUMB known as <em>Moore’s Law</em> implies that you can expect to have a computer with about double the speed and double the memory 18 months from now, or a computer with about 10 times the speed and 10 times the memory in about 5 years. The table below demonstrates that you cannot keep pace with Moore’s Law if you are using a quadratic or a cubic algorithm, and you can quickly determine whether that is the case by doing a doubling ratio test and checking that the ratio of running times as the input size doubles approaches 2, not 4 or 8.</p>
<p class="image"><img alt="image" src="graphics/t0194-01.jpg"/></p>
<p><a id="ch01sec2lev38"/></p>
<h4><a id="page_195"/>Caveats</h4>
<p>There are many reasons that you might get inconsistent or misleading results when trying to analyze program performance in detail. All of them have to do with the idea that one or more of the basic assumptions underlying our hypotheses might be not quite correct. We can develop new hypotheses based on new assumptions, but the more details that we need to take into account, the more care is required in the analysis.</p>
<p><a id="ch01sec3lev130"/></p>
<h5><em>Large constants</em></h5>
<p>With leading-term approximations, we ignore constant coefficients in lower-order terms, which may not be justifed. For example, when we approximate the function 2<em>N</em><sup>2</sup> + <em>c N</em> by ~2<em>N</em><sup>2</sup>, we are assuming that <em>c</em> is small. If that is not the case (suppose that <em>c</em> is 10<sup>3</sup> or 10<sup>6</sup>) the approximation is misleading. Thus, we have to be sensitive to the possibility of large constants.</p>
<p><a id="ch01sec3lev131"/></p>
<h5><em>Nondominant inner loop</em></h5>
<p>The assumption that the inner loop dominates may not always be correct. The cost model might miss the true inner loop, or the problem size <em>N</em> might not be sufficiently large to make the leading term in the mathematical description of the frequency of execution of instructions in the inner loop so much larger than lower-order terms that we can ignore them. Some programs have a significant amount of code outside the inner loop that needs to be taken into consideration. In other words, the cost model may need to be refined.</p>
<p><a id="ch01sec3lev132"/></p>
<h5><em>Instruction time</em></h5>
<p>The assumption that each instruction always takes the same amount of time is not always correct. For example, most modern computer systems use a technique known as <em>caching</em> to organize memory, in which case accessing elements in huge arrays can take much longer if they are not close together in the array. You might observe the effect of caching for <code>ThreeSum</code> by letting <code>DoublingTest</code> run for a while. After seeming to converge to 8, the ratio of running times may jump to a larger value for large arrays because of caching.</p>
<p><a id="ch01sec3lev133"/></p>
<h5><em>System considerations</em></h5>
<p>Typically, there are many, many things going on in your computer. Java is one application of many competing for resources, and Java itself has many options and controls that significantly affect performance. A garbage collector or a just-in-time compiler or a download from the internet might drastically affect the results of experiments. Such considerations can interfere with the bedrock principle of the scientific method that experiments should be reproducible, since what is happening at this moment in your computer will never be reproduced again. Whatever else is going on in your system should <em>in principle</em> be negligible or possible to control.</p>
<p><a id="ch01sec3lev134"/></p>
<h5><em>Too close to call</em></h5>
<p>Often, when we compare two different programs for the same task, one might be faster in some situations, and slower in others. One or more of the considerations just mentioned could make the difference. There is a natural tendency among <a id="page_196"/>some programmers (and some students) to devote an extreme amount of energy running races to find the “best” implementation, but such work is best left for experts.</p>
<p><a id="ch01sec3lev135"/></p>
<h5><em>Strong dependence on inputs</em></h5>
<p>One of the first assumptions that we made in order to determine the order of growth of the program’s running time of a program was that the running time should be relatively insensitive to the inputs. When that is not the case, we may get inconsistent results or be unable to validate our hypotheses. For example, suppose that we modify <code>ThreeSum</code> to answer the question <em>Does the input have a triple that sums to 0</em> ? by changing it to return a <code>boolean</code> value, replacing <code>cnt++</code> by <code>return true</code> and adding <code>return false</code> as the last statement. The order of growth of the running time of this program is <em>constant</em> if the first three integers sum to 0 and <em>cubic</em> if there are no such triples in the input.</p>
<p><a id="ch01sec3lev136"/></p>
<h5><em>Multiple problem parameters</em></h5>
<p>We have been focusing on measuring performance as a function of a <em>single</em> parameter, generally the value of a command-line argument or the size of the input. However, it is not unusual to have several parameters. A typical example arises when an algorithm involves building a data structure and then performing a sequence of operations that use that data structure. Both the size of the data structure and the number of operations are parameters for such applications. We have already seen an example of this in our analysis of the problem of whitelisting using binary search, where we have <em>N</em> numbers in the whitelist and <em>M</em> numbers on standard input and a typical running time proportional to <em>M</em> log<em>N</em>.</p>
<p>Despite all these caveats, understanding the order of growth of the running time of each program is valuable knowledge for any programmer, and the methods that we have described are powerful and broadly applicable. Knuth’s insight was that we can carry these methods through to the last detail <em>in principle</em> to make detailed, accurate predictions. Typical computer systems are extremely complex and close analysis is best left for experts, but the same methods are effective for developing approximate estimates of the running time of any program. A rocket scientist needs to have some idea of whether a test flight will land in the ocean or in a city; a medical researcher needs to know whether a drug trial will kill or cure all the subjects; and any scientist or engineer using a computer program needs to have some idea of whether it will run for a second or for a year.</p>
<p><a id="ch01sec2lev39"/></p>
<h4><a id="page_197"/>Coping with dependence on inputs</h4>
<p>For many problems, one of the most significant of the caveats just mentioned is the dependence on inputs, because running times can vary widely. The running time of the modification of <code>ThreeSum</code> mentioned on the facing page ranges from constant to cubic, depending on the input, so a closer analysis is required if we want to predict performance. We briefly consider here some of the approaches that are effective and that we will consider for specific algorithms later in the book.</p>
<p><a id="ch01sec3lev137"/></p>
<h5><em>Input models</em></h5>
<p>One approach is to more carefully model the kind of input to be processed in the problems that we need to solve. For example, we might assume that the numbers in the input to <code>ThreeSum</code> are random <code>int</code> values. This approach is challenging for two reasons:</p>
<p class="indenthangingB">• The model may be unrealistic.</p>
<p class="indenthangingB">• The analysis may be extremely difficult, requiring mathematical skills quite beyond those of the typical student or programmer.</p>
<p>The first of these is the more significant, often because the goal of a computation is to <em>discover</em> characteristics of the input. For example, if we are writing a program to process a genome, how can we estimate its performance on a different genome? A good model describing the genomes found in nature is precisely what scientists seek, so estimating the running time of our programs on data found in nature actually amounts to contributing to that model! The second challenge leads to a focus on mathematical results only for our most important algorithms. We will see several examples where a simple and tractable input model, in conjunction with classical mathematical analysis, helps us predict performance.</p>
<p><a id="ch01sec3lev138"/></p>
<h5><em>Worst-case performance guarantees</em></h5>
<p>Some applications demand that the running time of a program be less than a certain bound, no matter what the input. To provide such performance <em>guarantees</em>, theoreticians take an extremely pessimistic view of the performance of algorithms: what would the running time be in the <em>worst case</em>? For example, such a conservative approach might be appropriate for the software that runs a nuclear reactor or a pacemaker or the brakes in your car. We want to guarantee that such software completes its job within the bounds that we set because the result could be catastrophic if it does not. Scientists normally do not contemplate the worst case when studying the natural world: in biology, the worst case might be the extinction of the human race; in physics, the worst case might be the end of the universe. But the worst case can be a very real concern in computer systems, where the input may be generated by another (potentially malicious) user, rather than by nature. For example, websites that do not use algorithms with performance guarantees are subject to <em>denial-of-service</em> attacks, where hackers flood them with pathological requests that make them <a id="page_198"/>run much more slowly than planned. Accordingly, many of our algorithms are designed to provide performance guarantees, such as the following:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb14"/></p>
<p><strong>Proposition D.</strong> In the linked-list implementations of <code>Bag</code> (<a href="#ch01sb08"><small>ALGORITHM 1.4</small></a>), <code>Stack</code> (<a href="#ch01sb06"><small>ALGORITHM 1.2</small></a>), and <code>Queue</code> (<a href="#ch01sb07"><small>ALGORITHM 1.3</small></a>), all operations take constant time in the worst case.</p>
<p><strong>Proof:</strong> Immediate from the code. The number of instructions executed for each operation is bounded by a small constant. <em>Caveat</em>: This argument depends upon the (reasonable) assumption that the Java system creates a new <code>Node</code> in constant time.</p>
<hr/>
</div>
<p><a id="ch01sec3lev139"/></p>
<h5><em>Randomized algorithms</em></h5>
<p>One important way to provide a performance guarantee is to introduce randomness. For example, the quicksort algorithm for sorting that we study in <a href="ch02.html#ch02sec1lev3"><small>SECTION 2.3</small></a> (perhaps the most widely used sorting algorithm) is quadratic in the worst case, but randomly ordering the input gives a probabilistic guarantee that its running time is linearithmic. Every time you run the algorithm, it will take a different amount of time, but the chance that the time will not be linearithmic is so small as to be negligible. Similarly, the hashing algorithms for symbol tables that we study in <a href="ch03.html#ch03sec1lev4"><small>SECTION 3.4</small></a> (again, perhaps the most widely used approach) are linear-time in the worst case, but constant-time under a probabilistic guarantee. These guarantees are not absolute, but the chance that they are invalid is less than the chance your computer will be struck by lightning. Thus, such guarantees are as useful in practice as worst-case guarantees.</p>
<p><a id="ch01sec3lev140"/></p>
<h5><em>Sequences of operations</em></h5>
<p>For many applications, the algorithm “input” might be not just data, but the sequence of operations performed by the client. For example, a pushdown stack where the client pushes <em>N</em> values, then pops them all, may have quite different performance characteristics from one where the client issues an alternating sequence <em>N</em> of push and pop operations. Our analysis has to take both situations into account (or to include a reasonable model of the sequence of operations).</p>
<p><a id="ch01sec3lev141"/></p>
<h5><em>Amortized analysis</em></h5>
<p>Accordingly, another way to provide a performance guarantee is to <em>amortize</em> the cost, by keeping track of the total cost of all operations, divided by the number of operations. In this setting, we can allow some expensive operations, while keeping the average cost of operations low. The prototypical example of this type of analysis is the study of the resizing array data structure for <code>Stack</code> that we considered in <a href="#ch01sec1lev5"><small>SECTION 1.3</small></a> (<a href="#ch01sb04"><small>ALGORITHM 1.1</small></a> on page <a href="#ch01sb04">141</a>). For simplicity, suppose that <em>N</em> is a power of 2. Starting with an empty structure, how many array entries are accessed for <em>N</em> consecutive calls to <code>push()</code>? This quantity is easy to calculate: the number of array accesses is</p>
<p class="center"><a id="page_199"/><em>N</em> + 4 + 8 + 16 + ... + 2<em>N</em> = 5<em>N</em> − 4</p>
<p>The first term accounts for the array access within each of the <em>N</em> calls to <code>push()</code>; the subsequent terms account for the array accesses to initialize the data structure each time it doubles in size. Thus the <em>average number of array accesses per operation</em> is constant, even though the last operation takes linear time. This is known as an “amortized” analysis because we spread the cost of the few expensive operations, by assigning a portion of it to each of a large number of inexpensive operations. Amortized analysis provides a <em>worst case</em> guarantee on any sequence of operations, starting from an empty data structure. <code>VisualAccumulator</code> provides illustrates the process, shown above.</p>
<p class="image"><img alt="image" src="graphics/01_44-amortize.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb15"/></p>
<p><strong>Proposition E.</strong> In the resizing array implementation of <code>Stack</code> (<a href="#ch01sb04"><small>ALGORITHM 1.1</small></a>), the average number of array accesses for any sequence of push and pop operations starting from an empty data structure is constant in the worst case.</p>
<p><strong>Proof sketch:</strong> For each push operation that causes the array to grow (say from size <em>N</em> to size 2<em>N</em>), consider the <em>N</em>/2 – 1 push operations that most recently caused the stack size to grow to <em>k</em>, for <em>k</em> from <em>N</em>/2 <em>+</em> 2 to <em>N</em>. Averaging the 4<em>N</em> array accesses to grow the array with <em>N</em>/2 array accesses (one for each push), we get an average cost of 9 array accesses for each of these <em>N/2</em> – 1 push operations. Establishing this proposition for any sequence of push and pop operations is more intricate (see <a href="#ch01qa4q32"><small>EXERCISE 1.4.32</small></a>)</p>
<hr/>
</div>
<p>This kind of analysis is widely applicable. In particular, we use resizing arrays as the underlying data structure for several algorithms that we consider later in this book.</p>
<p><small>IT IS THE TASK OF THE ALGORITHM ANALYST</small> to discover as much relevant information about an algorithm as possible, and it is the task of the applications programmer to apply that knowledge to develop programs that effectively solve the problems at hand. Ideally, we want algorithms that lead to clear and compact code that provides both a good guarantee and good performance on input values of interest. Many of the classic algorithms that we consider in this chapter are important for a broad variety of applications precisely because they have these properties. Using them as models, you can develop good solutions yourself for typical problems that you face while programming.</p>
<p><a id="ch01sec2lev40"/></p>
<h4><a id="page_200"/>Memory</h4>
<p>As with running time, a program’s memory usage connects directly to the physical world: a substantial amount of your computer’s circuitry enables your program to store values and later retrieve them. The more values you need to have stored at any given instant, the more circuitry you need. You probably are aware of limits on memory usage on your computer (even more so than for time) because you probably have paid extra money to get more memory.</p>
<p>Memory usage is well-defined for Java on your computer (every value requires precisely the same amount of memory each time that you run your program), but Java is implemented on a very wide range of computational devices, and memory consumption is implementation-dependent. For economy, we use the word <em>typical</em> to signal that values are subject to machine dependencies.</p>
<p>One of Java’s most significant features is its memory allocation system, which is supposed to relieve you from having to worry about memory. Certainly, you are well-advised to take advantage of this feature when appropriate. Still, it is your responsibility to know, at least approximately, when a program’s memory requirements will prevent you from solving a given problem.</p>
<p class="image"><img alt="image" src="graphics/t0200-01.jpg"/></p>
<p>Analyzing memory usage is much easier than analyzing running time, primarily because not as many program statements are involved (just declarations) and because the analysis reduces complex objects to the primitive types, whose memory usage is well-defined and simple to understand: we can count up the number of variables and weight them by the number of bytes according to their type. For example, since the Java <code>int</code> data type is the set of integer values between −2,147,483,648 and 2,147,483,647, a grand total of 2<sup>32</sup> different values, typical Java implementations use 32 bits to represent <code>int</code> values. Similar considerations hold for other primitive types: typical Java implementations use 8-bit bytes, representing each <code>char</code> value with 2 bytes (16 bits), each <code>int</code> value with 4 bytes (32 bits), each <code>double</code> and each <code>long</code> value with 8 bytes (64 bits), and each <code>boolean</code> value with 1 byte (since computers typically access memory one byte at a time). Combined with knowledge of the amount of memory available, you can calculate limitations from these values. For example, if you have 1GB of memory on your computer (1 billion bytes), you cannot fit more than about 32 million <code>int</code> values or 16 million <code>double</code> values in memory at any one time.</p>
<p>On the other hand, analyzing memory usage is subject to various differences in machine hardware and in Java implementations, so you should consider the specific examples that we give as indicative of how you might go about determining memory usage when warranted, not the final word for your computer. For example, many data structures involve representation of machine addresses, and the amount of memory <a id="page_201"/>needed for a machine address varies from machine to machine. For consistency, we assume that 8 bytes are needed to represent addresses, as is typical for 64-bit architectures that are now widely used, recognizing that many older machines use a 32-bit architecture that would involve just 4 bytes per machine address.</p>
<p class="image"><img alt="image" src="graphics/01_45-objects.jpg"/></p>
<p><a id="ch01sec3lev142"/></p>
<h5><em>Objects</em></h5>
<p>To determine the memory usage of an object, we add the amount of memory used by each instance variable to the overhead associated with each object, typically 16 bytes. The overhead includes a reference to the object’s class, garbage collection information, and synchronization information. Moreover, the memory usage is typically padded to be a multiple of 8 bytes (machine words, on a 64-bit machine). For example, an <code>Integer</code> object uses 24 bytes (16 bytes of overhead, 4 bytes for its <code>int</code> instance variable, and 4 bytes of padding). Similarly, a <code>Date</code> (page <a href="ch01.html#page_91">91</a>) object also uses 32 bytes: 16 bytes of overhead, 4 bytes for each of its three <code>int</code> instance variables, and 4 bytes of padding. A reference to an object typically is a memory address and thus uses 8 bytes of memory. For example, a <code>Counter</code> (page <a href="ch01.html#page_89">89</a>) object uses 32 bytes: 16 bytes of overhead, 8 bytes for its <code>String</code> instance variable (a reference), 4 bytes for its <code>int</code> instance variable, and 4 bytes of padding. When we account for the memory for a reference, we account separately for the memory for the object itself, so this total does not count the memory for the <code>String</code> value.</p>
<p><a id="ch01sec3lev143"/></p>
<h5><em>Linked lists</em></h5>
<p>A nested non-static (inner) class such as our <code>Node</code> class (page <a href="#ch01sec2lev26">142</a>) requires an extra 8 bytes of overhead (for a reference to the enclosing instance). Thus, a <code>Node</code> object uses 40 bytes (16 bytes of object overhead, 8 bytes each for the references to the <code>Item</code> and <code>Node</code> objects, and 8 bytes for the extra overhead). Thus, since an <code>Integer</code> object uses 24 bytes, a stack with <em>N</em> integers built with a linked-list representation (<a href="#ch01sb06"><small>ALGORITHM 1.2</small></a>) uses 32 + 64<em>N</em> bytes, the usual 16 for object overhead for <code>Stack</code>, 8 for its reference instance variable, 4 for its <code>int</code> instance variable, 4 for padding, and 64 for each entry, 40 for a <code>Node</code> and 24 for an <code>Integer</code>.</p>
<p><a id="ch01sec3lev144"/></p>
<h5><a id="page_202"/><em>Arrays</em></h5>
<p>Typical memory requirements for various types of arrays in Java are summarized in the diagrams on the facing page. Arrays in Java are implemented as objects, typically with extra overhead for the length. An <em>array of primitive-type values</em> typically requires 24 bytes of header information (16 bytes of object overhead, 4 bytes for the length, and 4 bytes of padding) plus the memory needed to store the values. For example, an array of <em>N</em> <code>int</code> values uses 24 + 4<em>N</em> bytes (rounded up to be a multiple of 8), and an array of <em>N</em> <code>double</code> values uses 24 + 8<em>N</em> bytes. An <em>array of objects</em> is an array of references to the objects, so we need to add the space for the references to the space required for the objects. For example, an array of <em>N</em> <code>Date</code> objects (page <a href="ch01.html#page_91">91</a>) uses 24 bytes (array overhead) plus 8<em>N</em> bytes (references) plus 32 bytes for each object, for a grand total of 24 + 40<em>N</em> bytes. A <em>two-dimensional array</em> is an array of arrays (each array is an object). For example, a two-dimensional <em>M</em>-by-<em>N</em> array of <code>double</code> values uses 24 bytes (overhead for the array of arrays) plus 8<em>M</em> bytes (references to the row arrays) plus <em>M</em> times 24 bytes (overhead from the row arrays) plus <em>M</em> times <em>N</em> times 8 bytes (for the <em>N</em> <code>double</code> values in each of the <em>M</em> rows) for a grand total of 8<em>NM</em> + 32<em>M</em> + 24 ~ 8<em>NM</em> bytes. When array entries are objects, a similar accounting leads to a total of 8<em>NM</em> + 32<em>M</em> + 24 ~ 8<em>NM</em> bytes for the array of arrays filled with references to objects, plus the memory for the objects themselves.</p>
<p><a id="ch01sec3lev145"/></p>
<h5><em>String objects</em></h5>
<p>We account for memory in Java’s <code>String</code> objects in the same way as for any other object, except that aliasing is common for strings. The standard <code>String</code> implementation has four instance variables: a reference to a character array (8 bytes) and three <code>int</code> values (4 bytes each). The first <code>int</code> value is an offset into the character array; the second is a count (the string length). In terms of the instance variable names in the drawing on the facing page, the string that is represented consists of the characters <code>value[offset]</code> through <code>value[offset + count - 1]</code>. The third <code>int</code> value in <code>String</code> objects is a hash code that saves recomputation in certain circumstances that need not concern us now. Therefore, each <code>String</code> object uses a total of 40 bytes (16 bytes for object overhead plus 4 bytes for each of the three <code>int</code> instance variables plus 8 bytes for the array reference plus 4 bytes of padding). This space requirement is in addition to the space needed for the characters themselves, which are in the array. The space needed for the characters is accounted for separately because the <code>char</code> array is often shared among strings. Since <code>String</code> objects are immutable, this arrangement allows the implementation to save memory when <code>String</code> objects have the same underlying <code>value[]</code>.</p>
<p><a id="ch01sec3lev146"/></p>
<h5><em>String values and substrings</em></h5>
<p>A <code>String</code> of length <em>N</em> typically uses 40 bytes (for the <code>String</code> object) plus 24 + 2<em>N</em> bytes (for the array that contains the characters) for a total of 64 + 2<em>N</em> bytes. But it is typical in string processing to work with substrings, and Java’s representation is meant to allow us to do so without having to make copies of <a id="page_204"/>the string’s characters. When you use the <code>substring()</code> method, you create a new <code>String</code> object (40 bytes) but reuse the same <code>value[]</code> array, so a substring of an existing string takes just 40 bytes. The character array containing the original string is aliased in the object for the substring; the offset and length fields identify the substring. In other words, <em>a substring takes constant extra memory and forming a substring takes constant time</em>, even when the lengths of the string and the substring are huge. A naive representation that requires copying characters to make substrings would take linear time and space. The ability to create a substring using space (and time) independent of its length is the key to efficiency in many basic string-processing algorithms.</p>
<p class="image"><a id="page_203"/><img alt="image" src="graphics/01_46-arraysall.jpg"/></p>
<p class="image"><img alt="image" src="graphics/01_47-strings.jpg"/></p>
<p><small>THESE BASIC MECHANISMS ARE EFFECTIVE</small> for estimating the memory usage of a great many programs, but there are numerous complicating factors that can make the task significantly more difficult. We have already noted the potential effect of aliasing. Moreover, memory consumption is a complicated dynamic process when function calls are involved because the system memory allocation mechanism plays a more important role, with more system dependencies. For example, when your program calls a method, the system allocates the memory needed for the method (for its local variables) from a special area of memory called the <em>stack</em> (a system pushdown stack), and when the method returns to the caller, the memory is returned to the stack. For this reason, creating arrays or other large objects in recursive programs is dangerous, since each recursive call implies significant memory usage. When you create an object with <code>new</code>, the system allocates the memory needed for the object from another special area of memory known as the <em>heap</em> (not the same as the binary heap data structure we consider in <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>), and you must remember that every object lives until no references to it remain, at which point a system process known as <em>garbage collection</em> reclaims its memory for the heap. Such dynamics can make the task of precisely estimating memory usage of a program challenging.</p>
<p><a id="ch01sec2lev41"/></p>
<h4><a id="page_205"/>Perspective</h4>
<p>Good performance is important. An impossibly slow program is almost as useless as an incorrect one, so it is certainly worthwhile to pay attention to the cost at the outset, to have some idea of which kinds of problems you might feasibly address. In particular, it is always wise to have some idea of which code constitutes the inner loop of your programs.</p>
<p>Perhaps the most common mistake made in programming is to pay too much attention to performance characteristics. Your first priority is to make your code clear and correct. Modifying a program for the sole purpose of speeding it up is best left for experts. Indeed, doing so is often counterproductive, as it tends to create code that is complicated and difficult to understand. C. A. R. Hoare (the inventor of quicksort and a leading proponent of writing clear and correct code) once summarized this idea by saying that “<em>premature optimization is the root of all evil</em>,” to which Knuth added the qualifier “(<em>or at least most of it</em>) <em>in programming.</em>” Beyond that, improving the running time is not worthwhile if the available cost benefits are insignificant. For example, improving the running time of a program by a factor of 10 is inconsequential if the running time is only an instant. Even when a program takes a few minutes to run, the total time required to implement and debug an improved algorithm might be substantially more than the time required simply to run a slightly slower one—you may as well let the computer do the work. Worse, you might spend a considerable amount of time and effort implementing ideas that should in theory improve a program but do not do so in practice.</p>
<p>Perhaps the second most common mistake made in programming is to ignore performance characteristics. Faster algorithms are often more complicated than brute-force ones, so you might be tempted to accept a slower algorithm to avoid having to deal with more complicated code. However, you can sometimes reap huge savings with just a few lines of good code. Users of a surprising number of computer systems lose substantial time unknowingly waiting for brute-force quadratic algorithms to finish solving a problem, when linear or linearithmic algorithms are available that could solve the problem in a fraction of the time. When we are dealing with huge problem sizes, we often have no choice but to seek better algorithms.</p>
<p>We generally take as implicit the methodology described in this section to estimate memory usage and to develop an order-of-growth hypothesis of the running time from a tilde approximation resulting from a mathematical analysis within a cost model, and to check those hypotheses with experiments. Improving a program to make it more clear, efficient, and elegant should be your goal every time that you work on it. If you pay attention to the cost all the way through the development of a program, you will reap the benefits every time you use it.</p>
<p><a id="ch01sec2lev42"/></p>
<h4><a id="page_206"/>Q&amp;A</h4>
<p><strong>Q.</strong> Why not use <code>StdRandom</code> to generate random values instead of maintaining the file <code>1Mints.txt</code> ?</p>
<p><strong>A.</strong> It is easier to debug code in development and to reproduce experiments. <code>StdRandom</code> produces different values each time it is called, so running a program after fixing a bug may not test the fix! You could use the <code>initialize()</code> method in <code>StdRandom</code> to address this problem, but a reference file such as <code>1Mints.txt</code> makes it easier to add test cases while debugging. Also, different programmers can compare performance on different computers, without worrying about the input model. Once you have debugged a program and have a good idea of how it performs, it is certainly worthwhile to test it on random data. For example, <code>DoublingTest</code> and <code>DoublingRatio</code> take this approach.</p>
<p><strong>Q.</strong> I ran <code>DoublingRatio</code> on my computer, but the results were not as consistent as in the book. Some of the ratios were not close to 8. Why?</p>
<p><strong>A.</strong> That is why we discussed “caveats” on page <a href="#ch01sec2lev38">195</a>. Most likely, your computer’s operating system decided to do something else during the experiment. One way to mitigate such problems is to invest more time in more experiments. For example, you could change <code>DoublingTest</code> to run the experiments 1,000 times for each <em>N</em>, giving a much more accurate estimate for the running time for each size (see <a href="#ch01qa4q39"><small>EXERCISE 1.4.39</small></a>).</p>
<p><strong>Q.</strong> What, exactly, does “as <em>N</em> grows” mean in the definition of the tilde notation?</p>
<p><strong>A.</strong> The formal definition of <em>f</em>(<em>N</em>) ~ <em>g</em>(<em>N</em>) is lim<sub><em>N</em>→∞</sub> <em>f</em>(<em>N</em>)/<em>g</em>(<em>N</em>) = 1.</p>
<p><strong>Q.</strong> I’ve seen other notations for describing order of growth. What’s the story?</p>
<p><strong>A.</strong> The “big-Oh” notation is widely used: we say that <em>f</em>(<em>N</em>) is <em>O</em>(<em>g</em>(<em>N</em>)) if there exist constants <em>c</em> and <em>N</em><sub>0</sub> such that |<em>f</em>(<em>N</em>)| &lt; <em>c g</em>(<em>N</em>) for all <em>N</em> &gt; <em>N</em><sub>0</sub>. This notation is very useful in providing asymptotic upper bounds on the performance of algorithms, which is important in the theory of algorithms. But it is not useful for predicting performance or for comparing algorithms.</p>
<p><strong>Q.</strong> Why not?</p>
<p><strong>A.</strong> The primary reason is that it describes only an <em>upper bound</em> on the running time. Actual performance might be much better. The running time of an algorithm might be both <em>O</em>(<em>N</em><sup>2</sup>) and ~ <em>a N</em> log <em>N</em>. As a result, it cannot be used to justify tests like our doubling ratio test (see <a href="#ch01sb13"><small>PROPOSITION C</small></a> on page <a href="#ch01sb13">193</a>).</p>
<p><a id="page_207"/><strong>Q.</strong> So why is the big-Oh notation so widely used?</p>
<p><strong>A.</strong> It facilitates development of bounds on the order of growth, even for complicated algorithms for which more precise analysis might not be feasible. Moreover, it is compatible with the “big-Omega” and “big-Theta” notations that theoretical computer scientists use to classify algorithms by bounding their worst-case performance. We say that <em>f</em>(<em>N</em>) is Ω(<em>g</em>(<em>N</em>)) if there exist constants <em>c</em> and <em>N</em><sub>0</sub> such that |<em>f</em>(<em>N</em>)| &gt; <em>c g</em>(<em>N</em>) for <em>N</em> &gt; <em>N</em><sub>0</sub>; and if <em>f</em>(<em>N</em>) is <em>O</em>(<em>g</em>(<em>N</em>)) and Ω(<em>g</em>(<em>N</em>)), we say that <em>f</em>(<em>N</em>) is Θ(<em>g</em>(<em>N</em>)). The “big-Omega” notation is typically used to describe a <em>lower bound</em> on the worst case, and the “big-Theta” notation is typically used to describe the performance of algorithms that are <em>optimal</em> in the sense that no algorithm can have better asymptotic worst-case order of growth. Optimal algorithms are certainly worth considering in practical applications, but there are many other considerations, as you will see.</p>
<p><strong>Q.</strong> Aren’t upper bounds on asymptotic performance important?</p>
<p><strong>A.</strong> Yes, but we prefer to discuss precise results in terms of frequency of statement exceution with respect to cost models, because they provide more information about algorithm performance and because deriving such results is feasible for the algorithms that we discuss. For example, we say “<code>ThreeSum</code> uses ~<em>N</em><sup>3</sup>/2 array accesses” and “the number of times <code>cnt++</code> is executed in <code>ThreeSum</code> is ~<em>N</em><sup>3</sup>/6 in the worst case,” which is a bit more verbose but much more informative than the statement “the running time of <code>ThreeSum</code> is <em>O</em>(<em>N</em><sup>3</sup>).”</p>
<p><strong>Q.</strong> When the order of growth of the running time of an algorithm is <em>N</em> log <em>N</em>, the doubling test will lead to the hypothesis that the running time is ~ <em>a N</em> for a constant <em>a</em>. Isn’t that a problem?</p>
<p><strong>A.</strong> We have to be careful not to try to infer that the experimental data implies a particular mathematical model, but when we are just predicting performance, this is not really a problem. For example, when <em>N</em> is between 16,000 and 32,000, the plots of 14<em>N</em> and <em>N</em> lg <em>N</em> are very close to one another. The data fits both curves. As <em>N</em> increases, the curves become closer together. It actually requires some care to experimentally check the hypothesis that an algorithm’s running time is linearithmic but not linear.</p>
<p><strong>Q.</strong> Does <code>int[] a = new int[N]</code> count as <em>N</em> array accesses (to initialize entries to 0)?</p>
<p><strong>A.</strong> Most likely yes, so we make that assumption in this book, though a sophisticated compiler implementation might try to avoid this cost for huge sparse arrays.</p>
<p><a id="ch01sec2lev43"/></p>
<h4><a id="page_208"/>Exercises</h4>
<p><a id="ch01qa4q1"/><strong>1.4.1</strong> Show that the number of different triples that can be chosen from <em>N</em> items is precisely <em>N</em>(<em>N</em>−1)(<em>N</em>−2)/6. <em>Hint</em>: Use mathematical induction.</p>
<p><a id="ch01qa4q2"/><strong>1.4.2</strong> Modify <code>ThreeSum</code> to work properly even when the <code>int</code> values are so large that adding two of them might cause overflow.</p>
<p><a id="ch01qa4q3"/><strong>1.4.3</strong> Modify <code>DoublingTest</code> to use <code>StdDraw</code> to produce plots like the standard and log-log plots in the text, rescaling as necessary so that the plot always fills a substantial portion of the window.</p>
<p><a id="ch01qa4q4"/><strong>1.4.4</strong> Develop a table like the one on page <a href="#page_181">181</a> for <code>TwoSum</code>.</p>
<p><a id="ch01qa4q5"/><strong>1.4.5</strong> Give tilde approximations for the following quantities:</p>
<p class="indenthangingN"><em>a. N</em> + 1</p>
<p class="indenthangingN"><em>b.</em> 1 + 1/<em>N</em></p>
<p class="indenthangingN"><em>c.</em> (1 + 1/<em>N</em>)(1 + 2/<em>N</em>)</p>
<p class="indenthangingN"><em>d.</em> 2<em>N</em><sup><em>3</em>−</sup> 15 <em>N<sup>2</sup></em> + <em>N</em></p>
<p class="indenthangingN"><em>e.</em> lg(2<em>N</em>)/lg <em>N</em></p>
<p class="indenthangingN"><em>f.</em> lg(<em>N</em><sup>2</sup> + 1) / lg <em>N</em></p>
<p class="indenthangingN"><em>g. N</em><sup>100</sup> / 2<em><sup>N</sup></em></p>
<p><a id="ch01qa4q6"/><strong>1.4.6</strong> Give the order of growth (as a function of <em>N</em>) of the running times of each of the following code fragments:</p>
<p class="indenthangingN"><img alt="image" src="graphics/p0208-01.jpg"/></p>
<p><a id="page_209"/><a id="ch01qa4q7"/><strong>1.4.7</strong> Analyze <code>ThreeSum</code> under a cost model that counts arithmetic operations (and comparisons) involving the input numbers.</p>
<p><a id="ch01qa4q8"/><strong>1.4.8</strong> Write a program to determine the number pairs of values in an input file that are equal. If your first try is quadratic, think again and use <code>Arrays.sort()</code> to develop a linearithmic solution.</p>
<p><a id="ch01qa4q9"/><strong>1.4.9</strong> Give a formula to predict the running time of a program for a problem of size <em>N</em> when doubling experiments have shown that the doubling factor is 2<em><sup>b</sup></em> and the running time for problems of size <em>N</em><sub>0</sub> is <em>T</em>.</p>
<p><a id="ch01qa4q10"/><strong>1.4.10</strong> Modify binary search so that it always returns the element with the smallest index that matches the search element (and still guarantees logarithmic running time).</p>
<p><a id="ch01qa4q11"/><strong>1.4.11</strong> Add an instance method <code>howMany()</code> to <code>StaticSETofInts</code> (page <a href="ch01.html#page_99">99</a>) that finds the number of occurrences of a given key in time proportional to log <em>N</em> in the worst case.</p>
<p><a id="ch01qa4q12"/><strong>1.4.12</strong> Write a program that, given two sorted arrays of <em>N</em> <code>int</code> values, prints all elements that appear in both arrays, in sorted order. The running time of your program should be proportional to <em>N</em> in the worst case.</p>
<p><a id="ch01qa4q13"/><strong>1.4.13</strong> Using the assumptions developed in the text, give the amount of memory needed to represent an object of each of the following types:</p>
<p class="indenthangingN"><em>a.</em> <code>Accumulator</code></p>
<p class="indenthangingN"><em>b.</em> <code>Transaction</code></p>
<p class="indenthangingN"><em>c.</em> <code>FixedCapacityStackOfStrings</code> with capacity <em>C</em> and <em>N</em> entries</p>
<p class="indenthangingN"><em>d.</em> <code>Point2D</code></p>
<p class="indenthangingN"><em>e.</em> <code>Interval1D</code></p>
<p class="indenthangingN"><em>f.</em> <code>Interval2D</code></p>
<p class="indenthangingN"><em>g.</em> <code>Double</code></p>
<p><a id="ch01sec2lev44"/></p>
<h4><a id="page_210"/>Creative Problems</h4>
<p><a id="ch01qa4q14"/><strong>1.4.14</strong> <em>4-sum.</em> Develop an algorithm for the <em>4-sum</em> problem.</p>
<p><a id="ch01qa4q15"/><strong>1.4.15</strong> <em>Faster 3-sum.</em> As a warmup, develop an implementation <code>TwoSumFaster</code> that uses a <em>linear</em> algorithm to count the pairs that sum to zero after the array is sorted (instead of the binary-search-based linearithmic algorithm). Then apply a similar idea to develop a quadratic algorithm for the 3-sum problem.</p>
<p><a id="ch01qa4q16"/><strong>1.4.16</strong> <em>Closest pair (in one dimension).</em> Write a program that, given an array <code>a[]</code> of <em>N</em> <code>double</code> values, finds a <em>closest pair</em>: two values whose difference is no greater than the the difference of any other pair (in absolute value). The running time of your program should be linearithmic in the worst case.</p>
<p><a id="ch01qa4q17"/><strong>1.4.17</strong> <em>Farthest pair (in one dimension).</em> Write a program that, given an array <code>a[]</code> of <em>N</em> <code>double</code> values, finds a <em>farthest pair</em>: two values whose difference is no smaller than the the difference of any other pair (in absolute value). The running time of your program should be linear in the worst case.</p>
<p><a id="ch01qa4q18"/><strong>1.4.18</strong> <em>Local minimum of an array.</em> Write a program that, given an array <code>a[]</code> of <em>N</em> distinct integers, finds a <em>local minimum</em>: an index <code>i</code> such that <code>a[i] &lt; a[i-1] and a[i] &lt; a[i+1]</code>. Your program should use ~2lg <em>N</em> compares in the worst case..</p>
<p><em>Answer</em>: Examine the middle value <code>a[N/2]</code> and its two neighbors <code>a[N/2 - 1]</code> and <code>a[N/2 + 1]</code>. If <code>a[N/2]</code> is a local minimum, stop; otherwise search in the half with the smaller neighbor.</p>
<p><a id="ch01qa4q19"/><strong>1.4.19</strong> <em>Local minimum of a matrix.</em> Given an <em>N</em>-by-<em>N</em> array <code>a[]</code> of <em>N</em><sup>2</sup> distinct integers, design an algorithm that runs in time proportional to <em>N</em> to find a <em>local minimum</em>: a pair of indices <code>i</code> and <code>j</code> such that <code>a[i][j] &lt; a[i+1][j]</code>, a<code>[i][j] &lt; a[i][j+1]</code>, <code>a[i][j] &lt; a[i-1][j]</code>, and <code>a[i][j] &lt; a[i][j-1]</code>. The running time of your program should be proportional to <em>N</em> in the worst case.</p>
<p><a id="ch01qa4q20"/><strong>1.4.20</strong> <em>Bitonic search.</em> An array is <em>bitonic</em> if it is comprised of an increasing sequence of integers followed immediately by a decreasing sequence of integers. Write a program that, given a bitonic array of <em>N</em> distinct <code>int</code> values, determines whether a given integer is in the array. Your program should use ~3lg <em>N</em> compares in the worst case.</p>
<p><a id="ch01qa4q21"/><strong>1.4.21</strong> <em>Binary search on distinct values.</em> Develop an implementation of binary search for <code>StaticSETofInts</code> (see page <a href="ch01.html#ch01sec3lev74">98</a>) where the running time of <code>contains()</code> is guaranteed <a id="page_211"/>to be ~ lg <em>R</em>, where <em>R</em> is the number of different integers in the array given as argument to the constructor.</p>
<p><a id="ch01qa4q22"/><strong>1.4.22</strong> <em>Binary search with only addition and subtraction.</em> [Mihai Patrascu] Write a program that, given an array of <em>N</em> distinct <code>int</code> values in ascending order, determines whether a given integer is in the array. You may use only additions and subtractions and a constant amount of extra memory. The running time of your program should be proportional to log <em>N</em> in the worst case.</p>
<p><em>Answer</em>: Instead of searching based on powers of two (binary search), use Fibonacci numbers (which also grow exponentially). Maintain the current search range to be the interval [<em>i</em>, <em>i</em> + <em>F <sub>k</sub></em>] and keep <em>F<sub>k</sub></em> and <em>F<sub>k</sub></em><sub>−1</sub> in two variables. At each step compute <em>F<sub>k</sub></em><sub>−2</sub> via subtraction, check element <em>i</em> + <em>F<sub>k</sub></em><sub>−2</sub>, and update the current range to either [<em>i</em>, <em>i</em> + <em>F<sub>k</sub></em><sub>−2</sub>] or [<em>i</em> + <em>F<sub>k</sub></em><sub>−2</sub>, <em>i</em> + <em>F<sub>k</sub></em><sub>−2</sub> + <em>F<sub>k</sub></em><sub>−1</sub>].</p>
<p><a id="ch01qa4q23"/><strong>1.4.23</strong> <em>Binary search for a fraction.</em> Devise a method that uses a logarithmic number of queries of the form <em>Is the number less than x?</em> to find a rational number <em>p</em>/<em>q</em> such that 0 &lt; <em>p</em> &lt; <em>q</em> &lt; <em>N</em>. <em>Hint</em>: Two fractions with denominators less than <em>N</em> cannot differ by more than 1/<em>N</em><sup>2</sup>.</p>
<p><a id="ch01qa4q24"/><strong>1.4.24</strong> <em>Throwing eggs from a building.</em> Suppose that you have an <em>N</em>-story building and plenty of eggs. Suppose also that an egg is broken if it is thrown off floor <em>F</em> or higher, and unhurt otherwise. First, devise a strategy to determine the value of <em>F</em> such that the number of broken eggs is ~lg <em>N</em> when using ~lg <em>N</em> throws, then find a way to reduce the cost to ~2lg <em>F</em>.</p>
<p><a id="ch01qa4q25"/><strong>1.4.25</strong> <em>Throwing two eggs from a building.</em> Consider the previous question, but now suppose you only have two eggs, and your cost model is the number of throws. Devise a strategy to determine <em>F</em> such that the number of throws is at most 2√<em>N</em>, then find a way to reduce the cost to ~<em>c</em>√<em>F</em>. This is analogous to a situation where search hits (egg intact) are much cheaper than misses (egg broken).</p>
<p><a id="ch01qa4q26"/><strong>1.4.26</strong> <em>3-collinearity.</em> Suppose that you have an algorithm that takes as input <em>N</em> distinct points in the plane and can return the number of triples that fall on the same line. Show that you can use this algorithm to solve the 3-sum problem. <em>Strong hint</em>: Use algebra to show that (<em>a</em>, <em>a</em><sup>3</sup>), (<em>b</em>, <em>b</em><sup>3</sup>), and (<em>c</em>, <em>c</em><sup>3</sup>) are collinear if and only if <em>a</em> + <em>b</em> + <em>c</em> = 0.</p>
<p><a id="ch01qa4q27"/><strong>1.4.27</strong> <em>Queue with two stacks.</em> Implement a queue with two stacks so that each queue <a id="page_212"/>operation takes a constant amortized number of stack operations. <em>Hint</em>: If you push elements onto a stack and then pop them all, they appear in reverse order. If you repeat this process, they’re now back in order.</p>
<p><a id="ch01qa4q28"/><strong>1.4.28</strong> <em>Stack with a queue.</em> Implement a stack with a single queue so that each stack operations takes a linear number of queue operations. <em>Hint</em>: To delete an item, get all of the elements on the queue one at a time, and put them at the end, except for the last one which you should delete and return. (This solution is admittedly very inefficient.)</p>
<p><a id="ch01qa4q29"/><strong>1.4.29</strong> <em>Steque with two stacks.</em> Implement a steque with two stacks so that each steque operation (see <a href="#ch01qa3q32"><small>EXERCISE 1.3.32</small></a>) takes a constant amortized number of stack operations.</p>
<p><a id="ch01qa4q30"/><strong>1.4.30</strong> <em>Deque with a stack and a steque.</em> Implement a deque with a stack and a steque (see <a href="#ch01qa3q32"><small>EXERCISE 1.3.32</small></a>) so that each deque operation takes a constant amortized number of stack and steque operations.</p>
<p><a id="ch01qa4q31"/><strong>1.4.31</strong> <em>Deque with three stacks.</em> Implement a deque with three stacks so that each deque operation takes a constant amortized number of stack operations.</p>
<p><a id="ch01qa4q32"/><strong>1.4.32</strong> <em>Amortized analysis.</em> Prove that, starting from an empty stack, the number of array accesses used by any sequence of <em>M</em> operations in the resizing array implementation of <code>Stack</code> is proportional to <em>M.</em></p>
<p><a id="ch01qa4q33"/><strong>1.4.33</strong> <em>Memory requirements on a 32-bit machine.</em> Give the memory requirements for <code>Integer</code>, <code>Date</code>, <code>Counter</code>, <code>int[]</code>, <code>double[]</code>, <code>double[][]</code>, <code>String</code>, <code>Node</code>, and <code>Stack</code> (linked-list representation) for a 32-bit machine. Assume that references are 4 bytes, object overhead is 8 bytes, and padding is to a multiple of 4 bytes.</p>
<p><a id="ch01qa4q34"/><strong>1.4.34</strong> <em>Hot or cold.</em> Your goal is to guess a secret integer between 1 and <em>N</em>. You repeatedly guess integers between 1 and <em>N</em>. After each guess you learn if your guess equals the secret integer (and the game stops). Otherwise, you learn if the guess is hotter (closer to) or colder (farther from) the secret number than your previous guess. Design an algorithm that finds the secret number in at most ~2 lg <em>N</em> guesses. Then design an algorithm that finds the secret number in at most ~ 1 lg <em>N</em> guesses.</p>
<p><a id="page_213"/><a id="ch01qa4q35"/><strong>1.4.35</strong> <em>Time costs for pushdown stacks.</em> Justify the entries in the table below, which shows typical time costs for various pushdown stack implementations, using a cost model that counts both <em>data reference</em>s (references to data pushed onto the stack, either an array reference or a reference to an object’s instance variable) and <em>objects created</em>.</p>
<p class="image"><img alt="image" src="graphics/t0213-01.jpg"/></p>
<p><a id="ch01qa4q36"/><strong>1.4.36</strong> <em>Space usage for pushdown stacks.</em> Justify the entries in the table below, which shows typical space usage for various pushdown stack implementations. Use a static nested class for linked-list nodes to avoid the non-static nested class overhead.</p>
<p class="image"><img alt="image" src="graphics/t0213-02.jpg"/></p>
<p><a id="ch01sec2lev45"/></p>
<h4><a id="page_214"/>Experiments</h4>
<p><a id="ch01qa4q37"/><strong>1.4.37</strong> <em>Autoboxing performance penalty.</em> Run experiments to determine the performance penalty on your machine for using autoboxing and auto-unboxing. Develop an implementation <code>FixedCapacityStackOfInts</code> and use a client such as <code>DoublingRatio</code> to compare its performance with the generic <code>FixedCapacityStack&lt;Integer&gt;</code>, for a large number of <code>push()</code> and <code>pop()</code> operations.</p>
<p><a id="ch01qa4q38"/><strong>1.4.38</strong> <em>Naive 3-sum implementation.</em> Run experiments to evaluate the following implementation of the inner loop of <code>ThreeSum</code>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0214-01.jpg"/></p>
<p>Do so by developing a version of <code>DoublingTest</code> that computes the ratio of the running times of this program and <code>ThreeSum</code>.</p>
<p><a id="ch01qa4q39"/><strong>1.4.39</strong> <em>Improved accuracy for doubling test.</em> Modify <code>DoublingRatio</code> to take a second command-line argument that specifies the number of calls to make to <code>timeTrial()</code> for each value of <code>N</code>. Run your program for 10, 100, and 1,000 trials and comment on the precision of the results.</p>
<p><a id="ch01qa4q40"/><strong>1.4.40</strong> <em>3-sum for random values.</em> Formulate and validate a hypothesis describing the number of triples of <em>N</em> random <code>int</code> values that sum to 0. If you are skilled in mathematical analysis, develop an appropriate mathematical model for this problem, where the values are uniformly distributed between –<em>M</em> and <em>M</em>, where <em>M</em> is not small.</p>
<p><a id="ch01qa4q41"/><strong>1.4.41</strong> <em>Running times.</em> Estimate the amount of time it would take to run <code>TwoSumFast</code>, <code>TwoSum</code>, <code>ThreeSumFast</code> and <code>ThreeSum</code> on your computer to solve the problems for a file of 1 million numbers. Use <code>DoublingRatio</code> to do so.</p>
<p><a id="ch01qa4q42"/><strong>1.4.42</strong> <em>Problem sizes.</em> Estimate the size of the largest value of <em>P</em> for which you can run <code>TwoSumFast</code>, <code>TwoSum</code>, <code>ThreeSumFast</code>, and <code>ThreeSum</code> on your computer to solve the problems for a file of 2<em><sup>P</sup></em> thousand numbers. Use <code>DoublingRatio</code> to do so.</p>
<p><a id="ch01qa4q43"/><strong>1.4.43</strong> <em>Resizing arrays versus linked lists.</em> Run experiments to validate the hypothesis that resizing arrays are faster than linked lists for stacks (see <a href="#ch01qa4q35"><small>EXERCISE 1.4.35</small></a> and <a href="#ch01qa4q36"><small>EXERCISE 1.4.36</small></a>). Do so by developing a version of <code>DoublingRatio</code> that computes the ratio <a id="page_215"/>of the running times of the two programs.</p>
<p><a id="ch01qa4q44"/><strong>1.4.44</strong> <em>Birthday problem.</em> Write a program that takes an integer <em>N</em> from the command line and uses <code>StdRandom.uniform()</code> to generate a random sequence of integers between 0 and <em>N</em> − 1. Run experiments to validate the hypothesis that the number of integers generated before the first repeated value is found is <img alt="image" src="graphics/root_n2.jpg"/>.</p>
<p><a id="ch01qa4q45"/><strong>1.4.45</strong> <em>Coupon collector problem.</em> Generating random integers as in the previous exercise, run experiments to validate the hypothesis that the number of integers generated before all possible values are generated is ~<em>N</em> H<em><sub>N</sub></em>.</p>
<p><a id="ch01sec1lev7"/></p>
<h3><a id="page_216"/>1.5 Case Study: Union-Find</h3>
<p><small>TO ILLUSTRATE</small> our basic approach to developing and analyzing algorithms, we now consider a detailed example. Our purpose is to emphasize the following themes.</p>
<p class="indenthangingB">• Good algorithms can make the difference between being able to solve a practical problem and not being able to address it at all.</p>
<p class="indenthangingB">• An efficient algorithm can be as simple to code as an inefficient one.</p>
<p class="indenthangingB">• Understanding the performance characteristics of an implementation can be an interesting and satisfying intellectual challenge.</p>
<p class="indenthangingB">• The scientific method is an important tool in helping us choose among different methods for solving the same problem.</p>
<p class="indenthangingB">• An iterative refinement process can lead to increasingly efficient algorithms.</p>
<p>These themes are reinforced throughout the book. This prototypical example sets the stage for our use of the same general methodology for many other problems.</p>
<p>The problem that we consider is not a toy problem; it is a fundamental computational task, and the solution that we develop is of use in a variety of applications, from percolation in physical chemistry to connectivity in communications networks. We start with a simple solution, then seek to understand that solution’s performance characteristics, which help us to see how to improve the algorithm.</p>
<p><a id="ch01sec2lev46"/></p>
<h4>Dynamic connectivity</h4>
<p>We start with the following problem specification: The input is a sequence of pairs of integers, where each integer represents an object of some type and we are to interpret the pair <code>p q</code> as meaning “<code>p</code> is connected to <code>q</code>.” We assume that “is connected to” is an <em>equivalence</em> relation, which means that it is</p>
<p class="indenthangingB">• <em>Reflexive</em>: <code>p</code> is connected to <code>p</code>.</p>
<p class="indenthangingB">• <em>Symmetric</em>: If <code>p</code> is connected to <code>q</code>, then <code>q</code> is connected to <code>p</code>.</p>
<p class="indenthangingB">• <em>Transitive</em>: If <code>p</code> is connected to <code>q</code> and <code>q</code> is connected to <code>r</code>, then <code>p</code> is connected to <code>r</code>.</p>
<p>An equivalence relation partitions the objects into <em>equivalence classes</em>. In this case, two objects are in the same equivalence class if and only if they are connected. Our goal is to write a program to filter out extraneous pairs (pairs where both objects are in the same equivalence class) from the sequence. In other words, when the program reads a pair <code>p q</code> from the input, it should write the pair to the output only if the pairs it has seen to that point <em>do not</em> imply that <code>p</code> is connected to <code>q</code>. If the previous pairs <em>do</em> imply that <code>p</code> is connected to <code>q</code>, then the program should ignore the pair <code>p q</code> and proceed to read in the next pair. The figure on the facing page gives an example of this process. To achieve the desired goal, we need to devise a data structure that can remember sufficient <a id="page_217"/>information about the pairs it has seen to be able to decide whether or not a new pair of objects is connected. Informally, we refer to the task of designing such a method as the <em>dynamic connectivity</em> problem. This problem arises applications such as the following:</p>
<p><a id="ch01sec3lev147"/></p>
<h5><em>Networks</em></h5>
<p>The integers might represent computers in a large network, and the pairs might represent connections in the network. Then, our program determines whether we need to establish a new direct connection for <code>p</code> and <code>q</code> to be able to communicate or whether we can use existing connections to set up a communications path. Or, the integers might represent contact sites in an electrical circuit, and the pairs might represent wires connecting the sites. Or, the integers might represent people in a social network, and the pairs might represent friendships. In such applications, we might need to process millions of objects and billions of connections.</p>
<p class="image"><img alt="image" src="graphics/01_48-ufconnecttiny.jpg"/></p>
<p><a id="ch01sec3lev148"/></p>
<h5><em>Variable-name equivalence</em></h5>
<p>In certain programming environments, it is possible to declare two variable names as being equivalent (references to the same object). After a sequence of such declarations, the system needs to be able to determine whether two given names are equivalent. This application is an early one (for the FORTRAN programming language) that motivated the development of the algorithms that we are about to consider.</p>
<p><a id="ch01sec3lev149"/></p>
<h5><em>Mathematical sets</em></h5>
<p>On a more abstract level, you can think of the integers as belonging to mathematical sets. When we process a pair <code>p q</code>, we are asking whether they belong to the same set. If not, we unite <code>p</code>’s set and <code>q</code>’s set, putting them in the same set.</p>
<p><small>TO FIX IDEAS</small>, we will use networking terminology for the rest of this section and refer to the objects as <em>sites</em>, the pairs as <em>connections</em>, and the equivalence classes as <em>connected components</em>, or just <em>components</em> for short. For simplicity, we assume that we have <code>N</code> sites with integer names, from <code>0</code> to <code>N-1</code>. We do so without loss of generality because we shall be considering a host of algorithms in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a> that can associate arbitrary names with such integer identifiers in an efficient manner.</p>
<p>A larger example that gives some indication of the difficulty of the connectivity problem is depicted in the figure at the top of the next page. You can quickly identify the component consisting of a single site in the left middle of the diagram and the <a id="page_218"/>component consisting of five sites at the bottom left, but you might have difficulty verifying that all of the other sites are connected to one another. For a program, the task is even more difficult, because it has to work just with site names and connections and has no access to the geometric placement of sites in the diagram. How can we tell quickly whether or not any given two sites in such a network are connected?</p>
<p class="image"><img alt="image" src="graphics/01_49-ufconnectmed.jpg"/></p>
<p>The first task that we face in developing an algorithm is to specify the problem in a precise manner. The more we require of an algorithm, the more time and space we may expect it to need to finish the job. It is impossible to quantify this relationship <em>a priori</em>, and we often modify a problem specification on finding that it is difficult or expensive to solve or, in happy circumstances, on finding that an algorithm can provide information more useful than what was called for in the original specification. For example, our <a id="page_219"/>connectivity problem specification requires only that our program be able to determine whether or not any given pair <code>p q</code> is connected, and not that it be able to demonstrate a set of connections that connect that pair. Such a requirement makes the problem more difficult and leads us to a different family of algorithms, which we consider in <a href="ch04.html#ch04sec1lev10"><small>SECTION 4.1</small></a>.</p>
<p>To specify the problem, we develop an API that encapsulates the basic operations that we need: initialize, add a connection between two sites, identify the component containing a site, determine whether two sites are in the same component, and count the number of components. Thus, we articulate the following API:</p>
<p class="image"><img alt="image" src="graphics/t0219-01.jpg"/></p>
<p>The <code>union()</code> operation merges two components if the two sites are in different components, the <code>find()</code> operation returns an integer component identifier for a given site, the <code>connected()</code> operation determines whether two sites are in the same component, and the <code>count()</code> method returns the number of components. We start with <code>N</code> components, and each <code>union()</code> that merges two different components decrements the number of components by 1.</p>
<p>As we shall soon see, the development of an algorithmic solution for dynamic connectivity thus reduces to the task of developing an implementation of this API. Every implementation has to</p>
<p class="indenthangingB">• Define a data structure to represent the known connections</p>
<p class="indenthangingB">• Develop efficient <code>union()</code>, <code>find()</code>, <code>connected()</code>, and <code>count()</code> implementations that are based on that data structure</p>
<p>As usual, the nature of the data structure has a direct impact on the efficiency of the algorithms, so data structure and algorithm design go hand in hand. The API already specifies the convention that both sites and components will be identified by <code>int</code> values between <code>0</code> and <code>N-1</code>, so it makes sense to use a <em>site-indexed array</em> <code>id[]</code> as our basic <a id="page_220"/>data structure to represent the components. We always use the name of one of the sites in a component as the component identifier, so you can think of each component as being represented by one of its sites. Initially, we start with <code>N</code> components, each site in its own component, so we initialize <code>id[i]</code> to <code>i</code> for all <code>i</code> from <code>0</code> to <code>N-1</code>. For each site <code>i</code>, we keep the information needed by <code>find()</code> to determine the component containing <code>i</code> in <code>id[i]</code>, using various algorithm-dependent strategies. All of our implementations use a one-line implementation of <code>connected()</code> that returns the <code>boolean</code> value <code>find(p) == find(q)</code>.</p>
<p><small>IN SUMMARY</small>, our starting point is <a href="#ch01sb17"><small>ALGORITHM 1.5</small></a> on the facing page. We maintain two instance variables, the count of components and the array <code>id[].</code> Implementations of <code>find()</code> and <code>union()</code> are the topic of the remainder of this section.</p>
<p class="image"><img alt="image" src="graphics/p0220-01.jpg"/></p>
<p>To test the utility of the API and to provide a basis for development, we include a client in <code>main()</code> that uses it to solve the dynamic connectivity problem. It reads the value of <code>N</code> followed by a sequence of pairs of integers (each in the range <code>0</code> to <code>N-1</code>), calling <code>find()</code> for each pair: If the two sites in the pair are already connected, it moves on to the next pair; if they are not, it calls <code>union()</code> and prints the pair. Before considering implementations, we also prepare test data: the file <code>tinyUF.txt</code> contains the 11 connections among 10 sites used in the small example illustrated on page <a href="#page_217">217</a>, the file <code>mediumUF.txt</code> contains the 900 connections among 625 sites illustrated on page <a href="#page_218">218</a>, and the file <code>largeUF.txt</code> is an example with 2 million connections among 1 millions sites. Our goal is to be able to handle inputs such as <code>largeUF.txt</code> in a reasonable amount of time.</p>
<p>To analyze the algorithms, we focus on the number of times each algorithm accesses an array entry. By doing so, we are implicitly formulating the hypothesis that the running times of the algorithms on a particular machine are within a constant factor of this quantity. This hypothesis is immediate from the code, is not difficult to validate through experimentation, and provides a useful starting point for comparing algorithms, as we will see.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb16"/></p>
<p><strong>Union-find cost model.</strong> When studying algorithms to implement the union-find API, we count <em>array accesses</em> (the number of times an array entry is accessed, for read or write).</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch01sb17"/></p>
<h3><a id="page_221"/>Algorithm 1.5 Union-find implementation</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0221-01.jpg"/></p>
<p class="programlisting2"><img alt="image" src="graphics/p0221-02.jpg"/></p>
<p>Our <code>UF</code> implementations are based on this code, which maintains an array of integers <code>id[]</code> such that the <code>find()</code> method returns the same integer for every site in each connected component. The <code>union()</code> method must maintain this invariant.</p>
<hr/>
</div>
<p><a id="ch01sec2lev47"/></p>
<h4><a id="page_222"/>Implementations</h4>
<p>We shall consider three different implementations, all based on using the site-indexed <code>id[]</code> array, to determine whether two sites are in the same connected component.</p>
<p><a id="ch01sec3lev150"/></p>
<h5><em>Quick-find</em></h5>
<p>One approach is to maintain the invariant that <code>p</code> and <code>q</code> are connected if and only if <code>id[p]</code> is equal to <code>id[q]</code>. In other words, all sites in a component must have the same value in <code>id[]</code>. This method is called <em>quick-find</em> because <code>find(p)</code> just returns <code>id[p]</code>, which immediately implies that <code>connected(p, q)</code> reduces to just the test <code>id[p] == id[q]</code> and returns <code>true</code> if and only if <code>p</code> and <code>q</code> are in the same component. To maintain the invariant for the call <code>union(p, q)</code>, we first check whether they are already in the same component, in which case there is nothing to do. Otherwise, we are faced with the situation that all of the <code>id[]</code> entries corresponding to sites in the same component as <code>p</code> have one value and all of the <code>id[]</code> entries corresponding to sites in the same component as <code>q</code> have another value. To combine the two components into one, we have to make all of the <code>id[]</code> entries corresponding to both sets of sites the same value, as shown in the example at right. To do so, we go through the array, changing all the entries with values equal to <code>id[p]</code> to the value <code>id[q]</code>. We could have decided to change all the entries equal to <code>id[q]</code> to the value <code>id[p]</code>—the choice between these two alternatives is arbitrary. The code for <code>find()</code> and <code>union()</code> based on these descriptions, given at left, is straightforward. A full trace for our development client with our sample test data <code>tinyUF.txt</code> is shown on the next page.</p>
<p class="image"><img alt="image" src="graphics/01_50-qfoverview.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0222-01.jpg"/></p>
<p><a id="ch01sec3lev151"/></p>
<h5><a id="page_223"/><em>Quick-find analysis.</em></h5>
<p>The <code>find()</code> operation is certainly quick, as it only accesses the <code>id[]</code> array once in order to complete the operation. But quick-find is typically not useful for large problems because <code>union()</code> needs to scan through the whole <code>id[]</code> array for each input pair.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb18"/></p>
<p><strong>Proposition F.</strong> The quick-find algorithm uses one array access for each call to <code>find()</code> and between <em>N</em> + 3 and 2<em>N</em>+ 1 array accesses for each call to <code>union()</code> that combines two components.</p>
<p><strong>Proof:</strong> Immediate from the code. Each call to <code>connected()</code> tests two entries in the <code>id[]</code> array, one for each of the two calls to <code>find()</code>. Each call to <code>union()</code> that combines two components does so by making two calls to <code>find()</code>, testing each of the <em>N</em> entries in the <code>id[]</code> array, and changing between 1 and <em>N</em>−1 of them.</p>
<hr/>
</div>
<p>In particular, suppose that we use quick-find for the dynamic connectivity problem and wind up with a single component. This requires at least <em>N</em>−1 calls to <code>union()</code>, and, consequently, at least (<em>N</em>+3)(<em>N</em>−1) ~ <em>N</em><sup>2</sup> array accesses—we are led immediately to the hypothesis that dynamic connectivity with quick-find can be a <em>quadratic</em>-time process. This analysis generalizes to say that quick-find is quadratic for typical applications where we end up with a small number of components. You can easily validate this hypothesis on your computer with a doubling test (see <a href="#ch01qa5q23"><small>EXERCISE 1.5.23</small></a> for an instructive example). Modern computers can execute hundreds of millions or billions of instructions per second, so this cost is not noticeable if <em>N</em> is small, but we also might find ourselves with millions or billions of sites and connections to process in a modern application, as represented by our test file <code>largeUF.txt</code>. If you are still not convinced and feel that you have a particularly fast computer, try using quick-find to determine the number of components implied by the pairs in <code>largeUF.txt</code>. The inescapable conclusion is that we cannot feasibly solve such a problem using the quick-find algorithm, so we seek better algorithms.</p>
<p class="image"><img alt="image" src="graphics/01_51-qf.jpg"/></p>
<p><a id="ch01sec3lev152"/></p>
<h5><a id="page_224"/><em>Quick-union</em></h5>
<p>The next algorithm that we consider is a complementary method that concentrates on speeding up the <code>union()</code> operation. It is based on the same data structure—the site-indexed <code>id[]</code> array—but we interpret the values differently, to define more complicated structures. Specifically, the <code>id[]</code> entry for each site is the name of another site in the same component (possibly itself)—we refer to this connection as a <em>link</em>. To implement <code>find()</code>, we start at the given site, follow its link to another site, follow that site’s link to yet another site, and so forth, following links until reaching a <em>root</em>, a site that has a link to itself (which is guaranteed to happen, as you will see). Two sites are in the same component if and only if this process leads them to the same root. To validate this process, we need <code>union(p, q)</code> to maintain this invariant, which is easily arranged: we follow links to find the roots associated with <code>p</code> and <code>q</code>, then rename one of the components by linking one of these roots to the other; hence the name <em>quick-union</em>. Again, we have an arbitrary choice of whether to rename the component containing <code>p</code> or the component containing <code>q</code>; the implementation above renames the one containing <code>p</code>. The figure on the next page shows a trace of the quick-union algorithm for <code>tinyUF.txt</code>. This trace is best understood in terms of the graphical representation depicted at left, which we consider next.</p>
<p class="image"><img alt="image" src="graphics/p0224-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/01_52-quoverview.jpg"/></p>
<p><a id="ch01sec3lev153"/></p>
<h5><a id="page_225"/><em>Forest-of-trees representation</em></h5>
<p>The code for quick-union is compact, but a bit opaque. Representing sites as <em>nodes</em> (labeled circles) and links as arrows from one node to another gives a graphical representation of the data structure that makes it relatively easy to understand the operation of the algorithm. The resulting structures are <em>trees</em>—in technical terms, our <code>id[]</code> array is a parent-link representation of a forest (set) of trees. To simplify the diagrams, we often omit both the arrowheads in the links (because they all point upwards) and the self-links in the roots of the trees. The forests corresponding to the <code>id[]</code> array for <code>tinyUF.txt</code> are shown at right. When we start at the node corresponding to any site and follow links, we eventually end up at the root of the tree containing that node. We can prove this property to be true by induction: It is true after the array is initialized to have every node link to itself, and if it is true before a given <code>union()</code> operation, it is certainly true afterward. Thus, the <code>find()</code> method on page <a href="#ch01sec3lev152">224</a> returns the name of the site at the root (so that <code>connected()</code> checks whether two sites are in the same tree). This representation is useful for this problem because the nodes corresponding to two sites are in the same tree if and only if the sites are in the same component. Moreover, the trees are not difficult to build: the <code>union()</code> implementation on page <a href="#ch01sec3lev152">224</a> combines two trees into one in a single statement, by making the root of one the parent of the other.</p>
<p class="image"><img alt="image" src="graphics/01_53-qu.jpg"/></p>
<p><a id="ch01sec3lev154"/></p>
<h5><a id="page_226"/><em>Quick-union analysis</em></h5>
<p>The quick-union algorithm would seem to be faster than the quick-find algorithm, because it does not have to go through the entire array for each input pair; but how much faster is it? Analyzing the cost of quick-union is more difficult than it was for quick-find, because the cost is more dependent on the nature of the input. In the best case, <code>find()</code> just needs one array access to find the identifier associated with a site, as in quick-find; in the worst case, it needs 2<em>N</em> − 1 array accesses, as for <code>0</code> in the example at left (this count is conservative since compiled code will typically <em>not</em> do an array access for the second reference to <code>id[p]</code> in the <code>while</code> loop). Accordingly, it is not difficult to construct a best-case input for which the running time of our dynamic connectivity client is linear; on the other hand it is also not difficult to construct a worst-case input for which the running time is quadratic (see the diagram at left and <a href="#ch01sb20"><small>PROPOSITION G</small></a> below). Fortunately, we do not need to face the problem of analyzing quick union and we will not dwell on comparative performance of quick-find and quick-union because we will next examine another variant that is far more efficient than either. For the moment, you can regard quick-union as an improvement over quick-find because it removes quick-find’s main liability (that <code>union()</code> always takes linear time). This difference certainly represents an improvement for typical data, but quick-union still has the liability that we cannot <em>guarantee</em> it to be substantially faster than quick-find in every case (for certain input data, quick-union is no faster than quick-find).</p>
<p class="image"><img alt="image" src="graphics/01_54-quworst.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb19"/></p>
<p><strong>Definition.</strong> The <em>size</em> of a tree is its number of nodes. The <em>depth</em> of a node in a tree is the number of links on the path from it to the root. The <em>height</em> of a tree is the maximum depth among its nodes.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb20"/></p>
<p><strong>Proposition G.</strong> The number of array accesses used by <code>find()</code> in quick-union is 1 plus the twice the depth of the node corresponding to the given site. The number of array accesses used by <code>union()</code> and <code>connected()</code> is the cost of the two <code>find()</code> operations (plus 1 for <code>union()</code> if the given sites are in different trees).</p>
<p><strong>Proof:</strong> Immediate from the code.</p>
<hr/>
</div>
<p><a id="page_227"/>Again, suppose that we use quick-union for the dynamic connectivity problem and wind up with a single component. An immediate implication of <a href="#ch01sb20"><small>PROPOSITION G</small></a> is that the running time is quadratic, in the worst case. Suppose that the input pairs come in the order <code>0-1</code>, then <code>0-2</code>, then <code>0-3</code>, and so forth. After <em>N</em>−1 such pairs, we have <em>N</em> sites all in the same set, and the tree that is formed by the quick-union algorithm has height <em>N</em>−1, with <code>0</code> linking to <code>1</code>, which links to <code>2</code>, which links to <code>3</code>, and so forth (see the diagram on the facing page). By <a href="#ch01sb20"><small>PROPOSITION G</small></a>, the number of array accesses for the <code>union()</code> operation for the pair <code>0 i</code> is exactly 2<em>i</em> + 2 (site <code>0</code> is at depth <em>i</em> and site <code>i</code> at depth 0). Thus, the total number of array accesses for the <code>find()</code> operations for these <em>N</em> pairs is 2 (1 + 2 + . . . + <em>N</em>) ~<em>N</em><sup>2</sup>.</p>
<p><a id="ch01sec3lev155"/></p>
<h5><em>Weighted quick-union</em></h5>
<p>Fortunately, there is an easy modification to quick-union that allows us to guarantee that bad cases such as this one do not occur. Rather than arbitrarily connecting the second tree to the first for <code>union()</code>, we keep track of the <em>size</em> of each tree and always connect the smaller tree to the larger. This change requires slightly more code and another array to hold the node counts, as shown on page <a href="#ch01sb21">228</a>, but it leads to substantial improvements in efficiency. We refer to this algorithm as the <em>weighted quick-union</em> algorithm. The forest of trees constructed by this algorithm for <code>tinyUF.txt</code> is shown in the figure at left on the top of page <a href="#page_229">229</a>. Even for this small example, the tree height is substantially smaller than the height for the unweighted version.</p>
<p class="image"><img alt="image" src="graphics/01_55-quweighted.jpg"/></p>
<p><a id="ch01sec3lev156"/></p>
<h5><em>Weighted quick-union analysis</em></h5>
<p>The figure at right on the top of page <a href="#page_229">229</a> illustrates the worst case for weighted quick union, when the sizes of the trees to be merged by <code>union()</code> are always equal (and a power of 2). These tree structures look complex, but they have the simple property that the height of a tree of 2<em><sup>n</sup></em> nodes is <em>n</em>. Furthermore, when we merge two trees of 2<em><sup>n</sup></em> nodes, we get a tree of 2<em><sup>n</sup></em><sup>+1</sup> nodes, and we increase the height of the tree to <em>n</em>+1. This observation generalizes to provide a proof that the weighted algorithm can guarantee <em>logarithmic</em> performance.</p>
<p class="image"><img alt="image" src="graphics/p0227-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch01sb21"/></p>
<h3><a id="page_228"/>Algorithm 1.5 (continued) Union-find implementation (weighted quick-union)</h3>
<p class="programlisting2"><img alt="image" src="graphics/t0228-01.jpg"/></p>
<p class="programlisting2"><img alt="image" src="graphics/t0228-02.jpg"/></p>
<p>This code is best understood in terms of the forest-of-trees representation described in the text. We add a site-indexed array <code>sz[]</code> as an instance variable so that <code>union()</code> can link the root of the smaller tree to the root of the larger tree. This addition makes it feasible to address large problems.</p>
<hr/>
</div>
<p class="image"><a id="page_229"/><img alt="image" src="graphics/01_56-quw.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb22"/></p>
<p><strong>Proposition H.</strong> The depth of any node in a forest built by weighted quick-union for <em>N</em> sites is at most lg <em>N</em>.</p>
<p><strong>Proof:</strong> We prove a stronger fact by (strong) induction: The height of every tree of size <em>k</em> in the forest is at most lg <em>k</em>. The base case follows from the fact that the tree height is 0 when <em>k</em> is 1. By the inductive hypothesis, assume that the tree height of a tree of size <em>i</em> is at most lg <em>i</em> for all <em>i</em> &lt; <em>k</em>. When we combine a tree of size <em>i</em> with a tree of size <em>j</em> with <em>i</em> ≤ <em>j</em> and <em>i</em>+<em>j</em> = <em>k</em>, we increase the depth of each node in the smaller set by 1, but they are now in a tree of size <em>i</em>+<em>j</em> = <em>k</em>, so the property is preserved because 1+lg <em>i</em> = lg(<em>i</em>+<em>i</em>) <em>≤</em> lg(<em>i</em>+<em>j</em>) = lg <em>k</em>.</p>
<hr/>
</div>
<p class="image"><a id="page_230"/><img alt="image" src="graphics/01_57-qu100.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch01sb23"/></p>
<p><strong>Corollary.</strong> For weighted quick-union with <em>N</em> sites, the worst-case order of growth of the cost of <code>find()</code>, <code>connected()</code>, and <code>union()</code> is log <em>N</em>.</p>
<p><strong>Proof.</strong> Each operation does at most a constant number of array accesses for each node on the path from a node to a root in the forest.</p>
<hr/>
</div>
<p>For dynamic connectivity, the practical implication of <a href="#ch01sb22"><small>PROPOSITION H</small></a> and its corollary is that weighted quick-union is the only one of the three algorithms that can feasibly be used for huge practical problems. The weighted quick-union algorithm uses <em>at most c M</em> lg <em>N</em> array accesses to process <em>M</em> connections among <em>N</em> sites for a small constant <em>c</em>. This result is in stark contrast to our finding that quick-find always (and quick-union sometimes) uses <em>at least MN</em> array accesses. Thus, with weighted quick-union, we can guarantee that we can solve huge practical dynamic connectivity problems in a reasonable amount of time. For the price of a few extra lines of code, we get a program that can be millions of times faster than the simpler algorithms for the huge dynamic connectivity problems that we might encounter in practical applications.</p>
<p>A 100-site example is shown on the top of this page. It is evident from this diagram that relatively few nodes fall far from the root with weighted quick-union. Indeed it is frequently the case that a 1-node tree is merged with a larger tree, which puts the node just one link from the root. Empirical studies on huge problems tell us that weighted quick-union typically solves practical problems in <em>constant</em> time per operation. We could hardly expect to find a more efficient algorithm.</p>
<p class="image"><a id="page_231"/><img alt="image" src="graphics/t0231-01.jpg"/></p>
<p><a id="ch01sec3lev157"/></p>
<h5><em>Optimal algorithms</em></h5>
<p>Can we find an algorithm that has <em>guaranteed</em> constant-time-per-operation performance? This question is an extremely difficult one that plagued researchers for many years. In pursuit of an answer, a number of variations of quick-union and weighted quick-union have been studied. For example, the following method, known as <em>path compression</em>, is easy to implement. Ideally, we would like every node to link directly to the root of its tree, but we do not want to pay the price of changing a large number of links, as we did in the quick-find algorithm. We can approach the ideal simply by making all the nodes that we <em>do</em> examine directly link to the root. This step seems drastic at first blush, but it is easy to implement, and there is nothing sacrosanct about the structure of these trees: if we can modify them to make the algorithm more efficient, we should do so. To implement path compression, we just add another loop to <code>find()</code> that sets the <code>id[]</code> entry corresponding to each node encountered along the way to link directly to the root. The net result is to flatten the trees almost completely, approximating the ideal achieved by the quick-find algorithm. The method is simple and effective, but you are not likely to be able to discern any improvement over weighted quick-union in a practical situation (see <a href="#ch01qa5q24"><small>EXERCISE 1.5.24</small></a>). Theoretical results about the situation are extremely complicated and quite remarkable. <em>Weighted quick union with path compression is optimal but not quite constant-time per operation.</em> That is, not only is weighted quick-union with path compression not constant-time per operation in the worst case (amortized), but also there exists <em>no</em> algorithm that can guarantee to perform each union-find operation in amortized constant time (under the very general “cell probe” model of computation). Weighted quick-union with path compression is very close to the best that we can do for this problem.</p>
<p><a id="ch01sec3lev158"/></p>
<h5><a id="page_232"/><em>Amortized cost plots</em></h5>
<p>As with any data type implementation, it is worthwhile to run experiments to test the validity of our performance hypotheses for typical clients, as discussion in <a href="#ch01sec1lev6"><small>SECTION 1.4</small></a>. The figure at left shows details of the performance of the algorithms for our dynamic connectivity development client when solving our 625-site connectivity example (<code>mediumUF.txt</code>). Such diagrams are easy to produce (see <a href="#ch01qa5q16"><small>EXERCISE 1.5.16</small></a>): For the <em>i</em>th connection processed, we maintain a variable <code>cost</code> that counts the number of array accesses (to <code>id[]</code> or <code>sz[]</code>) and a variable <code>total</code> that is the sum of the total number of array accesses so far. Then we plot a gray dot at <code>(i, cost)</code> and a red dot at <code>(i, total/i)</code>. The red dots are the average cost per operation, or amortized cost. These plots provide good insights into algorithm behavior. For <em>quick-find</em>, every <code>union()</code> operation uses at least 625 accesses (plus 1 for each component merged, up to another 625) and every <code>connected()</code> operation uses 2 accesses. Initially, most of the connections lead to a call on <code>union()</code>, so the cumulative average hovers around 625; later, most connections are calls to <code>connected()</code> that cause the call to <code>union()</code> to be skipped, so the cumulative average decreases, but still remains relatively high. (Inputs that lead to a large number of <code>connected()</code> calls that cause <code>union()</code> to be skipped will exhibit significantly better performance—see <a href="#ch01qa5q23"><small>EXERCISE 1.5.23</small></a> for an example). For <em>quick-union</em>, all operations initially require only a few array accesses; eventually, the height of the trees becomes a significant factor and the amortized cost grows noticably. For <em>weighted quick-union</em>, the tree height stays small, none of the operations are expensive, and the amortized cost is low. These experiments validate our conclusion that weighted quick-union is certainly worth implementing and that there is not much further room for improvement for practical problems.</p>
<p class="image"><img alt="image" src="graphics/01_58-ufallplots.jpg"/></p>
<p><a id="ch01sec2lev48"/></p>
<h4><a id="page_233"/>Perspective</h4>
<p>Each of the <code>UF</code> implementations that we considered is an improvement over the previous in some intuitive sense, but the process is artificially smooth because we have the benefit of hindsight in looking over the development of the algorithms as they were studied by researchers over the years. The implementations are simple and the problem is well specified, so we can evaluate the various algorithms directly by running empirical studies. Furthermore, we can use these studies to validate mathematical results that quantify the performance of these algorithms. When possible, we follow the same basic steps for fundamental problems throughout the book that we have taken for union–find algorithms in this section, some of which are highlighted in this list:</p>
<p class="indenthangingB">• Decide on a complete and specific problem statement, including identifying fundamental abstract operations that are intrinsic to the problem and an API.</p>
<p class="indenthangingB">• Carefully develop a succinct implementation for a straightforward algorithm, using a well-thought-out development client and realistic input data.</p>
<p class="indenthangingB">• Know when an implementation could not possibly be used to solve problems on the scale contemplated and must be improved or abandoned.</p>
<p class="indenthangingB">• Develop improved implementations through a process of stepwise refinement, validating the efficacy of ideas for improvement through empirical analysis, mathematical analysis, or both.</p>
<p class="indenthangingB">• Find high-level abstract representations of data structures or algorithms in operation that enable effective high-level design of improved versions.</p>
<p class="indenthangingB">• Strive for worst-case performance guarantees when possible, but accept good performance on typical data when available.</p>
<p class="indenthangingB">• Know when to leave further improvements for detailed in-depth study to skilled researchers and move on to the next problem.</p>
<p>The potential for spectacular performance improvements for practical problems such as those that we saw for union–find makes algorithm design a compelling field of study. What other design activities hold the potential to reap savings factors of millions or billions, or more?</p>
<p>Developing an efficient algorithm is an intellectually satisfying activity that can have direct practical payoff. As the dynamic connectivity problem indicates, a simply stated problem can lead us to study numerous algorithms that are not only both useful and interesting, but also intricate and challenging to understand. We shall encounter many ingenious algorithms that have been developed over the years for a host of practical problems. As the scope of applicability of computational solutions to scientific and commercial problems widens, so also grows the importance of being able to use efficient algorithms to solve known problems and of being able to develop efficient solutions to new problems.</p>
<p><a id="ch01sec2lev49"/></p>
<h4><a id="page_234"/>Q&amp;A</h4>
<p><strong>Q.</strong> I’d like to add a <code>delete()</code> method to the API that allows clients to delete connections. Any advice on how to proceed?</p>
<p><strong>A.</strong> No one has devised an algorithm as simple and efficient as the ones in this section that can handle deletions. This theme recurs throughout this book. Several of the data structures that we consider have the property that deleting something is much more difficult than adding something.</p>
<p><strong>Q.</strong> What is the cell-probe model?</p>
<p><strong>A.</strong> A model of computation where we only count accesses to a random-access memory large enough to hold the input and consider all other operations to be free.</p>
<p><a id="ch01sec2lev50"/></p>
<h4><a id="page_235"/>Exercises</h4>
<p><a id="ch01qa5q1"/><strong>1.5.1</strong> Show the contents of the <code>id[]</code> array and the number of times the array is accessed for each input pair when you use quick-find for the sequence <code>9-0 3-4 5-8 7-2 2-1 5-7 0-3 4-2</code>.</p>
<p><a id="ch01qa5q2"/><strong>1.5.2</strong> Do <a href="#ch01qa5q1"><small>EXERCISE 1.5.1</small></a>, but use quick-union (page <a href="#ch01sec3lev152">224</a>). In addition, draw the forest of trees represented by the <code>id[]</code> array after each input pair is processed.</p>
<p><a id="ch01qa5q3"/><strong>1.5.3</strong> Do <a href="#ch01qa5q1"><small>EXERCISE 1.5.1</small></a>, but use weighted quick-union (page <a href="#ch01sb21">228</a>).</p>
<p><a id="ch01qa5q4"/><strong>1.5.4</strong> Show the contents of the <code>sz[]</code> and <code>id[]</code> arrays and the number of array accesses for each input pair corresponding to the weighted quick-union examples in the text (both the reference input and the worst-case input).</p>
<p><a id="ch01qa5q5"/><strong>1.5.5</strong> Estimate the minimum amount of time (in days) that would be required for quick-find to solve a dynamic connectivity problem with 10<sup>9</sup> sites and 10<sup>6</sup> input pairs, on a computer capable of executing 10<sup>9</sup> instructions per second. Assume that each iteration of the inner <code>for</code> loop requires 10 machine instructions.</p>
<p><a id="ch01qa5q6"/><strong>1.5.6</strong> Repeat <a href="#ch01qa5q5"><small>EXERCISE 1.5.5</small></a> for weighted quick-union.</p>
<p><a id="ch01qa5q7"/><strong>1.5.7</strong> Develop classes <code>QuickUnionUF</code> and <code>QuickFindUF</code> that implement quick-union and quick-find, respectively.</p>
<p><a id="ch01qa5q8"/><strong>1.5.8</strong> Give a counterexample that shows why this intuitive implementation of <code>union()</code> for quick-find is not correct:</p>
<p class="programlisting"><img alt="image" src="graphics/p0235-01.jpg"/></p>
<p><a id="ch01qa5q9"/><strong>1.5.9</strong> Draw the tree corresponding to the <code>id[]</code> array depicted at right. Can this be the result of running weighted quick-union? Explain why this is impossible or give a sequence of operations that results in this array.</p>
<p class="image"><img alt="image" src="graphics/01_59-exid.jpg"/></p>
<p><a id="page_236"/><a id="ch01qa5q10"/><strong>1.5.10</strong> In the weighted quick-union algorithm, suppose that we set <code>id[find(p)]</code> to <code>q</code> instead of to <code>id[find(q)]</code>. Would the resulting algorithm be correct?</p>
<p><em>Answer</em>: Yes, but it would increase the tree height, so the performance guarantee would be invalid.</p>
<p><a id="ch01qa5q11"/><strong>1.5.11</strong> Implement <em>weighted quick-find</em>, where you always change the <code>id[]</code> entries of the smaller component to the identifier of the larger component. How does this change affect performance?</p>
<p><a id="ch01sec2lev51"/></p>
<h4><a id="page_237"/>Creative Problems</h4>
<p><a id="ch01qa5q12"/><strong>1.5.12</strong> <em>Quick-union with path compression.</em> Modify quick-union (page <a href="#ch01sec3lev152">224</a>) to include <em>path compression</em>, by adding a loop to <code>union()</code> that links every site on the paths from <code>p</code> and <code>q</code> to the roots of their trees to the root of the new tree. Give a sequence of input pairs that causes this method to produce a path of length 4. <em>Note</em>: The amortized cost per operation for this algorithm is known to be logarithmic.</p>
<p><a id="ch01qa5q13"/><strong>1.5.13</strong> <em>Weighted quick-union with path compression.</em> Modify weighted quick-union (<a href="#ch01sb17"><small>ALGORITHM 1.5</small></a>) to implement path compression, as described in <a href="#ch01qa5q12"><small>EXERCISE 1.5.12</small></a>. Give a sequence of input pairs that causes this method to produce a tree of height 4. <em>Note</em>: The amortized cost per operation for this algorithm is known to be bounded by a function known as the <em>inverse Ackermann function</em> and is less than 5 for any conceivable practical value of <em>N</em>.</p>
<p><a id="ch01qa5q14"/><strong>1.5.14</strong> <em>Weighted quick-union by height.</em> Develop a <code>UF</code> implementation that uses the same basic strategy as weighted quick-union but keeps track of tree height and always links the shorter tree to the taller one. Prove a logarithmic upper bound on the height of the trees for <em>N</em> sites with your algorithm.</p>
<p><a id="ch01qa5q15"/><strong>1.5.15</strong> <em>Binomial trees.</em> Show that the number of nodes at each level in the worst-case trees for weighted quick-union are <em>binomial coefficients</em>. Compute the average depth of a node in a worst-case tree with <em>N</em> = 2<em><sup>n</sup></em> nodes.</p>
<p><a id="ch01qa5q16"/><strong>1.5.16</strong> <em>Amortized costs plots.</em> Instrument your implementations from <a href="#ch01qa5q7"><small>EXERCISE 1.5.7</small></a> to make amortized costs plots like those in the text.</p>
<p><a id="ch01qa5q17"/><strong>1.5.17</strong> <em>Random connections.</em> Develop a <code>UF</code> client <code>ErdosRenyi</code> that takes an integer value <code>N</code> from the command line, generates random pairs of integers between <code>0</code> and <code>N-1</code>, calling <code>connected()</code> to determine if they are connected and then <code>union()</code> if not (as in our development client), looping until all sites are connected, and printing the number of connections generated. Package your program as a static method <code>count()</code> that takes <code>N</code> as argument and returns the number of connections and a <code>main()</code> that takes <code>N</code> from the command line, calls <code>count()</code>, and prints the returned value.</p>
<p><a id="ch01qa5q18"/><strong>1.5.18</strong> <em>Random grid generator.</em> Write a program <code>RandomGrid</code> that takes an <code>int</code> value <code>N</code> from the command line, generates all the connections in an <code>N</code>-by-<code>N</code> grid, puts them in random order, randomly orients them (so that <code>p q</code> and <code>q p</code> are equally likely to occur), and prints the result to standard output. To randomly order the connections, use a <code>RandomBag</code> (see <a href="#ch01qa3q34"><small>EXERCISE 1.3.34</small></a> on page <a href="#ch01sec2lev31">167</a>). To encapsulate <code>p</code> and <code>q</code> in a single object, <a id="page_238"/>use the <code>Connection</code> nested class shown below. Package your program as two static methods: <code>generate()</code>, which takes <code>N</code> as argument and returns an array of connections, and <code>main()</code>, which takes <code>N</code> from the command line, calls <code>generate()</code>, and iterates through the returned array to print the connections.</p>
<p><a id="ch01qa5q19"/><strong>1.5.19</strong> <em>Animation.</em> Write a <code>RandomGrid</code> client (see <a href="#ch01qa5q18"><small>EXERCISE 1.5.18</small></a>) that uses <code>UnionFind</code> as in our development client to check connectivity and uses <code>StdDraw</code> to draw the connections as they are processed.</p>
<p><a id="ch01qa5q20"/><strong>1.5.20</strong> <em>Dynamic growth.</em> Using linked lists or a resizing array, develop a weighted quick-union implementation that removes the restriction on needing the number of objects ahead of time. Add a method <code>newSite()</code> to the API, which returns an <code>int</code> identifier.</p>
<p class="image"><img alt="image" src="graphics/p0238-01.jpg"/></p>
<p><a id="ch01sec2lev52"/></p>
<h4><a id="page_239"/>Experiments</h4>
<p><a id="ch01qa5q21"/><strong>1.5.21</strong> <em>Erdös-Renyi model.</em> Use your client from <a href="#ch01qa5q17"><small>EXERCISE 1.5.17</small></a> to test the hypothesis that the number of pairs generated to get one component is ~ ½<em>N</em> ln <em>N</em>.</p>
<p><a id="ch01qa5q22"/><strong>1.5.22</strong> <em>Doubling test for Erdös-Renyi model.</em> Develop a performance-testing client that takes an <code>int</code> value <code>T</code> from the command line and performs <code>T</code> trials of the following experiment: Use your client from <a href="#ch01qa5q17"><small>EXERCISE 1.5.17</small></a> to generate random connections, using <code>UnionFind</code> to determine connectivity as in our development client, looping until all sites are connected. For each <code>N</code>, print the value of <code>N</code>, the average number of connections processed, and the ratio of the running time to the previous. Use your program to validate the hypotheses in the text that the running times for quick-find and quick-union are quadratic and weighted quick-union is near-linear.</p>
<p><a id="ch01qa5q23"/><strong>1.5.23</strong> <em>Compare quick-find with quick-union for Erdös-Renyi model.</em> Develop a performance-testing client that takes an <code>int</code> value <code>T</code> from the command line and performs <code>T</code> trials of the following experiment: Use your client from <a href="#ch01qa5q17"><small>EXERCISE 1.5.17</small></a> to generate random connections. Save the connections, so that you can use both quick-union and quick-find to determine connectivity as in our development client, looping until all sites are connected. For each <code>N</code>, print the value of <code>N</code> and the ratio of the two running times.</p>
<p><a id="ch01qa5q24"/><strong>1.5.24</strong> <em>Fast algorithms for Erdös-Renyi model.</em> Add weighted quick-union and weighted quick-union with path compression to your tests from <a href="#ch01qa5q23"><small>EXERCISE 1.5.23</small></a>. Can you discern a difference between these two algorithms?</p>
<p><a id="ch01qa5q25"/><strong>1.5.25</strong> <em>Doubling test for random grids.</em> Develop a performance-testing client that takes an <code>int</code> value <code>T</code> from the command line and performs <code>T</code> trials of the following experiement: Use your client from <a href="#ch01qa5q18"><small>EXERCISE 1.5.18</small></a> to generate the connections in an <code>N</code>-by-<code>N</code> square grid, randomly oriented and in random order, then use <code>UnionFind</code> to determine connectivity as in our development client, looping until all sites are connected. For each <code>N</code>, print the value of <code>N</code>, the average number of connections processed, and the ratio of the running time to the previous. Use your program to validate the hypotheses in the text that the running times for quick-find and quick-union are quadratic and weighted quick-union is near-linear. <em>Note</em>: As <code>N</code> doubles, the number of sites in the grid increases by a factor of 4, so expect a doubling factor of 16 for quadratic and 4 for linear.</p>
<p><a id="page_240"/><a id="ch01qa5q26"/><strong>1.5.26</strong> <em>Amortized plot for Erdös-Renyi.</em> Develop a client that takes an <code>int</code> value <code>N</code> from the command line and does an amortized plot of the cost of all operations in the style of the plots in the text for the process of generating random pairs of integers between <code>0</code> and <code>N-1</code>, calling <code>connected()</code> to determine if they are connected and then <code>union()</code> if not (as in our development client), looping until all sites are connected.</p>
</body>
</html>