<?xml version="1.0" encoding="UTF-8" standalone="no"?><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Six. Context</title>
<link href="9780132762564.css" rel="stylesheet" type="text/css"/>
<link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml"/>
<meta name="Adept.resource" value="urn:uuid:7baf5dbb-ffe1-4201-87bd-b993ed04f947"/>
</head>
<body>
<p><a id="ch06"/></p>
<h2><a id="page_852"/>Six. Context</h2>
<p><a id="page_853"/><small>COMPUTING DEVICES ARE UBIQUITOUS</small> in the modern world. In the last several decades, we have evolved from a world where computing devices were virtually unknown to a world where billions of people use them regularly. Moreover, today’s cellphones are orders of magnitude more powerful than the supercomputers that were available only to the privileged few as little as 30 years ago. But many of the underlying algorithms that enable these devices to work effectively are the same ones that we have studied in this book. Why? <em>Survival of the fittest.</em> Scalable (linear and linearithmic) algorithms have played a central role in the process and validate the idea that efficient algorithms are important. Researchers of the 1960s and 1970s built the basic infrastructure that we now enjoy with such algorithms. They knew that scalable algorithms are the key to the future; the developments of the past several decades have validated that vision. Now that the infrastructure is built, people are beginning to <em>use</em> it, for all sorts of purposes. As B. Chazelle has famously observed, the 20th century was the century of the equation, but the 21st century is the century of the <em>algorithm</em>.</p>
<p>Our treatment of fundamental algorithms in this book is only a starting point. The day is soon coming (if it is not already here) when one could build a college major around the study of algorithms. In commerical applications, scientific computing, engineering, operations research (OR), and countless other areas of inquiry too diverse to even mention, efficient algorithms make the difference between being able to solve problems in the modern world and not being able to address them at all. Our emphasis throughout this book has been to study <em>important</em> and <em>useful</em> algorithms. In this chapter, we reinforce this orientation by considering examples that illustrate the role of the algorithms that we have studied (and our approach to the study of algorithms) in <a id="page_854"/>several advanced contexts. To indicate the scope of the impact of the algorithms, we begin with a very brief description of several important areas of application. To indicate the depth, we later consider specific representative examples in detail and introduce the theory of algorithms. In both cases, this brief treatment at the end of a long book can only be indicative, not inclusive. For every area of application that we mention, there are dozens of others, equally broad in scope; for every point that we describe within an application, there are scores of others, equally important; and for every detailed example we consider, there are hundreds if not thousands of others, equally impactful.</p>
<p><a id="ch06sec1lev1"/></p>
<h4><em>Commercial applications</em></h4>
<p>The emergence of the internet has underscored the central role of algorithms in <em>commercial applications</em>. All of the applications that you use regularly benefit from the classic algorithms that we have studied:</p>
<p class="indenthangingB">• Infrastructure (operating systems, databases, communications)</p>
<p class="indenthangingB">• Applications (email, document processing, digital photography)</p>
<p class="indenthangingB">• Publishing (books, magazines, web content)</p>
<p class="indenthangingB">• Networks (wireless networks, social networks, the internet)</p>
<p class="indenthangingB">• Transaction processing (financial, retail, web search)</p>
<p>As a prominent example, we consider in this chapter <em>B-trees</em>, a venerable data structure that was developed for mainstream computers of the 1960s but still serve as the basis for modern database systems. We will also discuss <em>suffix arrays</em>, for text indexing.</p>
<p><a id="ch06sec1lev2"/></p>
<h4><em>Scientific computing</em></h4>
<p>Since von Neumann developed mergesort in 1950, algorithms have played a central role in <em>scientific computing</em>. Today’s scientists are awash in experimental data and are using both mathematical and computational models to understand the natural world for:</p>
<p class="indenthangingB">• Mathematical calculations (polynomials, matrices, differential equations)</p>
<p class="indenthangingB">• Data processing (experimental results and observations, especially genomics)</p>
<p class="indenthangingB">• Computational models and simulation</p>
<p>All of these can require complex and extensive computing with huge amounts of data. As a detailed example of an application in scientific computing, we consider in this chapter a classic example of <em>event-driven simulation</em>. The idea is to maintain a model of a complicated real-world system, controlling changes in the model over time. There are a vast number of applications of this basic approach. We also consider a fundamental data-processing problem in computational genomics.</p>
<p><a id="ch06sec1lev3"/></p>
<h4><em>Engineering</em></h4>
<p>Almost by definition, modern <em>engineering</em> is based on technology. Modern technology is computer-based, so algorithms play a central role for</p>
<p class="indenthangingB">• Mathematical calculations and data processing</p>
<p class="indenthangingB">• Computer-aided design and manufacturing</p>
<p class="indenthangingB"><a id="page_855"/>• Algorithm-based engineering (networks, control systems)</p>
<p class="indenthangingB">• Imaging and other medical systems</p>
<p>Engineers and scientists use many of the same tools and approaches. For example, scientists develop computational models and simulations for the purpose of understanding the natural world; engineers develop computational models and simulations for the purpose of designing, building, and controlling they artifacts they create.</p>
<p><a id="ch06sec1lev4"/></p>
<h4><em>Operations research</em></h4>
<p>Researchers and practitioners in OR develop and apply mathematical models for problem solving, including</p>
<p class="indenthangingB">• Scheduling</p>
<p class="indenthangingB">• Decision making</p>
<p class="indenthangingB">• Assignment of resources</p>
<p>The shortest-paths problem of <a href="ch04a.html#ch04sec1lev13"><small>SECTION 4.4</small></a> is a classic OR problem. We revisit this problem and consider the <em>maxflow</em> problem, illustrate the importance of <em>reduction,</em> and discuss implications for general problem-solving models, in particular the <em>linear programming</em> model that is central in OR.</p>
<p><small>ALGORITHMS PLAY AN IMPORTANT ROLE</small> in numerous subfields of computer science with applications in all of these areas, including, but certainly not limited to</p>
<p class="indenthangingB">• Computational geometry</p>
<p class="indenthangingB">• Cryptography</p>
<p class="indenthangingB">• Databases</p>
<p class="indenthangingB">• Programming languages and systems</p>
<p class="indenthangingB">• Artificial intelligence</p>
<p>In each field, articulating problems and finding efficient algorithms and data structures for solving them play an essential role. Some of the algorithms we have studied apply directly; more important, the general approach of designing, implementing, and analyzing algorithms that lies at the core of this book has proven successful in all of these fields. This effect is spreading beyond computer science to many other areas of inquiry, from games to music to linguistics to finance to neuroscience.</p>
<p>So many important and useful algorithms have been developed that learning and understanding relationships among them are essential. We finish this section (and this book!) with an introduction to the <em>theory of algorithms</em>, with particular focus on <em>intractability</em> and the <strong>P=NP?</strong> question that still stands as the key to understanding the practical problems that we aspire to solve.</p>
<p><a id="ch06sec1lev5"/></p>
<h3><a id="page_856"/>Event-driven simulation</h3>
<p>Our first example is a fundamental scientific application: simulate the motion of a system of moving particles that behave according to the laws of elastic collision. Scientists use such systems to understand and predict properties of physical systems. This paradigm embraces the motion of molecules in a gas, the dynamics of chemical reactions, atomic diffusion, sphere packing, the stability of the rings around planets, the phase transitions of certain elements, one-dimensional self-gravitating systems, front propagation, and many other situations. Applications range from molecular dynamics, where the objects are tiny subatomic particles, to astrophysics, where the objects are huge celestial bodies.</p>
<p>Addressing this problem requires a bit of high-school physics, a bit of software engineering, and a bit of algorithmics. We leave most of the physics for the exercises at the end of this section so that we can concentrate on the topic at hand: using a fundamental algorithmic tool (heap-based priority queues) to address an application, enabling calculations that would not otherwise be possible.</p>
<p><a id="ch06sec2lev1"/></p>
<h4><em>Hard-disc model</em></h4>
<p>We begin with an idealized model of the motion of atoms or molecules in a container that has the following salient features:</p>
<p class="indenthangingB">• Moving <em>particles</em> interact via elastic collisions with each other and with <em>walls</em>.</p>
<p class="indenthangingB">• Each particle is a disc with known position, velocity, mass, and radius.</p>
<p class="indenthangingB">• No other forces are exerted.</p>
<p>This simple model plays a central role in <em>statistical mechanics</em>, a field that relates macroscopic observables (such as temperature and pressure) to microscopic dynamics (such as the motion of individual atoms and molecules). Maxwell and Boltzmann used the model to derive the distribution of speeds of interacting molecules as a function of temperature; Einstein used the model to explain the Brownian motion of pollen grains immersed in water. The assumption that no other forces are exerted implies that particles travel in straight lines at constant speed between collisions. We could also extend the model to add other forces. For example, if we add friction and spin, we can more accurately model the motion of familiar physical objects such as billiard balls on a pool table.</p>
<p class="image"><img alt="image" src="graphics/06_01-collisiontime.jpg"/></p>
<p><a id="ch06sec2lev2"/></p>
<h4><em>Time-driven simulation</em></h4>
<p>Our primary goal is simply to maintain the model: that is, we want to be able to keep track of the positions and velocities of all the particles as time passes. The basic calculation that we have to do is the following: given the positions and velocities for a specific time <em>t</em>, update them to reflect the situation at <a id="page_857"/>a future time <em>t+dt</em> for a specific amount of time <em>dt</em>. Now, if the particles are sufficiently far from one another and from the walls that no collision will occur before <em>t+dt</em>, then the calculation is easy: since particles travel in a straight-line trajectory, we use each particle’s velocity to update its position. The challenge is to take the collisions into account. One approach, known as <em>time-driven simulation</em>, is based on using a fixed value of <em>dt</em>. To do each update, we need to check all pairs of particles, determine whether or not any two occupy the same position, and then back up to the moment of the first such collision. At that point, we are able to properly update the velocities of the two particles to reflect the collision (using calculations that we will discuss later). This approach is computationally intensive when simulating a large number of particles: if <em>dt</em> is measured in seconds (fractions of a second, usually), it takes time proportional to <em>N</em><sup>2</sup>/<em>dt</em> to simulate an <em>N</em>-particle system for 1 second. This cost is prohibitive (even worse than usual for quadratic algorithms)—in the applications of interest, <em>N</em> is very large and <em>dt</em> is very small. The challenge is that if we make <em>dt</em> too small, the computational cost is high, and if we make <em>dt</em> too large, we may miss collisions.</p>
<p class="image"><img alt="image" src="graphics/06_02-collisiontimebad.jpg"/></p>
<p><a id="ch06sec2lev3"/></p>
<h4><em>Event-driven simulation</em></h4>
<p>We pursue an alternative approach that focuses only on those times at which collisions occur. In particular, we are always interested in the <em>next</em> collision (because the simple update of all of the particle positions using their velocities is valid until that time). Therefore, we maintain a priority queue of <em>events,</em> where an event is a potential collision sometime in the future, either between two particles or between a particle and a wall. The priority associated with each event is its time, so when we <em>remove the minimum</em> from the priority queue, we get the next potential collision.</p>
<p><a id="ch06sec2lev4"/></p>
<h4><em>Collision prediction</em></h4>
<p>How do we identify potential collisions? The particle velocities provide precisely the information that we need. For example, suppose that we have, at time <em>t</em>, a particle of radius <em>s</em> at position (<em>r<sub>x</sub></em>, <em>r<sub>y</sub></em>) moving with velocity (<em>v<sub>x</sub></em>, <em>v<sub>y</sub></em>) in the unit box. Consider the vertical wall at <em>x =</em> 1 with <em>y</em> between 0 and 1. Our interest is in the horizontal component of the motion, so we can concentrate on the <em>x</em>-component of the position <em>r<sub>x</sub></em> and the <em>x</em>-component of the velocity <em>v<sub>x</sub></em>. If <em>v<sub>x</sub></em> is negative, the particle is not on a collision course with the wall, but if <em>v<sub>x</sub></em> is positive, there is a potential collision with the wall. Dividing the horizontal distance to the wall (1 − <em>s</em> − <em>r<sub>x</sub></em>) by the magnitude of the horizontal component of the velocity (<em>v<sub>x</sub></em>) we find that the particle will hit the wall after <em>dt</em> = (1 − <em>s</em> − <em>r<sub>x</sub></em>)/<em>v<sub>x</sub></em> time units, when the particle will be at (1 − <em>s</em>, <em>r<sub>y</sub></em> + <em>v<sub>y</sub> dt</em>), unless it hits some other particle or a horizontal wall before that time. Accordingly, we <a id="page_858"/>put an entry on the priority queue with priority <em>t + dt</em> (and appropriate information describing the particle-wall collision event). The collision-prediction calculations for other walls are similar (see <a href="#ch06qa1q1"><small>EXERCISE 6.1</small></a>). The calculation for two particles colliding is also similar, but more complicated. Note that it is often the case that the calculation leads to a prediction that the collision will <em>not</em> happen (if the particle is moving away from the wall, or if two particles are moving away from one another)—we do not need to put anything on the priority queue in such cases. To handle another typical situation where the predicted collision might be too far in the future to be of interest, we include a parameter <code>limit</code> that specifies the time period of interest, so we can also ignore any events that are predicted to happen at a time later than <code>limit</code>.</p>
<p class="image"><img alt="image" src="graphics/06_03-collisionwall.jpg"/></p>
<p><a id="ch06sec2lev5"/></p>
<h4><em>Collision resolution</em></h4>
<p>When a collision does occur, we need to resolve it by applying the physical formulas that specify the behavior of a particle after an elastic collision with a reflecting boundary or with another particle. In our example where the particle hits the vertical wall, if the collision does occur, the velocity of the particle will change from (<em>v<sub>x</sub></em>, <em>v<sub>y</sub></em>) to (– <em>v<sub>x</sub></em>, <em>v<sub>y</sub></em>) at that time. The collision-resolution calculations for other walls are similar, as are the calculations for two particles colliding, but these are more complicated (see <a href="#ch06qa1q1"><small>EXERCISE 6.1</small></a>).</p>
<p class="image"><img alt="image" src="graphics/06_04-collisionparticls.jpg"/></p>
<p><a id="ch06sec2lev6"/></p>
<h4><a id="page_859"/><em>Invalidated events</em></h4>
<p>Many of the collisions that we predict do not actually happen because some other collision intervenes. To handle this situation, we maintain an instance variable for each particle that counts the number of collisions in which it has been involved. When we remove an event from the priority queue for processing, we check whether the counts corresponding to its particle(s) have changed since the event was created. This approach to handling invalidated collisions is the so-called <em>lazy</em> approach: when a particle is involved in a collision, we leave the now-invalid events associated with it on the priority queue and essentially ignore them when they come off. An alternative approach, the so-called <em>eager</em> approach, is to remove from the priority queue all events involving any colliding particle before calculating all of the new potential collisions for that particle. This approach requires a more sophisticated priority queue (that implements the <em>remove</em> operation).</p>
<p class="image"><img alt="image" src="graphics/06_05-collisionevents.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_06-collisionnonevents.jpg"/></p>
<p><small>THIS DISCUSSION</small> sets the stage for a full event-driven simulation of particles in motion, interacting according to the physical laws of elastic collisions. The software architecture is to encapsulate the implementation in three classes: a <code>Particle</code> data type that encapsulates calculations that involve particles, an <code>Event</code> data type for predicted events, and a <code>CollisionSystem</code> client that does the simulation. The centerpiece of the simulation is a <code>MinPQ</code> that contains events, ordered by time. Next, we consider implementations of <code>Particle</code>, <code>Event</code>, and <code>CollisionSystem</code>.</p>
<p class="image"><img alt="image" src="graphics/06_07-collisioninvalid.jpg"/></p>
<p><a id="ch06sec2lev7"/></p>
<h4><a id="page_860"/><em>Particles</em></h4>
<p><a href="#ch06qa1q1"><small>EXERCISE 6.1</small></a> outlines the implementation of a data type particles, based on a direct application of Newton’s laws of motion. A simulation client needs to be able to move particles, draw them, and perform a number of calculations related to collisions, as detailed in the following API:</p>
<p class="image"><img alt="image" src="graphics/p0860-01.jpg"/></p>
<p>The three <code>timeToHit*()</code> methods all return <code>Double.POSITIVE_INFINITY</code> for the (rather common) case when there is no collision course. These methods allow us to predict all future collisions that are associated with a given particle, putting an event on a priority queue corresponding to each one that happens before a given time <code>limit</code>. We use the <code>bounce()</code> method each time that we process an event that corresponds to two particles colliding to change the velocities (of both particles) to reflect the collision, and the <code>bounceOff*()</code> methods for events corresponding to collisions between a particle and a wall.</p>
<p><a id="ch06sec2lev8"/></p>
<h4><a id="page_861"/><em>Events</em></h4>
<p>We encapsulate in a private class the description of the objects to be placed on the priority queue (events). The instance variable <code>time</code> holds the time when the event is predicted to happen, and the instance variables <code>a</code> and <code>b</code> hold the particles associated with the event. We have three different types of events: a particle may hit a vertical wall, a horizontal wall, or another particle. To develop a smooth dynamic display of the particles in motion, we add a fourth event type, a redraw event that is a command to draw all the particles at their current positions. A slight twist in the implementation of <code>Event</code> is that we use the fact that particle values may be null to encode these four different types of events, as follows:</p>
<p class="indenthangingB">• Neither <code>a</code> nor <code>b null</code>: particle-particle collision</p>
<p class="indenthangingB">• <code>a</code> not <code>null</code> and <code>b null</code>: collision between <code>a</code> and a vertical wall</p>
<p class="indenthangingB">• <code>a null</code> and <code>b</code> not <code>null</code>: collision between <code>b</code> and a horizontal wall</p>
<p class="indenthangingB">• Both <code>a</code> and <code>b null</code>: redraw event (draw all particles)</p>
<p>While not the finest object-oriented programming, this convention is a natural one that enables straightforward client code and leads to the implementation shown below.</p>
<p class="image"><img alt="image" src="graphics/p0861-01.jpg"/></p>
<p><a id="page_862"/>A second twist in the implementation of <code>Event</code> is that we maintain the instance variables <code>countA</code> and <code>countB</code> to record the number of collisions involving each of the particles <em>at the time the event is created</em>. If these counts are unchanged when the event is removed from the priority queue, we can go ahead and simulate the occurrence of the event, but if one of the counts changes between the time an event goes on the priority queue and the time it leaves, we know that the event has been invalidated and can ignore it. The method <code>isValid()</code> allows client code to test this condition.</p>
<p><a id="ch06sec2lev9"/></p>
<h4><em>Simulation code</em></h4>
<p>With the computational details encapsulated in <code>Particle</code> and <code>Event</code>, the simulation itself requires remarkably little code, as you can see in the implementation in the class <code>CollisionSystem</code> (see page <a href="#ch06sb01">863</a> and page <a href="#ch06sb02">864</a>). Most of the calculations are encapsulated in the <code>predictCollisions()</code> method shown on this page. This method calculates all potential future collisions involving particle <code>a</code> (either with another particle or with a wall) and puts an event corresponding to each onto the priority queue.</p>
<p class="image"><img alt="image" src="graphics/p0862-01.jpg"/></p>
<p>The heart of the simulation is the <code>simulate()</code> method shown on page <a href="#ch06sb02">864</a>. We initialize by calling <code>predictCollisions()</code> for each particle to fill the priority queue with the potential collisions involving all particle-wall and all particle-particle pairs. Then we enter the main event-driven simulation loop, which works as follows:</p>
<p class="indenthangingB">• Delete the impending event (the one with minimum priority <em>t</em>).</p>
<p class="indenthangingB">• If the event is invalid, ignore it.</p>
<p class="indenthangingB">• Advance all particles to time <em>t</em> on a straight-line trajectory.</p>
<p class="indenthangingB">• Update the velocities of the colliding particle(s).</p>
<p class="indenthangingB">• Use <code>predictCollisions()</code> to predict future collisions involving the colliding particle(s) and insert onto the priority queue an event corresponding to each.</p>
<p>This simulation can serve as the basis for computing all manner of interesting properties of the system, as explored in the exercises. For example, one fundamental property <a id="page_865"/>of interest is the amount of pressure exerted by the particles against the walls. One way to calculate the pressure is to keep track of the number and magnitude of wall collisions (an easy computation based on particle mass and velocity) so that we can easily compute the total. Temperature involves a similar calculation.</p>
<div class="sidebar">
<hr/>
<p><a id="ch06sb01"/></p>
<h3><a id="page_863"/>Event-driven simulation of colliding particles (scaffolding)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0863-01.jpg"/></p>
<p>This class is a priority-queue client that simulates the motion of a system of particles over time. The <code>main()</code> test client takes a command-line argument <em>N</em>, creates <em>N</em> random particles, creates a <code>CollisionSystem</code> consisting of the particles, and calls <code>simulate()</code> to do the simulation. The instance variables are a priority queue for the simulation, the time, and the particles.</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch06sb02"/></p>
<h3><a id="page_864"/>Event-driven simulation of colliding particles (primary loop)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0864-01.jpg"/></p>
<p>This method represents the main event-driven simulation. First, the priority queue is initialized with events representing all predicted future collisions involving each particle. Then the main loop takes an event from the queue, updates time and particle positions, and adds new events to reflect changes.</p>
<p class="image"><img alt="image" src="graphics/06_08-collision.jpg"/></p>
<hr/>
</div>
<p><a id="ch06sec2lev10"/></p>
<h4><em>Performance</em></h4>
<p>As described at the outset, our interest in event-driven simulation is to avoid the computationally intensive inner loop intrinsic in time-driven simulation.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb03"/></p>
<p><strong>Proposition A.</strong> An event-driven simulation of <em>N</em> colliding particles requires at most <em>N</em><sup>2</sup> priority queue operations for initialization, and at most <em>N</em> priority queue operations per collision (with one extra priority queue operation for each invalid collision).</p>
<p><strong>Proof</strong> Immediate from the code.</p>
<hr/>
</div>
<p>Using our standard guaranteed-logarithmic-time-per operation priority-queue implementation from <a href="ch02.html#ch02sec1lev4"><small>SECTION 2.4</small></a>, the time needed per collision is linearithmic. Simulations with large numbers of particles are therefore quite feasible.</p>
<p><small>EVENT-DRIVEN SIMULATION</small> applies to countless other domains that involve physical modeling of moving objects, from molecular modeling to astrophysics to robotics. Such applications may involve extending the model to add other kinds of bodies, to operate in three dimensions, to include other forces, and in many other ways. Each extension involves its own computational challenges. This event-driven approach results in a more robust, accurate, and efficient simulation than many other alternatives that we might consider, and the efficiency of the heap-based priority queue enables calculations that might not otherwise be possible.</p>
<p>Simulation plays a vital role in helping researchers to understand properties of the natural world in all fields of science and engineering. Applications ranging from manufacturing processes to biological systems to financial systems to complex engineered structures are too numerous to even list here. For a great many of these applications, the extra efficiency afforded by the heap-based priority queue data type or an efficient sorting algorithm can make a substantial difference in the quality and extent that are possible in the simulation.</p>
<p><a id="ch06sec1lev6"/></p>
<h3><a id="page_866"/>B-trees</h3>
<p>In <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>, we saw that algorithms that are appropriate for accessing items from huge collections of data are of immense practical importance. Searching is a fundamental operation on huge data sets, and such searching consumes a significant fraction of the resources used in many computing environments. With the advent of the web, we have the ability to access a vast amount of information that might be relevant to a task—our challenge is to be able to search through it efficiently. In this section, we describe a further extension of the balanced-tree algorithms from <a href="ch03.html#ch03sec1lev3"><small>SECTION 3.3</small></a> that can support <em>external search</em> in symbol tables that are kept on a disk or on the web and are thus potentially far larger than those we have been considering (which have to fit in addressable memory). Modern software systems are blurring the distinction between local files and web pages, which may be stored on a remote computer, so the amount of data that we might wish to search is virtually unlimited. Remarkably, the methods that we shall study can support search and insert operations on symbol tables containing trillions of items or more using only four or five references to small blocks of data.</p>
<p><a id="ch06sec2lev11"/></p>
<h4><em>Cost model</em></h4>
<p>Data storage mechanisms vary widely and continue to evolve, so we use a simple model to capture the essentials. We use the term <em>page</em> to refer to a contiguous block of data and the term <em>probe</em> to refer to the first access to a page. We assume that accessing a page involves reading its contents into local memory, so that subsequent accesses are relatively inexpensive. A page could be a file on your local computer or a web page on a distant computer or part of a file on a server, or whatever. Our goal is to develop search implementations that use a small number of probes to find any given key. We avoid making specific assumptions about the page size and about the ratio of the time required for a probe (which presumably requires communicating with a distant device) to the time required, subsequently, to access items within the block (which presumably happens in a local processor). In typical situations, these values are likely to be on the order of 100 or 1,000 or 10,000; we do not need to be more precise because the algorithms are not highly sensitive to differences in the values in the ranges of interest.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb04"/></p>
<p><strong>B-tree cost model.</strong> When studying algorithms for external searching, we count <em>page accesses</em> (the number of times a page is accessed, for read or write).</p>
<hr/>
</div>
<p><a id="ch06sec2lev12"/></p>
<h4><em>B-trees</em></h4>
<p>The approach is to extend the 2-3 tree data structure described in <a href="ch03.html#ch03sec1lev3"><small>SECTION 3.3</small></a>, with a crucial difference: rather than store the data in the tree, we build a tree with <em>copies</em> of the keys, each key copy associated with a link. This approach enables us to more easily separate the index from the table itself, much like the index in a book. As with 2-3 trees, we enforce upper and lower bounds on the number of key-link pairs <a id="page_867"/>that can be in each node: we choose a parameter <em>M</em> (an even number, by convention) and build multiway trees where every node must have <em>at most M −</em> 1 key-link pairs (we assume that <em>M</em> is sufficiently small that an <em>M</em>-way node will fit on a page) and <em>at least M</em>/2 key-link pairs (to provide the branching that we need to keep search paths short), except possibly the root, which can have fewer than <em>M</em>/2 key-link pairs but must have at least 2. Such trees were named <em>B-trees</em> by Bayer and McCreight, who, in 1970, were the first researchers to consider the use of multiway balanced trees for external searching. Some people reserve the term <em>B-tree</em> to describe the exact data structure built by the algorithm suggested by Bayer and McCreight; we use it as a generic term for data structures based on multiway balanced search trees with a fixed page size. We specify the value of <em>M</em> by using the terminology “<em>B-tree of order M</em>.” In a B-tree of order 4, each node has at most 3 and at least 2 key-link pairs; in a B-tree of order 6, each node has at most 5 and at least 3 link pairs (except possibly the root, which could have 2 key-link pairs), and so forth. The reason for the exception at the root for larger <em>M</em> will become clear when we consider the construction algorithm in detail.</p>
<p><a id="ch06sec2lev13"/></p>
<h4><em>Conventions</em></h4>
<p>To illustrate the basic mechanisms, we consider an (ordered) <code>SET</code> implementation (with keys and no values). Extending to provide an ordered <code>ST</code> to associate keys with values is an instructive exercise (see <a href="#ch06qa1q16"><small>EXERCISE 6.16</small></a>). Our goal is to support <code>add()</code> and <code>contains()</code> for a set of keys that could be huge. We use ordered keys because we are generalizing search trees, which are based on ordered keys. Extending our implementation to support other ordered operations is also an instructive exercise. In external searching applications, it is common to keep the index separate from the data. For B-trees, we do so by using two different kinds of nodes:</p>
<p class="indenthangingB">• <em>Internal</em> nodes, which associate copies of keys with pages</p>
<p class="indenthangingB">• <em>External</em> nodes, which have references to the actual data</p>
<p>Every key in an internal node is associated with another node that is the root of a tree containing all keys <em>greater than or equal to</em> that key and <em>less than</em> the next largest key, if <a id="page_868"/>any. It is convenient to use a special key, known as a <em>sentinel</em>, that is defined to be less than all other keys, and to start with a root node containing that key, associated with the tree containing all the keys. The symbol table does not contain duplicate keys, but we use copies of keys (in internal nodes) to guide the search. (In our examples, we use single-letter keys and the character <code>*</code> as the sentinel that is less than all other keys.) These conventions simplify the code somewhat and thus represent a convenient (and widely used) alternative to mixing all the data with links in the internal nodes, as we have done for other search trees.</p>
<p class="image"><img alt="image" src="graphics/06_09-banatomy.jpg"/></p>
<p><a id="ch06sec2lev14"/></p>
<h4><em>Search and insert</em></h4>
<p>Search in a B-tree is based on recursively searching in the unique subtree that could contain the search key. Every search ends in an external node that contains the key if and only if it is in the set. We might also terminate a <em>search hit</em> when encountering a copy of the search key in an internal node, but we always search to an external node because doing so simplifies extending the code to an ordered symbol-table implementation (also, this event rarely happens when <em>M</em> is large). To be specific, consider searching in a B-tree of order 6: it consists of 3-nodes with 3 key-link pairs, 4-nodes with 4 key-link pairs, and 5-nodes with 5 key-link pairs, with possibly a 2-node at the root. To search, we start at the root and move from one node to the next by finding the proper interval for the search key in the current node and then exiting through the corresponding link to get to the next node. Eventually, the search process leads us to a page containing keys at the bottom of the tree. We terminate the search with a search hit if the search key is in that page; we terminate with a search miss if it is not. As with 2-3 trees, we can use recursive code to insert a new key at the bottom of the tree. If there is no room for the key, we allow the node at the bottom to temporarily overflow (become a 6-node) and then split 6-nodes on the way up the tree, after the recursive call. If the root is an 6-node, we split it into a 2-node connected to two 3-nodes; elsewhere in the tree, we replace any <em>k</em>-node attached to a 6-node by a (<em>k</em>+1)-node attached to two 3-nodes. Replacing 3 by <em>M</em>/2 and 6 by <em>M</em> in this description converts it into a description of search and insert for B-trees of order <em>M</em> and leads to the following definition:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb05"/></p>
<p><strong>Definition.</strong> A <em>B-tree of order M</em> (where <em>M</em> is an even positive integer) is a tree that either is an external <em>k</em>-node (with <em>k</em> keys and associated information) or comprises internal <em>k</em>-nodes (each with <em>k</em> keys and <em>k</em> links to B-trees representing each of the <em>k</em> intervals delimited by the keys), having the following structural properties: every path from the root to an external node must be the same length (perfect balance); and <em>k</em> must be between 2 and <em>M −</em> 1 at the root and between <em>M</em>/2 and <em>M −</em> 1 at every other node.</p>
<hr/>
</div>
<p class="image"><a id="page_869"/><img alt="image" src="graphics/06_10-bsearch.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_11-binsert.jpg"/></p>
<p><a id="ch06sec2lev15"/></p>
<h4><a id="page_870"/><em>Representation</em></h4>
<p>As just discussed, we have a great deal of freedom in choosing concrete representations for nodes in B-trees. We encapsulate these choices in a <code>Page</code> API that associates keys with links to <code>Page</code> objects and supports the operations that we need to test for overfull pages, split them, and distinguish between internal and external pages. You can think of a <code>Page</code> as a symbol table, kept externally (in a file on your computer or on the web). The terms <em>open</em> and <em>close</em> in the API refer to the process of bringing an external page into internal memory and writing its contents back out (if necessary). The <code>add()</code> method for internal pages is a symbol-table operation that associates the given page with the minimum key in the tree rooted at that page. The <code>put()</code> and <code>contains()</code> methods for external pages are like their corresponding <code>SET</code> operations. The workhorse of any implementation is the <code>split()</code> method, which splits a full page by moving the <em>M</em>/2 key-value pairs of rank greater than <em>M</em>/2 to a new <code>Page</code> and returns a reference to that page. <a href="#ch06qa1q15"><small>EXERCISE 6.15</small></a> discusses an implementation of <code>Page</code> using <code>BinarySearchST</code>, which implements B-trees in memory, like our other search implementations. On some systems, this might suffice as an external searching implementation because a virtual-memory system might take care of disk references. More typical practical implementations might involve hardware-specific code that reads and <a id="page_871"/>writes pages. <a href="#ch06qa1q19"><small>EXERCISE 6.19</small></a> encourages you to think about implementing <code>Page</code> using web pages. We ignore such details here in the text to emphasize the utility of the B-tree concept in a broad variety of settings.</p>
<p class="image"><img alt="image" src="graphics/t0870-01.jpg"/></p>
<p>With these preparations, the code for <code>BTreeSET</code> on page <a href="#ch06sb07">872</a> is remarkably simple. For <code>contains()</code>, we use a recursive method that takes a <code>Page</code> as argument and handles three cases:</p>
<p class="indenthangingB">• If the page is external and the key is in the page, return <code>true</code>.</p>
<p class="indenthangingB">• If the page is external and the key is not in the page, return <code>false</code>.</p>
<p class="indenthangingB">• Otherwise, do a recursive call for the subtree that could contain the key.</p>
<p>For <code>put()</code> we use the same recursive structure, but insert the key at the bottom if it is not found during the search and then split any full nodes on the way up the tree.</p>
<p><a id="ch06sec2lev16"/></p>
<h4><em>Performance</em></h4>
<p>The most important property of B-trees is that for reasonable values of the parameter <em>M</em> the search cost is <em>constant</em>, for all practical purposes:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb06"/></p>
<p><strong>Proposition B.</strong> A search or an insertion in a B-tree of order <em>M</em> with <em>N</em> items requires between log<em><sub>M</sub> N</em> and log<em><sub>M</sub></em><sub>/2</sub> <em>N</em> probes—a constant number, for practical purposes.</p>
<p><strong>Proof</strong> This property follows from the observation that all the nodes in the interior of the tree (nodes that are not the root and are not external) have between <em>M</em>/2 and <em>M −</em> 1 links, since they are formed from a split of a full node with <em>M</em> keys and can only grow in size (when a child is split). In the best case, these nodes form a complete tree of branching factor <em>M −</em> 1, which leads immediately to the stated bound. In the worst case, we have a root with two entries each of which refers to a complete tree of degree <em>M</em>/2. Taking the logarithm to the base <em>M</em> results in a very small number—for example, when <em>M</em> is 1,000, the height of the tree is less than 4 for <em>N</em> less than 62.5 billion.</p>
<hr/>
</div>
<p>In typical situations, we can reduce the cost by one probe by keeping the root in internal memory. For searching on disk or on the web, we might take this step explicitly before embarking on any application involving a huge number of searches; in a virtual memory with caching, the root node will be the one most likely to be in fast memory, because it is the most frequently accessed node.</p>
<p><a id="ch06sec2lev17"/></p>
<h4><em>Space</em></h4>
<p>The space usage of B-trees is also of interest in practical applications. By construction, the pages are at least half full, so, in the worst case, B-trees use about double the space that is absolutely necessary for keys, plus extra space for links. For random keys, A. Yao proved in 1979 (using mathematical analysis that is beyond the scope of <a id="page_874"/>this book) that the average number of keys in a node is about <em>M</em> ln 2, so about 44 percent of the space is unused. As with many other search algorithms, this random model reasonably predicts results for key distributions that we observe in practice.</p>
<div class="sidebar">
<hr/>
<p><a id="ch06sb07"/></p>
<h3><a id="page_872"/>Algorithm 6.12 B-tree set implementation</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0872-01.jpg"/></p>
<p>This B-tree implementation implements multiway balanced search trees as described in the text, using a <code>Page</code> data type that supports search by associating keys with subtrees that could contain the key and supports insertion by including a test for overflow and a page split method.</p>
<hr/>
</div>
<p class="image"><a id="page_873"/><img alt="image" src="graphics/06_12-bsimpagenew.jpg"/></p>
<p><small>THE IMPLICATIONS OF</small> <a href="#ch06sb06"><small>PROPOSITION B</small></a> <small>ARE PROFOUND</small> and worth contemplating. Would you have guessed that you can develop a search implementation that can guarantee a cost of four or five probes for search and insert in files as large as you can reasonably contemplate needing to process? B-trees are widely used because they allow us to achieve this ideal. In practice, the primary challenge to developing an implementation is ensuring that space is available for the B-tree nodes, but even that challenge becomes easier to address as available storage space increases on typical devices.</p>
<p>Many variations on the basic B-tree abstraction suggest themselves immediately. One class of variations saves time by packing as many page references as possible in internal nodes, thereby increasing the branching factor and flattening the tree. Another class of variations improves storage efficiency by combining nodes with siblings before splitting. The precise choice of variant and algorithm parameter can be engineered to suit particular devices and applications. Although we are limited to getting a small constant factor improvement, such an improvement can be of significant importance for applications where the table is huge and/or huge numbers of transactions are involved, precisely the applications for which B-trees are so effective.</p>
<p><a id="ch06sec1lev7"/></p>
<h3><a id="page_875"/>Suffix arrays</h3>
<p>Efficient algorithms for string processing play a critical role in commercial applications and in scientific computing. From the countless strings that define web pages that are searched by billions of users to the extensive genomic databases that scientists are studying to unlock the secret of life, computing applications of the 21st century are increasingly string-based. As usual, some classic algorithms are effective, but remarkable new algorithms are being developed. Next, we describe a data structure and an API that support some of these algorithms. We begin by describing a typical (and a classic) string-processing problem.</p>
<p><a id="ch06sec2lev18"/></p>
<h4><em>Longest repeated substring</em></h4>
<p>What is the longest substring that appears at least twice in a given string? For example, the longest repeated substring in the string <code>"to be or not to be"</code> is the string <code>"to be"</code>. Think briefly about how you might solve it. Could you find the longest repeated substring in a string that has millions of characters? This problem is simple to state and has many important applications, including data compression, cryptography, and computer-assisted music analysis. For example, a standard technique used in the development of large software systems is <em>refactoring code</em>. Programmers often put together new programs by cutting and pasting code from old programs. In a large program built over a long period of time, replacing duplicate code by function calls to a single copy of the code can make the program much easier to understand and maintain. This improvement can be accomplished by finding long repeated substrings in the program. Another application is found in computational biology. Are substantial identical fragments to be found within a given genome? Again, the basic computational problem underlying this question is to find the longest repeated substring in a string. Scientists are typically interested in more detailed questions (indeed, the nature of the repeated substrings is precisely what scientists seek to understand), but such questions are certainly no easier to answer than the basic question of finding the longest repeated substring.</p>
<p><a id="ch06sec2lev19"/></p>
<h4><em>Brute-force solution</em></h4>
<p>As a warmup, consider the following simple task: given two strings, find their longest common <em>prefix</em> (the longest substring that is a prefix of both strings). For example, the longest common prefix of <code>acctgttaac</code> and <code>accgttaa</code> is <code>acc</code>. The code at right is a useful starting point for addressing more complicated tasks: it takes time proportional to the length of the match. Now, how do we find the longest repeated substring in a given string? With <code>lcp()</code>, the following <a id="page_876"/>brute-force solution immediately suggests itself: we compare the substring starting at each string position <code>i</code> with the substring starting at each other starting position <code>j</code>, keeping track of the longest match found. This code is not useful for long strings, because its running time is at least <em>quadratic</em> in the length of the string: the number of different pairs <code>i</code> and <code>j</code> is <em>N</em> (<em>N</em>−1)/2, so the number of calls on <code>lcp()</code> for this approach would be ~<em>N</em><sup>2</sup>/2. Using this solution for a genomic sequence with millions of characters would require trillions of <code>lcp()</code> calls, which is infeasible.</p>
<p class="image"><img alt="image" src="graphics/p0875-01.jpg"/></p>
<p><a id="ch06sec2lev20"/></p>
<h4><em>Suffix sort solution</em></h4>
<p>The following clever approach, which takes advantage of sorting in an unexpected way, is an effective way to find the longest repeated substring, even in a huge string: we use Java’s <code>substring()</code> method to make an array of strings that consists of the suffixes of <code>s</code> (the substrings starting at each position and going to the end), and then we sort this array. The key to the algorithm is that every substring appears somewhere as a prefix of one of the suffixes in the array. After sorting, the longest repeated substrings will appear in adjacent positions in the array. Thus, we can make a single pass through the sorted array, keeping track of the longest matching prefixes between adjacent strings. This approach is significantly more efficient than the brute-force method, but before implementing and analyzing it, we consider another application of suffix sorting.</p>
<p class="image"><img alt="image" src="graphics/06_13-lrs.jpg"/></p>
<p><a id="ch06sec2lev21"/></p>
<h4><a id="page_877"/><em>Indexing a string</em></h4>
<p>When you are trying to find a particular substring within a large text—for example, while working in a text editor or within a page you are viewing with a browser—you are doing a <em>substring search</em>, the problem we considered in <a href="ch05.html#ch05sec1lev9"><small>SECTION 5.3</small></a>. For that problem, we assume the text to be relatively large and focus on preprocessing the <em>substring</em>, with the goal of being able to efficiently find that substring in any given text. When you type search keys into your web browser, you are doing a <em>search with string keys</em>, the subject of <a href="ch05.html#ch05sec1lev8"><small>SECTION 5.2</small></a>. Your search engine must precompute an index, since it cannot afford to scan all the pages in the web for your keys. As we discussed in <a href="ch03a.html#ch03sec1lev5"><small>SECTION 3.5</small></a> (see <a href="ch03a.html#ch03sb38"><code>FileIndex</code></a> on page <a href="ch03a.html#ch03sb38">501</a>), this would ideally be an inverted index associating each possible search string with all web pages that contain it—a symbol table where each entry is a string key and each value is a set of pointers (each pointer giving the information necessary to locate an occurrence of the key on the web—perhaps a URL that names a web page and an integer offset within that page). In practice, such a symbol table would be far too big, so your search engine uses various sophisticated algorithms to reduce its size. One approach is to rank web pages by importance (perhaps using an algorithm like the PageRank algorithm that we discussed on page 502) and work only with highly-ranked pages, not all pages. Another approach to cutting down on the size of a symbol table to support search with string keys is to associate URLs with <em>words</em> (substrings delimited by whitespace) as keys in the precomputed index. Then, when you search for a word, the search engine can use the index to find the (important) pages containing your search keys (words) and then use substring search within each page to find them. But with this approach, if the text were to contain <code>"everything"</code> and you were to search for <code>"thing"</code>, you would not find it. For some applications, it <em>is</em> worthwhile to build an index to help find <em>any substring</em> within a given text. Doing so might be justified for a linguistic study of an important piece of literature, for a genomic sequence that might be an object of study for many scientists, or just for a widely <a id="page_878"/>accessed web page. Again, ideally, the index would associate all possible substrings of the text string with each position where it occurs in the text string, as depicted at right. The basic problem with this ideal is that the number of possible substrings is too large to have a symbol-table entry for each of them (an <em>N</em>-character text has <em>N</em> (<em>N</em>−1)/2 substrings). The table for the example at right would need entries for <code>b</code>, <code>be</code>, <code>bes</code>, <code>best</code>, <code>best o</code>, <code>best of</code>, <code>e</code>, <code>es</code>, <code>est</code>, <code>est o</code>, <code>est of</code>, <code>s</code>, <code>st</code>, <code>st o</code>, <code>st of</code>, <code>t</code>, <code>t o</code>, <code>t of</code>, <code>o</code>, <code>of</code>, and many, many other substrings. Again, we can use a suffix sort to address this problem in a manner analogous to our first symbol-table implementation using binary search, in <a href="ch03.html#ch03sec1lev1"><small>SECTION 3.1</small></a>. We consider each of the <em>N</em> suffixes to be keys, create a sorted array of our keys (the suffixes), and use binary search to search in that array, comparing the search key with each suffix.</p>
<p class="image"><img alt="image" src="graphics/06_14-websearchoverview.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_15-textindexoverview.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_16-suffixexample.jpg"/></p>
<p><a id="ch06sec2lev22"/></p>
<h4><a id="page_879"/><em>API and client code</em></h4>
<p>To support client code to solve these two problems, we articulate the API shown below. It includes a constructor; a <code>length()</code> method; methods <code>select()</code> and <code>index()</code>, which give the string and index of the suffix of a given rank in the sorted list of suffixes; a method <code>lcp()</code> that gives the length of the longest common prefix of each suffix and the one preceding it in the sorted list; and a method <code>rank()</code> that gives the number of suffixes less than the given key (just as we have been using since we first examined binary search in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>). We use the term <em>suffix array</em> to describe the abstraction of a sorted list of suffix strings, without necessarily committing to use an array of strings as the underlying data structure.</p>
<p class="image"><img alt="image" src="graphics/t0879-01.jpg"/></p>
<p>In the example on the facing page, <code>select(9)</code> is <code>"as the best of times..."</code>, <code>index(9)</code> is <code>4</code>, <code>lcp(20)</code> is <code>10</code> because <code>"it was the best of times..."</code> and <code>"it was the"</code> have the common prefix <code>"it was the"</code> which is of length 10, and <code>rank("th")</code> is <code>30</code>. Note also that the <code>select(rank(key))</code> is the first possible suffix in the sorted suffix list that has <code>key</code> as prefix and that all other occurrences of <code>key</code> in the text immediately follow (see the figure on the opposite page). With this API, the client code on the next two pages is immediate. <code>LRS</code> (page <a href="#page_880">880</a>) finds the longest repeated substring in the text on standard input by building a suffix array and then scanning through the sorted suffixes to find the maximum <code>lcp()</code> value. <code>KWIC</code> (page <a href="#page_881">881</a>) builds a suffix array for the text named as command-line argument, takes queries from standard input, and prints all occurrences of each query in the text (including a specified number of characters before and after to give context). The name KWIC stands for <em>keyword-in-context</em> search, a term dating at least to the 1960s. The simplicity and efficiency of this client code for these typical string-processing applications is remarkable, and testimony to the importance of careful API design (and the power of a simple but ingenious idea).</p>
<p class="image"><a id="page_880"/><img alt="image" src="graphics/p0880-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0880-02.jpg"/></p>
<p class="image"><a id="page_881"/><img alt="image" src="graphics/p0881-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0881-02.jpg"/></p>
<p><a id="ch06sec2lev23"/></p>
<h4><a id="page_882"/><em>Implementation</em></h4>
<p>The code on the facing page is a straightforward implementation of the <code>SuffixArray</code> API. Its instance variables are an array of strings and (for economy in code) a variable <code>N</code> that holds the length of the array (the length of the string and its number of suffixes). The constructor builds the suffix array and sorts it, so <code>select(i)</code> just returns <code>suffixes[i]</code>. The implementation of <code>index()</code> is also a one-liner, but it is a bit tricky, based on the observation that <em>the length of the suffix string uniquely determines its starting point</em>. The suffix of length <code>N</code> starts at position <code>0</code>, the suffix of length <code>N-1</code> starts at position <code>1</code>, the suufix of length <code>N-2</code> starts at position <code>2</code>, and so forth, so <code>index(i)</code> just returns <code>N - suffixes[i].length()</code>. The implementation of <code>lcp()</code> is immediate, given the static method <code>lcp()</code> on page <a href="#page_875">875</a>, and <code>rank()</code> is virtually the same as our implementation of binary search for symbol tables, on page <a href="ch03.html#page_381">381</a>. Again, the simplicity and elegance of this implementation should not mask the fact that it is a sophisticated algorithm that enables solution of important problems like the longest repeated substring problem that would otherwise seem to be infeasible.</p>
<p><a id="ch06sec2lev24"/></p>
<h4><em>Performance</em></h4>
<p>The efficiency of suffix sorting depends on the fact that Java substring extraction uses a constant amount of space—each substring is composed of standard object overhead, a pointer into the original, and a length. Thus, the size of the index is linear in the size of the string. This point is a bit counterintuitive because the total number of characters in the suffixes is ~<em>N</em><sup>2</sup>/2, a quadratic function of the size of the string. Moreover, that quadratic factor gives one pause when considering the cost of sorting the suffix array. It is very important to bear in mind that this approach is effective for long strings because of the Java representation for strings: when we exchange two strings, we are exchanging only references, not the whole string. Now, the cost of <em>comparing</em> two strings may be proportional to the length of the strings in the case when their common prefix is very long, but most comparisons in typical applications involve only a few characters. If so, the running time of the suffix sort is linearithmic. For example, in many applications, it is reasonable to use a random string model:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb08"/></p>
<p><strong>Proposition C.</strong> Using 3-way string quicksort, we can build a suffix array from a random string of length <em>N</em> with space proportional to <em>N</em> and <em>~</em> 2<em>N</em> ln <em>N</em> character compares, on the average.</p>
<p><strong>Discussion:</strong> The space bound is immediate, but the time bound is follows from a a detailed and difficult research result by P. Jacquet and W. Szpankowski, which implies that the cost of sorting the suffixes is asymptotically the same as the cost of sorting <em>N</em> random strings (see <a href="#ch06sb15"><small>PROPOSITION E</small></a> on page <a href="#ch06sb15">723</a>).</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch06sb09"/></p>
<h3><a id="page_883"/>Algorithm 6.13 Suffix array (elementary implementation)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0883-01.jpg"/></p>
<p>This implementation of our <code>SuffixArray</code> API depends for its efficiency on the fact that substrings are constant-size references and substring extraction takes constant time (see text).</p>
<hr/>
</div>
<p><a id="ch06sec2lev25"/></p>
<h4><a id="page_884"/><em>Improved implementations</em></h4>
<p>Our elementary implementation of <code>SuffixArray</code> has poor worst-case performance. For example, if all the characters are equal, the sort examines every character in each substring and thus takes <em>quadratic</em> time. For strings of the type we have been using as examples, such as genomic sequences or natural-language text, this is not likely to be problematic, but the algorithm can be slow for texts with long runs of identical characters. Another way of looking at the problem is to observe that the cost of finding the longest repeated substring is <em>quadratic in the length of the substring</em> because all of the prefixes of the repeat need to be checked (see the diagram at right). This is not a problem for a text such as <em>A Tale of Two Cities</em>, where the longest repeat</p>
<p class="programlisting"><img alt="image" src="graphics/p0884-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_17-lrsquadratic.jpg"/></p>
<p>has just 84 characters, but it is a serious problem for genomic data, where long repeated substrings are not unusual. How can this quadratic behavior for repeat searching be avoided? Remarkably, research by P. Weiner in 1973 showed that <em>it is possible to solve the longest repeated substring problem in guaranteed linear time</em>. Weiner’s algorithm was based on building a suffix tree data structure (essentially a trie for suffixes). With multiple pointers per character, suffix trees consume too much space for many practical problems, which led to the development of suffix arrays. In the 1990s, U. Manber and E. Myers presented a linearithmic algorithm for building suffix arrays directly and a method that does preprocessing at the same time as the suffix sort to support <em>constant-time</em> <code>lcp()</code>. Several linear-time suffix sorting algorithms have been developed since. With a bit more work, the Manber-Myers implementation can also support a two-argument <code>lcp()</code> that finds the longest common prefix of two given suffixes that are not necessarily adjacent in guaranteed constant time, again a remarkable improvement over the straightforard implementation. These results are quite surprising, as they achieve efficiencies quite beyond what you might have expected.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb10"/></p>
<p><a id="page_885"/><strong>Proposition D.</strong> With suffix arrays, we can solve both the suffix sorting and longest repeated substring problems in linear time.</p>
<p><strong>Proof:</strong> The remarkable algorithms for these tasks are just beyond our scope, but you can find on the booksite code that implements the <code>SuffixArray</code> constructor in linear time and <code>lcp()</code> queries in constant time.</p>
<hr/>
</div>
<p>A <code>SuffixArray</code> implementation based on these ideas supports efficient solutions of numerous string-processing problems, with simple client code, as in our <code>LRS</code> and <code>KWIC</code> examples.</p>
<p><small>SUFFIX ARRAYS ARE THE CULMINATION</small> of decades of research that began with the development of tries for KWIC indices in the 1960s. The algorithms that we have discussed were worked out by many researchers over several decades in the context of solving practical problems ranging from putting the <em>Oxford English Dictionary</em> online to the development of the first web search engines to sequencing the human genome. This story certainly helps put the importance of algorithm design and analysis in context.</p>
<p><a id="ch06sec1lev8"/></p>
<h3><a id="page_886"/>Network-flow algorithms</h3>
<p>Next, we consider a graph model that has been successful not just because it provides us with a simply stated problem-solving model that is useful in many practical applications but also because we have efficient algorithms for solving problems within the model. The solution that we consider illustrates the tension between our quest for implementations of general applicability and our quest for efficient solutions to specific problems. The study of network-flow algorithms is fascinating because it brings us tantalizingly close to compact and elegant implementations that achieve both goals. As you will see, we have straightforward implementations that are guaranteed to run in time proportional to a polynomial in the size of the network.</p>
<p class="image"><img alt="image" src="graphics/06_18-fnswitchintro.jpg"/></p>
<p>The classical solutions to network-flow problems are closely related to other graph algorithms that we studied in <a href="ch04.html#ch04"><small>CHAPTER 4</small></a>, and we can write surprisingly concise programs that solve them, using the algorithmic tools we have developed. As we have seen in many other situations, good algorithms and data structures can lead to substantial reductions in running times. Development of better implementations and better algorithms is still an area of active research, and new approaches continue to be discovered.</p>
<p><a id="ch06sec2lev26"/></p>
<h4><em>A physical model</em></h4>
<p>We begin with an idealized physical model in which several of the basic concepts are intuitive. Specifically, imagine a collection of interconnected oil pipes of varying sizes, with switches controlling the direction of flow at junctions, as in the example illustrated at right. Suppose further that the network has a single <em>source</em> (say, an oil field) and a single <em>sink</em> (say, a large refinery) to which all the pipes ultimately connect. At each vertex, the flowing oil reaches an equilibrium where the amount of oil flowing in is equal to the amount flowing out. We measure both flow and pipe capacity in the same units (say, gallons per second). If every switch has the property that the total capacity of the ingoing pipes is equal to the total capacity of the outgoing pipes, then there is no problem to solve: we <a id="page_887"/>simply fill all pipes to full capacity. Otherwise, not all pipes are full, but oil flows through the network, controlled by switch settings at the junctions, satisfying a <em>local equilibrium</em> condition at the junctions: the amount of oil flowing into each junction is equal to the amount of oil flowing out. For example, consider the network in the diagram on the opposite page. Operators might start the flow by opening the switches along the path <code>0-&gt;1-&gt;3-&gt;5</code>, which can handle 2 units of flow, then open switches along the path <code>0-&gt;2-&gt;4-&gt;5</code> to get another unit of flow in the network. Since <code>0-&gt;1, 2-&gt;4</code>, and <code>3-&gt;5</code> are full, there is no direct way to get more flow from <code>0</code> to <code>5</code>, but if we change the switch at <code>1</code> to redirect enough flow to fill <code>1-&gt;4</code>, we open up enough capacity in <code>3-&gt;5</code> to allow us to add a unit of flow on <code>0-&gt;2-&gt;3-&gt;5</code>. Even for this simple network, finding switch settings that increase the flow is not an easy task; for a complicated network, we are clearly interested in the following question: What switch settings will maximize the amount of oil flowing from source to sink? We can model this situation directly with an edge-weighted digraph that has a single source and a single sink. The edges in the network correspond to the oil pipes, the vertices correspond to the junctions with switches that control how much oil goes into each outgoing edge, and the weights on the edges correspond to the capacity of the pipes. We assume that the edges are directed, specifying that oil can flow in only one direction in each pipe. Each pipe has a certain amount of flow, which is less than or equal to its capacity, and every vertex satisfies the equilibrium condition that the flow in is equal to the flow out. This flow-network abstraction is a useful problem-solving model that applies directly to a variety of applications and indirectly to still more. We sometimes appeal to the idea of oil flowing through pipes for intuitive support of basic ideas, <a id="page_888"/>but our discussion applies equally well to goods moving through distribution channels and to numerous other situations. As with our use of distance in shortest-paths algorithms, we are free to abandon any physical intuition when convenient because all the definitions, properties, and algorithms that we consider are based entirely on an abstract model that does not necessarily obey physical laws. Indeed, a prime reason for our interest in the network-flow model is that it allows us to solve numerous other problems through reduction, as we see in the next section.</p>
<p class="image"><img alt="image" src="graphics/06_19-local.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_19-anatomyfn.jpg"/></p>
<p><a id="ch06sec2lev27"/></p>
<h4><em>Definitions</em></h4>
<p>Because of this broad applicability, it is worthwhile to consider precise statements of the terms and concepts that we have just informally introduced:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb11"/></p>
<p><strong>Definition.</strong> A <em>flow network</em> is an edge-weighted digraph with positive edge weights (which we refer to as <em>capacities</em>). An <em>st</em>-<em>flow network</em> has two identified vertices, a source <em>s</em> and a sink <em>t</em>.</p>
<hr/>
</div>
<p>We sometimes refer to edges as having infinite capacity or, equivalently, as being uncapacitated. That might mean that we do not compare flow against capacity for such edges, or we might use a sentinel value that is guaranteed to be larger than any flow value. We refer to the total flow into a vertex (the sum of the flows on its incoming edges) as the vertex’s <em>inflow,</em> the total flow out of a vertex (the sum of the flows on its outgoing edges) as the vertex’s <em>outflow,</em> and the difference between the two (inflow minus outflow) as the vertex’s <em>netflow.</em> To simplify the discussion, we also assume that there are no edges leaving <code>t</code> or entering <code>s</code>.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb12"/></p>
<p><strong>Definition.</strong> An <em>st</em>-<em>flow</em> in an <em>st</em>-flow network is a set of nonnegative values associated with each edge, which we refer to as <em>edge flows.</em> We say that a flow is <em>feasible</em> if it satisfies the condition that no edge’s flow is greater than that edge’s capacity and the local equilibrium condition that the every vertex’s netflow is zero (except <em>s</em> and <em>t</em>).</p>
<hr/>
</div>
<p>We refer to the sink’s inflow as the <code>st</code>-flow <em>value.</em> We will see in <a href="#ch06sb08"><small>PROPOSITION C</small></a> that the value is also equal to the source’s outflow. With these definitions, the formal statement of our basic problem is straightforward:</p>
<p class="indenthanging"><strong><em>Maximum st-flow.</em></strong> Given an <em>st</em>-flow network, find an <em>st</em>-flow such that no other flow from <em>s</em> to <em>t</em> has a larger value.</p>
<p>For brevity, we refer to such a flow as a <em>maxflow</em> and the problem of finding one in a network as the <em>maxflow problem</em>. In some applications, we might be content to know <a id="page_889"/>just the maxflow value, but we generally want to know a flow (edge flow values) that achieves that value.</p>
<p><a id="ch06sec2lev28"/></p>
<h4><em>APIs</em></h4>
<p>The <code>FlowEdge</code> and <code>FlowNetwork</code> APIs shown on page <a href="#page_890">890</a> are straightforward extensions of APIs from <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>. We will consider on page <a href="#ch06sb21">896</a> an implementation of <code>FlowEdge</code> that is based on adding an instance variable containing the flow to our <code>WeightedEdge</code> class from page <a href="ch04.html#ch04sb38">610</a>. Flows have a direction, but we do not base <code>FlowEdge</code> on <code>WeightedDirectedEdge</code> because we work with a more general abstraction known as the <em>residual network</em> that is described below, and we need each edge to appear in the adjacency lists of both its vertices to implement the residual network. The residual network allows us to both add and subtract flow and to test whether an edge is full to capacity (no more flow can be added) or empty (no flow can be subtracted). This abstraction is implemented via the methods <code>residualCapacity()</code> and <code>addResidualFlow()</code> that we will consider later. The implementation of <code>FlowNetwork</code> is virtually identical to our <code>EdgeWeightedGraph</code> implementation on page <a href="ch04.html#ch04sb39">611</a>, so we omit it. To simplify the file format, we adopt the convention that the source is 0 and the sink is <em>V</em> − 1. These APIs leave a straightforward goal for maxflow algorithms: build a network, then assign values to the flow instance variables in the client’s edges that maximize flow through the network. Shown at right are client methods for certifying whether a flow is feasible. Typically, we might do such a check as the final action of a maxflow algorithm.</p>
<p class="image"><img alt="image" src="graphics/p0889-01.jpg"/></p>
<p class="image"><a id="page_890"/><img alt="image" src="graphics/t0890-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/06_20-weightednetrep.jpg"/></p>
<p><a id="ch06sec2lev29"/></p>
<h4><a id="page_891"/><em>Ford-Fulkerson algorithm</em></h4>
<p>An effective approach to solving max-flow problems was developed by L. R. Ford and D. R. Fulkerson in 1962. It is a generic method for increasing flows incrementally along paths from source to sink that serves as the basis for a family of algorithms. It is known as the <em>Ford-Fulkerson algorithm</em> in the classical literature; the more descriptive term <em>augmenting-path algorithm</em> is also widely used. Consider any directed path from source to sink through an <em>st</em>-flow network. Let <em>x</em> be the minimum of the unused capacities of the edges on the path. We can increase the network’s flow value by at least <em>x</em> by increasing the flow in all edges on the path by that amount. Iterating this action, we get a first attempt at computing flow in a network: find another path, increase the flow along that path, and continue until all paths from source to sink have at least one full edge (so that we can no longer increase flow in this way). This algorithm will compute the maxflow in some cases but will fall short in other cases. Our introductory example on page <a href="#page_886">886</a> is such an example. To improve the algorithm such that it always finds a maxflow, we consider a more general way to increase the flow, along a path from source to sink through the network’s underlying <em>undirected</em> graph. The edges on any such path are either <em>forward</em> edges, which go with the flow (when we traverse the path from source to sink, we traverse the edge from its source vertex to its destination vertex), or <em>backward</em> edges, which go against the flow (when we traverse the path from source to sink, we traverse the edge from its destination vertex to its source vertex). Now, for any path from source to sink with no full forward edges and no empty backward edges, we can increase the amount of flow in the network by increasing flow in forward edges and decreasing flow in backward edges. The amount by which the flow can be increased is limited by the minimum of the unused capacities in the forward edges and the flows in the backward edges. Such a path is called an <em>augmenting path</em>. An example is shown at right. In the new flow, at least one of the forward edges along the path becomes full or at least one of the backward edges along the path becomes empty. The process just sketched is the basis for the classical Ford-Fulkerson maxflow algorithm (augmenting-path method). We summarize it as follows:</p>
<p class="image"><img alt="image" src="graphics/06_21-fnaugment.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb13"/></p>
<p><a id="page_892"/><strong>Ford-Fulkerson maxflow algorithm.</strong> Start with zero flow everywhere. Increase the flow along any augmenting path from source to sink (with no full forward edges or empty backward edges), continuing until there are no such paths in the network.</p>
<hr/>
</div>
<p>Remarkably (under certain technical conditions about numeric properties of the flow), this method always finds a maxflow, no matter how we choose the paths. Like the greedy MST algorithm discussed in <a href="ch04.html#ch04sec1lev12"><small>SECTION 4.3</small></a> and the generic shortest-paths method discussed in <a href="ch04a.html#ch04sec1lev13"><small>SECTION 4.4</small></a>, it is a generic algorithm that is useful because it establishes the correctness of a whole family of more specific algorithms. We are free to use any method whatever to choose the path. Several algorithms that compute sequences of augmenting paths have been developed, all of which lead to a maxflow. The algorithms differ in the number of augmenting paths they compute and the costs of finding each path, but they all implement the Ford-Fulkerson algorithm and find a maxflow.</p>
<p><a id="ch06sec2lev30"/></p>
<h4><em>Maxflow-mincut theorem</em></h4>
<p>To show that any flow computed by any implementation of the Ford-Fulkerson algorithm is indeed a maxflow, we prove a key fact known as the <em>maxflow-mincut theorem</em>. Understanding this theorem is a crucial step in understanding network-flow algorithms. As suggested by its name, the theorem is based on a direct relationship between flows and cuts in networks, so we begin by defining terms that relate to cuts. Recall from <a href="ch04.html#ch04sec1lev12"><small>SECTION 4.3</small></a> that a <em>cut</em> in a graph is a partition of the vertices into two disjoint sets, and a crossing edge is an edge that connects a vertex in one set to a vertex in the other set. For flow networks, we refine these definitions as follows:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb14"/></p>
<p><strong>Definition.</strong> An <em>st-cut</em> is a cut that places vertex <em>s</em> in one of its sets and vertex <em>t</em> in the other.</p>
<hr/>
</div>
<p>Each crossing edge corresponding to an <em>st</em>-cut is either an <em>st-edge</em> that goes from a vertex in the set containing <em>s</em> to a vertex in the set containing <em>t</em>, or a <em>ts-edge</em> that goes in the other direction. We sometimes refer to the set of crossing <em>st</em>-edges as a <em>cut set</em>. The <em>capacity</em> of an <em>st</em>-cut in a flow network is the sum of the capacities of that cut’s <em>st</em>-edges, and the <em>flow across</em> an <em>st</em>-cut is the difference between the sum of the flows in that cut’s <em>st</em>-edges and the sum of the flows in that cut’s <em>ts</em>-edges. Removing all the <em>st</em>-edges (the cut set) in an <em>st</em>-cut of a network leaves no path from <em>s</em> to <em>t</em>, but adding any one of them back could create such a path. Cuts are the appropriate abstraction for many applications. For our oil-flow model, a cut provides a way to completely stop the flow of oil <a id="page_893"/>from the source to the sink. If we view the capacity of the cut as the cost of doing so, to stop the flow in the most economical manner is to solve the following problem:</p>
<p class="indenthanging"><strong><em>Minimum st-cut.</em></strong> Given an <em>st</em>-network, find an <em>st</em>-cut such that the capacity of no other cut is smaller. For brevity, we refer to such a cut as a <em>mincut</em> and to the problem of finding one in a network as the <em>mincut problem</em>.</p>
<p>The statement of the mincut problem includes no mention of flows, and these definitions might seem to digress from our discussion of the augmenting-path algorithm. On the surface, computing a mincut (a set of edges) seems easier than computing a maxflow (an assignment of weights to all the edges). On the contrary, the maxflow and mincut problems are intimately related. The augmenting-path method itself provides a proof. That proof rests on the following basic relationship between flows and cuts, which immediately gives a proof that local equilibrium in an <em>st</em>-flow implies global equilibrium as well (the first corollary) and an upper bound on the value of any <em>st</em>-flow (the second corollary):</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb15"/></p>
<p><strong>Proposition E.</strong> For any <em>st</em>-flow, the flow across each <em>st</em>-cut is equal to the value of the flow.</p>
<p><strong>Proof:</strong> Let <em>C<sub>s</sub></em> be the vertex set containing <em>s</em> and <em>C<sub>t</sub></em> the vertex set containing <em>t</em>. This fact follows immediately by induction on the size of <em>C<sub>t</sub></em>. The property is true by definition when <em>C<sub>t</sub></em> is <em>t</em> and when a vertex is moved from <em>C<sub>s</sub></em> to <em>C<sub>t</sub></em>, local equilibrium at that vertex implies that the stated property is preserved. Any <em>st</em>-cut can be created by moving vertices in this way.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/06_22-stcut.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb16"/></p>
<p><strong>Corollary.</strong> The outflow from <em>s</em> is equal to the inflow to <em>t</em> (the value of the <em>st</em>-flow).</p>
<p><strong>Proof:</strong> Let <em>C<sub>s</sub></em> be { <em>s</em> }.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb17"/></p>
<p><strong>Corollary.</strong> No <em>st</em>-flow’s value can exceed the capacity of any <em>st</em>-cut.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb18"/></p>
<p><a id="page_894"/><strong>Proposition F. (Maxflow-mincut theorem)</strong> Let <em>f</em> be an <em>st</em>-flow. The following three conditions are equivalent:</p>
<p class="indenthangingN1"><em>i.</em> There exists an <em>st</em>-cut whose capacity equals the value of the flow <em>f</em>.</p>
<p class="indenthangingN1"><em>ii. f</em> is a maxflow.</p>
<p class="indenthangingN1"><em>iii.</em> There is no augmenting path with respect to <em>f</em>.</p>
<p><strong>Proof:</strong> Condition <em>i.</em> implies condition <em>ii.</em> by the corollary to <a href="#ch06sb15"><small>PROPOSITION E</small></a>. Condition <em>ii.</em> implies condition <em>iii.</em> because the existence of an augmenting path implies the existence of a flow with a larger flow value, contradicting the maximality of <em>f</em>.</p>
<p>It remains to prove that condition <em>iii.</em> implies condition <em>i.</em> Let <em>C<sub>s</sub></em> be the set of all vertices that can be reached from <em>s</em> with an undirected path that does not contain a full forward or empty backward edge, and let <em>C<sub>t</sub></em> be the remaining vertices. Then, <em>t</em> must be in <em>C<sub>t</sub></em>, so (<em>C<sub>s</sub></em>, <em>C<sub>t</sub></em>) is an <em>st</em>-cut, whose cut set consists entirely of full forward or empty backward edges. The flow across this cut is equal to the cut’s capacity (since forward edges are full and the backward edges are empty) and also to the value of the flow (by <a href="#ch06sb15"><small>PROPOSITION E</small></a>).</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb19"/></p>
<p><strong>Corollary. (Integrality property)</strong> When capacities are integers, there exists an integer-valued maxflow, and the Ford-Fulkerson algorithm finds it.</p>
<p><strong>Proof:</strong> Each augmenting path increases the flow by a positive integer (the minimum of the unused capacities in the forward edges and the flows in the backward edges, all of which are always positive integers).</p>
<hr/>
</div>
<p>It is possible to design a maxflow with noninteger flows, even when capacities are all integers, but we do not need to consider such flows. From a theoretical standpoint, this observation is important: allowing capacities and flows that are real numbers, as we have done and as is common in practice, can lead to unpleasant anomalous situations. For example, it is known that the Ford-Fulkerson algorithm could, in principle, lead to an infinite sequence of augmenting paths that does not even converge to the maxflow value. The version of the algorithm that we consider is known to always converge, even when capacities and flows are real-valued. No matter what method we choose to find an augmenting path and no matter what paths we find, we always end up with a flow that does not admit an augmenting path, which therefore must be a maxflow.</p>
<p><a id="ch06sec2lev31"/></p>
<h4><a id="page_895"/><em>Residual network</em></h4>
<p>The generic Ford-Fulkerson algorithm does not specify any particular method for finding an augmenting path. How can we find a path with no full forward edges and no empty backward edges? To this end, we begin with the following definition:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb20"/></p>
<p><strong>Definition.</strong> Given a <em>st-</em> flow network and an <em>st-</em> flow, the <em>residual network</em> for the flow has the same vertices as the original and one or two edges in the residual network for each edge in the original, defined as follows: For each edge <em>e</em> from <code>v</code> to <code>w</code> in the original, let <em>f<sub>e</sub></em> be its flow and <em>c<sub>e</sub></em> its capacity. If <em>f<sub>e</sub></em> is positive, include an edge <code>w-&gt;v</code> in the residual with capacity <em>f<sub>e</sub>;</em> and if <em>f<sub>e</sub></em> is less than <em>c<sub>e</sub>,</em> include an edge <code>v-&gt;w</code> in the residual with capacity <em>c<sub>e</sub></em> − <em>f<sub>e</sub>.</em></p>
<hr/>
</div>
<p>If an edge <em>e</em> from <code>v</code> to <code>w</code> is empty (<em>f<sub>e</sub></em> is equal to 0), there is a single corresponding edge <code>v-&gt;w</code> with capacity <em>c<sub>e</sub></em> in the residual; if it is full (<em>f<sub>e</sub></em> is equal to <em>c<sub>e</sub></em>), there is a single corresponding edge <code>w-&gt;v</code> with capacity <em>f<sub>e</sub></em> in the residual; and if it is neither empty nor full, both <code>v-&gt;w</code> and <code>w-&gt;v</code> are in the residual with their respective capacities. An example is shown at the bottom of this page. At first, the residual network representation is a bit confusing because the edges corresponding to flow go in the <em>opposite</em> direction of the flow itself. The forward edges represent the remaining capacity (the amount of flow we can add if traversing that edge); the backward edges represent the flow (the amount of flow we can remove if traversing that edge). The code on page <a href="#ch06sb21">896</a> gives the methods in the <code>FlowEdge</code> class that we need to implement the residual network abstraction. With these implementations, our algorithms work with the residual network, but they are actually examining capacities and changing flow (through edge references) in the client’s edges. The methods <code>from()</code> and <code>other()</code> allow us to process edges in either orientation:</p>
<p class="image"><img alt="image" src="graphics/06_23-anatomyfnrevisit.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch06sb21"/></p>
<h3><a id="page_896"/>Flow edge data type (residual network)</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0896-01.jpg"/></p>
<p>This <code>FlowEdge</code> implementation adds to the weighted <code>DirectedEdge</code> implementation of <a href="ch04a.html#ch04sec1lev13"><small>SECTION 4.4</small></a> (see page <a href="ch04a.html#ch04sec1lev13">642</a>) a <code>flow</code> instance variable and two methods to implement the residual flow network.</p>
<hr/>
</div>
<p><a id="page_897"/><code>e.other(v)</code> returns the endpoint of <code>e</code> that is not <code>v</code>. The methods <code>residualCapTo()</code> and <code>addResidualFlowTo()</code> implement the residual network. Residual networks allow us to use graph search to find an augmenting path, since any path from source to sink in the residual network corresponds directly to an augmenting path in the original network. Increasing the flow along the path implies making changes in the residual network: for example, at least one edge on the path becomes full or empty, so at least one edge in the residual network changes direction or disappears (but our use of an abstract residual network means that we just check for positive capacity and do not need to actually insert and delete edges).</p>
<p><a id="ch06sec2lev32"/></p>
<h4><em>Shortest-augmenting-path method</em></h4>
<p>Perhaps the simplest Ford-Fulkerson implementation is to use a <em>shortest</em> augmenting path (as measured by the number of edges on the path, not flow or capacity). This method was suggested by J. Edmonds and R.Karp in 1972. In this case, the search for an augmenting path amounts to breadth-first search (BFS) in the residual network, precisely as described in <a href="ch04.html#ch04sec1lev10"><small>SECTION 4.1</small></a>, as you can see by comparing the <code>hasAugmentingPath()</code> implementation below to our breadth-first search implemention in <a href="ch04.html#ch04sb10"><small>ALGORITHM 4.2</small></a> on page <a href="ch04.html#ch04sb10">540</a> (the residual graph is a digraph, and this is fundamentally a digraph processing algorithm, as mentioned on page <a href="ch04a.html#ch04sec2lev44">685</a>). This method forms the basis for the full implementation in <a href="#ch06sb22"><small>ALGORITHM 6.14</small></a> on the next page, a remarkably concise implementation based on the tools we have developed. For brevity, we refer to this method as the <em>shortest-augmenting-path</em> maxflow algorithm. A trace for our example is shown in detail on page <a href="#page_899">899</a>.</p>
<p class="image"><img alt="image" src="graphics/p0897-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch06sb22"/></p>
<h3><a id="page_898"/>Algorithm 6.14 Ford-Fulkerson shortest-augmenting path maxflow algorithm</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0898-01.jpg"/></p>
<p>This implementation of the Ford-Fulkerson algorithm finds the shortest augmenting path in the residual network, finds the bottneck capacity in that path, and augments the flow along that path, continuing until no path from source to sink exists.</p>
<hr/>
</div>
<p class="image"><a id="page_899"/><img alt="image" src="graphics/06_24-fnaugmentingpath.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0899-01.jpg"/></p>
<p class="image"><a id="page_900"/><img alt="image" src="graphics/06_25-fnbig.jpg"/></p>
<p><a id="ch06sec2lev33"/></p>
<h4><em>Performance</em></h4>
<p>A larger example is shown in the figure above. As is evident from the figure, the lengths of the augmenting paths form a nondecreasing sequence. This fact is a first key to analyzing the performance of the algorithm.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb23"/></p>
<p><strong>Proposition G.</strong> The number of augmenting paths needed in the shortest-augmenting-path implementation of the Ford-Fulkerson maxflow algorithm for a flow network with <em>V</em> vertices and <em>E</em> edges is at most <em>EV</em>/2.</p>
<p><strong>Proof sketch:</strong> Every augmenting path has a <em>critical edge</em>—an edge that is deleted from the residual network because it corresponds either to a forward edge that becomes filled to capacity or a backward edge that is emptied. Each time an edge is a critical edge, the length of the augmenting path through it must increase by 2 (see <a href="#ch06qa1q39"><small>EXERCISE 6.39</small></a>). Since an augmenting path is of length at most <em>V</em> each edge can be on at most <em>V</em>/2 augmenting paths, and the total number of augmenting paths is at most <em>EV</em>/2.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb24"/></p>
<p><a id="page_901"/><strong>Corollary.</strong> The shortest-augmenting-path implementation of the Ford-Fulkerson maxflow algorithm takes time proportional to <em>VE</em><sup>2</sup> in the worst case.</p>
<p><strong>Proof:</strong> Breadth-first search examines at most <em>E</em> edges.</p>
<hr/>
</div>
<p>The upper bound of <a href="#ch06sb23"><small>PROPOSITION G</small></a> is very conservative. For example, the graph shown in the figure at the top of page <a href="#page_900">900</a> has 11 vertices and 20 vertices, so the bound says that the algorithm uses no more than 110 augmenting paths. In fact, it uses 14.</p>
<p><a id="ch06sec2lev34"/></p>
<h4><em>Other implementations</em></h4>
<p>Another Ford-Fulkerson implementation, suggested by Edmonds and Karp, is the following: Augment along the path that increases the flow by the largest amount. For brevity, we refer to this method as the <em>maximum-capacity-augmenting-path</em> maxflow algorithm. We can implement this (and other approaches) by using a priority queue and slightly modifying our implementation of Dijkstra’s shortest-paths algorithm, choosing edges from the priority queue to give the maximum amount of flow that can be pushed through a forward edge or diverted from a backward edge. Or, we might look for a longest augmenting path, or make a random choice. A complete analysis establishing which method is best is a complex task, because their running times depend on</p>
<p class="indenthangingB">• The number of augmenting paths needed to find a maxflow</p>
<p class="indenthangingB">• The time needed to find each augmenting path</p>
<p>These quantities can vary widely, depending on the network being processed and on the graph-search strategy. Several other approaches to solving the maxflow problem have also been devised, some of which compete well with the Ford-Fulkerson algorithm in practice. Developing a mathematical model of maxflow algorithms that can validate such hypotheses, however, is a significant challenge. The analysis of maxflow algorithms remains an interesting and active area of research. From a theoretical standpoint, worst-case performance bounds for numerous maxflow algorithms have been developed, but the bounds are generally substantially higher than the actual costs observed in applications and also quite a bit higher than the trivial (linear-time) lower bound. This gap between what is known and what is possible is larger than for any other problem that we have considered (so far) in this book.</p>
<h4><a id="page_902"/><em>Computing a minimum st-cut.</em></h4>
<p>Remarkably, the Ford-Fulkerson algorithm computes not only a maximum <em>st</em>-flow but also a minimum <em>st</em>-cut. The augmenting path algorithm terminates when there are no more augmenting paths with respect to the flow <em>f</em>. Upon termination, let <em>C<sub>s</sub></em> be the set of all vertices that can be reached from <em>s</em> with an undirected path that does not contain a full forward or empty backward edge, and let <em>C<sub>t</sub></em> be the remaining vertices. Then, as in the proof of <small>PROPOSITION F</small>, (<em>C<sub>s</sub> , C<sub>t</sub></em>) is a minimum <em>st</em>-cut. <small>ALGORITHM 6.14</small> provides an <code>inCut()</code> method that identifies the vertices on the <em>s</em>-side of the mincut. It accomplishes this by using the information left over in <code>marked[]</code> from the last call to <code>hasAugmentingPath()</code>.</p>
<p><small>THE PRACTICAL APPLICATION</small> of maxflow algorithms remains both an art and a science. The art lies in picking the strategy that is most effective for a given practical situation; the science lies in understanding the essential nature of the problem. Are there new data structures and algorithms that can solve the maxflow problem in linear time, or can we prove that none exist?</p>
<p class="image"><img alt="image" src="graphics/t0902-01.jpg"/></p>
<p><a id="ch06sec1lev9"/></p>
<h3><a id="page_903"/>Reduction</h3>
<p>Throughout this book, we have focused on articulating specific problems, then developing algorithms and data structures to solve them. In several cases (many of which are listed below), we have found it convenient to solve a problem by formulating it as an instance of another problem that we have already solved. Formalizing this notion is a worthwhile starting point for studying relationships among the diverse problems and algorithms that we have studied.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb25"/></p>
<p><strong>Definition.</strong> We say that a problem <em>A reduces to</em> another problem <em>B</em> if we can use an algorithm that solves <em>B</em> to develop an algorithm that solves <em>A</em>.</p>
<hr/>
</div>
<p>This concept is certainly a familiar one in software development: when you use a library method to solve a problem, you are reducing your problem to the one solved by the library method. In this book, we have informally referred to problems that we can reduce to a given problem as <em>applications</em>.</p>
<p><a id="ch06sec2lev35"/></p>
<h4><em>Sorting reductions</em></h4>
<p>We first encountered reduction in <a href="ch02.html#ch02"><small>CHAPTER 2</small></a>, to express the idea that an efficient sorting algorithm is useful for efficiently solving many other problems, that may not seem to be at all related to sorting. For example, we considered the following problems, among many others:</p>
<p class="indenthanging"><strong><em>Finding the median.</em></strong> Given a set of numbers, find the median value.</p>
<p class="indenthanging"><strong><em>Distinct values.</em></strong> Determine the number of distinct values in a set of numbers.</p>
<p class="indenthanging"><strong><em>Scheduling to minimize average completion time.</em></strong> Given a set of jobs of specified duration to be completed, how can we schedule the jobs on a single processor so as to minimize their average completion time?</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb26"/></p>
<p><strong>Proposition H.</strong> The following problems reduce to sorting:</p>
<p class="indenthangingB">• Finding the median</p>
<p class="indenthangingB">• Counting distinct values</p>
<p class="indenthangingB">• Scheduling to minimize average completion time</p>
<p><strong>Proof:</strong> See page <a href="ch02.html#page_345">345</a> and <a href="ch02.html#ch02qa5q12"><small>EXERCISE 2.5.12</small></a>.</p>
<hr/>
</div>
<p>Now, we have to pay attention to cost when doing a reduction. For example, we can find the median of a set of numbers in linear time, but using the reduction to sorting will <a id="page_904"/>end up costing linearithmic time. Even so, such extra cost might be acceptable, since we can use an exisiting sort implementation. Sorting is valuable for three reasons:</p>
<p class="indenthangingB">• It is useful in its own right.</p>
<p class="indenthangingB">• We have an efficient algorithms for solving it.</p>
<p class="indenthangingB">• Many problems reduce to it.</p>
<p>Generally, we refer to a problem with these properties as a <em>problem-solving model</em>. Like well-engineered software libraries, well-designed problem-solving models can greatly expand the universe of problems that we can efficiently address. One pitfall in focusing on problem-solving models is known as <em>Maslow’s hammer</em>, an idea widely attributed to A. Maslow in the 1960s: <em>If all you have is a hammer, everything seems to be a nail</em>. By focusing on a few problem-solving models, we may use them like Maslow’s hammer to solve every problem that comes along, depriving ourselves of the opportunity to discover better algorithms to solve the problem, or even new problem-solving models. While the models we consider are important, powerful, and broadly useful, it is also wise to consider other possibilities.</p>
<p><a id="ch06sec2lev36"/></p>
<h4><em>Shortest-paths reductions</em></h4>
<p>In <a href="ch04a.html#ch04sec1lev13"><small>SECTION 4.4</small></a>, we revisited the idea of reduction in the context of shortest-paths algorithms. We considered the following problems, among many others:</p>
<p class="indenthanging"><strong><em>Single-source shortest paths in undirected graphs.</em></strong> Given an edge-weighted <em>undirected</em> graph with nonnegative weights and a source vertex <code>s</code>, support queries of the form <em>Is there a path from</em> <code>s</code> <em>to a given target vertex</em> <code>v</code>? If so, find a <em>shortest</em> such path (one whose total weight is minimal).</p>
<p class="indenthanging"><strong><em>Parallel precedence-constrained scheduling.</em></strong> Given a set of jobs of specified duration to be completed, with precedence constraints that specify that certain jobs have to be completed before certain other jobs are begun, how can we schedule the jobs on identical processors (as many as needed) such that they are all completed in the minimum amount of time while still respecting the constraints?</p>
<p class="indenthanging"><strong><em>Arbitrage.</em></strong> Find an arbitrage opportunity in a given table of currency-conversion rates.</p>
<p>Again, the latter two problems do not seem to be directly related to shortest-paths problems, but we saw that shortest paths is an effective way to address them. These examples, while important, are merely indicative. A large number of important problems, too many to survey here, are known to reduce to shortest paths—it is an effective and important problem-solving model.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb27"/></p>
<p><a id="page_905"/><strong>Proposition I.</strong> The following problems reduce to shortest paths in weighted digraphs:</p>
<p class="indenthangingB">• Single-source shortest paths in undirected graphs with nonnegative weights</p>
<p class="indenthangingB">• Parallel precedence-constrained scheduling</p>
<p class="indenthangingB">• Arbitrage</p>
<p class="indenthangingB">• [many other problems]</p>
<p><strong>Proof examples:</strong> See page <a href="ch04a.html#ch04sb55">654</a>, page <a href="ch04a.html#ch04sb61">665</a>, and page <a href="ch04a.html#ch04sb71">680</a>.</p>
<hr/>
</div>
<p><a id="ch06sec2lev37"/></p>
<h4><em>Maxflow reductions</em></h4>
<p>Maxflow algorithms are also important in a broad context. We can remove various restrictions on the flow network and solve related flow problems; we can solve other network- and graph-processing problems; and we can solve problems that are not network problems at all. For example, consider the following problems.</p>
<p class="indenthanging"><strong><em>Job placement.</em></strong> A college’s job-placement office arranges interviews for a set of students with a set of companies; these interviews result in a set of job offers. Assuming that an interview followed by a job offer represents mutual interest in the student taking a job at the company, it is in everyone’s best interests to maximize the number of job placements. Is it possible to match every student with a job? What is the maximum number of jobs that can be filled?</p>
<p class="indenthanging"><strong><em>Product distribution.</em></strong> A company that manufactures a single product has factories, where the product is produced; distribution centers, where the product is stored temporarily; and retail outlets, where the product is sold. The company must distribute the product from factories through distribution centers to retail outlets on a regular basis, using distribution channels that have varying capacities. Is it possible to get the product from the warehouses to the retail outlets such that supply meets demand everywhere?</p>
<p class="indenthanging"><strong><em>Network reliability.</em></strong> A simplified model considers a computer network as consisting of a set of trunk lines that connect computers through switches such that there is the possibility of a switched path through trunk lines connecting any two given computers. What is the minimum number of trunk lines that can be cut to disconnect some pair of computers?</p>
<p>Again, these problems seem to be unrelated to one another and to flow networks, but they all reduce to maxflow.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb28"/></p>
<p><a id="page_906"/><strong>Proposition J.</strong> The following problems reduce to the maxflow problem:</p>
<p class="indenthangingB">• Job placement</p>
<p class="indenthangingB">• Product distribution</p>
<p class="indenthangingB">• Network reliability</p>
<p class="indenthangingB">• [many other problems]</p>
<p><strong>Proof example:</strong> We prove the first (which is known as the <em>maximum bipartite matching problem</em>) and leave the others for exercises. Given a job-placement problem, construct an instance of the maxflow problem by directing all edges from students to companies, adding a source vertex with edges directed to all the students and adding a sink vertex with edges directed from all the companies. Assign each edge capacity 1. Now, any integral solution to the maxflow problem for this network provides a solution to the corresponding bipartite matching problem (see the corollary to <a href="#ch06sb18"><small>PROPOSITION F</small></a>). The matching corresponds exactly to those edges between vertices in the two sets that are filled to capacity by the maxflow algorithm. First, the network flow always gives a legal matching: since each vertex has an edge of capacity 1 either coming in (from the sink) or going out (to the source), at most 1 unit of flow can go through each vertex, implying in turn that each vertex will be included at most once in the matching. Second, no matching can have more edges, since any such matching would lead directly to a better flow than that produced by the maxflow algorithm.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/06_26-bipartitenew.jpg"/></p>
<p><a id="page_907"/>For example, as illustrated in the figure at right, an augmenting-path max-flow algorithm might use the paths <code>s-&gt;1-&gt;7-&gt;t</code>, <code>s-&gt;2-&gt;8-&gt;t</code>, <code>s-&gt;3-&gt;9-&gt;t</code>, <code>s-&gt;5-&gt;10-&gt;t</code>, <code>s-&gt;6-&gt;11-&gt;t</code>, and <code>s-&gt;4-&gt;7-&gt;1-&gt;8-&gt;2-&gt;12-&gt;t</code> to compute the matching <code>1-8</code>, <code>2-12</code>, <code>3-9</code>, <code>4-7</code>, <code>5-10</code>, and <code>6-11</code>. Thus, there is a way to match all the students to jobs in our example. Each augmenting path fills one edge from the source and one edge into the sink. Note that these edges are never used as back edges, so there are at most <em>V</em> augmenting paths. and a total running time proportional to <em>VE</em>.</p>
<p class="image"><img alt="image" src="graphics/06_27-bipartiteaug.jpg"/></p>
<p><small>SHORTEST PATHS AND MAXFLOW ARE IMPORTANT</small> problem-solving models because they have the same properties that we articulated for sorting:</p>
<p class="indenthangingB">• They are useful in their own right.</p>
<p class="indenthangingB">• We have efficient algorithms for solving them.</p>
<p class="indenthangingB">• Many problems reduce to them.</p>
<p>This short discussion serves only to introduce the idea. If you take a course in operations research, you will learn many other problems that reduce to these and many other problem-solving models.</p>
<p><a id="ch06sec2lev38"/></p>
<h4><em>Linear programming</em></h4>
<p>One of the cornerstones of operations research is <em>linear programming</em> (LP). It refers to the idea of reducing a given problem to the following mathematical formulation:</p>
<p class="indenthanging"><strong><em>Linear programming.</em></strong> Given a set of <em>M linear inequalities</em> and linear equations involving <em>N</em> variables, and a linear <em>objective function</em> of the <em>N</em> variables, find an assignment of values to the variables that maximizes the objective function, or report that no feasible assignment exists.</p>
<p class="image"><img alt="image" src="graphics/t0907-01.jpg"/></p>
<p>Linear programming is an extremely important problem-solving model because</p>
<p class="indenthangingB">• A great many important problems reduce to linear programming</p>
<p class="indenthangingB">• We have efficient algorithms for solving linear-programming problems</p>
<p>The “useful in its own right” phrase is not needed in this litany that we have stated for other problem-solving models because <em>so many</em> practical problems reduce to linear programming.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb29"/></p>
<p><a id="page_908"/><strong>Proposition K.</strong> The following problems reduce to linear programming</p>
<p class="indenthangingB">• Maxflow</p>
<p class="indenthangingB">• Shortest paths</p>
<p class="indenthangingB">• [many, many other problems]</p>
<p><strong>Proof example:</strong> We prove the first and leave the second to <a href="#ch06qa1q49"><small>EXERCISE 6.49</small></a>. We consider a system of inequalities and equations that involve one variable corresponding to each edge, two inequalities corresponding to each edge, and one equation corresponding to each vertex (except the source and the sink). The value of the variable is the edge flow, the inequalities specify that the edge flow must be between 0 and the edge’s capacity, and the equations specify that the total flow on the edges that go into each vertex must be equal to the total flow on the edges that go out of that vertex. Any max flow problem can be converted into an instance of a linear programming problem in this way, and the solution is easily converted to a solution of the maxflow problem. The illustration below gives the details for our example.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/06_28-maxflowlp.jpg"/></p>
<p><a id="page_909"/>The “many, many other problems” in the statement of <a href="#ch06sb29"><small>PROPOSITION K</small></a> refers to three ideas. First, <em>it is very easy to extend a model and to add constraints</em>. Second, <em>reduction is transitive</em>, so all the problems that reduce to shortest paths and maximum flow also reduce to linear programming. Third, and more generally, <em>optimization problems of all sorts can be directly formulated as linear programming problems</em>. Indeed, the term <em>linear programming</em> means “formulate an optimization problem as a linear programming problem.” This use predates the use of the word <em>programming</em> for computers. Equally important as the idea that a great many problems reduce to linear programming is the fact that efficient algorithms have been known for linear programming for many decades. The most famous, developed by G. Dantzig in the 1940s, is known as the <em>simplex algorithm</em>. Simplex is not difficult to understand (see the bare-bones implementation on the booksite). More recently, the <em>ellipsoid algorithm</em> presented by L. G. Khachian in 1979 led to the development of <em>interior point methods</em> in the 1980s that have proven to be an effective complement to the simplex algorithm for the huge linear programming problems that people are solving in modern applications. Nowadays, linear programming solvers are robust, extensively tested, efficient, and critical to the basic operation of modern corporations. Uses in scientific contexts and even in applications programming are also greatly expanding. If you can model your problem as a linear programming problem, you are likely to be able to solve it.</p>
<p><small>IN A VERY REAL SENSE, LINEAR PROGRAMMING IS THE PARENT</small> of problem-solving models, since so many problems reduce to it. Naturally, this idea leads to the question of whether there is an even more powerful problem-solving model than linear programming. What sorts of problems do <em>not</em> reduce to linear programming? Here is an example of such a problem:</p>
<p class="indenthanging"><strong><em>Load balancing.</em></strong> Given a set of jobs of specified duration to be completed, how can we schedule the jobs on two identical processors so as to minimize the completion time of all the jobs?</p>
<p>Can we articulate a more general problem-solving model and solve instances of problems within that model efficiently? This line of thinking leads to the idea of <em>intractability</em>, our last topic.</p>
<p><a id="ch06sec1lev10"/></p>
<h3><a id="page_910"/>Intractability</h3>
<p>The algorithms that we have studied in this book generally are used to solve practical problems and therefore consume reasonable amounts of resources. The practical utility of most of the algorithms is obvious, and for many problems we have the luxury of several efficient algorithms to choose from. Unfortunately, many other problems arise in practice that do not admit such efficient solutions. What’s worse, for a large class of such problems we cannot even tell whether or not an efficient solution exists. This state of affairs has been a source of extreme frustration for programmers and algorithm designers, who cannot find any efficient algorithm for a wide range of practical problems, and for theoreticians, who have been unable to find any proof that these problems are difficult. A great deal of research has been done in this area and has led to the development of mechanisms by which new problems can be classified as being “hard to solve” in a particular technical sense. Though much of this work is beyond the scope of this book, the central ideas are not difficult to learn. We introduce them here because every programmer, when faced with a new problem, should have some understanding of the possibility that there exist problems for which no one knows any algorithm that is guaranteed to be efficient.</p>
<p><a id="ch06sec2lev39"/></p>
<h4><em>Groundwork</em></h4>
<p>One of the most beautiful and intriguing intellectual discoveries of the 20th century, developed by A. Turing in the 1930s, is the <em>Turing machine</em>, a simple model of computation that is general enough to embody any computer program or computing device. A Turing machine is a finite-state machine that can read inputs, move from state to state, and write outputs. Turing machines form the foundation of theoretical computer science, starting with the following two ideas:</p>
<p class="indenthangingB">• <em>Universality</em>. All physically realizable computing devices can be simulated by a Turing machine. This idea is known as the <em>Church-Turing thesis</em>. This is a statement about the natural world and cannot be proven (but it can be falsified). The evidence in favor of the thesis is that mathematicians and computer scientists have developed numerous models of computation, but they all have been proven equivalent to the Turing machine.</p>
<p class="indenthangingB">• <em>Computability</em>. There exist problems that cannot be solved by a Turing machine (or by any other computing device, by universality). This is a mathematical truth. The halting problem (no program can guarantee to determine whether a given program will halt) is a famous example of such a problem.</p>
<p>In the present context, we are interested in a third idea, which speaks to the efficiency of computing devices:</p>
<p class="indenthangingB">• <em>Extended Church-Turing thesis</em>. The order of growth of the running time of a program to solve a problem on any computing device is within a polynomial factor of some program to solve the problem on a Turing machine (or any other computing device).</p>
<p><a id="page_911"/>Again, this is a statement about the natural world, buttressed by the idea that all known computing devices can be simulated by a Turing machine, with at most a polynomial factor increase in cost. In recent years, the idea of <em>quantum computing</em> has given some researchers reason to doubt the extended Church-Turing thesis. Most agree that, from a practical point of view, it is probably safe for some time, but many researchers are hard at work on trying to falsify the thesis.</p>
<p><a id="ch06sec2lev40"/></p>
<h4><em>Exponential running time</em></h4>
<p>The purpose of the theory of intractability is to separate problems that can be solved in polynomial time from problems that (probably) require <em>exponential</em> time to solve in the worst case. It is useful to think of an exponential-time algorithm as one that, for some input of size <em>N</em>, takes time proportional to 2<sup><em>N</em></sup> (at least). The substance of the argument does not change if we replace 2 by any number α &gt; 1. We generally take as granted that an exponential-time algorithm cannot be guaranteed to solve a problem of size 100 (say) in a reasonable amount of time, because no one can wait for an algorithm to take 2<sup>100</sup> steps, regardless of the speed of the computer. Exponential growth dwarfs technological changes: a supercomputer may be a trillion times faster than an abacus, but neither can come close to solving a problem that requires 2<sup>100</sup> steps. Sometimes the line between “easy” and “hard” problems is a fine one. For example, we studied an algorithm in <a href="ch04.html#ch04sec1lev10"><small>SECTION 4.1</small></a> that can solve the following problem:</p>
<p class="indenthanging"><strong><em>Shortest-path length.</em></strong> What is the length of the shortest path from a given vertex <em>s</em> to a given vertex <em>t</em> in a given graph?</p>
<p class="image"><img alt="image" src="graphics/p0911-01.jpg"/></p>
<p>But we did not study algorithms for the following problem, which seems to be virtually the same:</p>
<p class="indenthanging"><strong><em>Longest-path length.</em></strong> What is the length of the longest simple path from a given vertex <em>s</em> to a given vertex <em>t</em> in a given graph?</p>
<p><a id="page_912"/>The crux of the matter is this: as far as we know, these problems are nearly at opposite ends of the spectrum with respect to difficulty. Breadth-first search yields a solution for the first problem in <em>linear</em> time, but all known algorithms for the second problem take <em>exponential</em> time in the worst case. The code at the bottom of the previous page shows a variant of depth-first search that accomplishes the task. It is quite similar to depth-first search, but it examines <em>all</em> simple paths from <em>s</em> to <em>t</em> in the digraph to find the longest one.</p>
<p><a id="ch06sec2lev41"/></p>
<h4><em>Search problems</em></h4>
<p>The great disparity between problems that can be solved with “efficient” algorithms of the type we have been studying in this book and problems where we need to look for a solution among a potentially huge number of possibilities makes it possible to study the interface between them with a simple formal model. The first step is to characterize the type of problem that we study:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb30"/></p>
<p><strong>Definition.</strong> A <em>search problem</em> is a problem having solutions with the property that the time needed to <em>certify</em> that any solution is correct is bounded by a polynomial in the size of the input. We say that an algorithm <em>solves</em> a search problem if, given any input, it either produces a solution or reports that none exists.</p>
<hr/>
</div>
<p>Four particular problems that are of interest in our discussion of intractability are shown at the top of the facing page. These problems are known as <em>satisfiability</em> problems. Now, all that is required to establish that a problem is a search problem is to show that any solution is sufficiently well-characterized that you can efficiently certify that it is correct. Solving a search problem is like searching for a “needle in a haystack” with the sole proviso that you can recognize the needle when you see it. For example, if you are given an assignment of values to variables in each of the satisfiability problems at the top of page <a href="#page_913">913</a>, you easily can certify that each equality or inequality is satisfied, but searching for such an assignment is a totally different task. The name <strong>NP</strong> is commonly used to describe search problems—we will describe the reason for the name on page <a href="#page_914">914</a>:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb31"/></p>
<p><strong>Definition. NP</strong> is the set of all search problems.</p>
<hr/>
</div>
<p><strong>NP</strong> is nothing more than a precise characterization of all the problems that scientists, engineers, and applications programmers <em>aspire to solve</em> with programs that are guaranteed to finish in a feasible amount of time.</p>
<p class="indenthanging"><a id="page_913"/><strong><em>Linear equation satisfiability.</em></strong> Given a set of <em>M linear equations</em> involving <em>N</em> variables, find an assignment of values to the variables that satisfies all of the equations, or report that none exists.</p>
<p class="indenthanging"><strong><em>Linear inequality satisfiability</em> (<em>search formulation of linear programming</em>).</strong> Given a set of <em>M linear inequalities</em> involving <em>N</em> variables, find an assignment of values to the variables that satisfies all of the inequalities, or report that none exists.</p>
<p class="indenthanging"><strong><em>0-1 integer linear inequality satisfiability</em> (<em>search formulation of 0-1 integer linear programming</em>).</strong> Given a set of <em>M</em> linear inequalities involving <em>N integer</em> variables, find an assignment of the values 0 or 1 to the variables that satisfies all of the inequalities, or report that none exists.</p>
<p class="indenthanging"><strong><em>Boolean satisfiability.</em></strong> Given a set of <em>M equations</em> involving <em>and</em> and <em>or</em> operations on <em>N boolean</em> variables, find an assignment of values to the variables that satisfies all of the equations, or report that none exists.</p>
<p class="center"><strong><span class="pd_red">Selected search problems</span></strong></p>
<p><a id="ch06sec2lev42"/></p>
<h4><em>Other types of problems</em></h4>
<p>The concept of search problems is one of many ways to characterize the set of problems that form the basis of the study of intractability. Other possibilities are <em>decision</em> problems (does a solution exist?) and <em>optimization</em> problems (what is the best solution)? For example, the longest-paths length problem on page <a href="#page_911">911</a> is an optimization problem, not a search problem (given a solution, we have no way to verify that it is a longest-path length). A search version of this problem is to <em>find</em> a simple path connecting all the vertices (this problem is known as the <em>Hamiltonian path problem</em>). A decision version of the problem is to ask whether <em>there exists</em> a simple path connecting all the vertices. Arbitrage, boolean satisfiability, and Hamiltonian path are search problems; to ask whether a solution exists to any of these problems is a decision problem; and shortest/longest paths, maxflow, and linear programming are all optimization problems. While not technically equivalent, search, decision, and optimization problems typically reduce to one another (see <a href="#ch06qa1q58"><small>EXERCISE 6.58</small></a> and <a href="#ch06qa1q59">6.59</a>) and the main conclusions we draw apply to all three types of problems.</p>
<p><a id="ch06sec2lev43"/></p>
<h4><a id="page_914"/><em>Easy search problems</em></h4>
<p>The definition of <strong>NP</strong> says nothing about the difficulty of <em>finding</em> the solution, just certifying that it is a solution. The second of the two sets of problems that form the basis of the study of intractability, which is known as <strong>P</strong>, is concerned with the difficulty of finding the solution. In this model, the efficiency of an algorithm is a function of the number of bits used to encode the input.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb32"/></p>
<p><strong>Definition. P</strong> is the set of all search problems that can be solved in polynomial time.</p>
<hr/>
</div>
<p>Implicit in the definition is the idea that the polynomial time bound is a <em>worst-case</em> bound. For a problem to be in <strong>P</strong>, there must exist an algorithm that can <em>guarantee</em> to solve it in polynomial time. Note that the polynomial is not specified at all. Linear, linearithmic, quadratic, and cubic are all polynomial time bounds, so this definition certainly covers the standard algorithms we have studied so far. The time taken by an algorithm depends on the computer used, but the extended Church-Turing thesis renders that point moot—it says that a polynomial-time solution on any computing device implies the existence of a polynomial-time solution on any other computing device. Sorting belongs to <strong>P</strong> because (for example) insertion sort runs in time proportional to <em>N</em><sup>2</sup> (the existence of linearithmic sorting algorithms is not relevant in this context), as does shortest paths, linear equation satisfiability, and many others. Having an efficient algorithm to solve a problem is a proof that the problem is in <strong>P</strong>. In other words, <strong>P</strong> is nothing more than a precise characterization of all the problems that scientists, engineers, and applications programmers <em>do solve</em> with programs that are guaranteed to finish in a feasible amount of time.</p>
<p><a id="ch06sec2lev44"/></p>
<h4><em>Nondeterminism</em></h4>
<p>The <strong>N</strong> in <strong>NP</strong> stands for <em>nondeterminism</em>. It represents the idea that one way (in theory) to extend the power of a computer is to endow it with the power of nondeterminism: to assert that when an algorithm is faced with a choice of several options, it has the power to “guess” the right one. For the purposes of our discussion, we can think of an algorithm for a nondeterministic machine as “guessing” the solution to a problem, then certifying that the solution is valid. In a Turing machine, nondeterminism is as simple as defining two different successor states for a given state and a given input and characterizing solutions as all legal paths to the desired result. Nondeterminism may be a mathematical fiction, but it is a useful idea. For example, in <a href="ch05.html#ch05sec1lev10"><small>SECTION 5.4</small></a>, we used nondeterminism as a tool for algorithm design—our regular expression pattern-matching algorithm is based on efficiently simulating a nondeterministic machine.</p>
<p class="image"><a id="page_915"/><img alt="image" src="graphics/t0915-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/t0915-02.jpg"/></p>
<p><a id="ch06sec3lev1"/></p>
<h5><a id="page_916"/><em>The main question</em></h5>
<p>Nondeterminism is such a powerful notion that it seems almost absurd to consider it seriously. Why bother considering an imaginary tool that makes difficult problems seem trivial? The answer is that, powerful as nondeterminism may seem, no one has been able to <em>prove</em> that it helps for any particular problem! Put another way, no one has been able to find a single problem that can be proven to be in <strong>NP</strong> but not in <strong>P</strong> (or even prove that one exists), leaving the following question open:</p>
<p class="center"><em>Does <strong>P</strong> = <strong>NP</strong>?</em></p>
<p>This question was first posed in a famous letter from K. Gödel to J. von Neumann in 1950 and has completely stumped mathematicians and computer scientists ever since. Other ways of posing the question shed light on its fundamental nature:</p>
<p class="indenthangingB">• Are there <em>any</em> hard-to-solve search problems?</p>
<p class="indenthangingB">• Would we be able to solve some search problems more efficiently if we could build a nondeterministic computing device?</p>
<p>Not knowing the answers to these questions is extremely frustrating because many important practical problems belong to <strong>NP</strong> but may or may not belong to <strong>P</strong> (the best known deterministic algorithms could take exponential time). If we could prove that a problem does not belong to <strong>P</strong>, then we could abandon the search for an efficient solution to it. In the absence of such a proof, there is the possibility that some efficient algorithm has gone undiscovered. In fact, given the current state of our knowledge, there could be some efficient algorithm for <em>every</em> problem in <strong>NP</strong>, which would imply that many efficient algorithms have gone undiscovered. Virtually no one believes that <strong>P</strong> = <strong>NP</strong>, and a considerable amount of effort has gone into proving the contrary, but this remains the outstanding open research problem in computer science.</p>
<p><a id="ch06sec2lev45"/></p>
<h4><em>Poly-time reductions</em></h4>
<p>Recall from page <a href="#page_903">903</a> that we show that a problem <em>A reduces to</em> another problem <em>B</em> by demonstrating that we can solve any instance of <em>A</em> in three steps:</p>
<p class="indenthangingB">• Transform it to an instance of <em>B</em>.</p>
<p class="indenthangingB">• Solve that instance of <em>B</em>.</p>
<p class="indenthangingB">• Transform the solution of <em>B</em> to be a solution of <em>A</em>.</p>
<p>As long as we can perform the transformations (and solve <em>B</em>) efficiently, we can solve <em>A</em> efficiently. In the present context, for <em>efficient</em> we use the weakest conceivable definition: to solve <em>A</em> we solve at most a polynomial number of instances of <em>B</em>, using transformations that require at most polynomial time. In this case, we say that A <em>poly-time reduces</em> to B. Before, we used reduction to introduce the idea of problem-solving models that can significantly expand the range of problems that we can solve with efficient algorithms. Now, we use reduction in another sense: <em>to prove a problem to be hard to solve</em>. If a problem <em>A</em> is known to be hard to solve, and <em>A</em> poly-time reduces to <em>B</em>, then <em>B</em> must be hard to solve, too. Otherwise, a guaranteed polynomial-time solution to <em>B</em> would give a guaranteed polynomial-time solution to <em>A</em>.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb33"/></p>
<p><a id="page_917"/><strong>Proposition L.</strong> Boolean satisfiability poly-time reduces to 0-1 integer linear inequality satisfiability.</p>
<p><strong>Proof:</strong> Given an instance of boolean satisfiability, define a set of inequalities with one 0-1 variable corresponding to each boolean variable and one 0-1 variable corresponding to each clause, as illustrated in the example at right. With this construction, we can tranform a solution to the integer 0-1 linear inequality satisfiability problem to a solution to the boolean satisfiability problem by assigning each boolean variable to be <em>true</em> if the corresponding integer variable is 1 and <em>false</em> if it is 0.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/06_31-satreduce.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb34"/></p>
<p><strong>Corollary.</strong> If satisfiability is hard to solve, then so is integer linear programming.</p>
<hr/>
</div>
<p>This statement is a meaningful statement about the relative difficulty of solving these two problems even in the absence of a precise definition of <em>hard to solve.</em> In the present context, by “hard to solve,” we mean “not in <strong>P</strong>.” We generally use the word <em>intractable</em> to refer to problems that are not in <strong>P</strong>. Starting with the seminal work of R. Karp in 1972, researchers have shown literally tens of thousands of problems from a wide variety of applications areas to be related by reduction relationships of this sort. Moreover, these relationships imply much more than just relationships between the individual problems, a concept that we now address.</p>
<p><a id="ch06sec2lev46"/></p>
<h4><em>NP-completeness</em></h4>
<p>Many, many problems are known to belong to <strong>NP</strong> but probably do not belong to <strong>P</strong>. That is, we can easily <em>certify</em> that any given solution is valid, but, despite considerable effort, no one has been able to develop an efficient algorithm to <em>find</em> a solution. Remarkably, all of these many, many problems have an additional property that provides convincing evidence that <strong>P</strong> ≠ <strong>NP</strong>:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb35"/></p>
<p><a id="page_918"/><strong>Definition.</strong> A search problem <em>A</em> is said to be <strong><em>NP-complete</em></strong> if all problems in <strong>NP</strong> polytime reduce to <em>A</em>.</p>
<hr/>
</div>
<p>This definition enables us to upgrade our definition of “hard to solve” to mean “intractable unless <strong>P</strong> = <strong>NP</strong>.” If <em>any</em> <strong>NP</strong>-complete problem can be solved in polynomial time on a deterministic machine, then so can <em>all problems in</em> <strong><em>NP</em></strong> (i.e., <strong>P</strong> = <strong>NP</strong>). That is, the collective failure of all researchers to find efficient algorithms for all of these problems might be viewed as a collective failure to prove that <strong>P</strong> = <strong>NP. NP</strong>-complete problems, meaning that we do not expect to find guaranteed polynomial-time algorithms. Most practical search problems are known to be either in <strong>P</strong> or <strong>NP</strong>-complete.</p>
<p><a id="ch06sec2lev47"/></p>
<h4><em>Cook-Levin theorem</em></h4>
<p>Reduction uses the <strong>NP</strong>-completeness of one problem to imply the <strong>NP</strong>-completeness of another. But reduction cannot be used in one case: how was the <em>first</em> problem proven to be <strong>NP</strong>-complete? This was done independently by S. Cook and L. Levin in the early 1970s.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch06sb36"/></p>
<p><strong>Proposition M. (Cook-Levin theorem)</strong> Boolean satisfiability is <strong>NP</strong>-complete.</p>
<p><strong>Extremely brief proof sketch:</strong> The goal is to show that if there is a polynomial time algorithm for boolean satisfiability, then all problems in <strong>NP</strong> can be solved in polynomial time. Now, a nondeterministic Turing machine can solve any problem in <strong>NP</strong>, so the first step in the proof is to describe each feature of the machine in terms of logical formulas such as appear in the boolean satisfiability problem. This construction establishes a correspondence between every problem in <strong>NP</strong> (which can be expressed as a program on the nondeterministic Turing machine) and some instance of satisfiability (the translation of that program into a logical formula). Now, the solution to the satisfiability problem essentially corresponds to a simulation of the machine running the given program on the given input, so it produces a solution to an instance of the given problem. Further details of this proof are well beyond the scope of this book. Fortunately, only one such proof is really necessary: it is much easier to use reduction to prove <strong>NP</strong>-completeness.</p>
<hr/>
</div>
<p>The Cook-Levin theorem, in conjunction with the thousands and thousands of polytime reductions from <strong>NP</strong>-complete problems that have followed it, leaves us with two possible universes: either <strong>P = NP</strong> and no intractable search problems exist (all search problems can be solved in polynomial time); or <strong>P ≠ NP</strong>, there do exist intractable search problems (some search problems cannot be solved in polynomial time). <strong>NP</strong>-complete <a id="page_919"/>problems arise frequently in important natural practical applications, so there has been strong motivation to find good algorithms to solve them. The fact that no good algorithm has been found for any of these problems is surely strong evidence that <strong>P ≠ NP</strong>, and most researchers certainly believe this to be the case. On the other hand, the fact that no one has been able to prove that any of these problems do not belong to <strong>P</strong> could be construed to comprise a similar body of circumstantial evidence on the other side. Whether or not <strong>P = NP</strong>, the practical fact is that the best known algorithm for any of the NP-complete problems takes exponential time in the worst case.</p>
<p class="image"><img alt="image" src="graphics/06_32-pnp.jpg"/></p>
<p><a id="ch06sec2lev48"/></p>
<h4><em>Classifying problems</em></h4>
<p>To prove that a search problem is in <strong>P</strong>, we need to exhibit a polynomial-time algorithm for solving it, perhaps by reducing it to a problem known to be in <strong>P</strong>. To prove that a problem in <strong>NP</strong> is <strong>NP</strong>-complete, we need to show that some known <strong>NP</strong>-complete problem is poly-time reducible to it: that is, that a polynomial-time algorithm for the new problem could be used to solve the <strong>NP</strong>-complete problem, and then could, in turn, be used to solve all problems in <strong>NP</strong>. Thousands and thousands of problems have been shown to be <strong>NP</strong>-complete in this way, as we did for integer linear programming in <a href="#ch06sb33"><small>PROPOSITION L</small></a>. The list on page <a href="#page_920">920</a>, which includes several of the problems addressed by Karp, is representative, but contains only a tiny fraction of the known <strong>NP</strong>-complete problems. Classifying problems as being easy to solve (in <strong>P</strong>) or hard to solve (<strong>NP</strong>-complete) can be:</p>
<p class="indenthangingB">• <em>Straightforward</em>. For example, the venerable Gaussian elimination algorithm proves that linear equation satisfiability is in <strong>P</strong>.</p>
<p class="indenthangingB">• <em>Tricky but not difficult.</em> For example, developing a proof like the proof of <a href="#ch06sb03"><small>PROPOSITION A</small></a> takes some experience and practice, but it is easy to understand.</p>
<p class="indenthangingB">• <em>Extremely challenging</em>. For example, linear programming was long unclassified, but Khachian’s ellipsoid algorithm proves that linear programming is in <strong>P</strong>.</p>
<p class="indenthangingB">• <em>Open</em>. For example, <em>graph isomorphism</em> (given two graphs, find a way to rename the vertices of one to make it identical to the other) and <em>factor</em> (given an integer, find a nontrivial factor) are still unclassified.</p>
<p>This is a rich and active area of current research, still involving thousands of research papers per year. As indicated by the last few entries on the list on page <a href="#page_920">920</a>, all areas of scientific inquiry are affected. Recall that our definition of <strong>NP</strong> encompasses the problems that scientists, engineers, and applications programmers <em>aspire to solve</em> feasibly—all such problems certainly need to be classified!</p>
<p class="indenthanging"><a id="page_920"/><strong><em>Boolean satisfiability.</em></strong> Given a set of <em>M equations</em> involving <em>N boolean</em> variables, find an assignment of values to the variables that satisfies all of the equations, or report that none exists.</p>
<p class="indenthanging"><strong><em>Integer linear programming.</em></strong> Given a set of <em>M</em> linear inequalities involving <em>N integer</em> variables, find an assignment of values to the variables that satisfies all of the inequalities, or report that none exists.</p>
<p class="indenthanging"><strong><em>Load balancing.</em></strong> Given a set of jobs of specified duration to be completed and a time bound <em>T</em>, how can we schedule the jobs on two identical processors so as to complete them all by time <em>T</em>?</p>
<p class="indenthanging"><strong><em>Vertex cover.</em></strong> Given a graph and a integer <em>C</em>, find a set of <em>C</em> vertices such that each edge of the graph is incident to at least one vertex of the set.</p>
<p class="indenthanging"><strong><em>Hamiltonian path.</em></strong> Given a graph, find a simple path that visits each vertex exactly once, or report that none exists.</p>
<p class="indenthanging"><strong><em>Protein folding.</em></strong> Given energy level <em>M</em>, find a folded three-dimensional conformation of a protein having potential energy less than <em>M.</em></p>
<p class="indenthanging"><strong><em>Ising model.</em></strong> Given an Ising model on a lattice of dimension three and an energy threshhold <em>E</em>, is there a subgraph with free energy less than <em>E</em>?</p>
<p class="indenthanging"><strong><em>Risk portfolio of a given return.</em></strong> Given an investment portfolio with a given total cost, a given return, risk values assigned to each investment, and a threshold <em>M</em>, find a way to allocate the investments such that the risk is less than <em>M.</em></p>
<p class="center1"><strong><span class="pd_red">Some famous NP-complete problems</span></strong></p>
<p><a id="ch06sec2lev49"/></p>
<h4><a id="page_921"/><em>Coping with NP-completeness</em></h4>
<p>Some sort of solution to this vast panoply of problems must be found in practice, so there is intense interest in finding ways to address them. It is impossible to do justice to this vast field of study in one paragraph, but we can briefly describe various approaches that have been tried. One approach is to change the problem and find an “approximation” algorithm that finds not the best solution but a solution guaranteed to be close to the best. For example, it is easy to find a solution to the Euclidean traveling salesperson problem that is within a factor of 2 of the optimal. Unfortunately, this approach is often not sufficient to ward off <strong>NP</strong>-completeness, when seeking improved approximations. Another approach is to develop an algorithm that solves efficiently virtually all of the instances that do arise in practice, even though there exist worst-case inputs for which finding a solution is infeasible. The most famous example of this approach are the integer linear programming solvers, which have been workhorses for many decades in solving huge optimizaiton problems in countless industrial applications. Even though they could require exponential time, the inputs that arise in practice evidently are not worst-case inputs. A third approach is to work with “efficient” exponential algorithms, using a technique known as <em>backtracking</em> to avoid having to check all possible solutions. Finally, there is quite a large gap between polynomial and exponential time that is not addressed by the theory. What about an algorithm that runs in time proportional to <em>N</em><sup>log <em>N</em></sup> or 2<sup>√<em>N</em></sup>?</p>
<p><small>ALL THE APPLICATIONS AREAS</small> we have studied in this book are touched by <strong>NP</strong>-completeness: <strong>NP</strong>-complete problems arise in elementary programming, in sorting and searching, in graph processing, in string processing, in scientific computing, in systems programming, in operations research, and in any conceivable area where computing plays a role. The most important practical contribution of the theory of <strong>NP</strong>-completeness is that it provides a mechanism to discover whether a new problem from any of these diverse areas is “easy” or “hard.” If one can find an efficient algorithm to solve a new problem, then there is no difficulty. If not, a proof that the problem is <strong>NP</strong>-complete tells us that developing an efficient algorithm would be a stunning achievement (and suggests that a different approach should perhaps be tried). The scores of efficient algorithms that we have examined in this book are testimony that we have learned a great deal about efficient computational methods since Euclid, but the theory of <strong>NP</strong> completeness shows that, indeed, we still have a great deal to learn.</p>
<p><a id="ch06sec2lev50"/></p>
<h4><a id="page_922"/>Exercises on collision simulation</h4>
<p><a id="ch06qa1q1"/><strong>6.1</strong> Complete the implementation <code>predictCollisions()</code> and <code>Particle</code> as described in the text. There are three equations governing the elastic collision between a pair of hard discs: (<em>a</em>) conservation of linear momentum, (<em>b</em>) conservation of kinetic energy, and (<em>c</em>) upon collision, the normal force acts perpendicular to the surface at the collision point (assuming no friction or spin). See the booksite for more details.</p>
<p><a id="ch06qa1q2"/><strong>6.2</strong> Develop a version of <code>CollisionSystem</code>, <code>Particle</code>, and <code>Event</code> that handles multiparticle collisions. Such collisions are important when simulating the break in a game of billiards. (This is a difficult exercise!)</p>
<p><a id="ch06qa1q3"/><strong>6.3</strong> Develop a version of <code>CollisionSystem</code>, <code>Particle</code>, and <code>Event</code> that works in three dimensions.</p>
<p><a id="ch06qa1q4"/><strong>6.4</strong> Explore the idea of improving the performance of <code>simulate()</code> in <code>CollisionSystem</code> by dividing the region into rectangular cells and adding a new event type so that you only need to predict collisions with particles in one of nine adjacent cells in any time quantum. This approach reduces the number of predictions to calculate at the cost of monitoring the movement of particles from cell to cell.</p>
<p><a id="ch06qa1q5"/><strong>6.5</strong> Introduce the concept of <em>entropy</em> to <code>CollisionSystem</code> and use it to confirm classical results.</p>
<p><a id="ch06qa1q6"/><strong>6.6</strong> <em>Brownian motion.</em> In 1827, the botanist Robert Brown observed the motion of wildflower pollen grains immersed in water using a microscope. He observed that the pollen grains were in a random motion, following what would become known as Brownian motion. This phenomenon was discussed, but no convincing explanation was provided until Einstein provided a mathematical one in 1905. Einstein’s explanation: the motion of the pollen grain particles was caused by millions of tiny molecules colliding with the larger particles. Run a simulation that illustrates this phenomenon.</p>
<p><a id="ch06qa1q7"/><strong>6.7</strong> <em>Temperature.</em> Add a method <code>temperature()</code> to <code>Particle</code> that returns the product of its mass and the square of the magitude of its velocity divided by <em>dk<sub>B</sub></em> where <em>d</em> =2 is the dimension and <em>k<sub>B</sub></em> =1.3806503 × 10<sup>−23</sup> is Boltzmann’s constant. The temperature of the system is the average value of these quantities. Then add a method <code>temperature()</code> to <code>CollisionSystem</code> and write a driver that plots the temperature periodically, to check that it is constant.</p>
<p><a id="page_923"/><a id="ch06qa1q8"/><strong>6.8</strong> <em>Maxwell-Boltzmann.</em> The distribution of velocity of particles in the hard disc model obeys the <em>Maxwell-Boltzmann distribution</em> (assuming that the system has thermalized and particles are sufficiently heavy that we can discount quantum-mechanical effects), which is known as the Rayleigh distribution in two dimensions. The distribution shape depends on temperature. Write a driver that computes a histogram of the particle velocities and test it for various temperatures.</p>
<p><a id="ch06qa1q9"/><strong>6.9</strong> <em>Arbitrary shape.</em> Molecules travel very quickly (faster than a speeding jet) but diffuse slowly because they collide with other molecules, thereby changing their direction. Extend the model to have a boundary shape where two vessels are connected by a pipe containing two different types of particles. Run a simulation and measure the fraction of particles of each type in each vessel as a function of time.</p>
<p><a id="ch06qa1q10"/><strong>6.10</strong> <em>Rewind.</em> After running a simulation, negate all velocities and then run the system backward. It should return to its original state! Measure roundoff error by measuring the difference between the final and original states of the system.</p>
<p><a id="ch06qa1q11"/><strong>6.11</strong> <em>Pressure.</em> Add a method <code>pressure()</code> to <code>Particle</code> that measures pressure by accumulating the number and magnitude of collisions against walls. The pressure of the system is the sucm of these quantities. Then add a method <code>pressure()</code> to <code>CollisionSystem</code> and write a client that validates the equation <em>pv</em> = <em>nRT</em>.</p>
<p><a id="ch06qa1q12"/><strong>6.12</strong> <em>Index priority queue implementation.</em> Develop a version of <code>CollisionSystem</code> that uses an index priority queue to guarantee that the size of the priority queue is at most linear in the number of particles (instead of quadratic or worse).</p>
<p><a id="ch06qa1q13"/><strong>6.13</strong> <em>Priority queue performance.</em> Instrument the priority queue and test <code>Pressure</code> at various temperatures to identify the computational bottleneck. If warranted, try switching to a different priority-queue implementation for better performance at high temperatures.</p>
<p><a id="ch06sec2lev51"/></p>
<h4><a id="page_924"/>Exercises on B-Trees</h4>
<p><a id="ch06qa1q14"/><strong>6.14</strong> Suppose that, in a three-level tree, we can afford to keep a links in internal memory, between <em>b</em> and 2<em>b</em> links in pages representing internal nodes, and between <em>c</em> and 2<em>c</em> items in pages representing external nodes. What is the maximum number of items that we can hold in such a tree, as a function of <em>a</em>, <em>b</em>, and <em>c</em>?</p>
<p><a id="ch06qa1q15"/><strong>6.15</strong> Develop an implementation of <code>Page</code> that represents each B-tree node as a <code>BinarySearchST</code> object.</p>
<p><a id="ch06qa1q16"/><strong>6.16</strong> Extend <code>BTreeSET</code> to develop a <code>BTreeST</code> implementation that associates keys with values and supports our full ordered symbol table API that includes <code>min()</code>, <code>max()</code>, <code>floor()</code>, <code>ceiling()</code>, <code>deleteMin()</code>, <code>deleteMax()</code>, <code>select()</code>, <code>rank()</code>, and the two-argument versions of <code>size()</code> and <code>get()</code>.</p>
<p><a id="ch06qa1q17"/><strong>6.17</strong> Write a program that uses <code>StdDraw</code> to visualize B-trees as they grow, as in the text.</p>
<p><a id="ch06qa1q18"/><strong>6.18</strong> Estimate the average number of probes per search in a B-tree for <em>S</em> random searches, in a typical cache system, where the <em>T</em> most-recently-accessed pages are kept in memory (and therefore add 0 to the probe count). Assume that <em>S</em> is much larger than <em>T</em>.</p>
<p><a id="ch06qa1q19"/><strong>6.19</strong> <em>Web search.</em> Develop an implementation of <code>Page</code> that represents B-tree nodes as text files on web pages, for the purposes of indexing (building a concordance for) the web. Use a file of search terms. Take web pages to be indexed from standard input. To keep control, take a command-line parameter <em>m</em>, and set an upper limit of 10<sup><em>m</em></sup> internal nodes (check with your system administrator before running for large <em>m</em>). Use an <em>m</em> digit number to name your internal nodes. For example, when <em>m</em> is 4, your nodes names might be <code>BTreeNode0000</code>, <code>BTreeNode0001</code>, <code>BTreeNode0002</code>, and so forth. Keep pairs of strings on pages. Add a <code>close()</code> operation to the API, to sort and write. To test your implementation, ook for yourself and your friends on your university’s website.</p>
<p><a id="ch06qa1q20"/><strong>6.20</strong> <em>B* trees.</em> Consider the sibling split (or <em>B*-tree)</em> heuristic for B-trees: When it comes time to split a node because it contains <em>M</em> entries, we combine the node with its sibling. If the sibling has <em>k</em> entries with k &lt; <em>M</em> − 1, we reallocate the items giving the sibling and the full node each about (<em>M+k</em>)/2 entries. Otherwise, we create a new node and give each of the three nodes about 2<em>M</em>/3 entries. Also, we allow the root to grow to hold about 4<em>M</em>/3 items, splitting it and creating a new root node with two entries when it reaches that bound. State bounds on the number of probes used for a search or an insertion in a B*-tree of order <em>M</em> with <em>N</em> items. Compare your bounds with the <a id="page_925"/>corresponding bounds for B-trees (see <a href="#ch06sb06"><small>PROPOSITION B</small></a>). Develop an <em>insert</em> implementation for B*-trees.</p>
<p><a id="ch06qa1q21"/><strong>6.21</strong> Write a program to compute the average number of external pages for a B-tree of order <em>M</em> built from <em>N</em> random insertions into an initially empty tree. Run your program for reasonable values of <em>M</em> and <em>N</em>.</p>
<p><a id="ch06qa1q22"/><strong>6.22</strong> If your system supports virtual memory, design and conduct experiments to compare the performance of B-trees with that of binary search, for random searches in a huge symbol table.</p>
<p><a id="ch06qa1q23"/><strong>6.23</strong> For your internal-memory implementation of <code>Page</code> in <a href="#ch06qa1q15"><small>EXERCISE 6.15</small></a>, run experiments to determine the value of <em>M</em> that leads to the fastest search times for a B-tree implementation supporting random search operations in a huge symbol table. Restrict your attention to values of <em>M</em> that are multiples of 100.</p>
<p><a id="ch06qa1q24"/><strong>6.24</strong> Run experiments to compare search times for internal B-trees (using the value of <em>M</em> determined in the previous exercise), linear probing hashing, and red-black trees for random search operations in a huge symbol table.</p>
<p><a id="ch06sec1lev11"/></p>
<h3><a id="page_926"/>Exercises on suffix arrays</h3>
<p><a id="ch06qa1q25"/><strong>6.25</strong> Give, in the style of the figure on page <a href="#page_882">882</a>, the suffixes, sorted suffixes, <code>index()</code> and <code>lcp()</code> tables for the following strings:</p>
<p class="indenthangingN"><em>a.</em> <code>abacadaba</code></p>
<p class="indenthangingN"><em>b.</em> <code>mississippi</code></p>
<p class="indenthangingN"><em>c.</em> <code>abcdefghij</code></p>
<p class="indenthangingN"><em>d.</em> <code>aaaaaaaaaa</code></p>
<p><a id="ch06qa1q26"/><strong>6.26</strong> Identify the problem with the following code fragment to compute all the suffixes for suffix sort:</p>
<p class="programlisting"><img alt="image" src="graphics/p0926-01.jpg"/></p>
<p><em>Answer</em>: It uses quadratic time and quadratic space.</p>
<p><a id="ch06qa1q27"/><strong>6.27</strong> Some applications require a sort of <em>cyclic rotations</em> of a text, which all contain all the characters of the text. For i from 0 to <em>N</em> − 1, the <em>i</em>th cyclic rotation of a text of length <em>N</em> is the last <em>N</em> − <em>i</em> characters followed by the first <em>i</em> characters. Identify the problem with the following code fragment to compute all the cyclic rotations:</p>
<p class="programlisting"><img alt="image" src="graphics/p0926-02.jpg"/></p>
<p><em>Answer</em>: It uses quadratic time and quadratic space.</p>
<p><a id="ch06qa1q28"/><strong>6.28</strong> Design a linear-time algorithm to compute all the cyclic rotations of a text string.</p>
<p><em>Answer</em>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0926-03.jpg"/></p>
<p><a id="page_927"/><strong>6.29</strong> Under the assumptions described in <a href="ch01a.html#ch01sec1lev6"><small>SECTION 1.4</small></a>. give the memory usage of a <code>SuffixArray</code> object with a string of length <em>N</em>.</p>
<p><a id="ch06qa1q30"/><strong>6.30</strong> <em>Longest common substring.</em> Write a <code>SuffixArray</code> client <code>LCS</code> that take two filenames as command-line arguments, reads the two text files, and finds the longest substring that appears in both in linear time. (In 1970, D. Knuth conjectured that this task was impossible.) <em>Hint</em>: Create a suffix array for <code>s#t</code> where <code>s</code> and <code>t</code> are the two text strings and <code>#</code> is a character that does not appear in either.</p>
<p><a id="ch06qa1q31"/><strong>6.31</strong> <em>Burrows-Wheeler transform.</em> The <em>Burrows-Wheeler transform</em> (BWT) is a transformation that is used in data compression algorithms, including <code>bzip2</code> and in high-throughput sequencing in genomics. Write a <code>SuffixArray</code> client that computes the BWT in linear time, as follows: Given a string of length <em>N</em> (terminated by a special end-of-file character $ that is smaller than any other character), consider the <em>N</em>-by-<em>N</em> matrix in which each row contains a different cyclic rotation of the original text string. Sort the rows lexicographically. The Burrows-Wheeler transform is the rightmost column in the sorted matrix. For example, the BWT of <code>mississippi$</code> is <code>ipssm$pissii</code>. The <em>Burrows-Wheeler inverse transform</em> (BWI) inverts the BWT. For example, the BWI of <code>ipssm$pissii</code> is <code>mississippi$</code>. Also write a client that, given the BWT of a text string, computes the BWI in linear time.</p>
<p><a id="ch06qa1q32"/><strong>6.32</strong> <em>Circular string linearization.</em> Write a <code>SuffixArray</code> client that, given a string, finds the cyclic rotation that is the smallest lexicographically in linear time. This problem arises in chemical databases for circular molecules, where each molecule is represented as a circular string, and a canonical representation (smallest cyclic rotation) is used to support search with any rotation as key. (See <a href="#ch06qa1q27"><small>EXERCISE 6.27</small></a> and <a href="#ch06qa1q28"><small>EXERCISE 6.28</small></a>.)</p>
<p><a id="ch06qa1q33"/><strong>6.33</strong> <em>Longest k-repeated substring.</em> Write a <code>SuffixArray</code> client that, given a string and an integer <code>k</code>, find the longest substring that is repeated <code>k</code> or more times.</p>
<p><a id="ch06qa1q34"/><strong>6.34</strong> <em>Long repeated substrings.</em> Write a <code>SuffixArray</code> client that, given a string and an integer <code>L</code>, finds all repeated substrings of length <code>L</code> or more.</p>
<p><a id="ch06qa1q35"/><strong>6.35</strong> <em>k-gram frequency counts.</em> Develop and implement an ADT for preprocessing a string to support efficiently answering queries of the form <em>How many times does a given k-gram appear</em>? Each query should take time proportional to <em>k</em> log <em>N</em> in the worst case, where <em>N</em> is the length of the string.</p>
<p><a id="ch06sec2lev52"/></p>
<h4><a id="page_928"/>Exercises on maxflow</h4>
<p><a id="ch06qa1q36"/><strong>6.36</strong> If capacities are positive integers less than <em>M</em>, what is the maximum possible flow value for any <em>st</em>-network with <em>V</em> vertices and <em>E</em> edges? Give two answers, depending on whether or not parallel edges are allowed.</p>
<p><a id="ch06qa1q37"/><strong>6.37</strong> Give an algorithm to solve the maxflow problem for the case that the network forms a tree if the sink is removed.</p>
<p><a id="ch06qa1q38"/><strong>6.38</strong> <em>True or false.</em> If true provide a short proof, if false give a counterexample:</p>
<p class="indenthangingN"><em>a.</em> In any max flow, there is no directed cycle on which every edge carries positive flow</p>
<p class="indenthangingN"><em>b.</em> There exists a max flow for which there is no directed cycle on which every edge carries positive flow</p>
<p class="indenthangingN"><em>c.</em> If all edge capacities are distinct, the max flow is unique</p>
<p class="indenthangingN"><em>d.</em> If all edge capacities are increased by an additive constant, the min cut remains unchanged</p>
<p class="indenthangingN"><em>e.</em> If all edge capacities are multiplied by a positive integer, the min cut remains unchanged</p>
<p><a id="ch06qa1q39"/><strong>6.39</strong> Complete the proof of <a href="#ch06sb23"><small>PROPOSITION G</small></a>: Show that each time an edge is a critical edge, the length of the augmenting path through it must increase by 2.</p>
<p><a id="ch06qa1q40"/><strong>6.40</strong> Find a large network online that you can use as a vehicle for testing flow algorithms on realistic data. Possibilities include transportation networks (road, rail, or air), communications networks (telephone or computer connections), or distribution networks. If capacities are not available, devise a reasonable model to add them. Write a program that uses the interface to implement flow networks from your data. If warranted, develop additional private methods to clean up the data.</p>
<p><a id="ch06qa1q41"/><strong>6.41</strong> Write a random-network generator for sparse networks with integer capacities between 0 and 2<sup>20</sup>. Use a separate class for capacities and develop two implementations: one that generates uniformly distributed capacities and another that generates capacities according to a Gaussian distribution. Implement client programs that generate random networks for both weight distributions with a well-chosen set of values of <em>V</em> and <em>E</em> so that you can use them to run empirical tests on graphs drawn from various distributions of edge weights.</p>
<p><a id="page_929"/><a id="ch06qa1q42"/><strong>6.42</strong> Write a program that generates <em>V</em> random points in the plane, then builds a flow network with edges (in both directions) connecting all pairs of points within a given distance <em>d</em> of each other, setting each edge’s capacity using one of the random models described in the previous exercise.</p>
<p><a id="ch06qa1q43"/><strong>6.43</strong> <em>Basic reductions.</em> Develop <code>FordFulkerson</code> clients for finding a maxflow in each of the following types of flow networks:</p>
<p class="indenthangingB">• Undirected</p>
<p class="indenthangingB">• No constraint on the number of sources or sinks or on edges entering the source or leaving the sink</p>
<p class="indenthangingB">• Lower bounds on capacities</p>
<p class="indenthangingB">• Capacity constraints on vertices</p>
<p><a id="ch06qa1q44"/><strong>6.44</strong> <em>Product distribution.</em> Suppose that a flow represents products to be transferred by trucks between cities, with the flow on edge <code>u-v</code> representing the amount to be taken from city <code>u</code> to city <code>v</code> in a given day. Write a client that prints out daily orders for truckers, telling them how much and where to pick up and how much and where to drop off. Assume that there are no limits on the supply of truckers and that nothing leaves a given distribution point until everything has arrived.</p>
<p><a id="ch06qa1q45"/><strong>6.45</strong> <em>Job placement.</em> Develop a <code>FordFulkerson</code> client that solves the job-placement problem, using the reduction in <a href="#ch06sb28"><small>PROPOSITION J</small></a>. Use a symbol table to convert symbolic names into integers for use in the flow network.</p>
<p><a id="ch06qa1q46"/><strong>6.46</strong> Construct a family of bipartite matching problems where the average length of the augmenting paths used by any augmenting-path algorithm to solve the corresponding maxflow problem is proportional to <em>E</em>.</p>
<p><a id="ch06qa1q47"/><strong>6.47</strong> <em>st-connectivity.</em> Develop a <code>FordFulkerson</code> client that, given an undirected graph <em>G</em> and vertices <em>s</em> and <em>t</em>, finds the minimum number of edges in <em>G</em> whose removal will disconnect <em>t</em> from <em>s</em>.</p>
<p><a id="ch06qa1q48"/><strong>6.48</strong> <em>Disjoint paths.</em> Develop a <code>FordFulkerson</code> client that, given an undirected graph <em>G</em> and vertices <em>s</em> and <em>t</em>, finds the maximum number of edge-disjoint paths from <em>s</em> to <em>t</em>.</p>
<p><a id="ch06sec2lev53"/></p>
<h4><a id="page_930"/>Exercises on reductions and intractability</h4>
<p><a id="ch06qa1q49"/><strong>6.49</strong> Find a nontrivial factor of <code>37703491</code>.</p>
<p><a id="ch06qa1q50"/><strong>6.50</strong> Prove that the shortest-paths problem reduces to linear programming.</p>
<p><a id="ch06qa1q51"/><strong>6.51</strong> Could there be an algorithm that solves an <strong>NP</strong>-complete problem in an average time of <em>N</em><sup>log <em>N</em></sup>, if <strong>P ≠ NP</strong>? Explain your answer.</p>
<p><a id="ch06qa1q52"/><strong>6.52</strong> Suppose that someone discovers an algorithm that is guranteed to solve the boolean satisfiability problem in time proportional to 1.1<sup><em>N</em></sup> Does this imply that we can solve other <strong><em>NP</em></strong>-complete problems in time proportional to 1.1<sup><em>N</em></sup>?</p>
<p><a id="ch06qa1q53"/><strong>6.53</strong> What would be the significance of a program that could solve the integer linear programming problem in time proportional to 1.1<sup><em>N</em></sup>?</p>
<p><a id="ch06qa1q54"/><strong>6.54</strong> Give a poly-time reduction from vertex cover to 0-1 integer linear inequality satisfiability.</p>
<p><a id="ch06qa1q55"/><strong>6.55</strong> Prove that the problem of finding a Hamiltonian path in a <em>directed</em> graph is <strong>NP</strong> complete, using the <strong><em>NP</em></strong>-completeness of the Hamiltonian-path problem for undirected graphs.</p>
<p><a id="ch06qa1q56"/><strong>6.56</strong> Suppose that two problems are known to be <strong><em>NP</em></strong>-complete. Does this imply that there is a poly-time reduction from one to the other?</p>
<p><a id="ch06qa1q57"/><strong>6.57</strong> Suppose that <em>X</em> is <strong><em>NP</em></strong>-complete, <em>X</em> poly-time reduces to <em>Y</em>, and <em>Y</em> poly-time reduces to <em>X</em>. Is <em>Y</em> necessarily <strong><em>NP</em></strong>-complete?</p>
<p><em>Answer</em>: No, since Y may not be in <strong><em>NP</em></strong>.</p>
<p><a id="ch06qa1q58"/><strong>6.58</strong> Suppose that we have an algorithm to solve the decision version of boolean satisfiability, which indicates that there exists an assignment of truth values to the variables that satisfies the boolean expression. Show how to find the assignment.</p>
<p><a id="ch06qa1q59"/><strong>6.59</strong> Suppose that we have an algorithm to solve the decision version of the vertex cover problem, which indicates that there exists a vertex cover of a given size. Show how to solve the optimization version of finding the vertex cover of minimum cardinality.</p>
<p><a id="ch06qa1q60"/><strong>6.60</strong> Explain why the optimization version of the vertex cover problem is not necessarily a search problem.</p>
<p><a id="page_931"/><em>Answer</em>: There does not appear to be an efficient way to certify that a purported solution is the best possible (even though we could use binary search on the search version of the problem to find the best solution).</p>
<p><a id="ch06qa1q61"/><strong>6.61</strong> Suppose that <em>X</em> and <em>Y</em> are two search problems an that <em>X</em> poly-time reduces to <em>Y</em>. Which of the following can we infer?</p>
<p class="indenthangingN"><em>a.</em> If <em>Y</em> is <strong><em>NP</em></strong>-complete then so is <em>X</em>.</p>
<p class="indenthangingN"><em>b.</em> If <em>X</em> is <strong><em>NP</em></strong>-complete then so is <em>Y</em>.</p>
<p class="indenthangingN"><em>c.</em> If <em>X</em> is in <strong><em>P</em></strong>, then <em>Y</em> is in <strong><em>P</em></strong>.</p>
<p class="indenthangingN"><em>d.</em> If <em>Y</em> is in <strong><em>P</em></strong>, then <em>X</em> is in <strong><em>P</em></strong>.</p>
<p><a id="ch06qa1q62"/><strong>6.62</strong> Suppose that <strong>P ≠ NP</strong>. Which of the following can we infer?</p>
<p class="indenthangingN"><em>e.</em> If <em>X</em> is <strong><em>NP</em></strong>-complete, then <em>X</em> cannot be solved in polynomial time.</p>
<p class="indenthangingN"><em>f.</em> If <em>X</em> is in <strong><em>NP</em></strong>, then <em>X</em> cannot be solved in polynomial time.</p>
<p class="indenthangingN"><em>g.</em> If <em>X</em> is in <strong><em>NP</em></strong> but not <strong><em>NP</em></strong>-complete, then <em>X</em> can be solved in polynomial time.</p>
<p class="indenthangingN"><em>h.</em> If <em>X</em> is in <strong><em>P</em></strong>, then <em>X</em> is not <strong><em>NP</em></strong>-complete.</p>
</body>
</html>