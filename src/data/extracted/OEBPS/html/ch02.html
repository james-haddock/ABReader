<?xml version="1.0" encoding="UTF-8" standalone="no"?><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Two. Sorting</title>
<link href="9780132762564.css" rel="stylesheet" type="text/css"/>
<link href="page-template.xpgt" rel="stylesheet" type="application/vnd.adobe-page-template+xml"/>
<meta name="Adept.resource" value="urn:uuid:7baf5dbb-ffe1-4201-87bd-b993ed04f947"/>
</head>
<body>
<p><a id="ch02"/></p>
<h2><a id="page_242"/>Two. Sorting</h2>
<div class="sidebar">
<hr/>
<p><a id="ch02sb01"/></p>
<p class="indenthangingN"><strong><a href="#ch02sec1lev1">2.1</a></strong> <a href="#ch02sec1lev1">Elementary Sorts</a> <a href="#ch02sec1lev1">244</a></p>
<p class="indenthangingN"><strong><a href="#ch02sec1lev2">2.2</a></strong> <a href="#ch02sec1lev2">Mergesort</a> <a href="#ch02sec1lev2">270</a></p>
<p class="indenthangingN"><strong><a href="#ch02sec1lev3">2.3</a></strong> <a href="#ch02sec1lev3">Quicksort</a> <a href="#ch02sec1lev3">288</a></p>
<p class="indenthangingN"><strong><a href="#ch02sec1lev4">2.4</a></strong> <a href="#ch02sec1lev4">Priority Queues</a> <a href="#ch02sec1lev4">308</a></p>
<p class="indenthangingN"><strong><a href="#ch02sec1lev5">2.5</a></strong> <a href="#ch02sec1lev5">Applications</a> <a href="#ch02sec1lev5">336</a></p>
<hr/>
</div>
<p><a id="page_243"/>Sorting is the process of rearranging a sequence of objects so as to put them in some logical order. For example, your credit card bill presents transactions in order by date—they were likely put into that order by a sorting algorithm. In the early days of computing, the common wisdom was that up to 30 percent of all computing cycles was spent sorting. If that fraction is lower today, one likely reason is that sorting algorithms are relatively efficient, not that sorting has diminished in relative importance. Indeed, the ubiquity of computer usage has put us awash in data, and the first step to organizing data is often to sort it. All computer systems have implementations of sorting algorithms, for use by the system and by users.</p>
<p>There are three practical reasons for you to study sorting algorithms, even though you might just use a system sort:</p>
<p class="indenthangingB">• Analyzing sorting algorithms is a thorough introduction to the approach that we use to compare algorithm performance throughout the book.</p>
<p class="indenthangingB">• Similar techniques are effective in addressing other problems.</p>
<p class="indenthangingB">• We often use sorting algorithms as a starting point to solve other problems.</p>
<p>More important than these practical reasons is that the algorithms are elegant, classic, and effective.</p>
<p>Sorting plays a major role in commercial data processing and in modern scientific computing. Applications abound in transaction processing, combinatorial optimization, astrophysics, molecular dynamics, linguistics, genomics, weather prediction, and many other fields. Indeed, a sorting algorithm (quicksort, in <a href="#ch02sec1lev3"><small>SECTION 2.3</small></a>) was named as one of the top ten algorithms for science and engineering of the 20th century.</p>
<p>In this chapter, we consider several classical sorting methods and an efficient implementation of a fundamental data type known as the priority queue. We discuss the theoretical basis for comparing sorting algorithms and conclude the chapter with a survey of applications of sorting and priority queues.</p>
<p><a id="ch02sec1lev1"/></p>
<h3><a id="page_244"/>2.1 Elementary Sorts</h3>
<p><small>FOR OUR FIRST EXCURSION</small> into the area of sorting algorithms, we shall study two elementary sorting methods and a variation of one of them. Among the reasons for studying these relatively simple algorithms in detail are the following: First, they provide context in which we can learn terminology and basic mechanisms. Second, these simple algorithms are more effective in some applications than the sophisticated algorithms that we shall discuss later. Third, they are useful in improving the efficiency of more sophisticated algorithms, as we will see.</p>
<p><a id="ch02sec2lev1"/></p>
<h4>Rules of the game</h4>
<p>Our primary concern is algorithms for rearranging <em>arrays of items</em> where each item contains a <em>key</em>. The objective of the sorting algorithm is to rearrange the items such that their keys are ordered according to some well-defined ordering rule (usually numerical or alphabetical order). We want to rearrange the array so that each entry’s key is no smaller than the key in each entry with a lower index and no larger than the key in each entry with a larger index. Specific characteristics of the keys and the items can vary widely across applications. In Java, items are just objects, and the abstract notion of a key is captured in a built-in mechanism—the <code>Comparable</code> interface—that is described on page <a href="#page_247">247</a>.</p>
<p>The class <code>Example</code> on the facing page illustrates the conventions that we shall use: we put our sort code in a <code>sort()</code> method within a single class along with private helper functions <code>less()</code> and <code>exch()</code> (and perhaps some others) and a sample client <code>main()</code>. <code>Example</code> also illustrates code that might be useful for initial debugging: its test client <code>main()</code> sorts strings from standard input using the private method <code>show()</code> to print the contents of the array. Later in this chapter, we will examine various test clients for comparing algorithms and for studying their performance. To differentiate sorting methods, we give our various sort classes different names. Clients can call different implementations by name: <code>Insertion.sort()</code>, <code>Merge.sort()</code>, <code>Quick.sort()</code>, and so forth.</p>
<p>With but a few exceptions, our sort code refers to the data only through two operations: the method <code>less()</code> that compares items and the method <code>exch()</code> that exchanges them. The <code>exch()</code> method is easy to implement, and the <code>Comparable</code> interface makes it easy to implement <code>less()</code>. Restricting data access to these two operations makes our code readable and portable, and makes it easier for us certify that algorithms are correct, to study performance and to compare algorithms. Before proceeding to consider sort implementations, we discuss a number of important issues that need to be carefully considered for every sort.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb02"/></p>
<h3><a id="page_245"/>Template for sort classes</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0245-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0245-02.jpg"/></p>
<p>This class illustrates our conventions for implementing array sorts. For each sorting algorithm that we consider, we present a <code>sort()</code> method for a class like this with <code>Example</code> changed to a name that corresponds to the algorithm. The test client sorts strings taken from standard input, but, with this code, our sort methods are effective for any type of data that implements <code>Comparable</code>.</p>
<hr/>
</div>
<p><a id="ch02sec3lev1"/></p>
<h5><a id="page_246"/><em>Certification</em></h5>
<p>Does the sort implementation always put the array in order, no matter what the initial order? As a conservative practice, we include the statement <code>assert isSorted(a);</code> in our test client to certify that array entries are in order after the sort. It is reasonable to include this statement in <em>every</em> sort implementation, even though we normally test our code and develop mathematical arguments that our algorithms are correct. Note that this test is sufficient only if we use <code>exch()</code> exclusively to change array entries. When we use code that stores values into the array directly, we do not have full assurance (for example, code that destroys the original input array by setting all values to be the same would pass this test).</p>
<p><a id="ch02sec3lev2"/></p>
<h5><em>Running time</em></h5>
<p>We also test algorithm <em>performance</em>. We start by proving facts about the number of basic operations (compares and exchanges, or perhaps the number of times the array is accessed, for read or write) that the various sorting algorithms perform for various natural input models. Then we use these facts to develop hypotheses about the comparative performance of the algorithms and present tools that you can use to experimentally check the validity of such hypotheses. We use a consistent coding style to facilitate the development of valid hypotheses about performance that will hold true for typical implementations.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb03"/></p>
<p><strong>Searching cost model.</strong> When studying sorting algorithms, we count <em>compares</em> and <em>exchanges.</em> For algorithms that do not use exchanges, we count <em>array accesses.</em></p>
<hr/>
</div>
<p><a id="ch02sec3lev3"/></p>
<h5><em>Extra memory</em></h5>
<p>The amount of extra memory used by a sorting algorithm is often as important a factor as running time. The sorting algorithms divide into two basic types: those that sort <em>in place</em> and use no extra memory except perhaps for a small function-call stack or a constant number of instance variables, and those that need enough extra memory to hold another copy of the array to be sorted.</p>
<p><a id="ch02sec3lev4"/></p>
<h5><em>Types of data</em></h5>
<p>Our sort code is effective for any item type that implements the <code>Comparable</code> interface. Adhering to Java’s convention in this way is convenient because many of the types of data that you might want to sort implement <code>Comparable</code>. For example, Java’s numeric wrapper types such as <code>Integer</code> and <code>Double</code> implement <code>Comparable</code>, as do <code>String</code> and various advanced types such as <code>File</code> or <code>URL</code>. Thus, you can just call one of our sort methods with an array of any of these types as argument. For example, the code at right uses quicksort (see <a href="#ch02sec1lev3"><small>SECTION 2.3</small></a>) to sort <code>N</code> random <code>Double</code> values. When we create types of our own, we can enable client code to sort that type of data by implementing the <code>Comparable</code> interface. To do so, we just need to implement a <code>compareTo()</code> method that defines an ordering on objects of that type known as the <em>natural <a id="page_247"/>order</em> for that type, as shown here for our <code>Date</code> data type (see page <a href="ch01.html#page_91">91</a>). Java’s convention is that the call <code>v.compareTo(w)</code> returns an integer that is negative, zero, or positive (usually <code>-1</code>, <code>0</code>, or <code>+1</code>) when <code>v &lt; w</code>, <code>v = w</code>, or <code>v &gt; w</code>, respectively. For economy, we use standard notation like <code>v&gt;w</code> as shorthand for code like <code>v.compareTo(w)&gt;0</code> for the remainder of this paragraph. By convention, <code>v.compareTo(w)</code> throws an exception if <code>v</code> and <code>w</code> are incompatible types or either is <code>null</code>. Furthermore, <code>compareTo()</code> must implement a <em>total order</em>: it must be</p>
<p class="indenthangingB">• <em>Reflexive</em> (for all <code>v</code>, <code>v = v</code>)</p>
<p class="indenthangingB">• <em>Antisymmetric</em> (for all <code>v</code> and <code>w</code>, if <code>v &lt; w</code> then <code>w &gt; v</code> and if <code>v = w</code> then <code>w = v</code>)</p>
<p class="indenthangingB">• <em>Transitive</em> (for all <code>v</code>, <code>w</code>, and <code>x</code>, if <code>v &lt;= w</code> and <code>w &lt;= x</code> then <code>v &lt;=x</code>)</p>
<p class="image"><img alt="image" src="graphics/p0246-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0247-01.jpg"/></p>
<p>These rules are intuitive and standard in mathematics—you will have little difficulty adhering to them. In short, <code>compareTo()</code> implements our <em>key</em> abstraction—it defines the ordering of the items (objects) to be sorted, which can be any type of data that implements <code>Comparable</code>. Note that <code>compareTo()</code> need not use all of the instance variables. Indeed, the key might be a small part of each item.</p>
<p><small>FOR THE REMAINDER OF THIS CHAPTER</small>, we shall address numerous algorithms for sorting arrays of objects having a natural order. To compare and contrast the algorithms, we shall examine a number of their properties, including the number of compares and exchanges that they use for various types of inputs and the amount of extra memory that they use. These properties lead to the development of hypotheses about performance properties, many of which have been validated on countless computers over the past several decades. Specific implementations always need to be checked, so we also consider tools for doing so. After considering the classic selection sort, insertion sort, shellsort, mergesort, quicksort, and heapsort algorithms, we will consider practical issues and applications, in <a href="#ch02sec1lev5"><small>SECTION 2.5</small></a>.</p>
<p><a id="ch02sec2lev2"/></p>
<h4><a id="page_248"/>Selection sort</h4>
<p>One of the simplest sorting algorithms works as follows: First, find the smallest item in the array and exchange it with the first entry (itself if the first entry is already the smallest). Then, find the next smallest item and exchange it with the second entry. Continue in this way until the entire array is sorted. This method is called <em>selection sort</em> because it works by repeatedly selecting the smallest remaining item.</p>
<p>As you can see from the implementation in <a href="#ch02sb05"><small>ALGORITHM 2.1</small></a>, the inner loop of selection sort is just a compare to test a current item against the smallest item found so far (plus the code necessary to increment the current index and to check that it does not exceed the array bounds); it could hardly be simpler. The work of moving the items around falls outside the inner loop: each exchange puts an item into its final position, so the number of exchanges is <em>N</em>. Thus, the running time is dominated by the number of compares.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb04"/></p>
<p><strong>Proposition A.</strong> Selection sort uses ~<em>N</em><sup>2</sup>/2 compares and <em>N</em> exchanges to sort an array of length <em>N</em>.</p>
<p><strong>Proof:</strong> You can prove this fact by examining the trace, which is an <em>N</em>-by-<em>N</em> table in which unshaded letters correspond to compares. About one-half of the entries in the table are unshaded—those on and above the diagonal. The entries on the diagonal each correspond to an exchange. More precisely, examination of the code reveals that, for each <em>i</em> from 0 to <em>N</em> − 1, there is one exchange and <em>N</em> − 1 − <em>i</em> compares, so the totals are <em>N</em> exchanges and (<em>N</em> − 1) + (<em>N</em> − 2) + . . . + 2 + 1+ 0 = <em>N</em>(<em>N</em> − 1) / 2 ~ <em>N</em><sup>2</sup> / 2 compares.</p>
<hr/>
</div>
<p><small>IN SUMMARY</small>, selection sort is a simple sorting method that is easy to understand and to implement and is characterized by the following two signature properties:</p>
<p><a id="ch02sec3lev5"/></p>
<h5><em>Running time is insensitive to input</em></h5>
<p>The process of finding the smallest item on one pass through the array does not give much information about where the smallest item might be on the next pass. This property can be disadvantageous in some situations. For example, the person using the sort client might be surprised to realize that it takes about as long to run selection sort for an array that is already in order or for an array with all keys equal as it does for a randomly-ordered array! As we shall see, other algorithms are better able to take advantage of initial order in the input.</p>
<p><a id="ch02sec3lev6"/></p>
<h5><em>Data movement is minimal</em></h5>
<p>Each of the <em>N</em> exchanges changes the value of two array entries, so selection sort uses <em>N</em> exchanges—the number of array accesses is a <em>linear</em> function of the array size. None of the other sorting algorithms that we consider have this property (most involve linearithmic or quadratic growth).</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb05"/></p>
<h3><a id="page_249"/>Algorithm 2.1 Selection sort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0249-01.jpg"/></p>
<p>For each <code>i</code>, this implementation puts the <code>i</code>th smallest item in <code>a[i]</code>. The entries to the left of position <code>i</code> are the <code>i</code> smallest items in the array and are not examined again.</p>
<p class="image"><img alt="image" src="graphics/02_01-selection.jpg"/></p>
<hr/>
</div>
<p><a id="ch02sec2lev3"/></p>
<h4><a id="page_250"/>Insertion sort</h4>
<p>The algorithm that people often use to sort bridge hands is to consider the cards one at a time, inserting each into its proper place among those already considered (keeping them sorted). In a computer implementation, we need to make space to insert the current item by moving larger items one position to the right, before inserting the current item into the vacated position. <a href="#ch02sb07"><small>ALGORITHM 2.2</small></a> is an implementation of this method, which is called <em>insertion sort</em>.</p>
<p>As in selection sort, the items to the left of the current index are in sorted order during the sort, but they are not in their final position, as they may have to be moved to make room for smaller items encountered later. The array is, however, fully sorted when the index reaches the right end.</p>
<p>Unlike that of selection sort, the running time of insertion sort depends on the initial order of the items in the input. For example, if the array is large and its entries are already in order (or nearly in order), then insertion sort is much, much faster than if the entries are randomly ordered or in reverse order.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb06"/></p>
<p><strong>Proposition B.</strong> Insertion sort uses ~<em>N</em><sup>2</sup>/4 compares and ~<em>N</em><sup>2</sup>/4 exchanges to sort a randomly ordered array of length <em>N</em> with distinct keys, on the average. The worst case is ~<em>N</em><sup>2</sup>/2 compares and ~<em>N</em><sup>2</sup>/2 exchanges and the best case is <em>N</em> − 1 compares and 0 exchanges.</p>
<p><strong>Proof:</strong> Just as for <a href="#ch02sb04"><small>PROPOSITION A</small></a>, the number of compares and exchanges is easy to visualize in the <em>N</em>-by-<em>N</em> diagram that we use to illustrate the sort. We count entries below the diagonal—all of them, in the worst case, and none of them, in the best case. For randomly ordered arrays, we expect each item to go about halfway back, on the average, so we count one-half of the entries below the diagonal.</p>
<p>The number of compares is the number of exchanges plus an additional term equal to <em>N</em> minus the number of times the item inserted is the smallest so far. In the worst case (array in reverse order), this term is negligible in relation to the total; in the best case (array in order) it is equal to <em>N</em> − 1.</p>
<hr/>
</div>
<p>Insertion sort works well for certain types of nonrandom arrays that often arise in practice, even if they are huge. For example, as just mentioned, consider what happens when you use insertion sort on an array that is already sorted. Each item is immediately determined to be in its proper place in the array, and the total running time is linear. (The running time of selection sort is quadratic for such an array.) The same is true for arrays whose keys are all equal (hence the condition in <a href="#ch02sb06"><small>PROPOSITION B</small></a> that the keys must be distinct).</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb07"/></p>
<h3><a id="page_251"/>Algorithm 2.2 Insertion sort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0251-01.jpg"/></p>
<p>For each <code>i</code> from <code>0</code> to <code>N-1</code>, exchange <code>a[i]</code> with the entries that are smaller in <code>a[0]</code> through <code>a[i-1]</code>. As the index <code>i</code> travels from left to right, the entries to its left are in sorted order in the array, so the array is fully sorted when <code>i</code> reaches the right end.</p>
<p class="image"><img alt="image" src="graphics/02_02-insertion.jpg"/></p>
<hr/>
</div>
<p><a id="page_252"/>More generally, we consider the concept of a <em>partially sorted</em> array, as follows: An <em>inversion</em> is a pair of entries that are out of order in the array. For instance, <code>E X A M P L E</code> has 11 inversions: <code>E-A</code>, <code>X-A</code>, <code>X-M</code>, <code>X-P</code>, <code>X-L</code>, <code>X-E</code>, <code>M-L</code>, <code>M-E</code>, <code>P-L</code>, <code>P-E</code>, and <code>L-E</code>. If the number of inversions in an array is less than a constant multiple of the array size, we say that the array is <em>partially sorted</em>. Typical examples of partially sorted arrays are the following:</p>
<p class="indenthangingB">• An array where each entry is not far from its final position</p>
<p class="indenthangingB">• A small array appended to a large sorted array</p>
<p class="indenthangingB">• An array with only a few entries that are not in place</p>
<p>Insertion sort is an efficient method for such arrays; selection sort is not. Indeed, when the number of inversions is low, insertion sort is likely to be faster than any sorting method that we consider in this chapter.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb08"/></p>
<p><strong>Proposition C.</strong> The number of exchanges used by insertion sort is equal to the number of inversions in the array, and the number of compares is at least equal to the number of inversions and at most equal to the number of inversions plus the array size minus 1.</p>
<p><strong>Proof:</strong> Every exchange involves two inverted adjacent entries and thus reduces the number of inversions by one, and the array is sorted when the number of inversions reaches zero. Every exchange corresponds to a compare, and an additional compare might happen for each value of <code>i</code> from <code>1</code> to <code>N-1</code> (when <code>a[i]</code> does not reach the left end of the array).</p>
<hr/>
</div>
<p>It is not difficult to speed up insertion sort substantially, by shortening its inner loop to move the larger entries to the right one position rather than doing full exchanges (thus cutting the number of array accesses in half). We leave this improvement for an exercise (see <a href="#ch02qa1q25"><small>EXERCISE 2.1.25</small></a>).</p>
<p><small>IN SUMMARY</small>, insertion sort is an excellent method for partially sorted arrays and is also a fine method for tiny arrays. These facts are important not just because such arrays frequently arise in practice, but also because both types of arrays arise in intermediate stages of advanced sorting algorithms, so we will be considering insertion sort again in relation to such algorithms.</p>
<p><a id="ch02sec2lev4"/></p>
<h4><a id="page_253"/>Visualizing sorting algorithms</h4>
<p>Throughout this chapter, we will be using a simple visual representation to help describe the properties of sorting algorithms. Rather than tracing the progress of a sort with key values such as letters, numbers, or words, we use vertical bars, to be sorted by their heights. The advantage of such a representation is that it can give insights into the behavior of a sorting method.</p>
<p class="image"><img alt="image" src="graphics/02_03-insertselectbars.jpg"/></p>
<p>For example, you can see at a glance on the visual traces at right that insertion sort does not touch entries to the right of the scan pointer and selection sort does not touch entries to the left of the scan pointer. Moreover, it is clear from the visual traces that, since insertion sort also does not touch entries smaller than the inserted item, it uses about half the number of compares as selection sort, on the average.</p>
<p>With our <code>StdDraw</code> library, developing a visual trace is not much more difficult than doing a standard trace. We sort <code>Double</code> values, instrument the algorithm to call <code>show()</code> as appropriate (just as we do for a standard trace), and develop a version of <code>show()</code> that uses <code>StdDraw</code> to draw the bars instead of printing the results. The most complicated task is setting the scale for the <em>y</em>-axis so that the lines of the trace appear in the expected order. You are encouraged to work <a href="#ch02qa1q18"><small>EXERCISE 2.1.18</small></a> in order to gain a better appreciation of the value of visual traces and the ease of creating them.</p>
<p>An even simpler task is to <em>animate</em> the trace so that you can see the array dynamically evolve to the sorted result. Developing an animated trace involves essentially the same process described in the previous paragraph, but without having to worry about the <em>y</em>-axis (just clear the window and redraw the bars each time). Though we cannot make the case on the printed page, such animated representations are also effective in gaining insight into how an algorithm works. You are also encouraged to work <a href="#ch02qa1q17"><small>EXERCISE 2.1.17</small></a> to see for yourself.</p>
<p><a id="ch02sec2lev5"/></p>
<h4><a id="page_254"/>Comparing two sorting algorithms</h4>
<p>Now that we have two implementations, we are naturally interested in knowing which one is faster: selection sort (<a href="#ch02sb05"><small>ALGORITHM 2.1</small></a>) or insertion sort (<a href="#ch02sb07"><small>ALGORITHM 2.2</small></a>). Questions like this arise again and again and again in the study of algorithms and are a major focus throughout this book. We have discussed some fundamental ideas in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, but we use this first case in point to illustrate our basic approach to answering such questions. Generally, following the approach introduced in <a href="ch01a.html#ch01sec1lev6"><small>SECTION 1.4</small></a>, we compare algorithms by</p>
<p class="indenthangingB">• Implementing and debugging them</p>
<p class="indenthangingB">• Analyzing their basic properties</p>
<p class="indenthangingB">• Formulating a hypothesis about comparative performance</p>
<p class="indenthangingB">• Running experiments to validate the hypothesis</p>
<p>These steps are nothing more than the time-honored <em>scientific method</em>, applied to the study of algorithms.</p>
<p>In the present context, <a href="#ch02sb05"><small>ALGORITHM 2.1</small></a> and <a href="#ch02sb07"><small>ALGORITHM 2.2</small></a> are evidence of the first step; <a href="#ch02sb04"><small>PROPOSITIONS A</small></a>, <a href="#ch02sb06">B</a>, and <a href="#ch02sb08">C</a> constitute the second step; <a href="#ch02sb09"><small>PROPERTY D</small></a> on page <a href="#ch02sb09">255</a> constitutes the third step; and the class <code>SortCompare</code> on page <a href="#ch02sb10">256</a> enables the fourth step. These activities are all interrelated.</p>
<p>Our brief descriptions mask a substantial amount of effort that is required to properly implement, analyze, and test algorithms. Every programmer knows that such code is the product of a long round of debugging and refinement, every mathematician knows that proper analysis can be very difficult, and every scientist knows that formulating hypotheses and designing and executing experiments to validate them require great care. Full development of such results is reserved for experts studying our most important algorithms, but every programmer using an algorithm should be aware of the scientific context underlying its performance properties.</p>
<p>Having developed implementations, our next choice is to settle on an appropriate model for the input. For sorting, a natural model, which we have used for <a href="#ch02sb04"><small>PROPOSITIONS A</small></a>, <a href="#ch02sb06">B</a>, and <a href="#ch02sb08">C</a>, is to assume that the arrays are randomly ordered <em>and</em> that the key values are distinct. In applications where significant numbers of equal key values are present we will need a more complicated model.</p>
<p>How do we formulate a hypothesis about the running times of insertion sort and selection sort for randomly ordered arrays? Examining <a href="#ch02sb05"><small>ALGORITHMS 2.1</small></a> and <a href="#ch02sb07">2.2</a> and <a href="#ch02sb04"><small>PROPOSITIONS A</small></a> and <a href="#ch02sb06"><small>B</small></a>, it follows immediately that the running time of both algorithms should be quadratic for randomly ordered arrays. That is, the running time of insertion sort for such an input is proportional to some small constant times <em>N</em><sup>2</sup> and the running time of selection sort is proportional to some other small constant times <em>N</em><sup>2</sup>. The values of the two constants depend on the cost of compares and exchanges on the particular computer being used. For many types of data and for typical computers, it is reasonable <a id="page_255"/>to assume that these costs are similar (though we will see a few significant exceptions). The following hypothesis follows directly:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb09"/></p>
<p><strong>Property D.</strong> The running times of insertion sort and selection sort are quadratic and within a small constant factor of one another for randomly ordered arrays of distinct values.</p>
<p><strong>Evidence:</strong> This statement has been validated on many different computers over the past half-century. Insertion sort was about twice as fast as selection sort when the first edition of this book was written in 1980 and it still is today, even though it took several hours to sort 100,000 items with these algorithms then and just several seconds today. Is insertion sort a bit faster than selection sort on your computer? To find out, you can use the class <code>SortCompare</code> on the next page, which uses the <code>sort()</code> methods in the classes named as command-line arguments to perform the given number of experiments (sorting arrays of the given size) and prints the ratio of the observed running times of the algorithms.</p>
<hr/>
</div>
<p>To validate this hypothesis, we use <code>SortCompare</code> (see page <a href="#ch02sb10">256</a>) to perform the experiments. As usual, we use <code>Stopwatch</code> to compute the running time. The implementation of <code>time()</code> shown here does the job for the basic sorts in this chapter. The “randomly ordered” input model is embedded in the <code>timeRandomInput()</code> method in <code>SortCompare</code>, which generates random <code>Double</code> values, sorts them, and returns the total measured time of the sort for the given number of trials. Using random <code>Double</code> values between <code>0.0</code> and <code>1.0</code> is much simpler than the alternative of using a library function such as <code>StdRandom.shuffle()</code> and is effective because equal key values are very unlikely (see <a href="#ch02qa5q31"><small>EXERCISE 2.5.31</small></a>). As discussed in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, the number of trials is taken as an argument both to take advantage of the law of large numbers (the more trials, the total running time divided by the number of trials is a more accurate estimate of the true average running time) and to help damp out system effects. You are encouraged to experiment with <code>SortCompare</code> <a id="page_257"/>on your computer to learn the extent to which its conclusion about insertion sort and selection sort is robust.</p>
<p class="image"><img alt="image" src="graphics/p0255-01.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb10"/></p>
<h3><a id="page_256"/>Comparing two sorting algorithms</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0256-01.jpg"/></p>
<p>This client runs the two sorts named in the first two command-line arguments on arrays of <code>N</code> (the third command-line argument) random <code>Double</code> values between <code>0.0</code> and <code>1.0</code>, repeating the experiment <code>T</code> (the fourth command-line argument) times, then prints the ratio of the total running times.</p>
<p class="image"><img alt="image" src="graphics/p0256-02.jpg"/></p>
<hr/>
</div>
<p><a href="#ch02sb09">PROPERTY D</a> is intentionally a bit vague—the value of the small constant factor is left unstated and the assumption that the costs of compares and exchanges are similar is left unstated—so that it can apply in a broad variety of situations. When possible, we try to capture essential aspects of the performance of each of the algorithms that we study in statements like this. As discussed in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>, each <em>Property</em> that we consider needs to be tested scientifically in a given situation, perhaps supplemented with a more refined hypothesis based upon a related <em>Proposition</em> (mathematical truth).</p>
<p>For practical applications, there is one further step, which is crucial: <em>run experiments to validate the hypothesis on the data at hand</em>. We defer consideration of this step to <a href="#ch02sec1lev5"><small>SECTION 2.5</small></a> and the exercises. In this case, if your sort keys are not distinct and/or not randomly ordered, <a href="#ch02sb09">PROPERTY D</a> might not hold. You can randomly order an array with <code>StdRandom.shuffle()</code>, but applications with significant numbers of equal keys involve more careful analysis.</p>
<p>Our discussions of the analyses of algorithms are intended to be starting points, not final conclusions. If some other question about performance of the algorithms comes to mind, you can study it with a tool like <code>SortCompare</code>. Many opportunities to do so are presented in the exercises.</p>
<p><small>WE DO NOT DWELL</small> further on the comparative performance of insertion sort and selection sort because we are much more interested in algorithms that can run a hundred or a thousand or a million times faster than either. Still, understanding these elementary algorithms is worthwhile for several reasons:</p>
<p class="indenthangingB">• They help us work out the ground rules.</p>
<p class="indenthangingB">• They provide performance benchmarks.</p>
<p class="indenthangingB">• They often are the method of choice in some specialized situations.</p>
<p class="indenthangingB">• They can serve as the basis for developing better algorithms.</p>
<p>For these reasons, we will use the same basic approach and consider elementary algorithms for every problem that we study throughout this book, not just sorting. Programs like <code>SortCompare</code> play a critical role in this incremental approach to algorithm development. At every step along the way, we can use such a program to help evaluate whether a new algorithm or an improved version of a known algorithm provides the performance gains that we expect.</p>
<p><a id="ch02sec2lev6"/></p>
<h4><a id="page_258"/>Shellsort</h4>
<p>To exhibit the value of knowing properties of elementary sorts, we next consider a fast algorithm based on insertion sort. Insertion sort is slow for large unordered arrays because the only exchanges it does involve adjacent entries, so items can move through the array only one place at a time. For example, if the item with the smallest key happens to be at the end of the array, <em>N</em>−1 exchanges are needed to get that one item where it belongs. <em>Shellsort</em> is a simple extension of insertion sort that gains speed by allowing exchanges of array entries that are far apart, to produce partially sorted arrays that can be efficiently sorted, eventually by insertion sort.</p>
<p>The idea is to rearrange the array to give it the property that taking every <em>h</em>th entry (starting anywhere) yields a sorted subsequence. Such an array is said to be <em>h</em>-sorted. Put another way, an <em>h</em>-sorted array is <em>h</em> independent sorted subsequences, interleaved together. By <em>h</em>-sorting for some large values of <em>h</em>, we can move items in the array long distances and thus make it easier to <em>h</em>-sort for smaller values of <em>h</em>. Using such a procedure for any sequence of values of <em>h</em> that ends in 1 will produce a sorted array: that is shellsort. The implementation in <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> on the facing page uses the sequence of decreasing values ½(3<em><sup>k</sup></em>−1), starting at the largest increment less than <em>N</em>/3 and decreasing to 1. We refer to such a sequence as an <em>increment sequence</em>. <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> computes its increment sequence; another alternative is to store an increment sequence in an array.</p>
<p class="image"><img alt="image" src="graphics/02_04-shell3sorted.jpg"/></p>
<p>One way to implement shellsort would be, for each <em>h</em>, to use insertion sort independently on each of the <em>h</em> subsequences. Because the subsequences are independent, we can use an even simpler approach: when <em>h</em>-sorting the array, we insert each item among the previous items in its <em>h</em>-subsequence by exchanging it with those that have larger keys (moving them each one position to the right in the subsequence). We accomplish this task by using the insertion-sort code, but modified to decrement by <em>h</em> instead of 1 when moving through the array. This observation reduces the shellsort implementation to an insertion-sort-like pass through the array for each increment.</p>
<p>Shellsort gains efficiency by making a tradeoff between size and partial order in the subsequences. At the beginning, the subsequences are short; later in the sort, the subsequences are partially sorted. In both cases, insertion sort is the method of choice. The extent to which the subsequences are partially sorted is a variable factor that depends strongly on the increment sequence. Understanding shellsort’s performance is a challenge. Indeed, <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> is the only sorting method we consider whose performance on randomly ordered arrays has not been precisely characterized.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb11"/></p>
<h3><a id="page_259"/>Algorithm 2.3 Shellsort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0259-01.jpg"/></p>
<p>If we modify insertion sort (<a href="#ch02sb07"><small>ALGORITHM 2.2</small></a>) to <code>h</code>-sort the array and add an outer loop to decrease <code>h</code> through a sequence of increments starting at an increment as large as a constant fraction of the array length and ending at <code>1</code>, we are led to this compact shellsort implementation.</p>
<p class="image"><img alt="image" src="graphics/p0259-02.jpg"/></p>
<p class="image"><img alt="image" src="graphics/02_05-shelloverview.jpg"/></p>
<hr/>
</div>
<p><a id="page_260"/>How do we decide what increment sequence to use? In general, this question is a difficult one to answer. The performance of the algorithm depends not just on the number of increments, but also on arithmetical interactions among the increments such as the size of their common divisors and other properties. Many different increment sequences have been studied in the literature, but no provably best sequence has been found. The increment sequence that is used in <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> is easy to compute and use, and performs nearly as well as more sophisticated increment sequences that have been discovered that have provably better worst-case performance. Increment sequences that are substantially better still may be waiting to be discovered.</p>
<p class="image"><img alt="image" src="graphics/02_06-shell.jpg"/></p>
<p>Shellsort is useful even for large arrays, particularly by contrast with selection sort and insertion sort. It also performs well on arrays that are in arbitrary order (not necessarily random). Indeed, constructing an array for which shellsort runs slowly for a particular increment sequence is usually a challenging exercise.</p>
<p>As you can learn with <code>SortCompare</code>, shellsort is much faster than insertion sort and selection sort, and its speed advantage increases with the array size. Before reading further, try using <code>SortCompare</code> to compare shellsort with insertion sort and selection sort for array sizes that are increasing powers of 2 on your computer (see <a href="#ch02qa1q27"><small>EXERCISE 2.1.27</small></a>). You will see that shellsort makes it possible to address sorting <a id="page_261"/>problems that could not be addressed with the more elementary algorithms. This example is our first practical illustration of an important principle that pervades this book: <em>achieving speedups that enable the solution of problems that could not otherwise be solved is one of the prime reasons to study algorithm performance and design.</em></p>
<p class="image"><img alt="image" src="graphics/02_07-shellbars.jpg"/></p>
<p>The study of the performance characteristics of shellsort requires mathematical arguments that are beyond the scope of this book. If you want to be convinced, start by thinking about how you would prove the following fact: <em>when an h-sorted array is k-sorted, it remains h-sorted</em>. As for the performance of <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a>, the most important result in the present context is the knowledge that <em>the running time of shellsort is not necessarily quadratic</em>—for example, it is known that the worst-case number of compares for <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> is proportional to <em>N</em><sup>3/2</sup>. That such a simple modification <a id="page_262"/>can break the quadratic-running-time barrier is quite interesting, as doing so is a prime goal for many algorithm design problems.</p>
<p>No mathematical results are available about the average-case number of compares for shellsort for randomly ordered input. Increment sequences have been devised that drive the asymptotic growth of the worst-case number of compares down to <em>N</em><sup>4/3</sup>, <em>N</em><sup>5/4</sup>, <em>N</em><sup>6/5</sup>, . . ., but many of these results are primarily of academic interest because these functions are hard to distinguish from one another (and from a constant factor of <em>N</em>) for practical values of <em>N</em>.</p>
<p>In practice, you can safely take advantage of the past scientific study of shellsort just by using the increment sequence in <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> (or one of the increment sequences in the exercises at the end of this section, which may improve performance by 20 to 40 percent). Moreover, you can easily validate the following hypothesis:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb12"/></p>
<p><strong>Property E.</strong> The number of compares used by shellsort with the increments 1, 4, 13, 40, 121, 364, . . . is bounded by a small multiple of <em>N</em> times the number of increments used.</p>
<p><strong>Evidence:</strong> Instrumenting <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> to count compares and divide by the number of increments used is a straightforward exercise (see <a href="#ch02qa1q12"><small>EXERCISE 2.1.12</small></a>). Extensive experiments suggest that the average number of compares per increment might be <em>N</em><sup>1/5</sup>, but it is quite difficult to discern the growth in that function unless <em>N</em> is huge. This property also seems to be rather insensitive to the input model.</p>
<hr/>
</div>
<p><small>EXPERIENCED PROGRAMMERS</small> sometimes choose shellsort because it has acceptable running time even for moderately large arrays; it requires a small amount of code; and it uses no extra space. In the next few sections, we shall see methods that are more efficient, but they are perhaps only twice as fast (if that much) except for very large <em>N</em>, and they are more complicated. If you need a solution to a sorting problem, and are working in a situation where a system sort may not be available (for example, code destined for hardware or an embedded system), you can safely use shellsort, then determine sometime later whether it will be worthwhile to replace it with a more sophisticated method.</p>
<p><a id="ch02sec2lev7"/></p>
<h4><a id="page_263"/>Q&amp;A</h4>
<p><strong>Q.</strong> Sorting seems like a toy problem. Aren’t many of the other things that we do with computers much more interesting?</p>
<p><strong>A.</strong> Perhaps, but many of those interesting things are <em>made possible</em> by fast sorting algorithms. You will find many examples in <a href="#ch02sec1lev5"><small>SECTION 2.5</small></a> and throughout the rest of the book. Sorting is worth studying now because the problem is easy to understand, and you can appreciate the ingenuity behind the faster algorithms.</p>
<p><strong>Q.</strong> Why so many sorting algorithms?</p>
<p><strong>A.</strong> One reason is that the performance of many algorithms depends on the input values, so different algorithms might be appropriate for different applications having different kinds of input. For example, insertion sort is the method of choice for partially sorted or tiny arrays. Other constraints, such as space and treatment of equal keys, also come into play. We will revisit this question in <a href="#ch02sec1lev5"><small>SECTION 2.5</small></a>.</p>
<p><strong>Q.</strong> Why bother using the tiny helper methods <code>less()</code> and <code>exch()</code>?</p>
<p><strong>A.</strong> They are basic abstract operations needed by any sort algorithm, and the code is easier to understand in terms of these abstractions. Moreover, they make the code directly portable to other settings. For example, much of the code in <a href="#ch02sb05"><small>ALGORITHMS 2.1</small></a> and <a href="#ch02sb07">2.2</a> is legal code in several other programming languages. Even in Java, we can use this code as the basis for sorting primitive types (which are not <code>Comparable</code>): simply implement <code>less()</code> with the code <code>v &lt; w</code>.</p>
<p><strong>Q.</strong> When I run <code>SortCompare</code>, I get different values each time that I run it (and those are different from the values in the book). Why?</p>
<p><strong>A.</strong> For starters, you have a different computer from the one we used, not to mention a different operating system, Java runtime, and so forth. All of these differences might lead to slight differences in the machine code for the algorithms. Differences each time that you run it on your computer might be due to other applications that you are running or various other conditions. Running a very large number of trials should dampen the effect. The lesson is that small differences in algorithm performance are difficult to notice nowadays. That is a primary reason that we focus on large ones!</p>
<p><a id="ch02sec2lev8"/></p>
<h4><a id="page_264"/>Exercises</h4>
<p><a id="ch02qa1q1"/><strong>2.1.1</strong> Show, in the style of the example trace with <a href="#ch02sb05"><small>ALGORITHM 2.1</small></a>, how selection sort sorts the array <code>E A S Y Q U E S T I O N</code>.</p>
<p><a id="ch02qa1q2"/><strong>2.1.2</strong> What is the maximum number of exchanges involving any particular item during selection sort? What is the average number of exchanges involving an item?</p>
<p><a id="ch02qa1q3"/><strong>2.1.3</strong> Give an example of an array of <em>N</em> items that maximizes the number of times the test <code>a[j] &lt; a[min]</code> succeeds (and, therefore, <code>min</code> gets updated) during the operation of selection sort (<a href="#ch02sb05"><small>ALGORITHM 2.1</small></a>).</p>
<p><a id="ch02qa1q4"/><strong>2.1.4</strong> Show, in the style of the example trace with <a href="#ch02sb07"><small>ALGORITHM 2.2</small></a>, how insertion sort sorts the array <code>E A S Y Q U E S T I O N</code>.</p>
<p><a id="ch02qa1q5"/><strong>2.1.5</strong> For each of the two conditions in the inner <code>for</code> loop in insertion sort (<a href="#ch02sb07"><small>ALGORITHM 2.2</small></a>), describe an array of <em>N</em> items where that condition is always false when the loop terminates.</p>
<p><a id="ch02qa1q6"/><strong>2.1.6</strong> Which method runs faster for an array with all keys identical, selection sort or insertion sort?</p>
<p><a id="ch02qa1q7"/><strong>2.1.7</strong> Which method runs faster for an array in reverse order, selection sort or insertion sort?</p>
<p><a id="ch02qa1q8"/><strong>2.1.8</strong> Suppose that we use insertion sort on a randomly ordered array where items have only one of three values. Is the running time linear, quadratic, or something in between?</p>
<p><a id="ch02qa1q9"/><strong>2.1.9</strong> Show, in the style of the example trace with <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a>, how shellsort sorts the array <code>E A S Y S H E L L S O R T Q U E S T I O N</code>.</p>
<p><a id="ch02qa1q10"/><strong>2.1.10</strong> Why not use selection sort for <em>h</em>-sorting in shellsort?</p>
<p><a id="ch02qa1q11"/><strong>2.1.11</strong> Implement a version of shellsort that keeps the increment sequence in an array, rather than computing it.</p>
<p><a id="ch02qa1q12"/><strong>2.1.12</strong> Instrument shellsort to print the number of compares divided by the array size for each increment. Write a test client that tests the hypothesis that this number is a small constant, by sorting arrays of random <code>Double</code> values, using array sizes that are increasing powers of 10, starting at 100.</p>
<p><a id="ch02sec2lev9"/></p>
<h4><a id="page_265"/>Creative Problems</h4>
<p><a id="ch02qa1q13"/><strong>2.1.13</strong> <em>Deck sort.</em> Explain how you would put a deck of cards in order by suit (in the order spades, hearts, clubs, diamonds) and by rank within each suit, with the restriction that the cards must be laid out face down in a row, and the only allowed operations are to check the values of two cards and to exchange two cards (keeping them face down).</p>
<p><a id="ch02qa1q14"/><strong>2.1.14</strong> <em>Dequeue sort.</em> Explain how you would sort a deck of cards, with the restriction that the only allowed operations are to look at the values of the top two cards, to exchange the top two cards, and to move the top card to the bottom of the deck.</p>
<p><a id="ch02qa1q15"/><strong>2.1.15</strong> <em>Expensive exchange.</em> A clerk at a shipping company is charged with the task of rearranging a number of large crates in order of the time they are to be shipped out. Thus, the cost of compares is very low (just look at the labels) relative to the cost of exchanges (move the crates). The warehouse is nearly full—there is extra space sufficient to hold any one of the crates, but not two. What sorting method should the clerk use?</p>
<p><a id="ch02qa1q16"/><strong>2.1.16</strong> <em>Certification.</em> Write a <code>check()</code> method that calls <code>sort()</code> for a given array and returns <code>true</code> if <code>sort()</code> puts the array in order <em>and</em> leaves the same set of objects in the array as were there initially, <code>false</code> otherwise. Do not assume that <code>sort()</code> is restricted to move data only with <code>exch()</code>. You may use <code>Arrays.sort()</code> and assume that it is correct.</p>
<p><a id="ch02qa1q17"/><strong>2.1.17</strong> <em>Animation.</em> Add code to <code>Insertion</code> and <code>Selection</code> to make them draw the array contents as vertical bars like the visual traces in this section, redrawing the bars after each pass, to produce an animated effect, ending in a “sorted” picture where the bars appear in order of their height. <em>Hint</em>: Use a client like the one in the text that generates random <code>Double</code> values, insert calls to <code>show()</code> as appropriate in the sort code, and implement a <code>show()</code> method that clears the canvas and draws the bars.</p>
<p><a id="ch02qa1q18"/><strong>2.1.18</strong> <em>Visual trace.</em> Modify your solution to the previous exercise to make <code>Insertion</code> and <code>Selection</code> produce visual traces such as those depicted in this section. <em>Hint</em>: Judicious use of <code>setYscale()</code> makes this problem easy. <em>Extra credit</em>: Add the code necessary to produce red and gray color accents such as those in our figures.</p>
<p><a id="ch02qa1q19"/><strong>2.1.19</strong> <em>Shellsort worst case.</em> Construct an array of 100 elements containing the numbers 1 through 100 for which shellsort, with the increments <code>1 4 13 40</code>, uses as large a number of compares as you can find.</p>
<p><a id="ch02qa1q20"/><strong>2.1.20</strong> <em>Shellsort best case.</em> What is the <em>best</em> case for shellsort? Justify your answer.</p>
<p><a id="page_266"/><a id="ch02qa1q21"/><strong>2.1.21</strong> <em>Comparable transactions.</em> Using our code for <code>Date</code> (page <a href="#page_247">247</a>) as a model, expand your implementation of <code>Transaction</code> (<a href="ch01.html#ch01qa2q13"><small>EXERCISE 1.2.13</small></a>) so that it implements <code>Comparable</code>, such that transactions are kept in order by amount.</p>
<p><em>Solution</em>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0266-01.jpg"/></p>
<p><a id="ch02qa1q22"/><strong>2.1.22</strong> <em>Transaction sort test client.</em> Write a class <code>SortTransactions</code> that consists of a static method <code>main()</code> that reads a sequence of transactions from standard input, sorts them, and prints the result on standard output (see <a href="ch01a.html#ch01qa3q17"><small>EXERCISE 1.3.17</small></a>).</p>
<p><em>Solution</em>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0266-02.jpg"/></p>
<p><a id="ch02sec2lev10"/></p>
<h4><a id="page_267"/>Experiments</h4>
<p><a id="ch02qa1q23"/><strong>2.1.23</strong> <em>Deck sort.</em> Ask a few friends to sort a deck of cards (see <a href="#ch02qa1q13"><small>EXERCISE 2.1.13</small></a>). Observe them carefully and write down the method(s) that they use.</p>
<p><a id="ch02qa1q24"/><strong>2.1.24</strong> <em>Insertion sort with sentinel.</em> Develop an implementation of insertion sort that eliminates the <code>j&gt;0</code> test in the inner loop by first putting the smallest item into position. Use <code>SortCompare</code> to evaluate the effectiveness of doing so. <em>Note</em>: It is often possible to avoid an index-out-of-bounds test in this way—the element that enables the test to be eliminated is known as a <em>sentinel</em>.</p>
<p><a id="ch02qa1q25"/><strong>2.1.25</strong> <em>Insertion sort without exchanges.</em> Develop an implementation of insertion sort that moves larger elements to the right one position with one array access per entry, rather than using <code>exch()</code>. Use <code>SortCompare</code> to evaluate the effectiveness of doing so.</p>
<p><a id="ch02qa1q26"/><strong>2.1.26</strong> <em>Primitive types.</em> Develop a version of insertion sort that sorts arrays of <code>int</code> values and compare its performance with the implementation given in the text (which sorts <code>Integer</code> values and implicitly uses autoboxing and auto-unboxing to convert).</p>
<p><a id="ch02qa1q27"/><strong>2.1.27</strong> <em>Shellsort is subquadratic.</em> Use <code>SortCompare</code> to compare shellsort with insertion sort and selection sort on your computer. Use array sizes that are increasing powers of 2, starting at 128.</p>
<p><a id="ch02qa1q28"/><strong>2.1.28</strong> <em>Equal keys.</em> Formulate and validate hypotheses about the running time of insertion sort and selection sort for arrays that contain just two key values, assuming that the values are equally likely to occur.</p>
<p><a id="ch02qa1q29"/><strong>2.1.29</strong> <em>Shellsort increments.</em> Run experiments to compare the increment sequence in <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a> with the sequence 1, 5, 19, 41, 109, 209, 505, 929, 2161, 3905, 8929, 16001, 36289, 64769, 146305, 260609 (which is formed by merging together the sequences 9·4<em><sup>k</sup></em> − 9·2<em><sup>k</sup></em> + 1 and 4<em><sup>k</sup></em> − 3·2<em><sup>k</sup></em> + 1). See <a href="#ch02qa1q11"><small>EXERCISE 2.1.11</small></a>.</p>
<p><a id="ch02qa1q30"/><strong>2.1.30</strong> <em>Geometric increments.</em> Run experiments to determine a value of <em>t</em> that leads to the lowest running time of shellsort for random arrays for the increment sequence 1, <img alt="image" src="graphics/lfloor.jpg"/><em>t</em><img alt="image" src="graphics/rfloor.jpg"/>, <img alt="image" src="graphics/lfloor.jpg"/><em>t<sup>2</sup></em><img alt="image" src="graphics/rfloor.jpg"/>, <img alt="image" src="graphics/lfloor.jpg"/><em>t<sup>3</sup></em><img alt="image" src="graphics/rfloor.jpg"/>, <img alt="image" src="graphics/lfloor.jpg"/><em>t<sup>4</sup></em><img alt="image" src="graphics/rfloor.jpg"/>, . . . for <em>N</em> = 10<sup>6</sup>. Give the values of <em>t</em> and the increment sequences for the best three values that you find.</p>
<p><a id="page_268"/><em>The following exercises describe various clients for helping to evaluate sorting methods. They are intended as starting points for helping to understand performance properties, using random data. In all of them, use</em> <code>time()</code>, <em>as in</em> <code>SortCompare</code>, <em>so that you can get more accurate results by specifying more trials in the second command-line argument. We refer back to these exercises in later sections when evaluating more sophisticated methods.</em></p>
<p><a id="ch02qa1q31"/><strong>2.1.31</strong> <em>Doubling test.</em> Write a client that performs a doubling test for sort algorithms. Start at <code>N</code> equal to <code>1000</code>, and print <code>N</code>, the predicted number of seconds, the actual number of seconds, and the ratio as <code>N</code> doubles. Use your program to validate that insertion sort and selection sort are quadratic for random inputs, and formulate and test a hypothesis for shellsort.</p>
<p><a id="ch02qa1q32"/><strong>2.1.32</strong> <em>Plot running times.</em> Write a client that uses <code>StdDraw</code> to plot the average running times of the algorithm for random inputs and various values of the array size. You may add one or two more command-line arguments. Strive to design a useful tool.</p>
<p><a id="ch02qa1q33"/><strong>2.1.33</strong> <em>Distribution.</em> Write a client that enters into an infinite loop running <code>sort()</code> on arrays of the size given as the third command-line argument, measures the time taken for each run, and uses <code>StdDraw</code> to plot the average running times. A picture of the <em>distribution</em> of the running times should emerge.</p>
<p><a id="ch02qa1q34"/><strong>2.1.34</strong> <em>Corner cases.</em> Write a client that runs <code>sort()</code> on difficult or pathological cases that might turn up in practical applications. Examples include arrays that are already in order, arrays in reverse order, arrays where all keys are the same, arrays consisting of only two distinct values, and arrays of size 0 or 1.</p>
<p><a id="ch02qa1q35"/><strong>2.1.35</strong> <em>Nonuniform distributions.</em> Write a client that generates test data by randomly ordering objects using other distributions than uniform, including the following:</p>
<p class="indenthangingB">• Gaussian</p>
<p class="indenthangingB">• Poisson</p>
<p class="indenthangingB">• Geometric</p>
<p class="indenthangingB">• Discrete (see <a href="#ch02qa1q28"><small>EXERCISE 2.1.28</small></a> for a special case)</p>
<p>Develop and test hypotheses about the effect of such input on the performance of the algorithms in this section.</p>
<p><a id="page_269"/><a id="ch02qa1q36"/><strong>2.1.36</strong> <em>Nonuniform data.</em> Write a client that generates test <em>data</em> that is not uniform, including the following:</p>
<p class="indenthangingB">• Half the data is 0s, half 1s.</p>
<p class="indenthangingB">• Half the data is 0s, half the remainder is 1s, half the remainder is 2s, and so forth.</p>
<p class="indenthangingB">• Half the data is 0s, half random <code>int</code> values.</p>
<p>Develop and test hypotheses about the effect of such input on the performance of the algorithms in this section.</p>
<p><a id="ch02qa1q37"/><strong>2.1.37</strong> <em>Partially sorted.</em> Write a client that generates partially sorted arrays, including the following:</p>
<p class="indenthangingB">• 95 percent sorted, last percent random values</p>
<p class="indenthangingB">• All entries within 10 positions of their final place in the array</p>
<p class="indenthangingB">• Sorted except for 5 percent of the entries randomly dispersed throughout the array</p>
<p>Develop and test hypotheses about the effect of such input on the performance of the algorithms in this section.</p>
<p><a id="ch02qa1q38"/><strong>2.1.38</strong> <em>Various types of items.</em> Write a client that generates arrays of items of various types with random key values, including the following:</p>
<p class="indenthangingB">• <code>String</code> key (at least ten characters), one <code>double</code> value</p>
<p class="indenthangingB">• <code>double</code> key, ten <code>String</code> values (all at least ten characters)</p>
<p class="indenthangingB">• <code>int</code> key, one <code>int[20]</code> value</p>
<p>Develop and test hypotheses about the effect of such input on the performance of the algorithms in this section.</p>
<p><a id="ch02sec1lev2"/></p>
<h3><a id="page_270"/>2.2 Mergesort</h3>
<p><small>THE ALGORITHMS</small> that we consider in this section are based on a simple operation known as <em>merging</em>: combining two ordered arrays to make one larger ordered array. This operation immediately leads to a simple recursive sort method known as <em>mergesort</em>: to sort an array, divide it into two halves, sort the two halves (recursively), and then merge the results. As you will see, one of mergesort’s most attractive properties is that it guarantees to sort any array of <em>N</em> items in time proportional to <em>N</em> log <em>N</em>. Its prime disadvantage is that it uses extra space proportional to <em>N</em>.</p>
<p class="image"><img alt="image" src="graphics/02_08-mergesort.jpg"/></p>
<p><a id="ch02sec2lev11"/></p>
<h4>Abstract in-place merge</h4>
<p>The straightforward approach to implementing merging is to design a method that merges two disjoint ordered arrays of <code>Comparable</code> objects into a third array. This strategy is easy to implement: create an output array of the requisite size and then choose successively the smallest remaining item from the two input arrays to be the next item added to the output array.</p>
<p>However, when we mergesort a large array, we are doing a huge number of merges, so the cost of creating a new array to hold the output every time that we do a merge is problematic. It would be much more desirable to have an in-place method so that we could sort the first half of the array in place, then sort the second half of the array in place, then do the merge of the two halves by moving the items around within the array, without using a significant amount of other extra space. It is worthwhile to pause momentarily to consider how you might do that. At first blush, this problem seems to be one that must be simple to solve, but solutions that are known are quite complicated, especially by comparison to alternatives that use extra space.</p>
<p>Still, the <em>abstraction</em> of an in-place merge is useful. Accordingly, we use the method signature <code>merge(a, lo, mid, hi)</code> to specify a merge method that puts the result of merging the subarrays <code>a[lo..mid]</code> with <code>a[mid+1..hi]</code> into a single ordered array, leaving the result in <code>a[lo..hi]</code>. The code on the next page implements this merge method in just a few lines by copying everything to an auxiliary array and then merging back to the original. Another approach is described in <a href="#ch02qa2q10"><small>EXERCISE 2.2.10</small></a>.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb13"/></p>
<h3><a id="page_271"/>Abstract in-place merge</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0271-01.jpg"/></p>
<p>This method merges by first copying into the auxiliary array <code>aux[]</code> then merging back to <code>a[]</code>. In the merge (the second <code>for</code> loop), there are four conditions: left half exhausted (take from the right), right half exhausted (take from the left), current key on right less than current key on left (take from the right), and current key on right greater than or equal to current key on left (take from the left).</p>
<p class="image"><img alt="image" src="graphics/02_09-merge.jpg"/></p>
<hr/>
</div>
<p><a id="ch02sec2lev12"/></p>
<h4><a id="page_272"/>Top-down mergesort</h4>
<p><a href="#ch02sb15"><small>ALGORITHM 2.4</small></a> is a recursive mergesort implementation based on this abstract in-place merge. It is one of the best-known examples of the utility of the <em>divide-and-conquer</em> paradigm for efficient algorithm design. This recursive code is the basis for an inductive proof that the algorithm sorts the array: if it sorts the two subarrays, it sorts the whole array, by merging together the subarrays.</p>
<p class="image"><img alt="image" src="graphics/02_10-mergecalls.jpg"/></p>
<p>To understand mergesort, it is worthwhile to consider carefully the dynamics of the method calls, shown in the trace at right. To sort <code>a[0..15]</code>, the <code>sort()</code> method calls itself to sort <code>a[0..7]</code> then calls itself to sort <code>a[0..3]</code> and <code>a[0..1]</code> before finally doing the first merge of <code>a[0]</code> with <code>a[1]</code> after calling itself to sort <code>a[0]</code> and then <code>a[1]</code> (for brevity, we omit the calls for the base-case 1-entry sorts in the trace). Then the next merge is <code>a[2]</code> with <code>a[3]</code> and then <code>a[0..1]</code> with <code>a[2..3]</code> and so forth. From this trace, we see that the sort code simply provides an organized way to sequence the calls to the <code>merge()</code> method. This insight will be useful later in this section.</p>
<p>The recursive code also provides us with the basis for analyzing mergesort’s running time. Because mergesort is a prototype of the divide-and-conquer algorithm design paradigm, we will consider this analysis in detail.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb14"/></p>
<p><strong>Proposition F.</strong> Top-down mergesort uses between ½ <em>N</em> lg <em>N</em> and <em>N</em> lg <em>N</em> compares to sort any array of length <em>N</em>.</p>
<p><strong>Proof:</strong> Let <em>C</em>(<em>N</em>) be the number of compares needed to sort an array of length <em>N</em>. We have <em>C</em>(0) = <em>C</em>(1) = 0 and for <em>N &gt;</em> 0 we can write a recurrence relationship that directly mirrors the recursive <code>sort()</code> method to establish an upper bound:</p>
<p class="center"><em>C</em>(<em>N</em>) <em>≤ C</em>(<img alt="image" src="graphics/lceil.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/>) + <em>C</em>(<img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rceil.jpg"/>) + <em>N</em></p>
<p>The first term on the right is the number of compares to sort the left half of the array, the second term is the number of compares to sort the right half, and the third <a id="page_274"/>term is the number of compares for the merge. The lower bound</p>
<p class="center"><em>C</em>(<em>N</em>) ≥ <em>C</em>(<img alt="image" src="graphics/lceil.jpg"/><em>N</em>/2<img alt="image" src="graphics/rceil.jpg"/>)+ <em>C</em>(<img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/>) + <img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/></p>
<p>follows because the number of compares for the merge is at least <img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/>.</p>
<p>We derive an exact solution to the recurrence when equality holds and <em>N</em> is a power of 2 (say <em>N</em> = 2<em><sup>n</sup></em>). First, since <img alt="image" src="graphics/lfloor.jpg"/><em>N</em>/2<img alt="image" src="graphics/rfloor.jpg"/> = <img alt="image" src="graphics/lceil.jpg"/><em>N</em>/2<img alt="image" src="graphics/rceil.jpg"/> = 2<sup><em>n</em>−1</sup>, we have</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>) = 2<em>C</em>(2<sup><em>n</em>−1</sup>) + 2<em><sup>n</sup></em></p>
<p>Dividing both sides by 2<em><sup>n</sup></em> gives</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>)/2<em><sup>n</sup></em> = <em>C</em>(2<sup><em>n</em>−1</sup>)/2<sup><em>n</em>−1</sup> + 1</p>
<p>Applying the same equation to the first term on the right, we have</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>)/2<em><sup>n</sup></em> = <em>C</em>(2<sup><em>n</em>−2</sup>)/2<sup><em>n</em>−2</sup> + 1 + 1</p>
<p>Repeating the previous step <em>n</em> − 1 additional times gives</p>
<p class="center"><em>C</em>(2<em><sup>n</sup></em>)/2<em><sup>n</sup></em> = <em>C</em>(2<sup>0</sup>)/2<sup>0</sup> + <em>n</em></p>
<p>which, after multiplying both sides by 2<em><sup>n</sup></em>, leaves us with the solution</p>
<p class="center"><em>C</em>(<em>N</em>) = <em>C</em>(2<em><sup>n</sup></em>) = <em>n</em> 2<em><sup>n</sup></em> = <em>N</em> lg <em>N</em></p>
<p>Exact solutions for general <em>N</em> are more complicated, but it is not difficult to apply the same argument to the inequalities describing the bounds on the number of compares to prove the stated result for all values of <em>N</em>. This proof is valid no matter what the input values are and no matter in what order they appear.</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch02sb15"/></p>
<h3><a id="page_273"/>Algorithm 2.4 Top-down mergesort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0273-01.jpg"/></p>
<p>To sort a subarray <code>a[lo..hi]</code> we divide it into two parts: <code>a[lo..mid]</code> and <code>a[mid+1..hi]</code>, sort them independently (via recursive calls), and merge the resulting ordered subarrays to produce the result.</p>
<p class="image"><img alt="image" src="graphics/02_11mergeaiitd.jpg"/></p>
<hr/>
</div>
<p>Another way to understand <a href="#ch02sb14"><small>PROPOSITION F</small></a> is to examine the tree drawn below, where each node depicts a subarray for which <code>sort()</code> does a <code>merge()</code>. The tree has precisely <em>n</em> levels. For <em>k</em> from 0 to <em>n</em> − 1, the <em>k</em>th level from the top depicts 2<em><sup>k</sup></em> subarrays, each of length 2<sup><em>n</em>−<em>k</em></sup>, each of which thus requires at most 2<sup><em>n</em>−<em>k</em></sup> compares for the merge. Thus we have 2<em><sup>k</sup></em> · 2<sup><em>n</em>−<em>k</em></sup> = 2<em><sup>n</sup></em> total cost for each of the <em>n</em> levels, for a total of <em>n</em> 2<em><sup>n</sup></em> = <em>N</em> lg<em>N</em>.</p>
<p class="image"><img alt="image" src="graphics/02_12-mergecallstree.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb16"/></p>
<p><a id="page_275"/><strong>Proposition G.</strong> Top-down mergesort uses at most 6<em>N</em> lg <em>N</em> array accesses to sort an array of length <em>N</em>.</p>
<p><strong>Proof:</strong> Each merge uses at most 6<em>N</em> array accesses (2<em>N</em> for the copy, 2<em>N</em> for the move back, and at most 2<em>N</em> for compares). The result follows from the same argument as for <a href="#ch02sb14"><small>PROPOSITION F</small></a>.</p>
<hr/>
</div>
<p><a href="#ch02sb14"><small>PROPOSITIONS F</small></a> and <a href="#ch02sb16"><small>G</small></a> tell us that we can expect the time required by mergesort to be proportional to <em>N</em> log <em>N.</em> That fact brings us to a different level from the elementary methods in <a href="#ch02sec1lev1"><small>SECTION 2.1</small></a> because it tells us that we can sort huge arrays using just a logarithmic factor more time than it takes to examine every entry. You can sort millions of items (or more) with mergesort, but not with insertion sort or selection sort. The primary drawback of mergesort is that it requires extra space proportional to <em>N</em>, for the auxiliary array for merging. If space is at a premium, we need to consider another method. On the other hand, we can cut the running time of mergesort substantially with some carefully considered modifications to the implementation.</p>
<p><a id="ch02sec3lev7"/></p>
<h5><em>Use insertion sort for small subarrays</em></h5>
<p>We can improve most recursive algorithms by handling small cases differently, because the recursion <em>guarantees</em> that the method will be used often for small cases, so improvements in handling them lead to improvements in the whole algorithm. In the case of sorting, we know that insertion sort (or selection sort) is simple and therefore likely to be faster than mergesort for tiny subarrays. As usual, a visual trace provides insight into the operation of mergesort. The visual trace on the facing page shows the operation of a mergesort implementation with a cutoff for small subarrays. Switching to insertion sort for small subarrays (length 15 or less, say) will improve the running time of a typical mergesort implementation by 10 to 15 percent (see <a href="#ch02qa2q23"><small>EXERCISE 2.2.23</small></a>).</p>
<p><a id="ch02sec3lev8"/></p>
<h5><em>Test whether the array is already in order</em></h5>
<p>We can reduce the running time to be linear for arrays that are already in order by adding a test to skip the call to <code>merge()</code> if <code>a[mid]</code> is less than or equal to <code>a[mid+1]</code>. With this change, we still do all the recursive calls, but the running time for any sorted subarray is linear (see <a href="#ch02qa2q8"><small>EXERCISE 2.2.8</small></a>).</p>
<p><a id="ch02sec3lev9"/></p>
<h5><em>Eliminate the copy to the auxiliary array</em></h5>
<p>It is possible to eliminate the time (but not the space) taken to copy to the auxiliary array used for merging. To do so, we use two invocations of the sort method: one takes its input from the given array and puts the sorted output in the auxiliary array; the other takes its input from the auxiliary array and puts the sorted output in the given array. With this approach, in a bit of recursive trickery, we can arrange the recursive calls such that the computation switches the roles of the input array and the auxiliary array at each level (see <a href="#ch02qa2q11"><small>EXERCISE 2.2.11</small></a>).</p>
<p class="image"><a id="page_276"/><img alt="image" src="graphics/02_13-mergetdbars.jpg"/></p>
<p><a id="page_277"/>It is appropriate to repeat here a point raised in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a> that is easily forgotten and needs reemphasis. Locally, we treat each algorithm in this book as if it were critical in some application. Globally, we try to reach general conclusions about which approach to recommend. Our discussion of such improvements is not necessarily a recommendation to always implement them, rather a warning not to draw absolute conclusions about performance from initial implementations. When addressing a new problem, your best bet is to use the simplest implementation with which you are comfortable and then refine it if it becomes a bottleneck. Addressing improvements that decrease running time just by a constant factor may not otherwise be worthwhile. You need to test the effectiveness of specific improvements by running experiments, as we indicate in exercises throughout.</p>
<p>In the case of mergesort, the three improvements just listed are simple to implement and are of interest when mergesort is the method of choice—for example, in situations discussed at the end of this chapter.</p>
<p><a id="ch02sec2lev13"/></p>
<h4>Bottom-up mergesort</h4>
<p>The recursive implementation of mergesort is prototypical of the <em>divide-and-conquer</em> algorithm design paradigm, where we solve a large problem by dividing it into pieces, solving the subproblems, then using the solutions for the pieces to solve the whole problem. Even though we are thinking in terms of merging together two large subarrays, the fact is that most merges are merging together tiny subarrays. Another way to implement mergesort is to organize the merges so that we do all the merges of tiny subarrays on one pass, then do a second pass to merge those subarrays in pairs, and so forth, continuing until we do a merge that encompasses the whole array. This method requires even less code than the standard recursive implementation. We start by doing a pass of 1-by-1 merges (considering individual items as subarrays of size 1), then a pass of 2-by-2 merges (merge subarrays of size 2 to make subarrays of size 4), then 4-by-4 merges, and so forth. The second subarray may be smaller than the first in the last merge on each pass (which is no problem for <code>merge()</code>), but otherwise all merges involve subarrays of equal size, doubling the sorted subarray size for the next pass.</p>
<p class="image"><img alt="image" src="graphics/02_14-mergeubbars.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb17"/></p>
<h3><a id="page_278"/>Bottom-up mergesort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0278-01.jpg"/></p>
<p>Bottom-up mergesort consists of a sequence of passes over the whole array, doing <code>sz</code>-by-<code>sz</code> merges, starting with <code>sz</code> equal to <code>1</code> and doubling <code>sz</code> on each pass. The final subarray is of size <code>sz</code> only when the array size is an even multiple of <code>sz</code> (otherwise it is less than <code>sz</code>).</p>
<p class="image"><img alt="image" src="graphics/02_15-mergeaiibu.jpg"/></p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb18"/></p>
<p><a id="page_279"/><strong>Proposition H.</strong> Bottom-up mergesort uses between ½ <em>N</em> lg <em>N</em> and <em>N</em> lg <em>N</em> compares and at most 6<em>N</em> lg <em>N</em> array accesses to sort an array of length <em>N</em>.</p>
<p><strong>Proof:</strong> The number of passes through the array is precisely <img alt="image" src="graphics/lfloor.jpg"/>lg <em>N</em><img alt="image" src="graphics/rfloor.jpg"/> (that is precisely the value of <em>n</em> such that 2<em><sup>n</sup></em> ≤ <em>N &lt;</em> 2<sup><em>n</em>+1</sup>). For each pass, the number of array accesses is exactly 6<em>N</em> and the number of compares is at most <em>N</em> and no less than <em>N</em>/2.</p>
<hr/>
</div>
<p><small>WHEN THE ARRAY LENGTH IS A POWER OF 2</small>, top-down and bottom-up mergesort perform precisely the same compares and array accesses, just in a different order. When the array length is not a power of 2, the sequence of compares and array accesses for the two algorithms will be different (see <a href="#ch02qa2q5"><small>EXERCISE 2.2.5</small></a>).</p>
<p>A version of bottom-up mergesort is the method of choice for sorting data organized in a <em>linked list</em>. Consider the list to be sorted sublists of size 1, then pass through to make sorted subarrays of size 2 linked together, then size 4, and so forth. This method rearranges the links to sort the list <em>in place</em> (without creating any new list nodes).</p>
<p>Both the top-down and bottom-up approaches to implementing a divide-and-conquer algorithm are intuitive. The lesson that you can take from mergesort is this: Whenever you encounter an algorithm based on one of these approaches, it is worth considering the other. Do you want to solve the problem by breaking it up into smaller problems (and solving them recursively) as in <code>Merge.sort()</code> or by building small solutions into larger ones as in <code>MergeBU.sort()</code>?</p>
<p><a id="ch02sec2lev14"/></p>
<h4>The complexity of sorting</h4>
<p>One important reason to know about mergesort is that we use it as the basis for proving a fundamental result in the field of <em>computational complexity</em> that helps us understand the intrinsic difficulty of sorting. In general, computational complexity plays an important role in the design of algorithms, and this result in particular is directly relevant to the design of sorting algorithms, so we next consider it in detail.</p>
<p>The first step in a study of complexity is to establish a model of computation. Generally, researchers strive to understand the simplest model relevant to a problem. For sorting, we study the class of <em>compare-based</em> algorithms that make their decisions about items only on the basis of comparing keys. A compare-based algorithm can do an arbitrary amount of computation between compares, but cannot get any information about a key except by comparing it with another one. Because of our restriction to the <code>Comparable</code> API, all of the algorithms in this chapter are in this class (note that we are ignoring the cost of array accesses), as are many algorithms that we might imagine. In <a href="ch05.html#ch05"><small>CHAPTER 5</small></a>, we consider algorithms that are not restricted to <code>Comparable</code> items.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb19"/></p>
<p><a id="page_280"/><strong>Proposition I.</strong> No compare-based sorting algorithm can guarantee to sort <em>N</em> items with fewer than lg(<em>N</em>!) ~ <em>N</em> lg <em>N</em> compares.</p>
<p><strong>Proof:</strong> First, we assume that the keys are all distinct, since any algorithm must be able to sort such inputs. Now, we use a binary tree to describe the sequence of compares. Each <em>node</em> in the tree is either a <em>leaf</em> <img alt="image" src="graphics/02_16-lowerboundleaf.jpg"/> that indicates that the sort is complete and has discovered that the original inputs were in the order <code>a[i<sub>0</sub>], a[i<sub>1</sub>], ...a[i<sub>N-1</sub>]</code>, or an <em>internal node</em> <img alt="image" src="graphics/02_17-lowerboundnode.jpg"/> that corresponds to a compare operation between <code>a[i]</code> and <code>a[j]</code>, with a left subtree corresponding to the sequence of compares in the case that <code>a[i]</code> is less than <code>a[j]</code>, and a right subtree corresponding to what happens if <code>a[i]</code> is greater than <code>a[j]</code>. Each path from the root to a leaf corresponds to the sequence of compares that the algorithm uses to establish the ordering given in the leaf. For example, here is a compare tree for <em>N</em> = 3:</p>
<p class="image"><img alt="image" src="graphics/02_18-lowerboundtree.jpg"/></p>
<p>We never explicitly construct such a tree—it is a mathematical device for describing the compares used by any algorithm.</p>
<p>The first key observation in the proof is that the tree must have at least <em>N</em>! leaves because there are <em>N</em>! different permutations of <em>N</em> distinct keys. If there are fewer than <em>N</em>! leaves, then some permutation is missing from the leaves, and the algorithm would fail for that permutation.</p>
<p>The number of internal nodes on a path from the root to a leaf in the tree is the number of compares used by the algorithm for some input. We are interested in the length of the longest such path in the tree (known as the tree <em>height</em>) since it measures the worst-case number of compares used by the algorithm. Now, it is a basic combinatorial property of binary trees that a tree of height <em>h</em> has no more than 2<em><sup>h</sup></em> leaves—the tree of height <em>h</em> with the maximum number of leaves is perfectly balanced, or <em>complete</em>. An example for <em>h</em> = 4 is diagrammed on the next page.</p>
<p class="image"><a id="page_281"/><img alt="image" src="graphics/02_19-lowerbouncomplete.jpg"/></p>
<p>Combining the previous two paragraphs, we have shown that any compare-based sorting algorithm corresponds to a compare tree of height <em>h</em> with</p>
<p class="center"><em>N</em>! ≤ <em>number of leaves</em> ≤ 2<em><sup>h</sup></em></p>
<p class="image"><img alt="image" src="graphics/02_20-lowerboundtreebo.jpg"/></p>
<p>The value of <em>h</em> is precisely the worst-case number of compares, so we can take the logarithm (base 2) of both sides of this equation and conclude that the number of compares used by any algorithm must be at least lg <em>N</em>!. The approximation lg <em>N</em>! ~ <em>N</em> lg <em>N</em> follows immediately from Stirling’s approximation to the factorial function (see page <a href="ch01a.html#page_185">185</a>).</p>
<hr/>
</div>
<p>This result serves as a guide for us to know, when designing a sorting algorithm, how well we can expect to do. For example, without such a result, one might set out to try to design a compare-based sorting algorithm that uses half as many compares as does mergesort, in the worst case. The lower bound in <a href="#ch02sb19"><small>PROPOSITION I</small></a> says that such an effort is futile—<em>no such algorithm exists</em>. It is an extremely strong statement that applies to any conceivable compare-based algorithm.</p>
<p><a href="#ch02sb18"><small>PROPOSITION H</small></a> asserts that the number of compares used by mergesort in the worst case is ~ <em>N</em> lg <em>N</em>. This result is an <em>upper bound</em> on the difficulty of the sorting problem in the sense that a better algorithm would have to guarantee to use a smaller number of compares. <a href="#ch02sb19"><small>PROPOSITION I</small></a> asserts that no sorting algorithm can guarantee to use fewer <a id="page_282"/>than ~ <em>N</em> lg <em>N</em> compares. It is a <em>lower bound</em> on the difficulty of the sorting problem in the sense that even the best possible algorithm must use at least that many compares in the worst case. Together, they imply:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb20"/></p>
<p><strong>Proposition J.</strong> Mergesort is an asymptotically optimal compare-based sorting algorithm.</p>
<p><strong>Proof:</strong> Precisely, we mean by this statement that <em>both the number of compares used by mergesort in the worst case and the minimum number of compares that any compare-based sorting algorithm can guarantee are</em> ~<em>N</em> lg <em>N</em>. <a href="#ch02sb18"><small>PROPOSITIONS H</small></a> and <a href="#ch02sb19"><small>I</small></a> establish these facts.</p>
<hr/>
</div>
<p>It is important to note that, like the model of computation, we need to precisely define what we mean by an optimal algorithm. For example, we might tighten the definition of optimality and insist that an optimal algorithm for sorting is one that uses <em>precisely</em> lg <em>N</em>! compares. We do not do so because we could not notice the difference between such an algorithm and (for example) mergesort for large <em>N.</em> Or, we might broaden the definition of optimality to include any sorting algorithm whose worst-case number of compares is <em>within a constant factor</em> of <em>N</em> lg <em>N.</em> We do not do so because we might very well notice the difference between such an algorithm and mergesort for large <em>N.</em></p>
<p><small>COMPUTATIONAL COMPLEXITY MAY SEEM RATHER ABSTRACT</small>, but fundamental research on the intrinsic difficulty of solving computational problems hardly needs justification. Moreover, when it does apply, it is emphatically the case that computational complexity affects the development of good software. First, good upper bounds allow software engineers to provide performance guarantees; there are many documented instances where poor performance has been traced to someone using a quadratic sort instead of a linearithmic one. Second, good lower bounds spare us the effort of searching for performance improvements that are not attainable.</p>
<p>But the optimality of mergesort is not the end of the story and should not be misused to indicate that we need not consider other methods for practical applications. That is not the case because the theory in this section has a number of limitations. For example:</p>
<p class="indenthangingB">• Mergesort is not optimal with respect to space usage.</p>
<p class="indenthangingB">• The worst case may not be likely in practice.</p>
<p class="indenthangingB">• Operations other than compares (such as array accesses) may be important.</p>
<p class="indenthangingB">• One can sort certain data without using <em>any</em> compares.</p>
<p>Thus, we shall be considering several other sorting methods in this book.</p>
<p><a id="ch02sec2lev15"/></p>
<h4><a id="page_283"/>Q&amp;A</h4>
<p><strong>Q.</strong> Is mergesort faster than shellsort?</p>
<p><strong>A.</strong> In practice, their running times are within a small constant factor of one another (when shellsort is using a well-tested increment sequence like the one in <a href="#ch02sb11"><small>ALGORITHM 2.3</small></a>), so comparative performance depends on the implementations.</p>
<p class="programlisting"><img alt="image" src="graphics/p0283-01.jpg"/></p>
<p>In theory, no one has been able to prove that shellsort is linearithmic for random data, so there remains the possibility that the asymptotic growth of the average-case performance of shellsort is higher. Such a gap has been proven for worst-case performance, but it is not relevant in practice.</p>
<p><strong>Q.</strong> Why not make the <code>aux[]</code> array local to <code>merge()</code>?</p>
<p><strong>A.</strong> To avoid the overhead of creating an array for every merge, even the tiny ones. This cost would dominate the running time of mergesort (see <a href="#ch02qa2q26"><small>EXERCISE 2.2.26</small></a>). A more proper solution (which we avoid in the text to reduce clutter in the code) is to make <code>aux[]</code> local to <code>sort()</code> and pass it as an argument to <code>merge()</code> (see <a href="#ch02qa2q9"><small>EXERCISE 2.2.9</small></a>).</p>
<p><strong>Q.</strong> How does mergesort fare when there are duplicate values in the array?</p>
<p><strong>A.</strong> If all the items have the same value, the running time is linear (with the extra test to skip the merge when the array is sorted), but if there is more than one duplicate value, this performance gain is not necessarily realized. For example, suppose that the input array consists of <em>N</em> items with one value in odd positions and <em>N</em> items with another value in even positions. The running time is linearithmic for such an array (it satisfies the same recurrence as for items with distinct values), not linear.</p>
<p><a id="ch02sec2lev16"/></p>
<h4><a id="page_284"/>Exercises</h4>
<p><a id="ch02qa2q1"/><strong>2.2.1</strong> Give a trace, in the style of the trace given at the beginning of this section, showing how the keys <code>A E Q S U Y E I N O S T</code> are merged with the abstract in-place <code>merge()</code> method.</p>
<p><a id="ch02qa2q2"/><strong>2.2.2</strong> Give traces, in the style of the trace given with <a href="#ch02sb15"><small>ALGORITHM 2.4</small></a>, showing how the keys <code>E A S Y Q U E S T I O N</code> are sorted with top-down mergesort.</p>
<p><a id="ch02qa2q3"/><strong>2.2.3</strong> Answer <a href="#ch02qa2q2"><small>EXERCISE 2.2.2</small></a> for bottom-up mergesort.</p>
<p><a id="ch02qa2q4"/><strong>2.2.4</strong> Does the abstract in-place merge produce proper output if and only if the two input subarrays are in sorted order? Prove your answer, or provide a counterexample.</p>
<p><a id="ch02qa2q5"/><strong>2.2.5</strong> Give the sequence of subarray sizes in the merges performed by both the top-down and the bottom-up mergesort algorithms, for <em>N</em> = 39.</p>
<p><a id="ch02qa2q6"/><strong>2.2.6</strong> Write a program to compute the exact value of the number of array accesses used by top-down mergesort and by bottom-up mergesort. Use your program to plot the values for <em>N</em> from 1 to 512, and to compare the exact values with the upper bound 6<em>N</em> lg <em>N</em>.</p>
<p><a id="ch02qa2q7"/><strong>2.2.7</strong> Show that the number of compares used by mergesort is monotonically increasing (<em>C</em>(<em>N</em>+1) <em>&gt; C</em>(<em>N</em>) for all <em>N &gt;</em> 0).</p>
<p><a id="ch02qa2q8"/><strong>2.2.8</strong> Suppose that <a href="#ch02sb15"><small>ALGORITHM 2.4</small></a> is modified to skip the call on <code>merge()</code> whenever <code>a[mid] &lt;= a[mid+1]</code>. Prove that the number of compares used to mergesort a sorted array is linear.</p>
<p><a id="ch02qa2q9"/><strong>2.2.9</strong> Use of a static array like <code>aux[]</code> is inadvisable in library software because multiple clients might use the class concurrently. Give an implementation of <code>Merge</code> that does not use a static array. Do <em>not</em> make <code>aux[]</code> local to <code>merge()</code> (see the <a href="#ch02sec2lev31">Q&amp;A</a> for this section). <em>Hint</em>: Pass the auxiliary array as an argument to the recursive <code>sort()</code>.</p>
<p><a id="ch02sec2lev17"/></p>
<h4><a id="page_285"/>Creative Problems</h4>
<p><a id="ch02qa2q10"/><strong>2.2.10</strong> <em>Faster merge.</em> Implement a version of <code>merge()</code> that copies the second half of <code>a[]</code> to <code>aux[]</code> in <em>decreasing order</em> and then does the merge back to <code>a[]</code>. This change allows you to remove the code to test that each of the halves has been exhausted from the inner loop. <em>Note</em>: The resulting sort is not stable (see page <a href="#ch02sec3lev41">341</a>).</p>
<p><a id="ch02qa2q11"/><strong>2.2.11</strong> <em>Improvements.</em> Implement the three improvements to mergesort that are described in the text on page <a href="#ch02sec3lev7">275</a>: Add a cutoff for small subarrays, test whether the array is already in order, and avoid the copy by switching arguments in the recursive code.</p>
<p><a id="ch02qa2q12"/><strong>2.2.12</strong> <em>Sublinear extra space.</em> Develop a merge implementation that reduces the extra space requirement to max<em>(M, N/M)</em>, based on the following idea: Divide the array into <em>N/M</em> blocks of size <em>M</em> (for simplicity in this description, assume that <em>N</em> is a multiple of <em>M</em>). Then, <em>(i)</em> considering the blocks as items with their first key as the sort key, sort them using selection sort; and <em>(ii)</em> run through the array merging the first block with the second, then the second block with the third, and so forth.</p>
<p><a id="ch02qa2q13"/><strong>2.2.13</strong> <em>Lower bound for average case.</em> Prove that the <em>expected</em> number of compares used by any compare-based sorting algorithm must be at least ~<em>N</em> lg <em>N</em> (assuming that all possible orderings of the input are equally likely). <em>Hint</em>: The expected number of compares is at least the external path length of the compare tree (the sum of the lengths of the paths from the root to all leaves), which is minimized when it is balanced.</p>
<p><a id="ch02qa2q14"/><strong>2.2.14</strong> <em>Merging sorted queues.</em> Develop a static method that takes two queues of sorted items as arguments and returns a queue that results from merging the queues into sorted order.</p>
<p><a id="ch02qa2q15"/><strong>2.2.15</strong> <em>Bottom-up queue mergesort.</em> Develop a bottom-up mergesort implementation based on the following approach: Given <em>N</em> items, create <em>N</em> queues, each containing one of the items. Create a queue of the <em>N</em> queues. Then repeatedly apply the merging operation of <a href="#ch02qa2q14"><small>EXERCISE 2.2.14</small></a> to the first two queues and reinsert the merged queue at the end. Repeat until the queue of queues contains only one queue.</p>
<p><a id="ch02qa2q16"/><strong>2.2.16</strong> <em>Natural mergesort.</em> Write a version of bottom-up mergesort that takes advantage of order in the array by proceeding as follows each time it needs to find two arrays to merge: find a sorted subarray (by incrementing a pointer until finding an entry that is smaller than its predecessor in the array), then find the next, then merge them. Analyze the running time of this algorithm in terms of the array size and the number of <a id="page_286"/>maximal increasing sequences in the array.</p>
<p><a id="ch02qa2q17"/><strong>2.2.17</strong> <em>Linked-list sort.</em> Implement a natural mergesort for linked lists. (This is the method of choice for sorting linked lists because it uses no extra space and is guaranteed to be linearithmic.)</p>
<p><a id="ch02qa2q18"/><strong>2.2.18</strong> <em>Shuffling a linked list.</em> Develop and implement a divide-and-conquer algorithm that randomly shuffles a linked list in linearithmic time and logarithmic extra space.</p>
<p><a id="ch02qa2q19"/><strong>2.2.19</strong> <em>Inversions.</em> Develop and implement a linearithmic algorithm for computing the number of inversions in a given array (the number of exchanges that would be performed by insertion sort for that array—see <a href="#ch02sec1lev1"><small>SECTION 2.1</small></a>). This quantity is related to the <em>Kendall tau distance</em>; see <a href="#ch02sec1lev5"><small>SECTION 2.5</small></a>.</p>
<p><a id="ch02qa2q20"/><strong>2.2.20</strong> <em>Index sort.</em> Develop and implement a version of mergesort that does not rearrange the array, but returns an <code>int[]</code> array <code>perm</code> such that <code>perm[i]</code> is the index of the <em>i</em>th smallest entry in the array.</p>
<p><a id="ch02qa2q21"/><strong>2.2.21</strong> <em>Triplicates.</em> Given three lists of <em>N</em> names each, devise a linearithmic algorithm to determine if there is any name common to all three lists, and if so, return the first such name.</p>
<p><a id="ch02qa2q22"/><strong>2.2.22</strong> <em>3-way mergesort.</em> Suppose instead of dividing in half at each step, you divide into thirds, sort each third, and combine using a 3-way merge. What is the order of growth of the overall running time of this algorithm?</p>
<p><a id="ch02sec2lev18"/></p>
<h4><a id="page_287"/>Experiments</h4>
<p><a id="ch02qa2q23"/><strong>2.2.23</strong> <em>Improvements.</em> Run empirical studies to evaluate the effectiveness of each of the three improvements to mergesort that are described in the text (see <a href="#ch02qa2q11"><small>EXERCISE 2.2.11</small></a>). Also, compare the performance of the merge implementation given in the text with the merge described in <a href="#ch02qa2q10"><small>EXERCISE 2.2.10</small></a>. In particular, empirically determine the best value of the parameter that decides when to switch to insertion sort for small subarrays.</p>
<p><a id="ch02qa2q24"/><strong>2.2.24</strong> <em>Sort-test improvement.</em> Run empirical studies for large randomly ordered arrays to study the effectiveness of the modification described in <a href="#ch02qa2q8"><small>EXERCISE 2.2.8</small></a> for random data. In particular, develop a hypothesis about the average number of times the test (whether an array is sorted) succeeds, as a function of <em>N</em> (the original array size for the sort).</p>
<p><a id="ch02qa2q25"/><strong>2.2.25</strong> <em>Multiway mergesort.</em> Develop a mergesort implementation based on the idea of doing <em>k</em>-way merges (rather than 2-way merges). Analyze your algorithm, develop a hypothesis regarding the best value of <em>k,</em> and run experiments to validate your hypothesis.</p>
<p><a id="ch02qa2q26"/><strong>2.2.26</strong> <em>Array creation.</em> Use <code>SortCompare</code> to get a rough idea of the effect on performance on your machine of creating <code>aux[]</code> in <code>merge()</code> rather than in <code>sort()</code>.</p>
<p><a id="ch02qa2q27"/><strong>2.2.27</strong> <em>Subarray lengths.</em> Run mergesort for large random arrays, and make an empirical determination of the average length of the other subarray when the first subarray exhausts, as a function of <em>N</em> (the sum of the two subarray sizes for a given merge).</p>
<p><a id="ch02qa2q28"/><strong>2.2.28</strong> <em>Top-down versus bottom-up.</em> Use <code>SortCompare</code> to compare top-down and bottom-up mergesort for <em>N</em>=10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p>
<p><a id="ch02qa2q29"/><strong>2.2.29</strong> <em>Natural mergesort.</em> Determine empirically the number of passes needed in a natural mergesort (see <a href="#ch02qa2q16"><small>EXERCISE 2.2.16</small></a>) for random <code>Long</code> keys with <em>N</em>=10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>. <em>Hint</em>: You do not need to implement a sort (or even generate full 64-bit keys) to complete this exercise.</p>
<p><a id="ch02sec1lev3"/></p>
<h3><a id="page_288"/>2.3 Quicksort</h3>
<p><small>THE SUBJECT OF THIS SECTION</small> is the sorting algorithm that is probably used more widely than any other, <em>quicksort</em>. Quicksort is popular because it is not difficult to implement, works well for a variety of different kinds of input data, and is substantially faster than any other sorting method in typical applications. The quicksort algorithm’s desirable features are that it is in-place (uses only a small auxiliary stack) and that it requires time proportional to <em>N</em> log <em>N</em> on the average to sort an array of length <em>N</em>. None of the algorithms that we have so far considered combine these two properties. Furthermore, quicksort has a shorter inner loop than most other sorting algorithms, which means that it is fast in practice as well as in theory. Its primary drawback is that it is fragile in the sense that some care is involved in the implementation to be sure to avoid bad performance. Numerous examples of mistakes leading to quadratic performance in practice are documented in the literature. Fortunately, the lessons learned from these mistakes have led to various improvements to the algorithm that make it of even broader utility, as we shall see.</p>
<p><a id="ch02sec2lev19"/></p>
<h4>The basic algorithm</h4>
<p>Quicksort is a divide-and-conquer method for sorting. It works by <em>partitioning</em> an array into two subarrays, then sorting the subarrays independently. Quicksort is complementary to mergesort: for mergesort, we break the array into two subarrays to be sorted and then combine the ordered subarrays to make the whole ordered array; for quicksort, we rearrange the array such that, when the two subarrays are sorted, the whole array is ordered. In the first instance, we do the two recursive calls <em>before</em> working on the whole array; in the second instance, we do the two recursive calls <em>after</em> working on the whole array. For mergesort, the array is divided in half; for quicksort, the position of the partition depends on the contents of the array.</p>
<p class="image"><img alt="image" src="graphics/02_21-quick.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb21"/></p>
<h3><a id="page_289"/>Algorithm 2.5 Quicksort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0289-01.jpg"/></p>
<p>Quicksort is a recursive program that sorts a subarray <code>a[lo...hi]</code> by using a <code>partition()</code> method that puts <code>a[j]</code> into position and arranges the rest of the entries such that the recursive calls finish the sort.</p>
<p class="image"><img alt="image" src="graphics/02_22-quickall.jpg"/></p>
<hr/>
</div>
<p><a id="page_290"/>The crux of the method is the partitioning process, which rearranges the array to make the following three conditions hold:</p>
<p class="indenthangingB">• The entry <code>a[j]</code> is in its final place in the array, for some <code>j</code>.</p>
<p class="indenthangingB">• No entry in <code>a[lo]</code> through <code>a[j-1]</code> is greater than <code>a[j]</code>.</p>
<p class="indenthangingB">• No entry in <code>a[j+1]</code> through <code>a[hi]</code> is less than <code>a[j]</code>.</p>
<p>We achieve a complete sort by partitioning, then recursively applying the method.</p>
<p>Because the partitioning process always fixes one item into its position, a formal proof by induction that the recursive method constitutes a proper sort is not difficult to develop: if the left subarray and the right subarray are both properly sorted, then the result array, made up of the left subarray (in order, with no entry larger than the partitioning item), the partitioning item, and the right subarray (in order, with no entry smaller that the partitioning item), is in order. <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> is a recursive program that implements this idea. It is a <em>randomized</em> algorithm, because it randomly shuffles the array before sorting it. Our reason for doing so is to be able to predict (and depend upon) its performance characteristics, as discussed below.</p>
<p class="image"><img alt="image" src="graphics/02_23-partition.jpg"/></p>
<p>To complete the implementation, we need to implement the partitioning method. We use the following general strategy: First, we arbitrarily choose <code>a[lo]</code> to be the <em>partitioning item</em>—the one that will go into its final position. Next, we scan from the left end of the array until we find an entry greater than (or equal to) the partitioning item, and we scan from the right end of the array until we find an entry less than (or equal to) the partitioning item. The two items that stopped the scans are out of place in the final partitioned array, so we exchange them. Continuing in this way, we ensure that no array entries to the left of the left index <code>i</code> are greater than the partitioning item, and no array entries to the right of the right index <code>j</code> are less than the partitioning item. When the scan indices cross, all that we need to do to complete the partitioning process is to exchange the partitioning item <code>a[lo]</code> with the rightmost entry of the left subarray (<code>a[j]</code>) and return its index <code>j</code>.</p>
<p>There are several subtle issues with respect to implementing quicksort that are reflected in this code and worthy of mention, because each either can lead to incorrect code or can significantly impact performance. Next, we discuss several of these issues. Later in this section, we will consider three important higher-level algorithmic improvements.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb22"/></p>
<h3><a id="page_291"/>Quicksort partitioning</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0291-01.jpg"/></p>
<p>This code partitions on the item <code>v</code> in <code>a[lo]</code>. The main loop exits when the scan indices <code>i</code> and <code>j</code> cross. Within the loop, we increment <code>i</code> while <code>a[i]</code> is less than <code>v</code> and decrement <code>j</code> while <code>a[j]</code> is greater than <code>v</code>, then do an exchange to maintain the invariant property that no entries to the left of <code>i</code> are greater than <code>v</code> and no entries to the right of <code>j</code> are smaller than <code>v</code>. Once the indices meet, we complete the partitioning by exchanging <code>a[lo]</code> with <code>a[j]</code> (thus leaving the partitioning value in <code>a[j]</code>).</p>
<p class="image"><img alt="image" src="graphics/02_24-partitionex.jpg"/></p>
<hr/>
</div>
<p><a id="ch02sec3lev10"/></p>
<h5><a id="page_292"/><em>Partitioning in place</em></h5>
<p>If we use an extra array, partitioning is easy to implement, but not so much easier that it is worth the extra cost of copying the partitioned version back into the original. A novice Java programmer might even create a new spare array within the recursive method, for each partition, which would drastically slow down the sort.</p>
<p><a id="ch02sec3lev11"/></p>
<h5><em>Staying in bounds</em></h5>
<p>If the smallest item or the largest item in the array is the partitioning item, we have to take care that the pointers do not run off the left or right ends of the array, respectively. Our <code>partition()</code> implementation has explicit tests to guard against this circumstance. The test <code>(j == lo)</code> is redundant, since the partitioning item is at <code>a[lo]</code> and not less than itself. With a similar technique on the right it is not difficult to eliminate both tests (see <a href="#ch02qa3q17"><small>EXERCISE 2.3.17</small></a>).</p>
<p><a id="ch02sec3lev12"/></p>
<h5><em>Preserving randomness</em></h5>
<p>The random shuffle puts the array in random order. Since it treats all items in the subarrays uniformly, <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> has the property that its two subarrays are also in random order. This fact is crucial to the predictability of the algorithm’s running time. An alternate way to preserve randomness is to choose a random item for partitioning within <code>partition()</code>.</p>
<p><a id="ch02sec3lev13"/></p>
<h5><em>Terminating the loop</em></h5>
<p>Experienced programmers know to take special care to ensure that any loop must always terminate, and the partitioning loop for quicksort is no exception. Properly testing whether the pointers have crossed is a bit trickier than it might seem at first glance. A common error is to fail to take into account that the array might contain other items with the same key value as the partitioning item.</p>
<p><a id="ch02sec3lev14"/></p>
<h5><em>Handling items with keys equal to the partitioning item’s key</em></h5>
<p>It is best to stop the left scan for items with keys greater than <em>or equal to</em> the partitioning item’s key and the right scan for items with key less than <em>or equal to</em> the partitioning item’s key, as in <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a>. Even though this policy might seem to create unnecessary exchanges involving items with keys equal to the partitioning item’s key, it is crucial to avoiding quadratic running time in certain typical applications (see <a href="#ch02qa3q11"><small>EXERCISE 2.3.11</small></a>). Later, we discuss a better strategy for the case when the array contains a large number of items with equal keys.</p>
<p><a id="ch02sec3lev15"/></p>
<h5><em>Terminating the recursion</em></h5>
<p>Experienced programmers also know to take special care to ensure that any recursive method must always terminate, and quicksort is again no exception. For instance, a common mistake in implementing quicksort involves not ensuring that one item is always put into position, then falling into an infinite recursive loop when the partitioning item happens to be the largest or smallest item in the array.</p>
<p><a id="ch02sec2lev20"/></p>
<h4><a id="page_293"/>Performance characteristics</h4>
<p>Quicksort has been subjected to very thorough mathematical analysis, so that we can make precise statements about its performance. The analysis has been validated through extensive empirical experience, and is a useful tool in tuning the algorithm for optimum performance.</p>
<p>The inner loop of quicksort (in the partitioning method) increments an index and compares an array entry against a fixed value. This simplicity is one factor that makes quicksort quick: it is hard to envision a shorter inner loop in a sorting algorithm. For example, mergesort and shellshort are typically slower than quicksort because they also do data movement within their inner loops.</p>
<p>The second factor that makes quicksort quick is that it uses few compares. Ultimately, the efficiency of the sort depends on how well the partitioning divides the array, which in turn depends on the value of the partitioning item’s key. Partitioning divides a large randomly ordered array into two smaller randomly ordered subarrays, but the actual split is equally likely (for distinct keys) to be anywhere in the array. Next, we consider the analysis of the algorithm, which allows us to see how this choice compares to the ideal choice.</p>
<p>The best case for quicksort is when each partitioning stage divides the array exactly in half. This circumstance would make the number of compares used by quicksort satisfy the divide-and-conquer recurrence <em>C<sub>N</sub></em> = 2<em>C</em><sub><em>N</em>/2</sub> + <em>N</em>. The 2<em>C</em><sub><em>N</em>/2</sub> term covers the cost of sorting the two subarrays; the <em>N</em> is the cost of examining each entry, using one partitioning index or the other. As in the proof of <a href="#ch02sb14"><small>PROPOSITION F</small></a> for mergesort, we know that this recurrence has the solution <em>C<sub>N</sub> ~ N</em> lg <em>N</em>. Although things do not always go this well, it is true that the partition falls in the middle <em>on the average</em>. Taking into account the precise probability of each partition position makes the recurrence more complicated and more difficult to solve, but the final result is similar. The proof of this result is the basis for our confidence in quicksort. If you are not mathematically inclined, you may wish to skip (and trust) it; if you <em>are</em> mathematically inclined, you may find it intriguing.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb23"/></p>
<p><strong>Proposition K.</strong> Quicksort uses <em>~</em> 2<em>N</em> ln <em>N</em> compares (and one-sixth that many exchanges) on the average to sort an array of length <em>N</em> with distinct keys.</p>
<p><strong>Proof:</strong> Let <em>C<sub>N</sub></em> be the average number of compares needed to sort <em>N</em> items with distinct values. We have <em>C</em><sub>0</sub> = <em>C</em><sub>1</sub> = 0 and for <em>N &gt;</em> 1 we can write a recurrence relationship that directly mirrors the recursive program:</p>
<p class="center"><a id="page_294"/><em>C<sub>N</sub></em> = <em>N</em> + 1 + (<em>C</em><sub>0</sub> + <em>C</em><sub>1</sub> + . . . + <em>C</em><sub><em>N</em>−2</sub> + <em>C</em><sub><em>N</em>−1</sub>) / <em>N</em> + (<em>C</em><sub><em>N</em>−1</sub> + <em>C</em><sub><em>N</em>−2</sub> + . . . + <em>C</em><sub>0</sub>)/<em>N</em></p>
<p>The first term is the cost of partitioning (always <em>N</em> + 1), the second term is the average cost of sorting the left subarray (which is equally likely to be any size from <em>0</em> to <em>N</em> − 1), and the third term is the average cost for the right subarray (which is the same as for the left subarray). Multiplying by <em>N</em> and collecting terms transforms this equation to</p>
<p class="center"><em>NC<sub>N</sub></em> = <em>N</em>(<em>N</em> + 1) + 2(<em>C</em><sub>0</sub> + <em>C</em><sub>1</sub>+ . . . +<em>C</em><sub><em>N</em>−2</sub>+<em>C</em><sub><em>N</em>−1</sub>)</p>
<p>Subtracting this from the same equation for <em>N</em> − 1 gives</p>
<p class="center"><em>NC<sub>N</sub></em> − (<em>N</em> − 1)<em>C</em><sub><em>N</em>−1</sub> = 2<em>N</em> + 2<em>C</em><sub><em>N</em>−1</sub></p>
<p>Rearranging terms and dividing by <em>N</em>(<em>N</em> + 1) leaves</p>
<p class="center"><em>C<sub>N</sub></em>/(<em>N</em> + 1) = <em>C</em><sub><em>N</em>−1</sub>/<em>N</em> + 2/(<em>N</em> + 1)</p>
<p>which telescopes to give the result</p>
<p class="center"><em>C<sub>N</sub></em> ~ 2 (<em>N</em> + 1)(1/3 + 1/4 + . . . + 1/(<em>N</em> + 1) )</p>
<p>The parenthesized quantity is the discrete estimate of the area under the curve 2/<em>x</em> from 3 to <em>N</em>, + 1 and <em>C<sub>N</sub></em> ~ 2<em>N</em> ln<em>N</em> by integration. Note that 2<em>N</em> ln <em>N</em> ≈ 1.39<em>N</em> lg <em>N</em>, so the average number of compares is only about 39 percent higher than in the best case.</p>
<p>A similar (but much more complicated) analysis is needed to establish the stated result for exchanges.</p>
<hr/>
</div>
<p>When keys may not be distinct, as is typical in practical applications, precise anaysis is considerably more complicated, but it is not difficult to show that the average number of compares is no greater than than <em>C<sub>N</sub></em>, even when duplicate keys may be present (on page <a href="#page_296">296</a>, we will look at a way to <em>improve</em> quicksort in this case).</p>
<p>Despite its many assets, the basic quicksort program has one potential liability: it can be extremely inefficient if the partitions are unbalanced. For example, it could be the case that the first partition is on the smallest item, the second partition on the next smallest item, and so forth, so that the program will remove just one item for each call, leading to an excessive number of partitions of large subarrays. Avoiding this situation is the primary reason that we randomly shuffle the array before using quicksort. This action makes it so unlikely that bad partitions will happen consistently that we need not worry about the possibility.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb24"/></p>
<p><a id="page_295"/><strong>Proposition L.</strong> Quicksort uses <em>~ N<sup>2</sup>/2</em> compares in the worst case, but random shuffling protects against this case.</p>
<p><strong>Proof:</strong> By the argument just given, the number of compares used when one of the subarrays is empty for every partition is</p>
<p class="center"><em>N</em> + (N − 1) + (<em>N</em> − 2) + . . . + 2 + 1 = (<em>N</em> + 1) <em>N</em> / 2</p>
<p>This behavior means not only that the time required will be quadratic but also that the space required to handle the recursion will be linear, which is unacceptable for large arrays. But (with quite a bit more work) it is possible to extend the analysis that we did for the average to find that the standard deviation of the number of compares is about .65 <em>N</em>, so the running time tends to the average as <em>N</em> grows and is unlikely to be far from the average. For example, even the rough estimate provided by Chebyshev’s inequality says that the probability that the running time is more than ten times the average for an array with a million elements is less than .00001 (and the true probability is far smaller). The probability that the running time for a large array is close to quadratic is so remote that we can safely ignore the possibility (see <a href="#ch02qa3q10"><small>EXERCISE 2.3.10</small></a>). For example, the probability that quicksort will use as many compares as insertion sort or selection sort when sorting a large array on your computer is much less than the probability that your computer will be struck by lightning during the sort!</p>
<hr/>
</div>
<p><small>IN SUMMARY</small>, you can be sure that the running time of <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> will be within a constant factor of 1.39<em>N</em> lg <em>N</em> whenever it is used to sort <em>N</em> items. The same is true of mergesort, but quicksort is typically faster because (even though it does 39 percent more compares) it does much less data movement. This mathematical assurance is probabilistic, but you can certainly rely upon it.</p>
<p><a id="ch02sec2lev21"/></p>
<h4>Algorithmic improvements</h4>
<p>Quicksort was invented in 1960 by C. A. R. Hoare, and many people have studied and refined it since that time. It is tempting to try to develop ways to improve quicksort: a faster sorting algorithm is computer science’s “better mousetrap,” and quicksort is a venerable method that seems to invite tinkering. Almost from the moment Hoare first published the algorithm, people began proposing ways to improve the algorithm. Not all of these ideas are fully successful, because the algorithm is so well-balanced that the effects of improvements can be more than offset by unexpected side effects, but a few of them, which we now consider, are quite effective.</p>
<p><a id="page_296"/>If your sort code is to be used a great many times or to sort a huge array (or, in particular, if it is to be used as a library sort that will be used to sort arrays of unknown characteristics), then it is worthwhile to consider the improvements that are discussed in the next few paragraphs. As noted, you need to run experiments to determine the effectiveness of these improvements and to determine the best choice of parameters for your implementation. Typically, improvements of 20 to 30 percent are available.</p>
<p><a id="ch02sec3lev16"/></p>
<h5><em>Cutoff to insertion sort</em></h5>
<p>As with most recursive algorithms, an easy way to improve the performance of quicksort is based on the following two observations:</p>
<p class="indenthangingB">• Quicksort is slower than insertion sort for tiny subarrays.</p>
<p class="indenthangingB">• Being recursive, quicksort’s <code>sort()</code> is certain to call itself for tiny subarrays.</p>
<p>Accordingly, it pays to switch to insertion sort for tiny subarrays. A simple change to <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> accomplishes this improvement: replace the statement</p>
<p class="programlisting">if (hi &lt;= lo) return;</p>
<p>in <code>sort()</code> with a statement that invokes insertion sort for small subarrays:</p>
<p class="programlisting">if (hi &lt;= lo + M) {  Insertion.sort(a, lo, hi); return;  }</p>
<p>The optimum value of the cutoff <code>M</code> is system-dependent, but any value between 5 and 15 is likely to work well in most situations (see <a href="#ch02qa3q25"><small>EXERCISE 2.3.25</small></a>).</p>
<p><a id="ch02sec3lev17"/></p>
<h5><em>Median-of-three partitioning</em></h5>
<p>A second easy way to improve the performance of quicksort is to use the median of a small sample of items taken from the subarray as the partitioning item. Doing so will give a slightly better partition, but at the cost of computing the median. It turns out that most of the available improvement comes from choosing a sample of size 3 and then partitioning on the middle item (see <a href="#ch02qa3q18"><small>EXERCISES 2.3.18</small></a> and <a href="#ch02qa3q19">2.3.19</a>). As a bonus, we can use the sample items as sentinels at the ends of the array and remove both array bounds tests in <code>partition()</code>.</p>
<p><a id="ch02sec3lev18"/></p>
<h5><em>Entropy-optimal sorting</em></h5>
<p>Arrays with large numbers of duplicate keys arise frequently in applications. For example, we might wish to sort a large personnel file by year of birth, or perhaps to separate females from males. In such situations, the quicksort implementation that we have considered has acceptable performance, but it can be substantially improved. For example, a subarray that consists solely of items that are equal (just one key value) does not need to be processed further, but our implementation keeps partitioning down to small subarrays. In a situation where there are large numbers of duplicate keys in the input array, the recursive nature of quicksort ensures that subarrays consisting solely of items with keys that are equal will occur often. There is potential for significant improvement, from the linearithmic-time performance of the implementations seen so far to linear-time performance.</p>
<p class="image"><a id="page_297"/><img alt="image" src="graphics/02_25-quickbars.jpg"/></p>
<p><a id="page_298"/>One straightforward idea is to partition the array into <em>three</em> parts, one each for items with keys smaller than, equal to, and larger than the partitioning item’s key. Accomplishing this partitioning is more complicated than the 2-way partitioning that we have been using, and various different methods have been suggested for the task. It was a classical programming exercise popularized by E. W. Dijkstra as the <em>Dutch National Flag</em> problem, because it is like sorting an array with three possible key values, which might correspond to the three colors on the flag.</p>
<p>Dijkstra’s solution to this problem leads to the remarkably simple partition code shown on the next page. It is based on a single left-to-right pass through the array that maintains a pointer <code>lt</code> such that <code>a[lo..lt-1]</code> is <em>less than</em> <code>v</code>, a pointer <code>gt</code> such that <code>a[gt+1, hi]</code> is <em>greater than</em> <code>v</code>, and a pointer <code>i</code> such that <code>a[lt..i-1]</code> are <em>equal to</em> <code>v</code> and <code>a[i..gt]</code> are not yet examined. Starting with <code>i</code> equal to <code>lo</code>, we process <code>a[i]</code> using the 3-way comparison given us by the <code>Comparable</code> interface (instead of using <code>less()</code>) to directly handle the three possible cases:</p>
<p class="indenthangingB">• <code>a[i]</code> less than <code>v</code>: exchange <code>a[lt]</code> with <code>a[i]</code> and increment both <code>lt</code> and <code>i</code></p>
<p class="indenthangingB">• <code>a[i]</code> greater than <code>v</code>: exchange <code>a[i]</code> with <code>a[gt]</code> and decrement <code>gt</code></p>
<p class="indenthangingB">• <code>a[i]</code> equal to <code>v</code>: increment <code>i</code></p>
<p>Each of these operations both maintains the invariant and decreases the value of <code>gt-i</code> (so that the loop terminates). Furthermore, every item encountered leads to an exchange <em>except</em> for those items with keys equal to the partitioning item’s key.</p>
<p>Though this code was developed not long after quicksort in the 1970s, it fell out of favor because it uses many more exchanges than the standard 2-way partitioning method for the common case when the number of duplicate keys in the array is not high. In the 1990s J. Bentley and D. McIlroy developed a clever implementation that overcomes this problem (see <a href="#ch02qa3q22"><small>EXERCISE 2.3.22</small></a>), and observed that 3-way partitioning makes quicksort asymptotically faster than mergesort and other methods in practical situations involving large numbers of equal keys. Later, J. Bentley and R. Sedgewick developed a proof of this fact, which we discuss next.</p>
<p class="image"><img alt="image" src="graphics/02_26-partitioneq.jpg"/></p>
<p>But we proved that mergesort is optimal. How have we defeated that lower bound? The answer to this question is that <a href="#ch02sb19"><small>PROPOSITION I</small></a> in <a href="#ch02sec1lev2"><small>SECTION 2.2</small></a> addresses worst-case performance over all possible inputs, while now we are looking at worst-case performance with some information about the key values at hand. Mergesort does not guarantee optimal performance for any given distribution of duplicates in the input: for example, mergesort is linearithmic for a randomly ordered array that has only a constant number of distinct key values, but quicksort with 3-way partitioning is linear for such an array. Indeed, by examining the visual trace above, you can see that <em>N</em> times the number of key values is a conservative bound on the running time.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb25"/></p>
<h3><a id="page_299"/>Quicksort with 3-way partitioning</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0299-01.jpg"/></p>
<p>This sort code partitions to put keys equal to the partitioning element in place and thus does not have to include those keys in the subarrays for the recursive calls. It is far more efficient than the standard quicksort implementation for arrays with large numbers of duplicate keys (see text).</p>
<p class="image"><img alt="image" src="graphics/02_27-partition3way.jpg"/></p>
<hr/>
</div>
<p class="image"><a id="page_300"/><img alt="image" src="graphics/02_28-quick3waybars.jpg"/></p>
<p>The analysis that makes these notions precise takes the distribution of key values into account. Given <em>N</em> keys with <em>k</em> distinct key values, for each <em>i</em> from 1 to <em>k</em> define <em>f<sub>i</sub></em> to be frequency of occurrence of the <em>i</em>th key value and <em>p<sub>i</sub></em> to be <em>f<sub>i</sub></em> / <em>N</em>, the probability that the <em>i</em>th key value is found when a random entry of the array is sampled. The <em>Shannon entropy</em> of the keys (a classic measure of their information content) is defined as</p>
<p class="center"><em>H</em> = − (<em>p</em><sub>1</sub> <em>lg p</em><sub>1</sub> + <em>p</em><sub>2</sub> <em>lg p</em><sub>2</sub> + . . . + <em>p<sub>k</sub> lg p<sub>k</sub></em>)</p>
<p>Given any array of items to be sorted, we can calculate its entropy by counting the frequency of each key value. Remarkably, we can also derive from the entropy both a lower bound on the number of compares and an upper bound on the number of compares used by quicksort with 3-way partitioning.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb26"/></p>
<p><strong>Proposition M.</strong> No compare-based sorting algorithm can guarantee to sort <em>N</em> items with fewer than <em>NH</em> − <em>N</em> compares, where <em>H</em> is the Shannon entropy, defined from the frequencies of key values.</p>
<p><strong>Proof sketch:</strong> This result follows from a (relatively easy) generalization of the lower bound proof of <a href="#ch02sb19"><small>PROPOSITION I</small></a> in <a href="#ch02sec1lev2"><small>SECTION 2.2</small></a>.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb27"/></p>
<p><a id="page_301"/><strong>Proposition N.</strong> Quicksort with 3-way partitioning uses ~ (2ln 2) <em>N H</em> compares to sort <em>N</em> items, where <em>H</em> is the Shannon entropy, defined from the frequencies of key values.</p>
<p><strong>Proof sketch:</strong> This result follows from a (relatively difficult) generalization of the average-case analysis of quicksort in <a href="#ch02sb23"><small>PROPOSITION K</small></a>. As with distinct keys, this costs about 39 percent more than the optimum (but within a constant factor).</p>
<hr/>
</div>
<p>Note that <em>H</em> = lg <em>N</em> when the keys are all distinct (all the probabilities are 1/<em>N</em>), which is consistent with <a href="#ch02sb19"><small>PROPOSITION I</small></a> in <a href="#ch02sec1lev2"><small>SECTION 2.2</small></a> and <a href="#ch02sb23"><small>PROPOSITION K</small></a>. The worst case for 3-way partitioning happens when the keys are distinct; when duplicate keys are present, it can do much better than mergesort. More important, these two properties together imply that quicksort with 3-way partitioning is <em>entropy-optimal</em>, in the sense that the average number of compares used by the best possible compare-based sorting algorithm and the average number of compares used by 3-way quicksort are within a constant factor of one another, for any given distribution of input key values.</p>
<p>As with standard quicksort, the running time tends to the average as the array size grows, and large deviations from the average are extremely unlikely, so that you can depend on 3-way quicksort’s running time to be proportional to <em>N</em> times the entropy of the distribution of input key values. This property of the algorithm is important in practice because <em>it reduces the time of the sort from linearithmic to linear for arrays with large numbers of duplicate keys</em>. The order of the keys is immaterial, because the algorithm shuffles them to protect against the worst case. The distribution of keys defines the entropy and no compare-based algorithm can use fewer compares than defined by the entropy. This ability to adapt to duplicates in the input makes 3-way quicksort the algorithm of choice for a library sort—clients that sort arrays containing large numbers of duplicate keys are not unusual.</p>
<p><small>A CAREFULLY TUNED VERSION</small> of quicksort is likely to run significantly faster on most computers for most applications than will any other compare-based sorting method. Quicksort is widely used throughout today’s computational infrastructure because the mathematical models that we have discussed suggest that it will outperform other methods in practical applications, and extensive experiments and experience over the past several decades have validated that conclusion.</p>
<p>We will see in <a href="ch05.html#ch05"><small>CHAPTER 5</small></a> that this is not quite the end of the story in the development of sorting algorithms, because is it possible to develop algorithms that do not use compares at all! But a version of quicksort turns out to be best in that situation, as well.</p>
<p><a id="ch02sec2lev22"/></p>
<h4><a id="page_302"/>Q &amp; A</h4>
<p><strong>Q.</strong> Is there some way to just divide the array into two halves, rather than letting the partitioning element fall where it may?</p>
<p><strong>A.</strong> That is a question that stumped experts for over a decade. It is tantamount to finding the <em>median</em> key value in the array and then partitioning on that value. We discuss the problem of finding the median on page <a href="#page_346">346</a>. It is possible to do so in linear time, but the cost of doing so with known algorithms (which are based on quicksort partitioning!) far exceeds the 39 percent savings available from splitting the array into equal parts.</p>
<p><strong>Q.</strong> Randomly shuffling the array seems to take a significant fraction of the total time for the sort. Is doing so really worthwhile?</p>
<p><strong>A.</strong> Yes. It protects against the worst case and makes the running time predictable. Hoare proposed this approach when he presented the algorithm in 1960—it is a prototypical (and among the first) randomized algorithm.</p>
<p><strong>Q.</strong> Why all the focus on items with equal keys?</p>
<p><strong>A.</strong> The issue directly impacts performance in practical situations. It was overlooked by many for decades, with the result that some older implementations of quicksort take quadratic time for arrays with large numbers of items with equal keys, which certainly do arise in applications. Better implementations such as <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> take linearithmic time for such arrays, but improving that to linear-time as in the entropy-optimal sort at the end of this section is worthwhile in many situations.</p>
<p><a id="ch02sec2lev23"/></p>
<h4><a id="page_303"/>Exercises</h4>
<p><a id="ch02qa3q1"/><strong>2.3.1</strong> Show, in the style of the trace given with <code>partition()</code>, how that method patitions the array <code>E A S Y Q U E S T I O N</code>.</p>
<p><a id="ch02qa3q2"/><strong>2.3.2</strong> Show, in the style of the quicksort trace given in this section, how quicksort sorts the array <code>E A S Y Q U E S T I O N</code> (for the purposes of this exercise, ignore the initial shuffle).</p>
<p><a id="ch02qa3q3"/><strong>2.3.3</strong> What is the maximum number of times during the execution of <code>Quick.sort()</code> that the largest item can be exchanged, for an array of length <em>N</em>?</p>
<p><a id="ch02qa3q4"/><strong>2.3.4</strong> Suppose that the initial random shuffle is omitted. Give six arrays of ten elements for which <code>Quick.sort()</code> uses the worst-case number of compares.</p>
<p><a id="ch02qa3q5"/><strong>2.3.5</strong> Give a code fragment that sorts an array that is known to consist of items having just two distinct keys.</p>
<p><a id="ch02qa3q6"/><strong>2.3.6</strong> Write a program to compute the exact value of <em>C<sub>N</sub></em>, and compare the exact value with the approximation 2<em>N</em> ln <em>N</em>, for <em>N</em> = 100, 1,000, and 10,000.</p>
<p><a id="ch02qa3q7"/><strong>2.3.7</strong> Find the expected number of subarrays of size 0, 1, and 2 when quicksort is used to sort an array of <em>N</em> items with distinct keys. If you are mathematically inclined, do the math; if not, run some experiments to develop hypotheses.</p>
<p><a id="ch02qa3q8"/><strong>2.3.8</strong> About how many compares will <code>Quick.sort()</code> make when sorting an array of <em>N</em> items that are all equal?</p>
<p><a id="ch02qa3q9"/><strong>2.3.9</strong> Explain what happens when <code>Quick.sort()</code> is run on an array having items with just two distinct keys, and then explain what happens when it is run on an array having just three distinct keys.</p>
<p><a id="ch02qa3q10"/><strong>2.3.10</strong> <em>Chebyshev’s inequality</em> says that the probability that a random variable is more than <em>k</em> standard deviations away from the mean is less than 1/<em>k</em><sup>2</sup>. For <em>N</em> = 1 million, use Chebyshev’s inequality to bound the probability that the number of compares used by quicksort is more than 100 billion (.1 <em>N</em><sup>2</sup>).</p>
<p><a id="ch02qa3q11"/><strong>2.3.11</strong> Suppose that we scan over items with keys equal to the partitioning item’s key instead of stopping the scans when we encounter them. Show that the running time of this version of quicksort is quadratic for all arrays with just a constant number of distinct keys.</p>
<p><a id="page_304"/><a id="ch02qa3q12"/><strong>2.3.12</strong> Show, in the style of the trace given with the code, how the entropy-optimal sort first partitions the array <code>B A B A B A B A C A D A B R A</code>.</p>
<p><a id="ch02qa3q13"/><strong>2.3.13</strong> What is the <em>recursive depth</em> of quicksort, in the best, worst, and average cases? This is the size of the stack that the system needs to keep track of the recursive calls. See <a href="#ch02qa3q20"><small>EXERCISE 2.3.20</small></a> for a way to guarantee that the recursive depth is logarithmic in the worst case.</p>
<p><a id="ch02qa3q14"/><strong>2.3.14</strong> Prove that when running quicksort on an array with <em>N</em> distinct items, the probability of comparing the <em>i</em>th and <em>j</em>th largest items is 2/(<em>j</em> − <em>i</em>). Then use this result to prove <a href="#ch02sb23"><small>PROPOSITION K</small></a>.</p>
<p><a id="ch02sec2lev24"/></p>
<h4><a id="page_305"/>Creative Problems</h4>
<p><a id="ch02qa3q15"/><strong>2.3.15</strong> <em>Nuts and bolts.</em> (G. J. E. Rawlins) You have a mixed pile of <em>N</em> nuts and <em>N</em> bolts and need to quickly find the corresponding pairs of nuts and bolts. Each nut matches exactly one bolt, and each bolt matches exactly one nut. By fitting a nut and bolt together, you can see which is bigger, but it is not possible to directly compare two nuts or two bolts. Give an efficient method for solving the problem.</p>
<p><a id="ch02qa3q16"/><strong>2.3.16</strong> <em>Best case.</em> Write a program that produces a best-case array (with no duplicates) for <code>sort()</code> in <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a>: an array of <em>N</em> items with distinct keys having the property that every partition will produce subarrays that differ in size by at most 1 (the same subarray sizes that would happen for an array of <em>N</em> equal keys). (For the purposes of this exercise, ignore the initial shuffle.)</p>
<p><em>The following exercises describe variants of quicksort. Each of them calls for an implementation, but naturally you will also want to use</em> <code>SortCompare</code> <em>for experiments to evaluate the effectiveness of each suggested modification.</em></p>
<p><a id="ch02qa3q17"/><strong>2.3.17</strong> <em>Sentinels.</em> Modify the code in <a href="#ch02sb21"><small>ALGORITHM 2.5</small></a> to remove both bounds checks in the inner <code>while</code> loops. The test against the left end of the subarray is redundant since the partitioning item acts as a sentinel (<code>v</code> is never less than <code>a[lo]</code>). To enable removal of the other test, put an item whose key is the largest in the whole array into <code>a[length-1]</code> just after the shuffle. This item will never move (except possibly to be swapped with an item having the same key) and will serve as a sentinel in all subarrays involving the end of the array. <em>Note</em>: When sorting interior subarrays, the leftmost entry in the subarray to the right serves as a sentinel for the right end of the subarray.</p>
<p><a id="ch02qa3q18"/><strong>2.3.18</strong> <em>Median-of-3 partitioning.</em> Add median-of-3 partitioning to quicksort, as described in the text (see page <a href="#page_296">296</a>). Run doubling tests to determine the effectiveness of the change.</p>
<p><a id="ch02qa3q19"/><strong>2.3.19</strong> <em>Median-of-5 partitioning.</em> Implement a quicksort based on partitioning on the median of a random sample of five items from the subarray. Put the items of the sample at the appropriate ends of the array so that only the median participates in partitioning. Run doubling tests to determine the effectiveness of the change, in comparison both to the standard algorithm and to median-of-3 partitioning (see the previous exercise). <em>Extra credit</em>: Devise a median-of-5 algorithm that uses fewer than seven compares on any input.</p>
<p><a id="page_306"/><a id="ch02qa3q20"/><strong>2.3.20</strong> <em>Nonrecursive quicksort.</em> Implement a nonrecursive version of quicksort based on a main loop where a subarray is popped from a stack to be partitioned, and the resulting subarrays are pushed onto the stack. <em>Note</em>: Push the larger of the subarrays onto the stack first, which guarantees that the stack will have at most lg <em>N</em> entries.</p>
<p><a id="ch02qa3q21"/><strong>2.3.21</strong> <em>Lower bound for sorting with equal keys.</em> Complete the first part of the proof of <a href="#ch02sb26"><small>PROPOSITION M</small></a> by following the logic in the proof of <a href="#ch02sb19"><small>PROPOSITION I</small></a> and using the observation that there are <em>N</em>! / <em>f</em><sub>1</sub>!<em>f</em><sub>2</sub>! . . . <em>f<sub>k</sub></em>! different ways to arrange keys with <em>k</em> different values, where the <em>i</em> th value appears with frequency <em>f<sub>i</sub></em> (= <em>Np<sub>i</sub></em>, in the notation of <a href="#ch02sb26"><small>PROPOSITION M</small></a>), with <em>f</em><sub>1</sub>+ . . . +<em>f<sub>k</sub></em> = <em>N</em>.</p>
<p><a id="ch02qa3q22"/><strong>2.3.22</strong> <em>Fast 3-way partitioning.</em> (J. Bentley and D. McIlroy) Implement an entropy-optimal sort based on keeping item’s with equal keys at both the left and right ends of the subarray. Maintain indices <code>p</code> and <code>q</code> such that <code>a[lo..p-1]</code> and <code>a[q+1..hi]</code> are all equal to <code>a[lo]</code>, an index <code>i</code> such that <code>a[p..i-1]</code> are all less than <code>a[lo]</code>, and an index <code>j</code> such that <code>a[j+1..q]</code> are all greater than <code>a[lo]</code>. Add to the inner partitioning loop code to swap <code>a[i]</code> with <code>a[p]</code> (and increment <code>p</code>) if it is equal to <code>v</code> and to swap <code>a[j]</code> with <code>a[q]</code> (and decrement <code>q</code>) if it is equal to <code>v</code> before the usual comparisons of <code>a[i]</code> and <code>a[j]</code> with <code>v</code>. After the partitioning loop has terminated, add code to swap the items with equal keys into position. <em>Note</em>: This code complements the code given in the text, in the sense that it does extra swaps for keys equal to the partitioning item’s key, while the code in the text does extra swaps for keys that are <em>not</em> equal to the partitioning item’s key.</p>
<p class="image"><img alt="image" src="graphics/02_29-partitionbentmac.jpg"/></p>
<p><a id="ch02qa3q23"/><strong>2.3.23</strong> <em>Java system sort.</em> Add to your implementation from <a href="#ch02qa3q22"><small>EXERCISE 2.3.22</small></a> code to use the <em>Tukey ninther</em> to compute the partitioning item—choose three sets of three items, take the median of each, then use the median of the three medians as the partitioning item. Also, add a cutoff to insertion sort for small subarrays.</p>
<p><a id="ch02qa3q24"/><strong>2.3.24</strong> <em>Samplesort.</em> (W. Frazer and A. McKellar) Implement a quicksort based on using a sample of size 2<em><sup>k</sup></em> − 1. First, sort the sample, then arrange to have the recursive routine partition on the median of the sample and to move the two halves of the rest of the sample to each subarray, such that they can be used in the subarrays, without having to be sorted again. This algorithm is called <em>samplesort</em>.</p>
<p><a id="ch02sec2lev25"/></p>
<h4><a id="page_307"/>Experiments</h4>
<p><a id="ch02qa3q25"/><strong>2.3.25</strong> <em>Cutoff to insertion sort.</em> Implement quicksort with a cutoff to insertion sort for subarrays with less than <em>M</em> elements, and empirically determine the value of <em>M</em> for which quicksort runs fastest in your computing environment to sort random arrays of <em>N</em> doubles, for <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>. Plot average running times for <em>M</em> from 0 to 30 for each value of <em>M</em>. <em>Note</em>: You need to add a three-argument <code>sort()</code> method to <a href="#ch02sb07"><small>ALGORITHM 2.2</small></a> for sorting subarrays such that the call <code>Insertion.sort(a, lo, hi)</code> sorts the subarray <code>a[lo..hi]</code>.</p>
<p><a id="ch02qa3q26"/><strong>2.3.26</strong> <em>Subarray sizes.</em> Write a program that plots a histogram of the subarray sizes left for insertion sort when you run quicksort for an array of size <em>N</em> with a cutoff for subarrays of size less than <em>M</em>. Run your program for <em>M</em>=10, 20, and 50 and <em>N</em> = 10<sup>5</sup>.</p>
<p><a id="ch02qa3q27"/><strong>2.3.27</strong> <em>Ignore small subarrays.</em> Run experiments to compare the following strategy for dealing with small subarrays with the approach described in <a href="#ch02qa3q25"><small>EXERCISE 2.3.25</small></a>: Simply ignore the small subarrays in quicksort, then run a single insertion sort after the quicksort completes. <em>Note</em>: You may be able to estimate the size of your computer’s cache memory with these experiments, as the performance of this method is likely to degrade when the array does not fit in the cache.</p>
<p><a id="ch02qa3q28"/><strong>2.3.28</strong> <em>Recursion depth.</em> Run empirical studies to determine the average recursive depth used by quicksort with cutoff for arrays of size <em>M</em>, when sorting arrays of <em>N</em> distinct elements, for <em>M</em>=10, 20, and 50 and <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p>
<p><a id="ch02qa3q29"/><strong>2.3.29</strong> <em>Randomization.</em> Run empirical studies to compare the effectiveness of the strategy of choosing a random partitioning item with the strategy of initially randomizing the array (as in the text). Use a cutoff for arrays of size <em>M</em>, and sort arrays of <em>N</em> distinct elements, for <em>M</em>=10, 20, and 50 and <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p>
<p><a id="ch02qa3q30"/><strong>2.3.30</strong> <em>Corner cases.</em> Test quicksort on large nonrandom arrays of the kind described in <a href="#ch02qa1q35"><small>EXERCISES 2.1.35</small></a> and <a href="#ch02qa1q36">2.1.36</a> both with and without the initial random shuffle. How does shuffling affect its performance for these arrays?</p>
<p><a id="ch02qa3q31"/><strong>2.3.31</strong> <em>Histogram of running times.</em> Write a program that takes command-line arguments <em>N</em> and <em>T</em>, does <em>T</em> trials of the experiment of running quicksort on an array of <em>N</em> random <code>Double</code> values, and plots a histogram of the observed running times. Run your program for <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>, with <em>T</em> as large as you can afford to make the curves smooth. Your main challenge for this exercise is to appropriately scale the experimental results.</p>
<p><a id="ch02sec1lev4"/></p>
<h3><a id="page_308"/>2.4 Priority Queues</h3>
<p><small>MANY APPLICATIONS REQUIRE</small> that we process items having keys in order, but not necessarily in full sorted order and not necessarily all at once. Often, we collect a set of items, then process the one with the largest key, then perhaps collect more items, then process the one with the current largest key, and so forth. For example, you are likely to have a computer (or a cellphone) that is capable of running several applications at the same time. This effect is typically achieved by assigning a priority to events associated with applications, then always choosing to process next the highest-priority event. For example, most cellphones are likely to process an incoming call with higher priority than a game application.</p>
<p>An appropriate data type in such an environment supports two operations: <em>remove the maximum</em> and <em>insert</em>. Such a data type is called a <em>priority queue</em>. Using priority queues is similar to using queues (remove the oldest) and stacks (remove the newest), but implementing them efficiently is more challenging.</p>
<p>In this section, after a short discussion of elementary representations where one or both of the operations take linear time, we consider a classic priority-queue implementation based on the <em>binary heap</em> data structure, where items are kept in an array, subject to certain ordering constraints that allow for efficient (logarithmic-time) implementations of <em>remove the maximum</em> and <em>insert</em>.</p>
<p>Some important applications of priority queues include simulation systems, where the keys correspond to event times, to be processed in chronological order; job scheduling, where the keys correspond to priorities indicating which tasks are to be performed first; and numerical computations, where the keys represent computational errors, indicating in which order we should deal with them. We consider in <a href="ch06.html#ch06"><small>CHAPTER 6</small></a> a detailed case study showing the use of priority queues in a particle-collision simulation.</p>
<p>We can use any priority queue as the basis for a sorting algorithm by inserting a sequence of items, then successively removing the smallest to get them out, in order. An important sorting algorithm known as <em>heapsort</em> also follows naturally from our heap-based priority-queue implementations. Later on in this book, we shall see how to use priority queues as building blocks for other algorithms. In <a href="ch04.html#ch04"><small>CHAPTER 4</small></a>, we shall see how priority queues are an appropriate abstraction for implementing several fundamental graph-searching algorithms; in <a href="ch05.html#ch05"><small>CHAPTER 5</small></a>, we shall develop a data-compression algorithm using methods from this section. These are but a few examples of the important role played by the priority queue as a tool in algorithm design.</p>
<p><a id="ch02sec2lev26"/></p>
<h4><a id="page_309"/>API</h4>
<p>The priority queue is a prototypical <em>abstract data type</em> (see <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a>): it represents a set of values and operations on those values, and it provides a convenient abstraction that allows us to separate application programs (clients) from various implementations that we will consider in this section. As in <a href="ch01.html#ch01sec1lev4"><small>SECTION 1.2</small></a>, we precisely define the operations by specifying an applications programming interface (API) that provides the information needed by clients. Priority queues are characterized by the <em>remove the maximum</em> and <em>insert</em> operations, so we shall focus on them. We use the method names <code>delMax()</code> for <em>remove the maximum</em> and <code>insert()</code> for <em>insert</em>. By convention, we will compare keys only with a helper <code>less()</code> method, as we have been doing for sorting. Thus, if items can have duplicate keys, <em>maximum</em> means <em>any</em> item with the largest key value. To complete the API, we also need to add constructors (like the ones we used for stacks and queues) and a <em>test if empty</em> operation. For flexibility, we use a generic implementation with a parameterized type <code>Key</code> that implements the <code>Comparable</code> interface. This choice eliminates our distinction between items and keys and enables clearer and more compact descriptions of data structures and algorithms. For example, we refer to the “largest key” instead of the “largest item” or the “item with the largest key.”</p>
<p>For convenience in client code, the API includes three constructors, which enable clients to build priority queues of an initial fixed size (perhaps initialized with a given array of keys). To clarify client code, we will use a separate class <code>MinPQ</code> whenever appropriate, which is the same as <code>MaxPQ</code> except that it has a <code>delMin()</code> method that deletes and returns an item with the smallest key in the queue. Any <code>MaxPQ</code> implementation is easily converted into a <code>MinPQ</code> implementation and vice versa, simply by reversing the sense of the comparison in <code>less()</code>.</p>
<p class="image"><img alt="image" src="graphics/t0309-01.jpg"/></p>
<p><a id="ch02sec3lev19"/></p>
<h5><a id="page_310"/><em>A priority-queue client</em></h5>
<p>To appreciate the value of the priority-queue abstraction, consider the following problem: You have a huge input stream of <code>N</code> strings and associated integer values, and your task is to find the largest or smallest <code>M</code> integers (and associated strings) in the input stream. You might imagine the stream to be financial transactions, where your interest is to find the big ones, or pesticide levels in an agricultural product, where your interest is to find the small ones, or requests for service, or results from a scientific experiment, or whatever. In some applications, the size of the input stream is so huge that it is best to consider it to be unbounded. One way to address this problem would be to sort the input stream and take the <code>M</code> largest keys from the result, but we have just stipulated that the input stream is too large for that. Another approach would be to compare each new key against the <code>M</code> largest seen so far, but that is also likely to be prohibitively expensive unless <code>M</code> is small. With priority queues, we can solve the problem with the <code>MinPQ</code> client <code>TopM</code> on the next page <em>provided</em> that we can develop efficient implementations of both <code>insert()</code> and <code>delMin()</code>. That is precisely our aim in this section. For the huge values of <code>N</code> that are likely to be encountered in our modern computational infrastructure, these implementations can make the difference between being able to address such a problem and not having the resources to do it at all.</p>
<p class="image"><img alt="image" src="graphics/t0310-01.jpg"/></p>
<p><a id="ch02sec2lev27"/></p>
<h4>Elementary implementations</h4>
<p>The basic data structures that we discussed in <a href="ch01.html#ch01"><small>CHAPTER 1</small></a> provide us with four immediate starting points for implementing priority queues. We can use an array or a linked list, kept in order or unordered. These implementations are useful for small priority queues, situations where one of the two operations are predominant, or situations where some assumptions can be made about the order of the keys involved in the operations. Since these implementations are elementary, we will be content with brief descriptions here in the text and leave the code for exercises (see <a href="#ch02qa4q3"><small>EXERCISE 2.4.3</small></a>).</p>
<p><a id="ch02sec3lev20"/></p>
<h5><em>Array representation (unordered)</em></h5>
<p>Perhaps the simplest priority-queue implementation is based on our code for pushdown stacks in <a href="#ch02sec1lev1"><small>SECTION 2.1</small></a>. The code for <em>insert</em> in the priority queue is the same as for <em>push</em> in the stack. To implement <em>remove the maximum</em>, we can add code like the inner loop of selection sort to exchange the maximum item with the item at the end and then delete that one, as we did with <code>pop()</code> for stacks. As with stacks, we can add resizing-array code to ensure that the data structure is always at least one-quarter full and never overflows.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb28"/></p>
<h3><a id="page_311"/>A priority-queue client</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0311-01.jpg"/></p>
<p>Given an integer <code>M</code> from the command line and an input stream where each line contains a transaction, this <code>MinPQ</code> client prints the <code>M</code> lines whose numbers are the highest. It does so by using our <code>Transaction</code> class (see page <a href="ch01.html#page_79">79</a>, <a href="ch01.html#ch01qa2q19"><small>EXERCISE 1.2.19</small></a>, and <a href="#ch02qa1q21"><small>EXERCISE 2.1.21</small></a>) to build a priority queue using the numbers as keys, deleting the minimum after each insertion once the size of the priority queue reaches <code>M</code>. Once all the transactions have been processed, the top <code>M</code> come off the priority queue in increasing order, so this code puts them on a stack, then iterates through the stack to reverse the order and print them in decreasing order.</p>
<p class="image"><img alt="image" src="graphics/p0311-02.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0311-03.jpg"/></p>
<hr/>
</div>
<p><a id="ch02sec3lev21"/></p>
<h5><a id="page_312"/><em>Array representation (ordered)</em></h5>
<p>Another approach is to add code for <em>insert</em> to move larger entries one position to the right, thus keeping the keys in the array in order (as in insertion sort). Thus, the largest entry is always at the end, and the code for <em>remove the maximum</em> in the priority queue is the same as for <em>pop</em> in the stack.</p>
<p><a id="ch02sec3lev22"/></p>
<h5><em>Linked-list representations</em></h5>
<p>Similarly, we can start with our linked-list code for pushdown stacks, modifying either the code for <code>pop()</code> to find and return the maximum or the code for <code>push()</code> to keep keys in <em>reverse</em> order and the code for <code>pop()</code> to unlink and return the first (maximum) item on the list.</p>
<p>Using unordered sequences is the prototypical <em>lazy</em> approach to this problem, where we defer doing work until necessary (to find the maximum); using ordered sequences is the prototypical <em>eager</em> approach to the problem, where we do as much work as we can up front (keep the list sorted on insertion) to make later operations efficient.</p>
<p class="image"><img alt="image" src="graphics/t0312-01.jpg"/></p>
<p>The significant difference between implementing stacks or queues and implementing priority queues has to do with performance. For stacks and queues, we were able to develop implementations of all the operations that take <em>constant</em> time; for priority queues, all of the elementary implementations just discussed have the property that either the <em>insert</em> or the <em>remove the maximum</em> operation takes <em>linear</em> time in the worst case. The <em>heap</em> data structure that we consider next enables implementations where <em>both</em> operations are guaranteed to be fast.</p>
<p class="image"><img alt="image" src="graphics/t0312-02.jpg"/></p>
<p><a id="ch02sec2lev28"/></p>
<h4><a id="page_313"/>Heap definitions</h4>
<p>The <em>binary heap</em> is a data structure that can efficiently support the basic priority-queue operations. In a binary heap, the keys are stored in an array such that each key is guaranteed to be larger than (or equal to) the keys at two other specific positions. In turn, each of those keys must be larger than (or equal to) two additional keys, and so forth. This ordering is easy to see if we view the keys as being in a binary tree structure with edges from each key to the two keys known to be smaller.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb29"/></p>
<p><strong>Definition.</strong> A binary tree is <em>heap-ordered</em> if the key in each node is larger than or equal to the keys in that node’s two children (if any).</p>
<hr/>
</div>
<p>Equivalently, the key in each node of a heap-ordered binary tree is smaller than or equal to the key in that node’s parent (if any). Moving up from any node, we get a nondecreasing sequence of keys; moving down from any node, we get a nonincreasing sequence of keys. In particular:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb30"/></p>
<p><strong>Proposition O.</strong> The largest key in a heap-ordered binary tree is found at the root.</p>
<p><strong>Proof:</strong> By induction on the size of the tree.</p>
<hr/>
</div>
<p><a id="ch02sec3lev23"/></p>
<h5><em>Binary heap representation</em></h5>
<p>If we use a linked representation for heap-ordered binary trees, we would need to have three links associated with each key to allow travel up and down the tree (each node would have one pointer to its parent and one to each child). It is particularly convenient, instead, to use a <em>complete</em> binary tree like the one drawn at right. We draw such a structure by placing the root node and then proceeding down the page and from left to right, drawing and connecting two nodes beneath each node on the previous level until we have drawn <em>N</em> nodes. Complete trees provide the opportunity to use a compact array representation that does not involve explicit links. Specifically, we represent complete binary trees sequentially within an array by putting the nodes in <em>level order</em>, with the root at position 1, its children at positions 2 and 3, their children in positions 4, 5, 6, and 7, and so on.</p>
<p class="image"><img alt="image" src="graphics/02_30-heaptree.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb31"/></p>
<p><a id="page_314"/><strong>Definition.</strong> A <em>binary heap</em> is a collection of keys arranged in a complete heap-ordered binary tree, represented in level order in an array (not using the first entry).</p>
<hr/>
</div>
<p>(For brevity, from now on we drop the “binary” modifier and use the term <em>heap</em> when referring to a binary heap.) In a heap, the parent of the node in position <em>k</em> is in position <img alt="image" src="graphics/lfloor.jpg"/><em>k</em>/2<img alt="image" src="graphics/rfloor.jpg"/> and, conversely, the two children of the node in position <em>k</em> are in positions 2<em>k</em> and 2<em>k</em> + 1. Instead of using explicit links (as in the binary tree structures that we will consider in <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>), we can travel up and down by doing simple arithmetic on array indices: to move <em>up</em> the tree from <code>a[k]</code> we set <code>k</code> to <code>k/2</code>; to move <em>down</em> the tree we set <code>k</code> to <code>2*k</code> or <code>2*k+1</code>.</p>
<p class="image"><img alt="image" src="graphics/02_31-Heap.jpg"/></p>
<p>Complete binary trees represented as arrays (heaps) are rigid structures, but they have just enough flexibility to allow us to implement efficient priority-queue operations. Specifically, we will use them to develop logarithmic-time <em>insert</em> and <em>remove the maximum</em> implementations. These algorithms take advantage of the capability to move up and down paths in the tree without pointers and have guaranteed logarithmic performance because of the following property of complete binary trees:</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb32"/></p>
<p><strong>Proposition P.</strong> The height of a complete binary tree of size <em>N</em> is <img alt="image" src="graphics/lfloor.jpg"/> lg <em>N</em> <img alt="image" src="graphics/rfloor.jpg"/>.</p>
<p><strong>Proof:</strong> The stated result is easy to prove by induction or by noting that the height increases by 1 when <em>N</em> is a power of 2.</p>
<hr/>
</div>
<p><a id="ch02sec2lev29"/></p>
<h4><a id="page_315"/>Algorithms on heaps</h4>
<p>We represent a heap of size <em>N</em> in private array <code>pq[]</code> of length <em>N</em> + 1, with <code>pq[0]</code> unused and the heap in <code>pq[1]</code> through <code>pq[N]</code>. As for sorting algorithms, we access keys only through private helper functions <code>less()</code> and <code>exch()</code>, but since all items are in the instance variable <code>pq[]</code>, we use the more compact implementations on the next page that do not involve passing the array name as a parameter. The heap operations that we consider work by first making a simple modification that could violate the heap condition, then traveling through the heap, modifying the heap as required to ensure that the heap condition is satisfied everywhere. We refer to this process as <em>reheapifying</em>, or <em>restoring heap order</em>.</p>
<p class="image"><img alt="image" src="graphics/p0315-01.jpg"/></p>
<p>There are two cases. When the priority of some node is increased (or a new node is added at the bottom of a heap), we have to travel <em>up</em> the heap to restore the heap order. When the priority of some node is decreased (for example, if we replace the node at the root with a new node that has a smaller key), we have to travel <em>down</em> the heap to restore the heap order. First, we will consider how to implement these two basic auxiliary operations; then, we shall see how to use them to implement <em>insert</em> and <em>remove the maximum</em>.</p>
<p><a id="ch02sec3lev24"/></p>
<h5><em>Bottom-up reheapify (swim)</em></h5>
<p>If the heap order is violated because a node’s key becomes <em>larger</em> than that node’s parent’s key, then we can make progress toward fixing the violation by exchanging the node with its parent. After the exchange, the node is larger than both its children (one is the old parent, and the other is smaller than the old parent because it was a child of that node) but the node may still be larger than its parent. We can fix that violation in the same way, and so forth, moving up the heap until we reach a node with a larger key, or the root. Coding this process is straightforward when you keep in mind that the parent of the node at position <code>k</code> in a heap is at position <code>k/2</code>. The loop in <code>swim()</code> preserves the invariant that the only place the heap <a id="page_316"/>order could be violated is when the node at position <code>k</code> might be larger than its parent. Therefore, when we get to a place where that node is not larger than its parent, we know that the heap order is satisfied throughout the heap. To justify the method’s name, we think of the new node, having too large a key, as having to <em>swim</em> to a higher level in the heap.</p>
<p class="image"><img alt="image" src="graphics/02_32-heapswim.jpg"/></p>
<p class="image"><img alt="image" src="graphics/p0316-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/02_33-heapsink.jpg"/></p>
<p><a id="ch02sec3lev25"/></p>
<h5><em>Top-down reheapify (sink)</em></h5>
<p>If the heap order is violated because a node’s key becomes <em>smaller</em> than one or both of that node’s children’s keys, then we can make progress toward fixing the violation by exchanging the node with the <em>larger</em> of its two children. This switch may cause a violation at the child; we fix that violation in the same way, and so forth, moving down the heap until we reach a node with both children smaller (or equal), or the bottom. The code again follows directly from the fact that the children of the node at position <code>k</code> in a heap are at positions <code>2k</code> and <code>2k+1</code>. To justify the method’s name, we think about the node, having too small a key, as having to <em>sink</em> to a lower level in the heap.</p>
<p class="image"><img alt="image" src="graphics/p0316-02.jpg"/></p>
<p><small>IF WE IMAGINE</small> the heap to represent a cutthroat corporate hierarchy, with each of the children of a node representing subordinates (and the parent representing the immediate superior), then these operations have amusing interpretations. The <code>swim()</code> operation corresponds to a promising new manager arriving on the scene, being promoted up the chain of command (by exchanging jobs with any lower-qualified boss) until the new person encounters a higher-qualified boss. The <code>sink()</code> operation is analogous to the situation when the president of the company resigns and is replaced by someone from the outside. If the president’s most powerful subordinate <a id="page_317"/>is stronger than the new person, they exchange jobs, and we move down the chain of command, demoting the new person and promoting others until the level of competence of the new person is reached, where there is no higher-qualified subordinate. These idealized scenarios may rarely be seen in the real world, but they may help you better understand basic operation on heaps.</p>
<p>These <code>sink()</code> and <code>swim()</code> operations provide the basis for efficient implementation of the priority-queue API, as diagrammed below and implemented in <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a><br/>on the next page.</p>
<p class="indenthanging"><strong><em><span class="pd_red">Insert.</span></em></strong> We add the new key at the end of the array, increment the size of the heap, and then swim up through the heap with that key to restore the heap condition.</p>
<p class="indenthanging"><strong><em><span class="pd_red">Remove the maximum.</span></em></strong> We take the largest item off the top, put the item from the end of the heap at the top, decrement the size of the heap, and then sink down through the heap with that key to restore the heap condition.</p>
<p><a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> solves the basic problem that we posed at the beginning of this section: it is a priority-queue API implementation for which both <em>insert</em> and <em>delete the maximum</em> are guaranteed to take time logarithmic in the size of the queue.</p>
<p class="image"><img alt="image" src="graphics/02_34-heapops.jpg"/></p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb33"/></p>
<h3><a id="page_318"/>Algorithm 2.6 Heap priority queue</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0318-01.jpg"/></p>
<p>The priority queue is maintained in a heap-ordered complete binary tree in the array <code>pq[]</code> with <code>pq[0]</code> unused and the <code>N</code> keys in the priority queue in <code>pq[1]</code> through <code>pq[N]</code>. To implement <code>insert()</code>, we increment <code>N</code>, add the new element at the end, then use <code>swim()</code> to restore the heap order. For <code>delMax()</code>, we take the value to be returned from <code>pq[1]</code>, then move <code>pq[N]</code> to <code>pq[1]</code>, decrement the size of the heap, and use <code>sink()</code> to restore the heap condition. We also set the now-unused position <code>pq[N+1]</code> to <code>null</code> to allow the system to reclaim the memory associated with it. Code for dynamic array resizing is omitted, as usual (see <a href="ch01a.html#ch01sec1lev5"><small>SECTION 1.3</small></a>). See <a href="#ch02qa4q19"><small>EXERCISE 2.4.19</small></a> for the other constructors.</p>
<hr/>
</div>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb34"/></p>
<p><a id="page_319"/><strong>Proposition Q.</strong> In an <em>N</em>-key priority queue, the heap algorithms require no more than 1 + lg <em>N</em> compares for <em>insert</em> and no more than 2 lg <em>N</em> compares for <em>remove the maximum</em>.</p>
<p><strong>Proof:</strong> By <a href="#ch02sb32"><small>PROPOSITION P</small></a>, both operations involve moving along a path between the root and the bottom of the heap whose number of links is no more than lg <em>N</em>. The <em>remove the maximum</em> operation requires two compares for each node on the path (except at the bottom): one to find the child with the larger key, the other to decide whether that child needs to be promoted.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/02_34-pqheap.jpg"/></p>
<p>For typical applications that require a large number of intermixed insert and remove the maximum operations in a large priority queue, <a href="#ch02sb34"><small>PROPOSITION Q</small></a> represents an important performance breakthrough, summarized in the table shown on page <a href="#page_312">312</a>. Where elementary implementations using an ordered array or an unordered array require linear time for one of the operations, a heap-based implementation provides a guarantee that both operations complete in logarithmic time. This improvement can make the difference between solving a problem and not being able to address it at all.</p>
<p><a id="ch02sec3lev26"/></p>
<h5><em>Multiway heaps</em></h5>
<p>It is not difficult to modify our code to build heaps based on an array representation of complete heap-ordered <em>ternary</em> trees, with an entry at position <em>k</em> larger than or equal to entries at positions 3<em>k</em>−1, 3<em>k,</em> and 3<em>k</em>+1 and smaller than or equal to entries at position <img alt="image" src="graphics/lfloor.jpg"/>(<em>k</em>+1) / 3<img alt="image" src="graphics/rfloor.jpg"/>, for all indices between 1 and <em>N</em> in an array of <em>N</em> items, and not much more difficult to use <em>d</em>-ary heaps for any given <em>d</em>. There is a tradeoff between the lower cost from the reduced tree height (log<em><sub>d</sub> N</em>) and the higher cost of finding the largest of the <em>d</em> children at each node. This tradeoff is dependent on details of the implementation and the expected relative frequency of operations.</p>
<p><a id="ch02sec3lev27"/></p>
<h5><a id="page_320"/><em>Array resizing</em></h5>
<p>We can add a no-argument constructor, code for array doubling in <code>insert()</code>, and code for array halving in <code>delMax()</code>, just as we did for stacks in <a href="ch01a.html#ch01sec1lev5"><small>SECTION 1.3</small></a>. Thus, clients need not be concerned about arbitrary size restrictions. The logarithmic time bounds implied by <a href="#ch02sb34"><small>PROPOSITION Q</small></a> are <em>amortized</em> when the size of the priority queue is arbitrary and the arrays are resized (see <a href="#ch02qa4q22"><small>EXERCISE 2.4.22</small></a>).</p>
<p><a id="ch02sec3lev28"/></p>
<h5><em>Immutability of keys</em></h5>
<p>The priority queue contains objects that are created by clients but assumes that client code does not change the keys (which might invalidate the heap-order invariant). It is possible to develop mechanisms to enforce this assumption, but programmers typically do not do so because they complicate the code and are likely to degrade performance.</p>
<p><a id="ch02sec3lev29"/></p>
<h5><em>Index priority queue</em></h5>
<p>In many applications, it makes sense to allow clients to refer to items that are already on the priority queue. One easy way to do so is to associate a unique integer <em>index</em> with each item. Moreover, it is often the case that clients have a universe of items of a known size <em>N</em> and perhaps are using (parallel) arrays to store information about the items, so other unrelated client code might already be using an integer index to refer to items. These considerations lead us to the following API:</p>
<p class="image"><img alt="image" src="graphics/t0320-01.jpg"/></p>
<p><a id="page_321"/>A useful way of thinking of this data type is as implementing an array, but with fast access to the smallest entry in the array. Actually it does even better—it gives fast access to the minimum entry in a <em>specified subset</em> of an array’s entries (the ones that have been inserted. In other words, you can think of an <code>IndexMinPQ</code> named <code>pq</code> as representing a subset of an array <code>pq[0..N-1]</code> of items. Think of the call <code>pq.insert(k, item)</code> as adding <code>k</code> to the subset and setting <code>pq[k] = item</code> and the call <code>pq.change(k, item)</code> as setting <code>pq[k] = item</code>, both also maintaining data structures needed to support the other operations, most importantly <code>delMin()</code> (remove and return the index of the minimum key) and <code>change()</code> (change the item associated with an index that is already in the data structure—just as in <code>pq[i] = item</code>). These operations are important in many applications and are enabled by our ability to refer to the key (with the index). <a href="#ch02qa4q33"><small>EXERCISE 2.4.33</small></a> describes how to extend <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> to implement index priority queues with remarkable efficiency and with remarkably little code. Intuitively, when an item in the heap changes, we can restore the heap invariant with a sink operation (if the key decreases) and a swim operation (if the key increases). To perform the operations, we use the index to find the item in the heap. The ability to locate an item in the heap also allows us to add the <code>delete()</code> operation to the API.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb35"/></p>
<p><strong>Proposition Q (continued).</strong> In an index priority queue of size <em>N</em>, the number of compares required is proportional to at most log <em>N</em> for <em>insert, change priority, delete,</em> and <em>remove the minimum</em>.</p>
<p><strong>Proof:</strong> Immediate from inspection of the code and the fact that all paths in a heap are of length at most ~lg <em>N</em>.</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/t0321-01.jpg"/></p>
<p>This discussion is for a minimum-oriented queue; as usual, we also implement on the booksite a maximum-oriented version <code>IndexMaxPQ</code>.</p>
<p><a id="ch02sec3lev30"/></p>
<h5><em>Index priority-queue client</em></h5>
<p>The <code>IndexMinPQ</code> client <code>Multiway</code> on page <a href="#ch02sec3lev21">322</a> solves the <em>multiway merge</em> problem: it merges together several sorted input streams into one sorted output stream. This problem arises in many applications: the streams might be the output of scientific instruments (sorted by time), lists of information from the web such as music or movies (sorted by title or artist name), commercial transactions (sorted by account number or time), or whatever. If you have the space, you might just read them all into an array and sort them, but with a priority queue, you can read input streams and put them in sorted order on the output <em>no matter how long they are</em>.</p>
<div class="sidebar">
<hr/>
<p><a id="ch02sb36"/></p>
<h3><a id="page_322"/>Multiway merge priority-queue client</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0322-01.jpg"/></p>
<p>This <code>IndexMinPQ</code> client merges together the sorted input streams given as command-line arguments into a single sorted output stream on standard output (see text). Each stream index is associated with a key (the next string in the stream). After initialization, it enters a loop that prints the smallest string in the queue and removes the corresponding entry, then adds a new entry for the next string in that stream. For economy, the output is shown on one line below—the actual output is one string per line.</p>
<p class="image"><img alt="image" src="graphics/p0322-02.jpg"/></p>
<hr/>
</div>
<p><a id="ch02sec2lev30"/></p>
<h4><a id="page_323"/>Heapsort</h4>
<p>We can use any priority queue to develop a sorting method. We insert all the items to be sorted into a minimum-oriented priority queue, then repeatedly use <em>remove the minimum</em> to remove them all in order. Using a priority queue represented as an unordered array in this way corresponds to doing a selection sort; using an ordered array corresponds to doing an insertion sort. What sorting method do we get if we use a heap? An entirely different one! Next, we use the heap to develop a classic elegant sorting algorithm known as <em>heapsort</em>.</p>
<p>Heapsort breaks into two phases: <em>heap construction</em>, where we reorganize the original array into a heap, and the <em>sortdown</em>, where we pull the items out of the heap in decreasing order to build the sorted result. For consistency with the code we have studied, we use a maximum-oriented priority queue and repeatedly remove the maximum. Focusing on the task of sorting, we abandon the notion of hiding the representation of the priority queue and use <code>swim()</code> and <code>sink()</code> directly. Doing so allows us to sort an array without needing any extra space, by maintaining the heap within the array to be sorted.</p>
<p><a id="ch02sec3lev31"/></p>
<h5><em>Heap construction</em></h5>
<p>How difficult is the process of building a heap from <em>N</em> given items? Certainly we can accomplish this task in time proportional to <em>N</em> log <em>N</em>, by proceeding from left to right through the array, using <code>swim()</code> to ensure that the items to the left of the scanning pointer make up a heap-ordered complete tree, like successive priority-queue insertions. A clever method that is much more efficient is to proceed from right to left, using <code>sink()</code> to make subheaps as we go. Every position in the array is the root of a small subheap; <code>sink()</code> works for such subheaps, as well. If the two children of a node are heaps, then calling <code>sink()</code> on that node makes the subtree rooted at the parent a heap. This process establishes the heap order inductively. The scan starts halfway back through the array because we can skip the subheaps of size 1. The scan ends at position 1, when we finish building the heap with one call to <code>sink().</code> As the first phase of a sort, heap construction is a bit counterintuitive, because its goal is to produce a heap-ordered result, which has the largest item first in the array (and other larger items near the beginning), not at the end, where it is destined to finish.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb37"/></p>
<p><strong>Proposition R.</strong> Sink-based heap construction uses fewer than 2<em>N</em> compares and fewer than <em>N</em> exchanges to construct a heap from <em>N</em> items.</p>
<p><strong>Proof:</strong> This fact follows from the observation that most of the heaps processed are small. For example, to build a heap of 127 items, we process 32 heaps of size 3, 16 heaps of size 7, 8 heaps of size 15, 4 heaps of size 31, 2 heaps of size 63, and 1 heap of size 127, so 32·1 + 16·2 + 8·3 + 4·4 + 2·5 + 1·6 = 120 exchanges (twice as many compares) are required (at worst). See <a href="#ch02qa4q20"><small>EXERCISE 2.4.20</small></a> for a complete proof.</p>
<hr/>
</div>
<div class="sidebar">
<hr/>
<p><a id="ch02sb38"/></p>
<h3><a id="page_324"/>Algorithm 2.7 Heapsort</h3>
<p class="programlisting2"><img alt="image" src="graphics/p0324-01.jpg"/></p>
<p>This code sorts <code>a[1]</code> through <code>a[N]</code> using the <code>sink()</code> method (modified to take <code>a[]</code> and <code>N</code> as arguments). The <code>for</code> loop constructs the heap; then the <code>while</code> loop exchanges the largest element <code>a[1]</code> with <code>a[N]</code> and then repairs the heap, continuing until the heap is empty. Decrementing the array indices in the implementations of <code>exch()</code> and <code>less()</code> gives an implementation that sorts <code>a[0]</code> through <code>a[N-1]</code>, consistent with our other sorts.</p>
<p class="image"><img alt="image" src="graphics/02_35-heapselection.jpg"/></p>
<hr/>
</div>
<p class="image"><a id="page_325"/><img alt="image" src="graphics/02_36-heapsort.jpg"/></p>
<p><a id="ch02sec3lev32"/></p>
<h5><a id="page_326"/><em>Sortdown</em></h5>
<p>Most of the work during heapsort is done during the second phase, where we remove the largest remaining item from the heap and put it into the array position vacated as the heap shrinks. This process is a bit like selection sort (taking the items in decreasing order instead of in increasing order), but it uses many fewer compares because the heap provides a much more efficient way to find the largest item in the unsorted part of the array.</p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb39"/></p>
<p><strong>Proposition S.</strong> Heapsort uses fewer than 2<em>N</em> lg <em>N</em> + 2<em>N</em> compares (and half that many exchanges) to sort <em>N</em> items.</p>
<p><strong>Proof:</strong> The 2 <em>N</em> term covers the cost of heap construction (see <a href="#ch02sb37"><small>PROPOSITION R</small></a>). The 2 <em>N</em> lg <em>N</em> term follows from bounding the cost of each sink operation during the sortdown by 2lg <em>N</em> (see <a href="#ch02sb32"><small>PROPOSITION P</small>Q</a>).</p>
<hr/>
</div>
<p class="image"><img alt="image" src="graphics/02_37-heapbars.jpg"/></p>
<p><a href="#ch02sb38"><small>ALGORITHM 2.7</small></a> is a full implementation based on these ideas, the classical <em>heapsort</em> algorithm, which was invented by J. W. J. Williams and refined by R. W. Floyd in 1964. Although the loops in this program seem to do different tasks (the first constructs the heap, and the second destroys the heap for the sortdown), they are both built around the <code>sink()</code> method. We provide an implementation outside of our priority-queue API to highlight the simplicity of the sorting algorithm (eight lines of code for <code>sort()</code> and another eight lines of code for <code>sink()</code>) and to make it an in-place sort.</p>
<p>As usual, you can gain some insight into the operation of the algorithm by studying a visual trace. At first, the process seems to do anything but sort, because large items are moving to the beginning of the array as the heap is being constructed. But then the method looks more like a mirror image of selection sort (except that it uses far fewer compares).</p>
<p>As for all of the other methods that we have studied, various people have investigated ways to improve heap-based priority-queue implementations and heapsort. We now briefly consider one of them.</p>
<p><a id="ch02sec3lev33"/></p>
<h5><a id="page_327"/><em>Sink to the bottom, then swim</em></h5>
<p>Most items reinserted into the heap during sortdown go all the way to the bottom. Floyd observed in 1964 that we can thus save time by avoiding the check for whether the item has reached its position, simply promoting the larger of the two children until the bottom is reached, then moving back up the heap to the proper position. This idea cuts the number of compares by a factor of 2 asymptotically—close to the number used by mergesort (for a randomly-ordered array). The method requires extra bookkeeping, and it is useful in practice only when the cost of compares is relatively high (for example, when we are sorting items with strings or other types of long keys).</p>
<p><small>HEAPSORT IS SIGNIFICANT</small> in the study of the complexity of sorting (see page <a href="#ch02sec2lev14">279</a>) because it is the only method that we have seen that is optimal (within a constant factor) in its use of both time and space—it is guaranteed to use ~2<em>N</em> lg <em>N</em> compares and constant extra space in the worst case. When space is very tight (for example, in an embedded system or on a low-cost mobile device) it is popular because it can be implemented with just a few dozen lines (even in machine code) while still providing optimal performance. However, it is rarely used in typical applications on modern systems because it has poor cache performance: array entries are rarely compared with nearby array entries, so the number of cache misses is far higher than for quicksort, mergesort, and even shellsort, where most compares are with nearby entries.</p>
<p>On the other hand, the use of heaps to implement priority queues plays an increasingly important role in modern applications, because it provides an easy way to guarantee logarithmic running time for dynamic situations where large numbers of <em>insert</em> and <em>remove the maximum</em> operations are intermixed. We will encounter several examples later in this book.</p>
<p><a id="ch02sec2lev31"/></p>
<h4><a id="page_328"/>Q&amp;A</h4>
<p><strong>Q.</strong> I’m still not clear on the purpose of priority queues. Why exactly don’t we just sort and then consider the items in increasing order in the sorted array?</p>
<p><strong>A.</strong> In some data-processing examples such as <code>TopM</code> and <code>Multiway</code>, the total amount of data is far too large to consider sorting (or even storing in memory). If you are looking for the top ten entries among a billion items, do you really want to sort a billion-entry array? With a priority queue, you can do it with a ten-entry priority queue. In other examples, all the data does not even exist together at any point in time: we take something from the priority queue, process it, and as a result of processing it perhaps add some more things to the priority queue.</p>
<p><strong>Q.</strong> Why not use <code>Comparable</code>, as we do for sorts, instead of the generic <code>Item</code> in <code>MaxPQ</code>?</p>
<p><strong>A.</strong> Doing so would require the client to cast the return value of <code>delMax()</code> to an actual type, such as <code>String</code>. Generally, casts in client code are to be avoided.</p>
<p><strong>Q.</strong> Why not use <code>a[0]</code> in the heap representation?</p>
<p><strong>A.</strong> Doing so simplifies the arithmetic a bit. It is not difficult to implement the heap methods based on a 0-based heap where the children of <code>a[0]</code> are <code>a[1]</code> and <code>a[2]</code>, the children of <code>a[1]</code> are <code>a[3]</code> and <code>a[4]</code>, the children of <code>a[2]</code> are <code>a[5]</code> and <code>a[6]</code>, and so forth, but most programmers prefer the simpler arithmetic that we use. Also, using <code>a[0]</code> as a sentinel value (in the parent of <code>a[1]</code>) is useful in some heap applications.</p>
<p><strong>Q.</strong> Building a heap in heapsort by inserting items one by one seems simpler to me than the tricky bottom-up method described on page <a href="#ch02sec2lev30">323</a> in the text. Why bother?</p>
<p><strong>A.</strong> For a sort implementation, it is 20 percent faster and requires half as much tricky code (no <code>swim()</code> needed). The difficulty of understanding an algorithm has not necessarily much to do with its simplicity, or its efficiency.</p>
<p><strong>Q.</strong> What happens if I leave off the <code>extends Comparable&lt;Key&gt;</code> phrase in an implementation like <code>MaxPQ</code> ?</p>
<p><strong>A.</strong> As usual, the easiest way for you to answer a question of this sort for yourself is to simply try it. If you do so for <code>MaxPQ</code> you will get a compile-time error:</p>
<p class="programlisting">MaxPQ.java:21: cannot find symbol<br/>
symbol  : method compareTo(Item)</p>
<p>which is Java’s way of telling you that it does not know about <code>compareTo()</code> in <code>Item</code> because you neglected to declare that <code>Item extends Comparable&lt;Item&gt;</code>.</p>
<p><a id="ch02sec2lev32"/></p>
<h4><a id="page_329"/>Exercises</h4>
<p><a id="ch02qa4q1"/><strong>2.4.1</strong> Suppose that the sequence <code>P R I O * R * * I * T * Y * * * Q U E * * * U * E</code> (where a letter means <em>insert</em> and an asterisk means <em>remove the maximum</em>) is applied to an initially empty priority queue. Give the sequence of letters returned by the <em>remove the maximum</em> operations.</p>
<p><a id="ch02qa4q2"/><strong>2.4.2</strong> Criticize the following idea: To implement <em>find the maximum</em> in constant time, why not use a stack or a queue, but keep track of the maximum value inserted so far, then return that value for <em>find the maximum</em>?</p>
<p><a id="ch02qa4q3"/><strong>2.4.3</strong> Provide priority-queue implementations that support <em>insert</em> and <em>remove the maximum</em>, one for each of the following underlying data structures: unordered array, ordered array, unordered linked list, and linked list. Give a table of the worst-case bounds for each operation for each of your four implementations.</p>
<p><a id="ch02qa4q4"/><strong>2.4.4</strong> Is an array that is sorted in decreasing order a max-oriented heap?</p>
<p><a id="ch02qa4q5"/><strong>2.4.5</strong> Give the heap that results when the keys <code>E A S Y Q U E S T I O N</code> are inserted in that order into an initially empty max-oriented heap.</p>
<p><a id="ch02qa4q6"/><strong>2.4.6</strong> Using the conventions of <a href="#ch02qa4q1"><small>EXERCISE 2.4.1</small></a>, give the sequence of heaps produced when the operations <code>P R I O * R * * I * T * Y * * * Q U E * * * U * E</code> are performed on an initially empty max-oriented heap.</p>
<p><a id="ch02qa4q7"/><strong>2.4.7</strong> The largest item in a heap must appear in position <code>1</code>, and the second largest must be in position <code>2</code> or position <code>3</code>. Give the list of positions in a heap of size 31 where the <em>k</em>th largest (<em>i</em>) can appear, and (<em>ii</em>) cannot appear, for <em>k</em>=2, 3, 4 (assuming the values to be distinct).</p>
<p><a id="ch02qa4q8"/><strong>2.4.8</strong> Answer the previous exercise for the <em>k</em>th <em>smallest</em> item.</p>
<p><a id="ch02qa4q9"/><strong>2.4.9</strong> Draw all of the different heaps that can be made from the five keys <code>A B C D E</code>, then draw all of the different heaps that can be made from the five keys <code>A A A B B</code>.</p>
<p><a id="ch02qa4q10"/><strong>2.4.10</strong> Suppose that we wish to avoid wasting one position in a heap-ordered array <code>pq[]</code>, putting the largest value in <code>pq[0]</code>, its children in <code>pq[1]</code> and <code>pq[2]</code>, and so forth, proceeding in level order. Where are the parents and children of <code>pq[k]</code>?</p>
<p><a id="ch02qa4q11"/><strong>2.4.11</strong> Suppose that your application will have a huge number of <em>insert</em> operations, but only a few <em>remove the maximum</em> operations. Which priority-queue implementation do you think would be most effective: heap, unordered array, or ordered array?</p>
<p><a id="page_330"/><a id="ch02qa4q12"/><strong>2.4.12</strong> Suppose that your application will have a huge number of <em>find the maximum</em> operations, but a relatively small number of <em>insert</em> and <em>remove the maximum</em> operations. Which priority-queue implementation do you think would be most effective: heap, unordered array, or ordered array?</p>
<p><a id="ch02qa4q13"/><strong>2.4.13</strong> Describe a way to avoid the <code>j &lt; N</code> test in <code>sink()</code>.</p>
<p><a id="ch02qa4q14"/><strong>2.4.14</strong> What is the minimum number of items that must be exchanged during a <em>remove the maximum</em> operation in a heap of size <em>N</em> with no duplicate keys? Give a heap of size 15 for which the minimum is achieved. Answer the same questions for two and three successive <em>remove the maximum</em> operations.</p>
<p><a id="ch02qa4q15"/><strong>2.4.15</strong> Design a linear-time certification algorithm to check whether an array <code>pq[]</code> is a min-oriented heap.</p>
<p><a id="ch02qa4q16"/><strong>2.4.16</strong> For <em>N</em>=32, give arrays of items that make heapsort use as <em>many</em> and as <em>few</em> compares as possible.</p>
<p><a id="ch02qa4q17"/><strong>2.4.17</strong> Prove that building a minimum-oriented priority queue of size <em>k</em> then doing <em>N</em> − <em>k replace the minimum</em> (<em>insert</em> followed by <em>remove the minimum</em>) operations leaves the <em>k</em> largest of the <em>N</em> items in the priority queue.</p>
<p><a id="ch02qa4q18"/><strong>2.4.18</strong> In <code>MaxPQ</code>, suppose that a client calls <code>insert()</code> with an item that is larger than all items in the queue, and then immediately calls <code>delMax()</code>. Assume that there are no duplicate keys. Is the resulting heap identical to the heap as it was before these operations? Answer the same question for two <code>insert()</code> operations (the first with a key larger than all keys in the queue and the second for a key larger than that one) followed by two <code>delMax()</code> operations.</p>
<p><a id="ch02qa4q19"/><strong>2.4.19</strong> Implement the constructor for <code>MaxPQ</code> that takes an array of items as argument, using the bottom-up heap construction method described on page <a href="#ch02sec2lev30">323</a> in the text.</p>
<p><a id="ch02qa4q20"/><strong>2.4.20</strong> Prove that sink-based heap construction uses fewer than 2<em>N</em> compares and fewer than <em>N</em> exchanges.</p>
<p><a id="ch02sec2lev33"/></p>
<h4><a id="page_331"/>Creative Problems</h4>
<p><a id="ch02qa4q21"/><strong>2.4.21</strong> <em>Elementary data structures.</em> Explain how to use a priority queue to implement the stack, queue, and randomized queue data types from <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>.</p>
<p><a id="ch02qa4q22"/><strong>2.4.22</strong> <em>Array resizing.</em> Add array resizing to <code>MaxPQ</code>, and prove bounds like those of <a href="#ch02sb34"><small>PROPOSITION Q</small></a> for array accesses, in an amortized sense.</p>
<p><a id="ch02qa4q23"/><strong>2.4.23</strong> <em>Multiway heaps.</em> Considering the cost of compares only, and assuming that it takes <em>t</em> compares to find the largest of <em>t</em> items, find the value of <em>t</em> that minimizes the coefficient of <em>N</em>lg <em>N</em> in the compare count when a <em>t</em>-ary heap is used in heapsort. First, assume a straightforward generalization of <code>sink()</code>; then, assume that Floyd’s method can save one compare in the inner loop.</p>
<p><a id="ch02qa4q24"/><strong>2.4.24</strong> <em>Priority queue with explicit links.</em> Implement a priority queue using a heap-ordered binary tree, but use a triply linked structure instead of an array. You will need three links per node: two to traverse down the tree and one to traverse up the tree. Your implementation should guarantee logarithmic running time per operation, even if no maximum priority-queue size is known ahead of time.</p>
<p><a id="ch02qa4q25"/><strong>2.4.25</strong> <em>Computational number theory.</em> Write a program that prints out all integers of the form <em>a</em><sup>3</sup> + <em>b</em><sup>3</sup> where <em>a</em> and <em>b</em> are integers between <em>0</em> and <em>N</em> in sorted order, without using excessive space. That is, instead of computing an array of the <em>N</em><sup>2</sup> sums and sorting them, build a minimum-oriented priority queue, initially containing (0<sup>3</sup>, 0, 0), (1<sup>3</sup>, 1, 0), (2<sup>3</sup>, 2, 0), . . ., (<em>N</em><sup>3</sup>, <em>N</em>, 0). Then, while the priority queue is nonempty, <em>remove the smallest</em> item <em>(i</em><sup>3</sup> + <em>j</em><sup>3</sup>, <em>i, j),</em> print it, and then, if <em>j &lt; N</em>, <em>insert</em> the item <em>(i</em><sup>3</sup> + <em>(j+1)</em><sup>3</sup>, <em>i, j+1)</em>. Use this program to find all distinct integers <em>a, b, c,</em> and <em>d</em> between 0 and 10<sup>6</sup> such that <em>a<sup>3</sup> + b</em><sup>3</sup> = <em>c</em><sup>3</sup> + <em>d</em><sup>3</sup>.</p>
<p><a id="ch02qa4q26"/><strong>2.4.26</strong> <em>Heap without exchanges.</em> Because the <code>exch()</code> primitive is used in the <code>sink()</code> and <code>swim()</code> operations, the items are loaded and stored twice as often as necessary. Give more efficient implementations that avoid this inefficiency, a la insertion sort (see <a href="#ch02qa1q25"><small>EXERCISE 2.1.25</small></a>).</p>
<p><a id="ch02qa4q27"/><strong>2.4.27</strong> <em>Find the minimum.</em> Add a <code>min()</code> method to <code>MaxPQ</code>. Your implementation should use constant time and constant extra space.</p>
<p><a id="ch02qa4q28"/><strong>2.4.28</strong> <em>Selection filter.</em> Write a <code>TopM</code> client that reads points <em>(x, y, z)</em> from standard input, takes a value <em>M</em> from the command line, and prints the <em>M</em> points that are closest to the origin in Euclidean distance. Estimate the running time of your client for <em>N</em> = 10<sup>8</sup> <a id="page_332"/>and <em>M</em> = 10<sup>4</sup>.</p>
<p><a id="ch02qa4q29"/><strong>2.4.29</strong> <em>Min/max priority queue.</em> Design a data type that supports the following operations: <em>insert</em>, <em>delete the maximum</em>, and <em>delete the minimum</em> (all in logarithmic time); and <em>find the maximum</em> and <em>find the minimum</em> (both in constant time). <em>Hint</em>: Use two heaps.</p>
<p><a id="ch02qa4q30"/><strong>2.4.30</strong> <em>Dynamic median-finding.</em> Design a data type that supports <em>insert</em> in logarithmic time, <em>find the median</em> in constant time, and <em>delete the median</em> in logarithmic time. <em>Hint</em>: Use a min-heap and a max-heap.</p>
<p><a id="ch02qa4q31"/><strong>2.4.31</strong> <em>Fast insert.</em> Develop a compare-based implementation of the <code>MinPQ</code> API such that <em>insert</em> uses ~ log log <em>N</em> compares and <em>delete the minimum</em> uses ~2 log <em>N</em> compares. <em>Hint</em>: Use binary search on parent pointers to find the ancestor in <code>swim()</code>.</p>
<p><a id="ch02qa4q32"/><strong>2.4.32</strong> <em>Lower bound.</em> Prove that it is impossible to develop a compare-based implementation of the <code>MinPQ</code> API such that both <em>insert</em> and <em>delete the minimum</em> guarantee to use ~<em>N</em> log log <em>N</em> compares.</p>
<p><a id="ch02qa4q33"/><strong>2.4.33</strong> <em>Index priority-queue implementation.</em> Implement the basic operations in the index priority-queue API on page <a href="#ch02sec3lev29">320</a> by modifying <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> as follows: Change <code>pq[]</code> to hold indices, add an array <code>keys[]</code> to hold the key values, and add an array <code>qp[]</code> that is the inverse of <code>pq[]</code> — <code>qp[i]</code> gives the position of <code>i</code> in <code>pq[]</code> (the index <code>j</code> such that <code>pq[j]</code> is <code>i</code>). Then modify the code in <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> to maintain these data structures. Use the convention that <code>qp[i] = -1</code> if <code>i</code> is not on the queue, and include a method <code>contains()</code> that tests this condition. You need to modify the helper methods <code>exch()</code> and <code>less()</code> but not <code>sink()</code> or <code>swim()</code>.</p>
<p><a id="page_333"/><em>Partial solution</em>:</p>
<p class="programlisting"><img alt="image" src="graphics/p0333-01.jpg"/></p>
<p><a id="page_334"/><a id="ch02qa4q34"/><strong>2.4.34</strong> <em>Index priority-queue implementation (additional operations).</em> Add <code>minIndex()</code>, <code>change()</code>, and <code>delete()</code> to your implementation of <a href="#ch02qa4q33"><small>EXERCISE 2.4.33</small></a>.</p>
<p><em>Solution</em>:</p>
<p class="programlisting3"><img alt="image" src="graphics/p0333-02.jpg"/></p>
<p><a id="ch02qa4q35"/><strong>2.4.35</strong> <em>Sampling from a discrete probability distribution.</em> Write a class <code>Sample</code> with a constructor that takes an array <code>p[]</code> of <code>double</code> values as argument and supports the following two operations: <code>random()</code>—return an index <code>i</code> with probability <code>p[i]/T</code> (where <code>T</code> is the sum of the numbers in <code>p[]</code>)—and <code>change(i, v)</code>—change the value of <code>p[i]</code> to <code>v</code>. <em>Hint</em>: Use a complete binary tree where each node has implied weight <code>p[i]</code>. Store in each node the cumulative weight of all the nodes in its subtree. To generate a random index, pick a random number between <code>0</code> and <code>T</code> and use the cumulative weights to determine which branch of the subtree to explore. When updating <code>p[i]</code>, change all of the weights of the nodes on the path from the root to <code>i</code>. Avoid explicit pointers, as we do for heaps.</p>
<p><a id="ch02sec2lev34"/></p>
<h4><a id="page_335"/>Experiments</h4>
<p><a id="ch02qa4q36"/><strong>2.4.36</strong> <em>Performance driver I.</em> Write a performance driver client program that uses <em>insert</em> to fill a priority queue, then uses <em>remove the maximum</em> to remove half the keys, then uses <em>insert</em> to fill it up again, then uses <em>remove the maximum</em> to remove all the keys, doing so multiple times on random sequences of keys of various lengths ranging from small to large; measures the time taken for each run; and prints out or plots the average running times.</p>
<p><a id="ch02qa4q37"/><strong>2.4.37</strong> <em>Performance driver II.</em> Write a performance driver client program that uses <em>insert</em> to fill a priority queue, then does as many <em>remove the maximum</em> and <em>insert</em> operations as it can do in 1 second, doing so multiple times on random sequences of keys of various lengths ranging from small to large; and prints out or plots the average number of <em>remove the maximum</em> operations it was able to do.</p>
<p><a id="ch02qa4q38"/><strong>2.4.38</strong> <em>Exercise driver.</em> Write an exercise driver client program that uses the methods in our priority-queue interface of <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> on difficult or pathological cases that might turn up in practical applications. Simple examples include keys that are already in order, keys in reverse order, all keys the same, and sequences of keys having only two distinct values.</p>
<p><a id="ch02qa4q39"/><strong>2.4.39</strong> <em>Cost of construction.</em> Determine empirically the percentage of time heapsort spends in the construction phase for <em>N</em> = 10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>.</p>
<p><a id="ch02qa4q40"/><strong>2.4.40</strong> <em>Floyd’s method.</em> Implement a version of heapsort based on Floyd’s sink-to-the-bottom-and-then-swim idea, as described in the text. Count the number of compares used by your program and the number of compares used by the standard implementation, for randomly ordered distinct keys with <em>N</em> = 10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>.</p>
<p><a id="ch02qa4q41"/><strong>2.4.41</strong> <em>Multiway heaps.</em> Implement a version of heapsort based on complete heap-ordered 3-ary and 4-ary trees, as described in the text. Count the number of compares used by each and the number of compares used by the standard implementation, for randomly ordered distinct keys with <em>N</em> = 10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>.</p>
<p><a id="ch02qa4q42"/><strong>2.4.42</strong> <em>Preorder heaps.</em> Implement a version of heapsort based on the idea of representing the heap-ordered tree in preorder rather than in level order. Count the number of compares used by your program and the number of compares used by the standard implementation, for randomly ordered keys with <em>N</em> = 10<sup>3</sup>, 10<sup>6</sup>, and 10<sup>9</sup>.</p>
<p><a id="ch02sec1lev5"/></p>
<h3><a id="page_336"/>2.5 Applications</h3>
<p><small>SORTING ALGORITHMS</small> and priority queues are widely used in a broad variety of applications. Our purpose in this section is to briefly survey some of these applications, consider ways in which the efficient methods that we have considered play a critical role in such applications, and discuss some of the steps needed to make use of our sort and priority-queue code.</p>
<p>A prime reason why sorting is so useful is that it is much easier to search for an item in a sorted array than in an unsorted one. For over a century, people found it easy to look up someone’s phone number in a phone book where items are sorted by last name. Now digital music players organize song files by artist name or song title; search engines display search results in descending order of importance; spreadsheets display columns sorted by a particular field; matrix-processing packages sort the real eigenvalues of a symmetric matrix in descending order; and so forth. Other tasks are also made easier once an array is in sorted order: from looking up an item in the sorted index in the back of this book; to removing duplicates from a long list such as a mailing list, a list of voters, or a list of websites; to performing statistical calculations such as removing outliers, finding the median, or computing percentiles.</p>
<p>Sorting also arises as a critical subproblem in many applications that appear to have nothing to do with sorting at all. Data compression, computer graphics, computational biology, supply-chain management, combinatorial optimization, social choice, and voting are but a few of many examples. The algorithms that we have considered in this chapter play a critical role in the development of effective algorithms in each of the later chapters in this book.</p>
<p>Most important is the system sort, so we begin by considering a number of practical considerations that come into play when building a sort for use by a broad variety of clients. While some of these topics are specific to Java, they each reflect challenges that need to be met in any system.</p>
<p>Our primary purpose is to demonstrate that, even though we have used mechanisms that are relatively simple, the sorting implementations that we are studying are widely applicable. The list of proven applications of fast sorting algorithms is vast, so we can consider just a small fraction of them: some scientific, some algorithmic, and some commercial. You will find many more examples in the exercises, and many more than that on the booksite. Moreover, we will often refer back to this chapter to effectively address the problems that we later consider <em>in this book</em>!</p>
<p><a id="ch02sec2lev35"/></p>
<h4><a id="page_337"/>Sorting various types of data</h4>
<p>Our implementations sort arrays of <code>Comparable</code> objects. This Java convention allows us to use Java’s <em>callback</em> mechanism to sort arrays of objects of any type that implements the <code>Comparable</code> interface. As described in <a href="#ch02sec1lev1"><small>SECTION 2.1</small></a>, implementing <code>Comparable</code> amounts to defining a <code>compareTo()</code> method that implements a <em>natural ordering</em> for the type. We can use our code immediately to sort arrays of type <code>String</code>, <code>Integer</code>, <code>Double</code>, and other types such as <code>File</code> and <code>URL</code>, because these data types all implement <code>Comparable</code>. Being able to use the same code for all of those types is convenient, but typical applications involve working with data types that are defined for use within the application. Accordingly it is common to implement a <code>compareTo()</code> method for user-defined data types, so that they implement <code>Comparable</code>, thus enabling client code to sort arrays of that type (and build priority queues of values of that type).</p>
<p><a id="ch02sec3lev34"/></p>
<h5><em>Transaction example</em></h5>
<p>A prototypical breeding ground for sorting applications is commercial data processing. For example, imagine that a company engaged in internet commerce maintains a record for each transaction involving a customer account that contains all of the pertinent information, such as the customer name, date, amount, and so forth. Nowadays, a successful company needs to be able to handle millions and millions of such transactions. As we saw in <a href="#ch02qa1q21"><small>EXERCISE 2.1.21</small></a>, it is reasonable to decide that a natural ordering of such transactions is that they be ordered by amount, which we can implement by adding an appropriate <code>compareTo()</code> method in the class definition. With such a definition, we could process an array <code>a[]</code> of <code>Transaction</code>s by, for example, first sorting it with the call <code>Quick.sort(a)</code>. Our sorting methods know nothing about our <code>Transaction</code> data type, but Java’s <code>Comparable</code> interface allows us to define a natural ordering so that we can use any of our methods to sort <code>Transaction</code> objects. Alternatively, we might specify that <code>Transaction</code> objects are to be ordered by date by implementing <code>compareTo()</code> to compare the <code>Date</code> fields. Since <code>Date</code> objects are themselves <code>Comparable</code>, we can just invoke the <code>compareTo()</code> method in <code>Date</code> rather than having to implement it from scratch. It is also reasonable to consider ordering this data by it customer field; arranging to allow clients the flexibility to switch among multiple different orders is an interesting challenge that we will soon consider.</p>
<p class="image"><img alt="image" src="graphics/p0337-01.jpg"/></p>
<p><a id="ch02sec3lev35"/></p>
<h5><a id="page_338"/><em>Pointer sorting</em></h5>
<p>The approach we are using is known in the classical literature as <em>pointer sorting</em>, so called because we process references to items and do not move the data itself. In programming languages such as C and C++, programmers explicitly decide whether to manipulate data or pointers to data; in Java, pointer manipulation is implicit. Except for primitive numeric types, we <em>always</em> manipulate references to objects (pointers), not the objects themselves. Pointer sorting adds a level of indirection: the array contains references to the objects to be sorted, not the objects themselves. We briefly consider some associated issues, in the context of sorting. With multiple reference arrays, we can have multiple different sorted representations of different parts of a single body of data (perhaps using multiple keys, as described below).</p>
<p><a id="ch02sec3lev36"/></p>
<h5><em>Keys are immutable</em></h5>
<p>It stands to reason that an array might not remain sorted if a client is allowed to change the values of keys after the sort. Similarly, a priority queue can hardly be expected to operate properly if the client can change the values of keys between operations. In Java, it is wise to ensure that key values do not change by using immutable keys. Most of the standard data types that you are likely to use as keys, such as <code>String</code>, <code>Integer</code>, <code>Double</code>, and <code>File</code>, are immutable.</p>
<p><a id="ch02sec3lev37"/></p>
<h5><em>Exchanges are inexpensive</em></h5>
<p>Another advantage of using references is that we avoid the cost of moving full items. The cost saving is significant for arrays with large items (and small keys) because the compare needs to access just a small part of the item, and most of the item is not even touched during the sort. The reference approach makes the cost of an exchange roughly equal to the cost of a compare for general situations involving arbitrarily large items (at the cost of the extra space for the references). Indeed, if the keys are long, the exchanges might even wind up being less costly than the compare. One way to study the performance of algorithms that sort arrays of numbers is to simply look at the total number of compares and exchanges they use, implicitly making the assumption that the cost of exchanges is the same as the cost of compares. Conclusions based on this assumption are likely to apply to a broad class of applications in Java, because we are sorting reference objects.</p>
<p><a id="ch02sec3lev38"/></p>
<h5><em>Alternate orderings</em></h5>
<p>There are many applications where we want to use different orders for the objects that we are sorting, depending on the situation. The Java <code>Comparator</code> interface allows us to build multiple orders within a single class. It has a single public method <code>compare()</code> that compares two objects. If we have a data type that implements this interface, we can pass a <code>Comparator</code> to <code>sort()</code> (which passes it to <code>less()</code>) as in the example on the next page. The <code>Comparator</code> mechanism allows us to sort arrays of any type of object, using any total order that we wish to define for them. Using a <code>Comparator</code> instead of working with <code>Comparable</code> types better separates the definition of the type from the definition of what it means to compare two objects of <a id="page_339"/>that type. Indeed, there are typically many possible ways to compare objects, and the <code>Comparator</code> mechanism allows us to choose among them. For instance, to sort an array <code>a[]</code> of strings without regard to whether characters are uppercase or lowercase you can just call <code>Insertion.sort(a, String.CASE_INSENSITIVE_ORDER)</code> which makes use of the <code>CASE_INSENSITIVE_ORDER</code> comparator defined in Java’s <code>String</code> class. As you can imagine, the precise rules for ordering strings are complicated and quite different for various natural languages, so Java has many <code>String</code> comparators.</p>
<p><a id="ch02sec3lev39"/></p>
<h5><em>Items with multiple keys</em></h5>
<p>In typical applications, items have multiple instance variables that might need to serve as sort keys. In our transaction example, one client may need to sort the transaction list by customer (for example, to bring together all transactions involving each customer); another client might need to sort the list by amount (for example, to identify high-value transactions); and other clients might need to use other fields as sort keys. The <code>Comparator</code> mechanism is precisely what we need to allow this flexibility. We can define multiple comparators, as in the alternate implementation of <code>Transaction</code> shown on the bottom of the next page. With this definition, a client can sort an array of <code>Transaction</code> objects by time with the call</p>
<p class="programlisting">Insertion.sort(a, new Transaction.WhenOrder())</p>
<p>or by amount with the call</p>
<p class="programlisting">Insertion.sort(a, new Transaction.HowMuchOrder()).</p>
<p>The sort does each compare through a <em>callback</em> to the <code>compare()</code> method in <code>Transaction</code> that is specified by the client code. To avoid the cost of making a new <code>Comparator</code> object for each sort, we could use <code>public final</code> instance variables to define the comparators (as Java does for <code>CASE_INSENSITIVE_ORDER</code>).</p>
<p class="image"><img alt="image" src="graphics/p0339-03.jpg"/></p>
<p><a id="ch02sec3lev40"/></p>
<h5><a id="page_340"/><em>Priority queues with comparators</em></h5>
<p>The same flexibility to use comparators is also useful for priority queues. Extending our standard implementation in <a href="#ch02sb33"><small>ALGORITHM 2.6</small></a> to support comparators involves the following steps:</p>
<p class="indenthangingB">• Import <code>java.util.Comparator</code>.</p>
<p class="indenthangingB">• Add to <code>MaxPQ</code> an instance variable <code>comparator</code> and a constructor that takes a comparator as argument and initializes <code>comparator</code> to that value.</p>
<p class="indenthangingB">• Add code to <code>less()</code> that checks whether <code>comparator</code> is <code>null</code> (and uses it if it is not <code>null</code>).</p>
<p>For example, with these changes, you could build different priority queues with <code>Transaction</code> keys, using the time, place, or account number for the ordering. If you remove the <code>Key extends Comparable&lt;Key&gt;</code> phrase from <code>MinPQ</code>, you even can support keys with no natural order.</p>
<p class="image"><img alt="image" src="graphics/p0340-01.jpg"/></p>
<p><a id="ch02sec3lev41"/></p>
<h5><a id="page_341"/><em>Stability</em></h5>
<p>A sorting method is <em>stable</em> if it preserves the relative order of equal keys in the array. This property is frequently important. For example, consider an internet commerce application where we have to process a large number of events that have locations and timestamps. To begin, suppose that we store events in an array as they arrive, so they are in order of the timestamp in the array. Now suppose that the application requires that the transactions be separated out by location for further processing. One easy way to do so is to sort the array by location. If the sort is unstable, the transactions for each city may <em>not</em> necessarily be in order by timestamp after the sort. Often, programmers who are unfamiliar with stability are surprised, when they first encounter the situation, by the way an unstable algorithm seems to scramble the data. Some of the sorting methods that we have considered in this chapter are stable (insertion sort and mergesort); many are not (selection sort, shellsort, quicksort, and heapsort). There are ways to trick any sort into stable behavior (see <a href="#ch02qa5q18"><small>EXERCISE 2.5.18</small></a>), but using a stable algorithm is generally preferable when stability is an essential requirement. It is easy to take stability for granted; actually, no practical method in common use achieves stability without using significant extra time or space (researchers have developed algorithms that do so, but applications programmers have judged them too complicated to be useful).</p>
<p class="image"><img alt="image" src="graphics/02_38-stability.jpg"/></p>
<p><a id="ch02sec2lev36"/></p>
<h4><a id="page_342"/>Which sorting algorithm should I use?</h4>
<p>We have considered numerous sorting algorithms in this chapter, so this question is natural. Knowing which algorithm is best possible depends heavily on details of the application and implementation, but we have studied some general-purpose methods that can be nearly as effective as the best possible for a wide variety of applications.</p>
<p>The table at the bottom of this page is a general guide that summarizes the important characteristics of the sort algorithms that we have studied in this chapter. In all cases but shellsort (where the growth rate is only an estimate), insertion sort (where the growth rate depends on the order of the input keys), and both versions of quicksort (where the growth rate is probabilitic and may depend on the distribution of input key values), multiplying these growth rates by appropriate constants gives an effective way to predict running time. The constants involved are partly algorithm-dependent (for example, heapsort uses twice the number of compares as mergesort and both do many more array accesses than quicksort) but are primarily dependent on the implementation, the Java compiler, and your computer, which determine the number of machine instructions that are executed and the time that each requires. Most important, since they are constants, you can generally predict the running time for large <em>N</em> by running experiments for smaller <em>N</em> and extrapolating, using our standard doubling protocol.</p>
<p class="image"><img alt="image" src="graphics/t0342-01.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb40"/></p>
<p><a id="page_343"/><strong>Property T.</strong> Quicksort is the fastest general-purpose sort.</p>
<p><strong>Evidence:</strong> This hypothesis is supported by countless implementations of quicksort on countless computer systems since its invention decades ago. Generally, the reason that quicksort is fastest is that it has only a few instructions in its inner loop (and it does well with cache memories because it most often references data sequentially) so that its running time is ~c <em>N</em> lg <em>N</em> with the value of c smaller than the corresponding constants for other linearithmic sorts. With 3-way partitioning, quicksort becomes linear for certain key distributions likely to arise in practice, where other sorts are linearithmic.</p>
<hr/>
</div>
<p>Thus, in most practical situations, quicksort is the method of choice. Still, given the broad reach of sorting and the broad variety of computers and systems, a flat statement like this is difficult to justify. For example, we have already seen one notable exception: if stability is important and space is available, mergesort might be best. We will see other exceptions in <a href="ch05.html#ch05"><small>CHAPTER 5</small></a>. With tools like <code>SortCompare</code> and a considerable amount of time and effort, you can do a more detailed study of comparative performance of these algorithms and the refinements that we have discussed for your computer, as discussed in several exercises at the end of this section. Perhaps the best way to interpret <a href="#ch02sb40">PROPERTY T</a> is as saying that you certainly should seriously consider using quicksort in any sort application where running time is important.</p>
<p><a id="ch02sec3lev42"/></p>
<h5><em>Sorting primitive types</em></h5>
<p>In some performance-critical applications, the focus may be on sorting numbers, so it is reasonable to avoid the costs of using references and sort primitive types instead. For example, consider the difference between sorting an array of <code>double</code> values and sorting an array of <code>Double</code> values. In the former case, we exchange the numbers themselves and put them in order in the array; in the latter, we exchange references to <code>Double</code> objects, which contain the numbers. If we are doing nothing more than sorting a huge array of numbers, we avoid paying the cost of storing an equal number of references plus the extra cost of accessing the numbers through the references, not to mention the cost of invoking <code>compareTo()</code> and <code>less()</code> methods. We can develop efficient versions of our sort codes for such purposes by replacing <code>Comparable</code> with the primitive type name, and redefining <code>less()</code> or just replacing calls to <code>less()</code> with code like <code>a[i] &lt; a[j]</code> (see <a href="#ch02qa1q26"><small>EXERCISE 2.1.26</small></a>).</p>
<p><a id="ch02sec3lev43"/></p>
<h5><em>Java system sort</em></h5>
<p>As an example of applying the information given in the table on page <a href="#page_342">342</a>, consider Java’s primary system sort method, <code>java.util.Arrays.sort()</code>. With overloading of argument types, this name actually represents a collection of methods:</p>
<p class="indenthangingB"><a id="page_344"/>• A different method for each primitive type</p>
<p class="indenthangingB">• A method for data types that implement <code>Comparable</code></p>
<p class="indenthangingB">• A method that uses a <code>Comparator</code></p>
<p>Java’s systems programmers have chosen to use quicksort (with 3-way partitioning) to implement the primitive-type methods, and mergesort for reference-type methods. The primary practical implications of these choices are, as just discussed, to trade speed and memory usage (for primitive types) for stability (for reference types).</p>
<p><small>THE ALGORITHMS AND IDEAS</small> that we have been considering are an essential part of many modern systems, including Java. When developing Java programs to address an application, you are likely to find that Java’s <code>Arrays.sort()</code> implementations (perhaps supplemented by your own implementation(s) of <code>compareTo()</code> and/or <code>compare()</code>) will meet your needs, because you will be using 3-way quicksort or mergesort, both proven classic algorithms.</p>
<p>In this book, we generally will use our own <code>Quick.sort()</code> (usually) or <code>Merge.sort()</code> (when stability is important and space is not) in sort clients. You may feel free to use <code>Arrays.sort()</code> unless you have a good reason to use another specific method.</p>
<p><a id="ch02sec2lev37"/></p>
<h4>Reductions</h4>
<p>The idea that we can use sorting algorithms to solve other problems is an example of a basic technique in algorithm design known as <em>reduction</em>. We consider reduction in detail in <a href="ch06.html#ch06"><small>CHAPTER 6</small></a> because of its importance in the theory of algorithms—in the meantime, we will consider several practical examples. A <em>reduction</em> is a situation where an algorithm developed for one problem is used to solve another. Applications programmers are quite used to the concept of reduction (whether or not it is explicitly articulated)—every time you make use of a method that solves problem <em>B</em> in order to solve problem <em>A</em>, you are doing a reduction from <em>A</em> to <em>B</em>. Indeed, one goal in implementing algorithms is to facilitate reductions by making the algorithms useful for as wide a variety as possible of applications. We begin with a few elementary examples for sorting. Many of these take the form of algorithmic puzzles where a quadratic brute-force algorithm is immediate. It is often the case that sorting the data first makes it easy to finish solving the problem in linear additional time, thus reducing the total cost from quadratic to linearithmic.</p>
<p><a id="ch02sec3lev44"/></p>
<h5><em>Duplicates</em></h5>
<p>Are there any duplicate keys in an array of <code>Comparable</code> objects? How many distinct keys are there? Which value appears most frequently? For small arrays, these kinds of questions are easy to answer with a quadratic algorithm that compares each array entry with each other array entry. For large arrays, using a quadratic algorithm is not feasible. With sorting, you can answer these questions in linearithmic time: <a id="page_345"/>first sort the array, then make a pass through the sorted array, taking note of duplicate keys that appear consecutively in the ordered array. For example, the code fragment at right counts the distinct keys in an array. With simple modifications to this code, you can answer the questions above and perform tasks such as printing all the distinct values, all the values that are duplicated, and so forth, even for huge arrays.</p>
<p><a id="ch02sec3lev45"/></p>
<h5><em>Rankings</em></h5>
<p>A <em>permutation</em> (or <em>ranking</em>) is an array of <em>N</em> integers where each of the integers between <em>0</em> and <em>N-1</em> appears exactly once. The <em>Kendall tau distance</em> between two rankings is the number of pairs that are in different order in the two rankings. For example, the Kendall tau distance between <code>0 3 1 6 2 5 4</code> and <code>1 0 3 6 4 2 5</code> is four because the pairs <code>0-1</code>, <code>3-1</code>, <code>2-4</code>, <code>5-4</code> are in different relative order in the two rankings, but all other pairs are in the same relative order. This statistic is widely used: in sociology to study social choice and voting theory, in molecular biology to compare genes using expression profiles, and in ranking search engine results on the web, among many other applications. The Kendall tau distance between a permutation and the identity permutation (where each entry is equal to its index) is the number of inversions in the permutation, and a quadratic algorithm based on insertion sort is not difficult to devise (recall <a href="#ch02sb08"><small>PROPOSITION C</small></a> in <a href="#ch02sec1lev1"><small>SECTION 2.1</small></a>). Efficiently computing the Kendall tau distance is an interesting exercise for a programmer (or a student!) who is familiar with the classical sorting algorithms that we have studied (see <a href="#ch02qa5q19"><small>EXERCISE 2.5.19</small></a>).</p>
<p class="image"><img alt="image" src="graphics/p0345-01.jpg"/></p>
<p><a id="ch02sec3lev46"/></p>
<h5><em>Priority-queue reductions</em></h5>
<p>In <a href="#ch02sec1lev4"><small>SECTION 2.4</small></a>, we considered two examples of problems that reduce to a sequence of operations on priority queues. <code>TopM</code>, on page <a href="#ch02sb28">311</a>, finds the <em>M</em> items in an input stream with the highest keys. <code>Multiway</code>, on page <a href="#ch02sb36">322</a>, merges <em>M</em> sorted input streams together to make a sorted output stream. Both of these problems are easily addressed with a priority queue of size <em>M</em>.</p>
<p><a id="ch02sec3lev47"/></p>
<h5><em>Median and order statistics</em></h5>
<p>An important application related to sorting but for which a full sort is not required is the operation of finding the <em>median</em> of a collection of keys (the value with the property that half the keys are no larger and half the keys are no smaller). This operation is a common computation in statistics and in various other data-processing applications. Finding the median is a special case of <em>selection</em>: finding the <em>k</em>th smallest of a collection of numbers. Selection has many applications in the processing of experimental and other data. The use of the median and other <em>order <a id="page_346"/>statistics</em> to divide an array into smaller groups is common. Often, only a small part of a large array is to be saved for further processing; in such cases, a program that can select, say, the top 10 percent of the items of the array might be more appropriate than a full sort. Our <code>TopM</code> application of <a href="#ch02sec1lev4"><small>SECTION 2.4</small></a> solves this problem for an unbounded input stream, using a priority queue. An effective alternative to <code>TopM</code> when you have the items in an array is to just sort it: after the call <code>Quick.sort(a)</code> the <em>k</em> smallest values in the array are in the first <em>k</em> array positions for all <em>k</em> less than the array length. But this approach involves a sort, so the running time is linearithmic. Can we do better? Finding the <em>k</em> smallest values in an array is easy when <em>k</em> is very small or very large, but more challenging when <em>k</em> is a constant fraction of the array size, such as finding the median (<em>k</em> = <em>N</em>/2). You might be surprised to learn that it is possible to solve this problem in <em>linear</em> time, as in the <code>select()</code> method above (this implementation requires a client cast; for the more pedantic code needed to avoid this requirement, see the booksite). To do the job, <code>select()</code> maintains the variables <code>lo</code> and <code>hi</code> to delimit the subarray that contains the index <code>k</code> of the item to be selected and uses quicksort partitioning to shrink the size of the subarray. Recall that <code>partition()</code> rearranges an array <code>a[lo]</code> through <code>a[hi]</code> and returns an integer <code>j</code> such that <code>a[lo]</code> through <code>a[j-1]</code> are less than or equal to <code>a[j]</code>, and <code>a[j+1]</code> through <code>a[hi]</code> are greater than or equal to <code>a[j]</code>. Now, if <code>k</code> is equal to <code>j</code>, then we are done. Otherwise, if <code>k &lt; j</code>, then we need to continue working in the left subarray (by changing the value of <code>hi</code> to <code>j-1</code>); if <code>k &gt; j</code>, then we need to continue working in the right subarray (by changing <code>lo</code> to <code>j+1</code>). The loop maintains the invariant that no entry to the left of <code>lo</code> is larger and no entry to the right of <code>hi</code> is smaller than any element within <code>a[lo..hi]</code>. After partitioning, we preserve this invariant and shrink the interval until it consists just of <a id="page_347"/><code>k</code>. Upon termination, <code>a[k]</code> contains the (<em>k</em> +1)st smallest entry, <code>a[0]</code> through <code>a[k-1]</code> are all smaller than (or equal to) <code>a[k]</code>, and <code>a[k+1]</code> through the end of the array are all greater than (or equal to) <code>a[k]</code>. To gain some insight into why this is a linear-time algorithm, suppose that partitioning divides the array exactly in half each time. Then the number of compares is <em>N</em> + <em>N</em>/2 + <em>N</em>/4 + <em>N</em>/8 + . . ., terminating when the <em>k</em>th smallest item is found. This sum is less than 2 <em>N</em>. As with quicksort, it takes a bit of math to find the true bound, which is a bit higher. Also as with quicksort, the analysis depends on partitioning on a random item, so that the guarantee is probabilistic.</p>
<p class="image"><img alt="image" src="graphics/p0346-01.jpg"/></p>
<p class="image"><img alt="image" src="graphics/02_39-selectbars.jpg"/></p>
<div class="sidebar1">
<hr/>
<p><a id="ch02sb41"/></p>
<p><strong>Proposition U.</strong> Partitioning-based selection is a linear-time algorithm, on average.</p>
<p><strong>Proof:</strong> An analysis similar to, but significantly more complex than, the proof of <a href="#ch02sb23"><small>PROPOSITION K</small></a> for quicksort leads to the result that the average number of compares is ~ 2<em>N</em> + 2<em>k</em> ln(<em>N/k</em>) + 2(<em>N</em> − <em>k</em>) ln(<em>N/(N</em> − <em>k</em>)), which is linear for any allowed value of <em>k</em>. For example, this formula says that finding the median (<em>k</em> = <em>N</em>/2) requires ~ (2 + 2 ln 2)<em>N</em> compares, on the average. Note that the worst case is quadratic but randomization protects against that possibility, as with quicksort.</p>
<hr/>
</div>
<p>Designing a selection algorithm that is guaranteed to use a linear number of compares in the <em>worst case</em> is a classic result in computational complexity, but it has not yet led to a useful practical algorithm.</p>
<p><a id="ch02sec2lev38"/></p>
<h4><a id="page_348"/>A brief survey of sorting applications</h4>
<p>Direct applications of sorting are familiar, ubiquitous, and far too numerous for us to list them all. You sort your music by song title or by artist name, your email or phone calls by time or origin, and your photos by date. Universities sort student accounts by name or ID. Credit card companies sort millions or even billions of transactions by date or amount. Scientists sort not only experimental data by time or other identifier but also to enable detailed simulations of the natural world, from the motion of particles or heavenly bodies to the structure of materials to social interations and relationships. Indeed, it is difficult to identify a computational application that does <em>not</em> involve sorting! To elaborate upon this point, we describe in this section examples of applications that are more complicated than the reductions just considered, including several that we will examine in detail later in this book.</p>
<p><a id="ch02sec3lev48"/></p>
<h5><em>Commercial computing</em></h5>
<p>The world is awash in information. Government organizations, financial institutions, and commercial enterprises organize much of this information by sorting it. Whether the information is accounts to be sorted by name or number, transactions to be sorted by date or amount, mail to be sorted by postal code or address, files to be sorted by name or date, or whatever, processing such data is sure to involve a sorting algorithm somewhere along the way. Typically, such information is organized in huge databases, sorted by multiple keys for efficient search. An effective strategy that is widely used is to collect new information, add it to the database, sort it on the keys of interest, and merge the sorted result for each key into the existing database. The methods that we have discussed have been used effectively since the early days of computing to build a huge infrastructure of sorted data and methods for processing it that serve as the basis for all of this commercial activity. Arrays having millions or even billions of entries are routinely processed today—without linearithmic sorting algorithms, such arrays could not be sorted, making such processing extremely difficult or impossible.</p>
<p><a id="ch02sec3lev49"/></p>
<h5><em>Search for information</em></h5>
<p>Keeping data in sorted order makes it possible to efficiently search through it using the classic <em>binary search</em> algorithm (see <a href="ch01.html#ch01"><small>CHAPTER 1</small></a>). You will also see that the same scheme makes it easy to quickly handle many other kinds of queries. How many items are smaller than a given item? Which items fall within a given range? In <a href="ch03.html#ch03"><small>CHAPTER 3</small></a>, we consider such questions. We also consider in detail various extensions to sorting and binary search that allow us to intermix such queries with operations that insert and remove objects from the set, still guaranteeing logarithmic performance for all operations.</p>
<p><a id="ch02sec3lev50"/></p>
<h5><a id="page_349"/><em>Operations research</em></h5>
<p>The field of <em>operations research</em> (OR) develops and applies mathematical models for problem-solving and decision-making. We will see several examples in this book of relationships between OR and the study of algorithms, beginning here with the use of sorting in a classic OR problem known as <em>scheduling</em>. Suppose that we have <em>N</em> jobs to complete, where job <em>j</em> requires <em>t<sub>j</sub></em> seconds of processing time. We need to complete all of the jobs but want to maximize customer satisfaction by minimizing the average completion time of the jobs. The <em>shortest processing time first</em> rule, where we schedule the jobs in increasing order of processing time, is known to accomplish this goal. Therefore we can sort the jobs by processing time or put them on a minimum-oriented priority queue. With various other constraints and restrictions, we get various other scheduling problems, which frequently arise in industrial applications and are well-studied. As another example, consider the <em>load-balancing problem</em>, where we have <em>M</em> identical processors and <em>N</em> jobs to complete, and our goal is to schedule all of the jobs on the processors so that the time at which the last job completes is as early as possible. This specific problem is <em>NP</em>-hard (see <a href="ch06.html#ch06"><small>CHAPTER 6</small></a>) so we do not expect to find a practical way to compute an optimal schedule. One method that is known to produce a good schedule is the <em>longest processing time first</em> rule, where we consider the jobs in descending order of processing time, assigning each job to the processor that becomes available first. To implement this algorithm, we first sort the jobs in reverse order. Then we maintain a priority queue of <em>M</em> processors, where the priority is the sum of the processing times of its jobs. At each step, we delete the processor with the minimum priority, add the next job to the processor, and reinsert that processor into the priority queue.</p>
<p><a id="ch02sec3lev51"/></p>
<h5><em>Event-driven simulation</em></h5>
<p>Many scientific applications involve simulation, where the point of the computation is to model some aspect of the real world in order to be able to better understand it. Before the advent of computing, scientists had little choice but to build mathematical models for this purpose; such models are now well-complemented by computational models. Doing such simulations efficiently can be challenging, and use of appropriate algorithms certainly can make the difference between being able to complete the simulation in a reasonable amount of time and being stuck with the choice of accepting inaccurate results or waiting for the simulation to do the computation necessary to get accurate results. We will consider in <a href="ch06.html#ch06"><small>CHAPTER 6</small></a> a detailed example that illustrates this point.</p>
<p><a id="ch02sec3lev52"/></p>
<h5><em>Numerical computations</em></h5>
<p>Scientific computing is often concerned with <em>accuracy</em> (how close are we to the true answer?). Accuracy is extremely important when we are performing millions of computations with estimated values such as the floating-point representation of real numbers that we commonly use on computers. Some numerical algorithms use priority queues and sorting to control accuracy in calculations. For <a id="page_350"/>example, one way to do numerical integration (quadrature), where the goal is to estimate the area under a curve, is to maintain a priority queue with accuracy estimates for a set of subintervals that comprise the whole interval. The process is to remove the least accurate subinterval, split it in half (thus achieving better accuracy for the two halves), and put the two halves back onto the priority queue, continuing until a desired tolerance is reached.</p>
<p><a id="ch02sec3lev53"/></p>
<h5><em>Combinatorial search</em></h5>
<p>A classic paradigm in artificial intelligence and in coping with intractable problems is to define a set of <em>configurations</em> with well-defined <em>moves</em> from one configuration to the next and a priority associated with each move. Also defined is a <em>start</em> configuration and a <em>goal</em> configuration (which corresponds to having solved the problem). The well-known <em>A* algorithm</em> is a problem-solving process where we put the start configuration on the priority queue, then do the following until reaching the goal: remove the highest-priority configuration and add to the queue all configurations that can be reached from that with one move (excluding the one just removed). As with event-driven simulation, this process is tailor-made for priority queues. It reduces solving the problem to defining an effective priority function. See <a href="#ch02qa5q32"><small>EXERCISE 2.5.32</small></a> for an example.</p>
<p><small>BEYOND SUCH DIRECT APPLICATIONS</small> (and we have only indicated a small fraction of those), sorting and priority queues are an essential abstraction in algorithm design, so they will surface frequently throughout this book. We next list some examples of applications from later in the book. All of these applications depend upon the efficient implementations of sorting algorithms and the priority-queue data type that we have considered in this chapter.</p>
<p><strong><em><span class="pd_red">Prim’s algorithm and Dijkstra’s algorithm</span></em></strong> are classical algorithms from <a href="ch04.html#ch04"><small>CHAPTER 4</small></a>. That chapter is about algorithms that process <em>graphs</em>, a fundamental model for <em>items</em> and <em>edges</em> that connect pairs of items. The basis for these and several other algorithms is <em>graph search</em>, where we proceed from item to item along edges. Priority queues play a fundamental role in organizing graph searches, enabling efficient algorithms.</p>
<p><strong><em><span class="pd_red">Kruskal’s algorithm</span></em></strong> is another classic algorithm for graphs whose edges have weights that depends upon processing the edges in order of their weight. Its running time is dominated by the cost of the sort.</p>
<p><strong><em><span class="pd_red">Huffman compression</span></em></strong> is a classic <em>data compression</em> algorithm that depends upon processing a set of items with integer weights by combining the two smallest to produce a new one whose weight is the sum of its two constituents. Implementing this operation <a id="page_351"/>is immediate, using a priority queue. Several other data-compression schemes are based upon sorting.</p>
<p><strong><em><span class="pd_red">String-processing</span></em></strong> algorithms, which are of critical importance in modern applications in cryptology and in genomics, are often based on sorting (generally using one of the specialized string sorts discussed in <a href="ch05.html#ch05"><small>CHAPTER 5</small></a>). For example, we will discuss in <a href="ch06.html#ch06"><small>CHAPTER 6</small></a> algorithms for finding the <em>longest repeated substring</em> in a given string that is based on first sorting suffixes of the strings.</p>
<p><a id="ch02sec2lev39"/></p>
<h4><a id="page_352"/>Q &amp; A</h4>
<p><strong>Q.</strong> Is there a priority-queue data type in the Java library?</p>
<p><strong>A.</strong> Yes, see <code>java.util.PriorityQueue</code>.</p>
<p><a id="ch02sec2lev40"/></p>
<h4><a id="page_353"/>Exercises</h4>
<p><a id="ch02qa5q1"/><strong>2.5.1</strong> Consider the following implementation of the <code>compareTo()</code> method for <code>String</code>. How does the third line help with efficiency?</p>
<p class="programlisting"><img alt="image" src="graphics/p0353-01.jpg"/></p>
<p><a id="ch02qa5q2"/><strong>2.5.2</strong> Write a program that reads a list of words from standard input and prints all two-word compound words in the list. For example, if <code>after</code>, <code>thought</code>, and <code>afterthought</code> are in the list, then <code>afterthought</code> is a compound word.</p>
<p><a id="ch02qa5q3"/><strong>2.5.3</strong> Criticize the following implementation of a class intended to represent account balances. Why is <code>compareTo()</code> a flawed implementation of the <code>Comparable</code> interface?</p>
<p class="programlisting"><img alt="image" src="graphics/p0353-02.jpg"/></p>
<p>Describe a way to fix this problem.</p>
<p><a id="ch02qa5q4"/><strong>2.5.4</strong> Implement a method <code>String[] dedup(String[] a)</code> that returns the objects in <code>a[]</code> in sorted order, with duplicates removed.</p>
<p><a id="ch02qa5q5"/><strong>2.5.5</strong> Explain why selection sort is not stable.</p>
<p><a id="page_354"/><a id="ch02qa5q6"/><strong>2.5.6</strong> Implement a recursive version of <code>select()</code>.</p>
<p><a id="ch02qa5q7"/><strong>2.5.7</strong> About how many compares are required, on the average, to find the smallest of <em>N</em> items using <code>select()</code>?</p>
<p><a id="ch02qa5q8"/><strong>2.5.8</strong> Write a program <code>Frequency</code> that reads strings from standard input and prints the number of times each string occurs, in descending order of frequency.</p>
<p><a id="ch02qa5q9"/><strong>2.5.9</strong> Develop a data type that allows you to write a client that can sort a file such as the one shown at right.</p>
<p class="image"><img alt="image" src="graphics/p0354-01.jpg"/></p>
<p><a id="ch02qa5q10"/><strong>2.5.10</strong> Create a data type <code>Version</code> that represents a software version number, such as <code>115.1.1</code>, <code>115.10.1</code>, <code>115.10.2</code>. Implement the <code>Comparable</code> interface so that <code>115.1.1</code> is less than <code>115.10.1</code>, and so forth.</p>
<p><a id="ch02qa5q11"/><strong>2.5.11</strong> One way to describe the result of a sorting algorithm is to specify a permutation <code>p[]</code> of the numbers <code>0</code> to <code>a.length-1</code>, such that <code>p[i]</code> specifies where the key originally in <code>a[i]</code> ends up. Give the permutations that describe the results of insertion sort, selection sort, shellsort, mergesort, quicksort, and heapsort for an array of seven equal keys.</p>
<p><a id="ch02sec2lev41"/></p>
<h4><a id="page_355"/>Creative Problems</h4>
<p><a id="ch02qa5q12"/><strong>2.5.12</strong> <em>Scheduling.</em> Write a program <code>SPT.java</code> that reads job names and processing times from standard input and prints a schedule that minimizes average completion time using the shortest processing time first rule, as described on page <a href="#page_349">349</a>.</p>
<p><a id="ch02qa5q13"/><strong>2.5.13</strong> <em>Load balancing.</em> Write a program <code>LPT.java</code> that takes an integer <code>M</code> as a command-line argument, reads job names and processing times from standard input and prints a schedule assigning the jobs to <code>M</code> processors that approximately minimizes the time when the last job completes using the longest processing time first rule, as described on page <a href="#page_349">349</a>.</p>
<p><a id="ch02qa5q14"/><strong>2.5.14</strong> <em>Sort by reverse domain.</em> Write a data type <code>Domain</code> that represents domain names, including an appropriate <code>compareTo()</code> method where the natural order is in order of the <em>reverse</em> domain name. For example, the reverse domain of <code>cs.princeton.edu</code> is <code>edu.princeton.cs</code>. This is useful for web log analysis. <em>Hint</em>: Use <code>s.split("\\.")</code> to split the string <code>s</code> into tokens, delimited by dots. Write a client that reads domain names from standard input and prints the reverse domains in sorted order.</p>
<p><a id="ch02qa5q15"/><strong>2.5.15</strong> <em>Spam campaign.</em> To initiate an illegal spam campaign, you have a list of email addresses from various domains (the part of the email address that follows the @ symbol). To better forge the return addresses, you want to send the email from another user at the same domain. For example, you might want to forge an email from <code>wayne@princeton.edu</code> to <code>rs@princeton.edu</code>. How would you process the email list to make this an efficient task?</p>
<p><a id="ch02qa5q16"/><strong>2.5.16</strong> <em>Unbiased election.</em> In order to thwart bias against candidates whose names appear toward the end of the alphabet, California sorted the candidates appearing on its 2003 gubernatorial ballot by using the following order of characters:</p>
<p class="programlisting">R W Q O J M V A H B S G Z X N T C I E K U P D Y F L</p>
<p>Create a data type where this is the natural order and write a client <code>California</code> with a single static method <code>main()</code> that sorts strings according to this ordering. Assume that each string is composed solely of uppercase letters.</p>
<p><a id="ch02qa5q17"/><strong>2.5.17</strong> <em>Check stability.</em> Extend your <code>check()</code> method from <a href="#ch02qa1q16"><small>EXERCISE 2.1.16</small></a> to call <code>sort()</code> for a given array and return <code>true</code> if <code>sort()</code> sorts the array in order <em>in a stable manner</em>, <code>false</code> otherwise. Do not assume that <code>sort()</code> is restricted to move data only with <code>exch()</code>.</p>
<p><a id="page_356"/><a id="ch02qa5q18"/><strong>2.5.18</strong> <em>Force stability.</em> Write a wrapper method that makes any sort stable by creating a new key type that allows you to append each key’s index to the key, call <code>sort()</code>, then restore the original key after the sort.</p>
<p><a id="ch02qa5q19"/><strong>2.5.19</strong> <em>Kendall tau distance.</em> Write a program <code>KendallTau.java</code> that computes the Kendall tau distance between two permutations in linearithmic time.</p>
<p><a id="ch02qa5q20"/><strong>2.5.20</strong> <em>Idle time.</em> Suppose that a parallel machine processes <em>N</em> jobs. Write a program that, given the list of job start and finish times, finds the largest interval where the machine is idle and the largest interval where the machine is <em>not</em> idle.</p>
<p><a id="ch02qa5q21"/><strong>2.5.21</strong> <em>Multidimensional sort.</em> Write a <code>Vector</code> data type for use in having the sorting methods sort multidimensional vectors of <em>d</em> integers, putting the vectors in order by first component, those with equal first component in order by second component, those with equal first and second components in order by third component, and so forth.</p>
<p><a id="ch02qa5q22"/><strong>2.5.22</strong> <em>Stock market trading.</em> Investors place buy and sell orders for a particular stock on an electronic exchange, specifying a maximum buy or minimum sell price that they are willing to pay, and how many shares they wish to trade at that price. Develop a program that uses priority queues to match up buyers and sellers and test it through simulation. Maintain two priority queues, one for buyers and one for sellers, executing trades whenever a new order can be matched with an existing order or orders.</p>
<p><a id="ch02qa5q23"/><strong>2.5.23</strong> <em>Sampling for selection.</em> Investigate the idea of using sampling to improve selection. <em>Hint</em>: Using the median may not always be helpful.</p>
<p><a id="ch02qa5q24"/><strong>2.5.24</strong> <em>Stable priority queue.</em> Develop a <em>stable</em> priority-queue implementation (which returns duplicate keys in the same order in which they were inserted).</p>
<p><a id="ch02qa5q25"/><strong>2.5.25</strong> <em>Points in the plane.</em> Write three <code>static</code> comparators for the <code>Point2D</code> data type of page <a href="ch01.html#page_77">77</a>, one that compares points by their <em>x</em> coordinate, one that compares them by their <em>y</em> coordinate, and one that compares them by their distance from the origin. Write two non-<code>static</code> comparators for the <code>Point2D</code> data type, one that compares them by their distance to a specified point and one that compares them by their polar angle with respect to a specified point.</p>
<p><a id="ch02qa5q26"/><strong>2.5.26</strong> <em>Simple polygon.</em> Given <em>N</em> points in the plane, draw a simple polygon with <em>N</em> <a id="page_357"/>points as vertices. <em>Hint</em>: Find the point <em>p</em> with the smallest <em>y</em> coordinate, breaking ties with the smallest <em>x</em> coordinate. Connect the points in increasing order of the polar angle they make with <em>p</em>.</p>
<p><a id="ch02qa5q27"/><strong>2.5.27</strong> <em>One-dimensional intervals.</em> Write three comparators for the <code>Interval1D</code> data type of page 77, one that compares intervals by their left endpoint, one that compares intervals by their right endpoint, and one that compares intervals by their length.</p>
<p><a id="ch02qa5q28"/><strong>2.5.28</strong> <em>Sort files by name.</em> Write a program <code>FileSorter</code> that takes the name of a directory as a command-line argument and prints out all of the files in the current directory, sorted by file name. <em>Hint</em>: Use the <code>File</code> data type.</p>
<p><a id="ch02qa5q29"/><strong>2.5.29</strong> <em>Sort files by size and date of last modification.</em> Write comparators for the type <code>File</code> to order by increasing/decreasing order of file size, ascending/descending order of file name, and ascending/descending order of last modification date. Use these comparators in a program <code>LS</code> that takes a command-line argument and lists the files in the current directory according to a specified order, e.g., <code>"-t"</code> to sort by timestamp. Support multiple flags to break ties. Be sure to use a stable sort.</p>
<p><a id="ch02qa5q30"/><strong>2.5.30</strong> <em>Boerner’s theorem.</em> True or false: If you sort each column of a matrix, then sort each row, the columns are still sorted. Justify your answer.</p>
<p><a id="ch02sec2lev42"/></p>
<h4><a id="page_358"/>Experiments</h4>
<p><a id="ch02qa5q31"/><strong>2.5.31</strong> <em>Duplicates.</em> Write a client that takes integers <code>M</code>, <code>N</code>, and <code>T</code> as command-line arguments, then uses the code given in the text to perform <em>T</em> trials of the following experiment: Generate <em>N</em> random <code>int</code> values between 0 and <em>M</em> − 1 and count the number of duplicates. Run your program for <em>T</em> = 10 and <em>N</em> = 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>, with <em>M</em> = <em>N</em>/2, and <em>N</em>, and 2<em>N</em>. Probability theory says that the number of duplicates should be about (1 − e<sup>−<strong><em>α</em></strong></sup>) where α = <em>N</em>/<em>M</em>—print a table to help you confirm that your experiments validate that formula.</p>
<p><a id="ch02qa5q32"/><strong>2.5.32</strong> <em>8 puzzle.</em> The 8 puzzle is a game popularized by S. Loyd in the 1870s. It is played on a 3-by-3 grid with 8 tiles labeled 1 through 8 and a blank square. Your goal is to rearrange the tiles so that they are in order. You are permitted to slide one of the available tiles horizontally or vertically (but not diagonally) into the blank square. Write a program that solves the puzzle using the <em>A* algorithm</em>. Start by using as priority the sum of the number of moves made to get to this board position plus the number of tiles in the wrong position. (Note that the number of moves you must make from a given board position is at least as big as the number of tiles in the wrong place.) Investigate substituting other functions for the number of tiles in the wrong position, such as the sum of the Manhattan distance from each tile to its correct position, or the sums of the squares of these distances.</p>
<p><a id="ch02qa5q33"/><strong>2.5.33</strong> <em>Random transactions.</em> Develop a generator that takes an argument <em>N</em>, generates <em>N</em> random <code>Transaction</code> objects (see <a href="#ch02qa1q21"><small>EXERCISES 2.1.21</small></a> and <a href="#ch02qa1q22">2.1.22</a>), using assumptions about the transactions that you can defend. Then compare the performance of shellsort, mergesort, quicksort, and heapsort for sorting <em>N</em> transactions, for <em>N</em>=10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 10<sup>6</sup>.</p>
</body>
</html>